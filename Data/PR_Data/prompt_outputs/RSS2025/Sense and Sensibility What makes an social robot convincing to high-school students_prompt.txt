=== PDF文件: Sense and Sensibility What makes an social robot convincing to high-school students.pdf ===
=== 时间: 2025-07-22 15:41:59.654271 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Sense and Sensibility: What makes a social robot
convincing to high-school students?
Pablo Gonzalez-Oliveras, Olov Engwall, Ali Reza Majlesi
Dept. of Intelligent Systems, KTH Royal Institute of Technology, Stockholm, Sweden
Dept. of Education, Stockholm University, Stockholm, Sweden
pabloolkth.se, engwallkth.se, ali.reza.majlesiedu.su.se
AbstractThis study with 40 high-school students demon-
strates the high influence of a social educational robot on
students decision-making for a set of eight true-false questions
on electric circuits, for which the theory had been covered in the
students courses. The robot argued for the correct answer on
six questions and the wrong on two, and 75 of the students
were persuaded by the robot to perform beyond their expected
when it was wrong. Students with more experience of using large
language models were even more likely to be influenced by the
robots stance  in particular for the two easiest questions on
which the robot was wrong  suggesting that familiarity with AI
can increase susceptibility to misinformation by AI.
We further examined how three different levels of portrayed
robot certainty, displayed using semantics, prosody and facial
on specific questions and how convincing they perceived the robot
to be on these questions. The students aligned with the robots
answers in 94.4 of the cases when the robot was portrayed
as Certain, 82.6 when it was Neutral and 71.4 when it
was Uncertain. The alignment was thus high for all conditions,
highlighting students general susceptibility to accept the robots
lower than in the Certain. Post-test questionnaire answers further
show that students found the robot most convincing when it
was portrayed as Certain. These findings highlight the need for
educational robots to adjust their display of certainty based
on the reliability of the information they convey, to promote
students critical thinking and reduce undue influence.
Index TermsSocial educational robots, AI Trust, Persuasion,
Certainty
I. INTRODUCTION
Educational robots are becoming more common and they
have significant potential in, e.g., STEM (science, technology,
engineering and mathematics) education [46, 69, 17], offering
students realistic and natural interactions, not the least by
employing Large Language Models (LLMs), as demonstrated
in several recent studies [41, 68, 67]. However, it is also well-
known that while the LLMs linguistic proficiency is often
Since robots can exert high informational social influence
[38, 24, 25, 55, 56] and students will align with the robots
views to large extents , the positive as well as negative
effects of learning with a social robot need to be considered:
Students need to use critical thinking to decide if they should
accept the robots propositions . Educators need to un-
derstand which students are more at risk of being misled by
a robot presenting incorrect STEM facts, to provide in-time
support. Developers need to find ways to signal how certain
the robot is about the presented facts to avoid overtrust .
In a prior exploratory study (currently submitted), we found
that Persuasion, i.e., robot arguments for an incorrect solution
to a maths problem, and Prejudice, i.e., students positive
attitudes towards robots and more experience of using LLMs,
influenced a large majority of students to conform with the
robots incorrect solution. On the other hand, taking Pride
in being a strong maths student increased resistance against
incorrect arguments.
The present study follows up on these findings, by systemat-
ically investigating Sense, i.e., the students reliable ability to
judge and decide with soundness, prudence, and intelligence
[Merriam-Webster] if the robots arguments are correct or
incorrect; and Sensibility, i.e., the students awareness of and
responsiveness toward [...] emotion in another [Merriam-
Webster] regarding their responses to the robots arguments
depending on the robots multimodal display of certainty
presented using semantics, prosody and facial signals.
A. Study objectives
To guide this study, we posed the following research questions:
by a social robots correct or incorrect arguments during
a series of truefalse questions?
align with its answers?
(being right or wrong; certain or uncertain) influence
student alignment in later interactions?
self-perception as a learner, or prior experience with
AIaffect the likelihood of students aligning with the
Our expectations, based on our previous exploratory study,
are that (H1) a majority of students will align with the robots
answer even when it is incorrect; (H2) students will be more
prone to follow the robot when it is portrayed as being certain.
(H3) robot argumentation on preceding questions will, to some
students with greater AI experience will align more frequently
with the robots answers, even when they are incorrect.
II. BACKGROUND  RELATED WORK
Like human actors, social robots impact their interaction
partners through informational and normative social influ-
ence [70, 6, 10]. These processes can lead to conformity
or persuasion, which differ in how the target perceives the
sources intent. In conformity, individuals adjust their stance
after being exposed to others opinions, without perceiving
an active attempt to change their minds . In persuasion,
the target typically perceives an intentional effort to influence
their stance, which can involve explicit argumentation and
This study focuses on how informational trust (RQ1) and
the robots persuasive certainty cues (RQ2) affect students
willingness to align with the robots answers, while also
exploring whether prior interactions influence subsequent de-
cisions (RQ3) and whether individual traits like AI experience
moderate these effects (RQ4). The following subsections elab-
orate these constructs.
A. Informational Trust in HRI
Informational trust plays a key role in informational social in-
making decisions under uncertainty . Cognitive dissonance
theory suggests that individuals weigh new information against
existing beliefs, with higher uncertainty leading to increased
reliance on external sources such as robots . This dynamic
is especially relevant in educational HRI, where students often
perceive robots as authoritative sources of knowledge . As
a result, overtrust can lead students to adopt information even
when the robot is clearly wrong [11, 23].
A meta-analysis  found that factors such as reliability,
false alarm rates, and failure rates significantly influence trust
development in HRI. Robots that adapt to a students learning
perceived as more trustworthy [44, 49]. Peer-like, anthropo-
morphic robots, such as Furhat used in this study, further
enhance trust through human-like gestures and responsiveness
[12, 52], particularly when their information is consistent and
accurate .
In this study, students ability to critically assess the robots
answers is shaped by their prior knowledge of the topic. Their
evaluation of the robots correctness, along with their percep-
tion of its confidence, determines the level of informational
trust they place in it, ultimately influencing how susceptible
they are to its argument-based persuasion.
B. Persuasion in HRI
Argument-based persuasion is explained by the Elaboration-
Likelihood Model, which describes how persuasion can in-
fluence attitudes via two distinct routes: the central route,
relying on detailed reasoning, and the peripheral route, de-
pendent on superficial cues like authority or likability . A
2022 systematic review confirmed that while persuasion has
been extensively explored in HRI , most studies focus
on peripheral strategies such as compliance, assertiveness,
and emotional cues [48, 2], with limited attention to logical
argumentation. Among 54 identified studies, only one partially
addressed structured argumentation  and this trend has
persisted in recent years [59, 58]. Studies on persuasion
by educational robots are even scarcer. A systematic review
of 89 studies found only two studies addressing real-world
educational tasks . These studies demonstrated positive
effects on compliance  and conformity , but their de-
signs lacked generalizability beyond experimental academical
contexts. Moreover, the review reaffirmed the heavy focus
on the peripheral route and revealed that approximately two-
thirds of the reviewed studies focused on affective rather than
cognitive outcomes.
C. Challenges to Students Critical Thinking in HRI
Learning gains in educational HRI are often measured
via pre- and post-tests , neglecting students academic
uncertainty and vulnerability of their existing knowledge.
robot confidence cues and learner certainty. This highlights
a gap in studies addressing how robots and students interact
with uncertain or conflicting prior knowledge.
Recent studies have shown the potential of tailored persua-
sive strategies, such as personalized storytelling to align with
users traits , or adjusting assertiveness to suit social con-
texts . However, they also highlight risks like persuasive
assertiveness can undermine trust in robots . These risks
have become more prominent with the recent development of
LLM-driven educational robots that have transformed inter-
actions with learners. They improve feedback and guidance
during problem-solving processes [68, 61, 60], but since these
robots tend to increase students trust in their outputs  and
the factual correctness of LLMs in STEM contexts has been
questioned [34, 3], there is a need for students to critically
evaluate AI-generated information, which they often fail to do
. Techniques such as adaptive feedback, enhanced multi-
modal communication, and transparency in decision-making
can improve improve students assessment of their interaction
with potentially unreliable robots [45, 66, 56, 27].
This study addresses research gaps by investigating how
variations in a robots displayed confidence influence the
outcome of the robots argument-based persuasion. Based
on these constructs, we hypothesize that (H1) informational
trust will lead students to align even when the robot is
persuasive cues, (H3) prior robot behavior might affect future
alignment via informational updating, and (H4) students AI
familiarity and personality traits will moderate these effects.
Fig. 1. Methodological flowchart outlining study phases: diagnostic test, robot
III. METHODOLOGY
We designed an experiment in which Swedish secondary
school students interacted one-on-one with a fully autonomous
educational robot, discussing eight electric circuits to deter-
mine whether statements about each were true or false (see
Fig. 2). The overall procedure is summarized in the method-
ological flowchart (Figure 1), which outlines the sequence
from diagnostic testing to robot interaction and post-session
assessments.
The recruitment of students and the questions were planned
together with two secondary school teachers to ensure that
topics corresponded to material that had been covered in
the students classes. One month before the experiment the
teachers distributed a diagnostic test with 19 three-choice
questions about electric circuits in their classes, respectively
in grade 10, 11 and 12 of a practically oriented electrical
engineering program, and grade 11 and 12 of a theoretical
natural sciences program. The students were unaware that the
diagnostic test was linked to the upcoming experiment.
The diagnostic test had two main objectives: 1) selecting
the eight questions for the experiment and 2) assessing student
subject knowledge, to balance group distribution and evaluate
their performance in the main experiment. An Item Response
Theory analysis of the diagnostic test answers (DA) was then
conducted to assess both student ability and question difficulty,
as input to the experimental design.
A. Item Response Theory
The item response theory (IRT)1 refers to a family of mathe-
matical models that attempt to explain the relationship between
latent traits (unobservable characteristic) and their observed
outcomes. We opted for the 3-Parameter Logistic (3PL) model,
includes a guessing parameter (c 0.25 to prevent over-
fitting), making it particularly appropriate for three-option
multiple-choice tests to account for guessing by low-ability
methodsitem-response-theory
Fig. 2. The eight electric circuit statements with ratio of correct diagnostic
answers (EEasy, MMedium, DDifficult). Robot certainty levels indicated
as Certain (C, pale blue background), Neutral (N, light gray background) and
Uncertain (U, soft yellow background), along with correctness (correct or
wrong). Cohort frames are coloured dark gray (N), yellow (U), and blue (C).
students. The following IRT assumptions were applied: mono-
tonicity (the probability of a correct response increases with
increased knowledge of electric circuits); unidimensionality
(the dominant latent trait, ability, is the driving force for the
observed DA); local independence (separate DA are mutually
independent given a certain level of ability) and invariance
(parameters can be estimated from DA for any sub-group
meeting the conditions).
The analysis was implemented in a two-stage process in
a Python-driven Jupyter Notes environment. The first stage
involved iteratively estimating the Item Characteristic Curves
(ICCs) for each question. This provided metrics for item
difficulty (a), discrimination (b), and guessing (c). Initial
parameters and bounds were set and refined through multiple
runs of the algorithm to achieve optimal values. Akaike In-
formation Criterion (AIC) and Bayesian Information Criterion
(BIC) were used as indicators of model fit, guiding the iterative
parameter adjustment to identify the best model configuration.
In the second stage, the calibrated ICC parameters were
used to estimate each students latent ability and the proba-
bility of a correct response for both DA (for verification) and
experiment (for analysis). In the 3PL model, the probability
that a student with ability  will correctly answer a specific
question is p()  c
1ea(b) . This probability can be used
to predict the students base performance in the experiment
and thus assess the robots influence. DA was lacking for four
students and their ability and probable correctness per question
was instead estimated through a second IRT analysis using the
40 students preliminary answers (PA) in the interaction with
the robot. The validity of these metrics was confirmed through
a correlation analysis (r0.43, p.009) between DA and PA.
B. Robot conditions
As outlined in the research questions, this study investigated
how robot correctness and certainty influence participants
experimental conditions. This section describes these robot
conditions and their validation.
The robot was portrayed as having three levels of certainty,
Uncertain U, Neutral N, and Certain C. These portrayals
were achieved through semantic, prosodic, and facial cues:
adjustments of arguments (Sec III-E), speech rate  (-10
for U and 10 for C relative to N), insertion of pauses
for U, and facial expressions . In the U condition,
the robot used filled and silent pauses, subtle smiles, slow
gaze shifts, head tilts, half-closed eyes, and pursed lips. In
the C condition, it exhibited open smiles, wider eyes, raised
An online validation survey was conducted to ensure that
the robots U, N and C portrayals were perceived as intended.
Fifteen short video clips (less than 8 s each) were created,
representing five distinct contexts (c1c5) combined with the
three certainty levels (U, N, C), showing the robot saying
the utterances in Table I in randomized order. The contexts
consisted of the robot disclosing its answer before knowing
the students response (c1), disclosing while disagreeing (c2)
or agreeing (c3) with the student, reminding the student of
its position (c4), and presenting an argument to support its
answer (c5). The utterances, which also occurred during the
main experiment, were delivered by the robot looking directly
at the camera against a black backdrop, as shown in Fig. 3.
Invitations were sent to 323 MSc, 166 BSc and 57 PhD stu-
dents at a technical university. The survey began by informing
about the purpose and procedures and the participants then
self-assessed their proficiency in Swedish on a 6-point Likert
scale. For the validation, we filtered the 125 responses to only
include subjects reporting medium to high proficiency (the top
three levels). This resulted in 76 participants, of which 15 were
excluded for incomplete responses and 2 for failing to watch
all videos. 59 participants thus rated the robots certainty on a
7-point Likert scale, where 1 indicated Highly Uncertain, 4
in 885 ratings (295 per condition).
Statistical analysis confirmed that the U condition (
2.86,   1.30) was perceived as less certain than the N
A one-way ANOVA revealed significant differences between
conditions (F  285.31, p < 0.001), and post-hoc Tukey tests
confirmed that all pairwise differences were statistically sig-
nificant (p < 0.001), as shown in Figure 3 (right). Cronbachs
0.74 indicated acceptable internal consistency, supporting
the reliability of participants assessments. It can be noted that,
in qualitative terms, the U condition was perceived as only
slightly uncertain, the N condition as slightly more certain
than neutral and the N condition as moderately certain.
C. Subjects, Groups  Cohorts
A modified mixed factorial design was used , with robot
correctness (within subjects, RQ1) and robot certainty (be-
tween and within subjects, RQ23) as factors. The robots
certainty levels were varied across three groups of participants
during the experiment. Group 1 (baseline) always experienced
the robot with neutral certainty (N condition), while Groups
2 and 3 alternately experienced robot uncertainty (U) or
certainty (C) after giving their preliminary answer.
47 subjects registered for the experiment and they were
distributed between groups by means of stratified assignment,
using the characteristics educational program, gender, grade
level and ability as criteria, categorised in canonical combi-
nations to assign students to groups. 40 subjects (31 male, 8
showed up. 28 were from the practical Electrical Engineering
(E) and 12 from the theoretical Natural Sciences (Na) program
with 9, 15 and 16 students respectively from grades 1012.
Due to the no-shows, the distribution became unbalanced:
Group 1: n14 (11 Male, 3 Female; 10 E, 4 Na), DA
performance (completed by 13 subjects): mDA:137.41.9.
Group 2: n11 (8M, 3F; 7E, 4Na), mDA:98.40.97.
Group 3: n15 (12M, 2F, 1NA; 11E, 4Na), mDA:148.91.7.
As the diagnostic score was similar for Groups 2 and 3 but
substantially lower for Group 1, a data analysis is required
controlling for abilities calculated through IRT as a covariate.
Three student cohorts were created, as illustrated in Fig. 2.
Cohort N corresponds to Group 1, exposed to Neutral condi-
tion throughout. Cohort U consists of Group 2 for questions
Distribution of certainty ratings for the three robot conditions (U, N, C),
with significant differences between conditions.
EXPERIMENT-USED ROBOT UTTERANCES CHOSEN FOR THE VALIDATION SURVEY, COVERING FIVE DISTINCT SITUATIONS (c1c5) FOR CONDITIONS N
(NEUTRAL), U (UNCERTAIN) AND C (CERTAIN). . . .  DENOTES PAUSES.
Q14 and Group 3 for Q58, interacting with an Uncertain
robot. Conversely, Cohort C includes Group 3 for questions
Q14 and Group 2 for Q58, interacting with a Certain
robot. Alternating Groups 2 and 3 between Cohorts U and C
served two purposes: first, to create a more natural interaction,
preventing individual students from experiencing a robot that
was always certain or always uncertain; and second, to enable
the investigation of how robot certainty levels on preceding
questions influenced students subsequent responses (RQ3).
D. Electric circuit problems
Based on the IRT analysis, the eight true or false problems
shown in Fig. 2 were selected so that the two halves of the test
would have similar questions (Q15, Q26, Q37, Q48).
The problems were four Conditioned questions of Medium
difficulty (Q1,2,5,6) with varying robot certainty levels and
correct robot arguments, two Easy Deception questions where
the robot provided incorrect answers (Q37) with neutral
tion and with neutral certainty (Q48). Using the diagnostic
answers (DA), we aimed for equidistant difficulty levels, but
as Q48 turned out to be notably more challenging for these
of more similar level than the Difficult (z: 1.67).
E. Robot Arguments for its Answer
For each question, the robot first asked what preliminary
answer the student had chosen (e.g., c1 in Table I) and agreed
or disagreed with this choice (e.g., c2  c3) and then presented
a set of four arguments (e.g., c5). The same four arguments
were used for a given question, but differed in presentation,
as shown in Table II, depending on if the student and robot
disagreed (arguments presented one by one), if they already
were in agreement (arguments grouped in pairs together with
expressions confirming the agreement), and the robot condition
(U, N, C). After presenting the four arguments, the robot
asked the student to give a final answer (e.g., c4 in Table I).
F. Pre- and post-test questionnaires
When signing up for the experiment, the students filled in a
pre-test-questionnaire (cf. the supplementary material) at home
to gather demographic data (gender, age, country of origin,
preferred language of communication, educational program
and grade); and answers to 10 questions based on the student
characteristics questionnaire  focused on the students self-
perception (using a four-point Likert scale) about their ability
to learn in different circumstances; and, on five-point Likert
8 questions from the Big Five Inventory  focused on
in teachers and in persons they like ; 3 questions about
AI attitudes (including frequency of interactions with LLMs,
attitudes towards educational robots in school and expectations
regarding collaborating with a robot on school problems).
After the session, the subjects were guided to an adjacent
room to fill in the post-session questionnaire (cf. the supple-
mentary material) on a tablet. The 11 questions focused on
how much confidence the students had in the robot as an
exercise partner, if the student or the robot prevailed in dissent
electric circuits provided), how the students thought before the
robot convinced them or they convinced it, the extent to which
the robot influenced their thinking and made them change their
mind and on which questions the robot was more and less
convincing (with pictures of the circuits provided).
G. Procedure  Robot Programming
The experiment was carried out in a study room at the stu-
dents school, with each student interacting individually with a
female Furhat robot  (using the Isabel face and the Elin-
Neural voice from Amazon Polly), placed on a table (Fig. 4).
To the right of the robot, a monitor presented the questions
in a web-based GUI with which the students interacted using
a mouse. The system connected the robot and the GUI via
Fig. 4. Experiment setup: Furhat robot (left), microphone (centre), screen with
GUI and mouse (right) and tripod with camera for close-up face recording.
TABLE II
EXAMPLES OF ROBOT ARGUMENTS FOR CONDITIONS N (NEUTRAL), U (UNCERTAIN) AND C (CERTAIN). . . .  DENOTES PAUSES.
a local web server using HTTP protocols, where events and
commands were exchanged in JSON format. This allowed the
robot to be contextually aware of student actions in the GUI,
such as submitting a response, but to foster a natural dialogue,
the robot acted as if it was unaware of the GUI interaction.
The session was recorded using a floor-standing video
camera (corresponding to the view in Fig. 4) and a table-
standing cam (see Fig. 4). The speech recognition results and
GUI interaction data were synchronized and logged to create a
multimodal interaction dataset. One experimenter prepared the
robot and the data logging for each student, gave instructions
on how to initiate the interaction, and then left the room.
After a brief welcome by the robot, the GUI showed the first
question and instructed to think silently before selecting TRUE
or FALSE. The robot then presented its stance (Sec. III-E).
After discussion, the robot prompted students to register their
final answer, with no feedback provided, before proceeding
to the next question. The robots responses to students input
were managed using an intent-based approach of the Furhat
such as adapting to perceived student (dis)agreement captured
via speech recognition and mapped to predefined user intents.
These intents were linked to corresponding states in the
interaction flow. Video sequences of the robots part of the
interaction are available in the supplementary material.
H. Statistical Analysis
Given the likely non-normality of behavioral data, the un-
balanced group sizes, and the correlated nature of repeated
student observations, our primary analyses used Generalized
Linear Mixed Models (GLMMs) to flexibly account for non-
tions of normal residuals. As covered in Sec III-A, IRT
modeling was used to estimate students baseline abilities
and question difficulties, providing a principled way to derive
expected performances without the need for traditional mixed-
effects random intercepts. In peripheral analyses where data
independence could be safely assumedsuch as group com-
parisons based on single aggregate measureswe employed
ANOVA models for their computational efficiency and easier
interpretation. This analytical strategy allowed us to match
model complexity to the nature of each analysis while ensuring
that critical statistical assumptions were respected throughout.
IV. RESULTS
With 40 students interacting with the robot across 8 ques-
the students preliminary answers (PA) and final answers
(FA). The eight questions (see Fig.2 and Sec.III-D) included
two Deception questions (Easy), four Conditioned questions
(Medium), and two Contrast questions (Difficult), correspond-
ing to robot correctness and question difficulty levels as
previously outlined. The main dependent variable, Alignment,
FA resisting the robots influence (A-1) or changing to align
with the robot (A1).
A. General findings on student alignment
Students could agree with the robot either by maintaining the
same PA as the robot (nA0  181) or by changing for their
FA after hearing the robots arguments (nA0  139). Of the
181 instances on which the student and robot agreed on the PA,
they were correct on 146 and wrong on 35. For these 35 events,
the students did not change their answer, despite hearing the
robots incorrect arguments for this answer, which provided
opportunities to detect errors and induce scepticism. Dissent
(A0) occurred 139 times: 45 for the Deception questions
(56.2 out of 80), 55 for the Conditioned (34.38 of 160),
and 39 for the Contrast (48.75 of 80). A logistic regression
model was fitted to assess the influence of question type
on dissent, using Deception (Easy) as reference. The model
revealed that dissent was significantly less likely for Condi-
tioned (Medium) questions than for Deception (  0.898,
p  .001), while no significant difference was found between
Contrast (Difficult) and Deception (  0.301, p  .343).
(Left) Alignment matrix between students (sorted by ability) and
questions. Blue and red cells (border or filled) indicate, respectively, students
aligning and resisting after PA dissent. Cell numbers describe the question-
student pairs contribution to beyond expectation results. (Right) z-score values
per student, showing deviation from expectations for PA (light bars) and FA
(dark), and which students performed beyond expectations (red line and dots).
(Bottom) z-score values per question, indicating deviation from expectations
for PA and FA, and for which questions the students answered beyond
expectations (red line and dots). Bars above zero indicate more correct answers
than expected, below zero less, based on the diagnostic test.
Student alignment as a function of question difficulty: Easy (E)
Students aligned with the robot after 117 of the PA dissents
and resisted in only 22 (11 for Deception, 9 for Conditioned,
and 2 for Contrast questions), as shown in blue and red
(S40), whose PA always agreed with the robots, all other
students aligned at least once (for 25 on at least one Deception
question). 13 students resisted the robots persuasion at least
once (10 for Deception, with one student resisting on both
Q37). An analysis of variance on alignment rates found no
relationship with ability (F0.16, p.696, R20), showing that
the robot influenced students across all ability levels.
The linear regression analysis shown in Fig. 6 indicated
that question difficulty significantly predicts alignment (
1.874, p  .026). A Mixed Linear Model analysis on alignment
across difficulty levels (Easy, Medium, Difficult), controlling
for ability, revealed a significant increase in alignment from
Easy to Difficult questions (-0.174, p.020). There was
no significant difference for Medium questions (-0.100, p
.161), and ability had no significant effect (-0.070, p.865).
It should be noted that E questions are identical to Deception,
and thus also differ in robot correctness.
A Generalized Linear Mixed Model (GLMM) was used
to assess the impact of Deception vs. non-Deception on
and condition as fixed covariates. The model showed no
significant effect (-0.055, p.469) and students did hence
not respond statistically differently when the robot presented
incorrect arguments. We implemented another model exploring
interactions between these factors and Deception, finding that
question difficulty has a substantial effect size (134.91,
SE101.91, p.186) on alignment in Deception, as will be
further explored in Sec. V.
B. Robot Influence on Student Performance
Following this broad overview of student alignment, we ex-
plore to what extent the changes between students preliminary
answers (PA) and final answers (FA) were driven by the
robots influence. Specifically, we measure how much the
deviation from expected outcomes shifted between PA and FA,
taking into account each students ability. The methodology
consisted of several sequential steps: 1. Data compilation into
a unified dataset of correctness probabilities (cp), PA and
FA per student. 2. Expected performance per question was
Comparison of Diagnostic (x-axis) and In-Experiment Performance
(y-axis) on Preliminary (light dots) and Final Answers (dark dots).
calculated using cp values and dispersion metrics, assessing
deviations in PA and FA with p-values. Thus, an 8-item vector
per student captured IRT-derived probabilities of correctness
for each question. 3. Segmented analysis of Deception and
non-Deception to separate negative and positive robot-induced
deviations. 4. Monte Carlo simulations estimated the probabil-
ity of observed PA and FA under the null hypothesis of random
chance. Per student and question, 10,000 simulations were run
based on cp values to generate p-value distributions. 5. Fishers
method for combined p-values synthesized Deception and non-
Deception questions to capture negative and positive perfor-
mance shifts, enhancing statistical power . 6. Deviation
factors were calculated as the difference between PA and FA
p-values from steps 2 and 5, for cases where PA performance
was within expectations (p0.05) and FA performance was
beyond (p<0.05), or where both deviated but FA performance
was more extreme.
The students and questions that contributed the most to
the results being beyond expectations (p<0.05) are indicated
in Fig. 5 (right and bottom). An analysis revealed that 98
out of 117 alignments directly contributed to the deviation
from expected results. While 35 students performed within
expectations in their PA, only seven did so in their FA. Of the
40 students, 36 deviated more in their FA than in their PA,
the exceptions being S12, S17, S18, and S40.
For students S12 and S17, FA deviated less from expec-
tations than PA, but both results were beyond expectations.
Only student S18 performed according to expectations in both
PA and FA and S40s performance remained unchanged (but
beyond expectations) since there was no dissent with the robot.
tween diagnostic and preliminary answers, establishing that the
student group performed within their expected ability shown
by the diagnostic test before hearing the robots input, there
was no significant correlation between diagnostic and final
FA. A thorough and conservative investigation revealed that
75 of the students performed beyond their expected capacity,
above expectations for non-Deception and below expectations
for Deception questions. This should be interpreted as a direct
result of robots influence.
Fig. 8. Alignment for cohorts U, N, C for (Left) Conditioned questions and
(Right) All questions, with significance levels for differences between cohorts.
C. Influence of robot certainty
We performed a series of GLMM analyses to examine the
impact of the robots displayed certainty on student alignment.
We controlled for covariates student Ability and question
sequence of robot condition.
Conditioned questions: The GLMM showed that with
cohort U as reference, cohort C demonstrated significantly
higher alignment (p  0.001), but not cohort N (p.083), as
shown in Fig. 8. This highlights the substantial impact of the
robots expressed certainty on alignment, beyond what could
be attributed to question difficulty.
Deception questions: The model showed no significant ef-
fects of cohorts (U, N, C) or ability on alignment. Coefficients
for cohort and ability are close to zero (p0.05) and group
variance is minimal, indicating low variability between groups.
Since the robot portrayed neutral certainty for all cohorts in
preceding questions did not have a significant effect.
All questions: A Mixed Linear Model regression analysis
was conducted using Alignment as dependent variable for all
139 dissents with Cohort C as reference. Cohort U showed
a marginal effect (-0.131, p.059), suggesting a possible,
although not statistically significant, reduced alignment com-
pared to Cohort C over all questions (i.e., not only the ones
on which the robot was displaying certainty). The effects of
Cohort N were not significant, and as for the co-variates,
alignment was significantly influenced by question difficulty
(  1.794, p  .020) but not student ability.
First half vs. last: A Mixed Linear Model regression anal-
ysis was conducted to examine the effects of robot condition,
question sequence, difficulty, and ability on alignment for the
139 instances of dissent. The question sequence (first vs. last
four questions: p  .907) had no impact on alignment and
interaction terms between condition and question sequence
were non-significant. This suggests that students were not
influenced by the fact that the robot had been uncertain (Group
2) or certain (Group 3) on questions in the first half when they
interacted on questions in the second half.
D. Pre-test questionnaire responses
The analysis of the pre-test questionnaire targeted finding
student characteristics that made them more prone to accept
the robots correct or incorrect arguments. We first performed
single-factor ANOVA on Deception and non-Deception ques-
an above-mean response for each characteristic and those
who had a below-mean. The most striking difference is that
students with more experience of using LLMs aligned 38
more (p0.029) over all questions than students with below-
mean experience. More importantly, when considering only the
Deception questions, students with more experience of LLMs
were significantly more aligning with the robots incorrect
solution (see Table III). The results in Table III further suggest
that extroversion and engagement may play a role in student
alignment with the robot. For non-Deception questions, being
full of energy and considering that efforts lead to success
(which could be interpreted as being open to constructively
interact with the robot on problem-solving) had a positive
supported by non-significant results for being social, 39,
p.065; talking a lot, 37, p.075; and Trying, I can learn
as being positive in difficult situations and being persuasive
(and thus presumably having higher confidence in ones own
stance) had a marginally significant negative effect, as this
lead to, respectively, 31 and 28 less alignment (p.050;
p.086). For Deception questions, being reserved and quiet
(and presumably less prone to follow others lead) had a
positive effect, as these students aligned less with the incorrect
answer (supported by results for being shy -38, p.072).
E. Post-test questionnaire responses
We focus on the students perception of how correct and
convincing the robot was, per questions and robot conditions.
correct to 78 (3.95, 0.87), compared to the true value
of 75. Groups 2 and 3, who interacted with the certain
uncertain robot, rated the robot correctness slightly (and
non-significantly) higher than the control Group 1 (13.75,
24.1, 33.93). The students further responded that the
robot had convinced them in 84 of the dissents (4.225,
0.79), compared to the true ratio of 81 (95 out of
117 cases), with Group 1 perceiving that they had been
convinced slightly more often (14.33, 24.13, 34.14).
The students rating of the extent to which the robot made
them change their mind (3.43, 1.26) and influenced their
TABLE III
CHARACTERISTICS IN THE PRE-TEST RESPONSES FOR WHICH THERE WAS
A SIGNIFICANT DIFFERENCE IN ALIGNMENT BETWEEN STUDENTS WITH
ABOVE- AND BELOW-MEAN RESPONSES IN THE PRETEST FOR EITHER THE
Deception OR NON-Deception QUESTIONS.
Deception
non-Deception
Characteristic
LLM usage
Being reserved
Being quiet
Efforts lead to success
Being full of energy
thinking (3.6, 1.27) was lower, but until a thorough
ethnomethodology conversation analysis is performed, it is not
possible to assess whether this discrepancy should be attributed
to the students not actually being convinced by the robot when
questionnaire indicating that the robot had not always been
correct. The students were, on average, neither certain nor
or they convinced the robot (2.92, 1.51).
Regarding on which questions there had been a dissent
and the robot had been more convincing on, two qualitative
observations can be made (as the measures are the number
and ratio of students mentioning each question, no statistical
test is applicable). Firstly, the ratios of questionmentions
indicate that the students assessed post-test that there had
been a higher degree of dissent on the Deception (r0.38)
than on non-Deception questions (r0.28). Secondly, by
calculating the difference in ratio of mentions between more
convincing and less convincing we find that the robot was
perceived as more convincing when it was
correct (r0.37) than wrong (r-0.19);
certain or neutral (r0.70; 0.60) than uncertain (r0.07).
V. DISCUSSION
Starting from the concept of informational trust (Sec. II-A),
we interpret the results at the event level and longitudinally.
At the event level, each dissent and each robot argument,
constitutes an independent opportunity for alignment or resis-
tance. A correct or incorrect robot argument allows the stu-
dents to evaluate their own position and the robots credibility,
potentially reinforcing their decision to align, strengthening
their resistance or even change their previous alignment. In
117 out of 139 instances where students preliminary answers
(PA) disagreed with the robot, they changed their final answers
(FA) to align with the robot, including 34 times when it was
there was a dissent, confirming H1. This high alignment rate
suggests that uncertainty in their own knowledge led students
to seek guidance from the robot, consistent with informational
trust concepts. The experimental setting, the robots social
contributed to the students uncertainty. Question difficulty
significantly influenced alignment, with greater conformity on
more difficult questions, aligning with prior research . A
stringent analysis demonstrated the robots influence, since
75 of students performed beyond their expected capacity.
Consistent with H2, students were more inclined to follow
the robot when it was portrayed as being certain. This effect
reflects the role of source reliability . When the robot
expressed certainty, students likely perceived it as a knowl-
edgeable and trustworthy source, increasing their willingness
to align. Conversely, expressions of uncertainty reduced align-
students continued to align at a high rate.
Confirming H4, we found that students with more expe-
rience of using LLMs like ChatGPT were more likely to
align with the robots answers, even when the robot was
incorrect. This indicates that prior experience with AI can
enhance the perceived reliability of AI sources, potentially
leading to over-reliance also in AI that are not actually
driven by LLMs [8, 9, 21]. Personality traits may have also
influenced alignment, in that students self-identifying as more
outgoing and energetic were more inclined to align with the
robot. Extrovert individuals may have a higher propensity for
social engagement and be more susceptible to social influence,
affecting their trust in the robot .
Alignment did not differ significantly between Deception
and non-Deception questions. While the flawed robot argu-
ments on Q37 might have provided students with oppor-
tunities to detect inconsistencies, this was not reflected in
statistically significant differences.
As these questions were the easiest on a topic that the
students should master, they should have sounded incorrect
or strange, prompting the students to critically evaluate and
potentially resist the robots influence. The rate of non-
alignment in Deception was in fact doubled compared to
non-Deception (24 vs. 12), and though not statistically
more resistant when the robots arguments are incorrect. The
rate of non-alignment in Deception was numerically higher
than in non-Deception questions (24 vs. 12), though this
difference was not statistically significant. This observation
may warrant further exploration in future studies examining
how prior knowledge interacts with flawed arguments.
This aligns with the Elaboration Likelihood Model ,
which posits that individuals are less likely to be persuaded
by weak or flawed arguments when they are motivated and
able to process the information carefully.
als may adjust their perceptions based on prior experiences
with the source [18, 29, 36]. However, our results did not
support H3, as no significant effects of preceding questions
on student alignment were found. Statistical analyses showed
that students were neither influenced by the robots certainty
on the previous two questions when confronted with the
Deception questions, nor by experiences on the first half when
coming to the second. This suggests students treated each
interaction independently, without adjusting their perceptions
of the robots reliability based on prior behaviour.
mean change between preliminary and final answers on the
Deception questions was the largest for cohort C (-0.58)
and clearly smaller for cohort U (-0.29), with cohort N in
between (-0.40), thus suggesting that it may be worth inves-
tigating carry-over effects on Deception from preceding robot
certainty in future work. Second, we explored the behaviour
of the 39 students who dissented at least once, categorizing
them into Aligners (the 32 who aligned with the robot at their
first dissent) and Non-aligners (the seven, S10, S12, S16, S18,
rate after the first dissent and conducted an independent
samples t-test. The results showed a statistically significant
difference between the groups, t(37)  3.73, p.001. Aligners
(n  32, 0.94, 0.21) had a higher Alignment rate than
Non-aligners (n7, 
