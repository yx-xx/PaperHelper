=== PDF文件: Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification.pdf ===
=== 时间: 2025-07-22 16:12:56.682837 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Uncertainty-Aware Trajectory Prediction via
Rule-Regularized Heteroscedastic Deep
Classication
Kumar Manas1, Christian Schlauch2,3, Adrian Paschke1,4,Christian Wirth2 and Nadja Klein3
AbstractDeep learning-based trajectory prediction models
have demonstrated promising capabilities in capturing complex
interactions. However, their out-of-distribution generalization
remains a signicant challenge, particularly due to unbalanced
data and a lack of enough data and diversity to ensure robustness
and calibration. To address this, we propose SHIFT (Spectral
Heteroscedastic Informed Forecasting for Trajectories), a novel
framework that uniquely combines well-calibrated uncertainty
modeling with informative priors derived through automated rule
extraction. SHIFT reformulates trajectory prediction as a clas-
sication task and employs heteroscedastic spectral-normalized
Gaussian processes to effectively disentangle epistemic and
aleatoric uncertainties. We learn informative priors from training
driving rules, such as stop rules and drivability constraints,
using a retrieval-augmented generation framework powered by
a large language model. Extensive evaluations over the nuScenes
displacement metrics. In particular, our model excels in complex
higher. Project page:
I. INTRODUCTION
Trajectory prediction represents a fundamental challenge
in autonomous driving and robotics, serving as a critical
bridge between perception and planning systems [37, 22].
As illustrated in Fig. 1, autonomous vehicles must simultane-
ously reason about multiple possible future trajectories while
accounting for road conditions, trafc rules, and multi-agent
interactions. This complex decision-making process becomes
particularly challenging in urban environments, where the in-
teractions between vehicles, pedestrians, and infrastructure are
fast-changing [25, 5]. Deep learning approaches for trajectory
prediction have demonstrated remarkable potential in tackling
these challenges . Nevertheless, the safety-critical nature
of autonomous driving introduces requirements that current
state-of-the-art deep learning-based trajectory predictors strug-
gle to address comprehensively.
given the high cost of data collection and long-tail distribution
of unique scenarios in urban environments. For example, most
observed scenarios involve relatively simple behaviors, such
1Department of Mathematics and Computer Science, Freie Universitat
2Continental Automotive Technologies GmbH, AI Lab Berlin.
3Karlsruhe Institute of Technology, Scientic Computing Center, Methods
for Big Data. 4Fraunhofer Institute for Open Communication Systems, Berlin,
Germany. Contact: kumar.manasfu-berlin.de.
as following a straight road, while rare safety-critical events,
like near-accidents, are particularly challenging to capture
due to their low frequency. Data efciency of deep learning-
based trajectory predictors has only recently gained attention
in scientic investigations [16, 56].
perception systems, and geographic locations. Social norms
and trafc rules provide valuable context for predicting com-
pliant agent behavior . However, informed or knowledge-
guided deep learning-based trajectory predictors have predom-
inantly focused on physical constraints [12, 1, 27, 55], leaving
broader contextual prior knowledge underexplored .
uncertainty that must be carefully quantied. Epistemic uncer-
tainty arises from model limitations and incomplete training
of agent behavior and environmental dynamics . Although
most trajectory prediction models address the predictive multi-
modality resulting from these uncertainties (e.g., by framing it
as a classication problem), many fail to provide sufciently
calibrated uncertainty estimates, leading to mode-collapse is-
To address these requirements, we propose SHIFT (Spectral
Heteroscedastic Informed Forecasting for Trajectories) to syn-
ergize uncertainty quantication with rules as soft constraints
in a scalable framework as visualized in Fig. 2:
1) We extend the Heteroscedastic Spectral-normalized
Gaussian Processes (HetSNGP) by Fortuin et al.
to the trajectory prediction using the CoverNet baseline
architecture . CoverNet frames the prediction as
a classication problem. HetSNGPs enable simultane-
ous estimation of epistemic and aleatoric uncertainties
through a distance-aware neural GP layer combined with
input-dependent noise modeling, improving calibration
and requiring minimal architectural change and compu-
tational overhead.
2) We model epistemic uncertainty while accounting for
the inherent stochasticity of agent interactions through
aleatoric uncertainty, enabling a sequential learning pro-
cess for informative prior weight distributions. In stage
capturing structured prior knowledge. These learned
trafc rule priors are then imposed as regularization
during stage 2, guiding model training on real-world
observations. This approach soft-constrains trajectory
Traffic Rule-Based Prior Trajectory
Posterior Trajectory
Ego Uncertainty Zone
V1 Predicted Trajectory
V2 Predicted Trajectory
Fig. 1: Visualization of SHIFTs trajectory prediction framework. Rule-based priors (blue dashed) generate trajectories without
interaction modeling, while interaction-aware HetSNGP posteriors (pink solid) produce uncertainty-calibrated ego paths.
Surrounding vehicles predictions (V1: green dashed, V2: purple dashed) with uncertainty regions (light greenpurple shading)
reveal multi-agent dynamics. Denser pink stripes (not size-based) highlight where vehicle interactions amplify prediction
predictions to align with trafc rules while preserving
the exibility to adapt to high-uncertainty and uncom-
mon driving scenarios.
3) We employ a large language model (LLM) pipeline
to semi-automatically generate synthetic training labels
from natural language descriptions. This approach al-
lows us to more easily scale the integration of multiple
trafc rules, either as (a) a single unied prior learned
from a single knowledge task or (b) a chained prior
sequentially learned from multiple knowledge tasks in
our rst training stage.
SHIFT increases data efciency and robustness by integrat-
ing sets of rules into the training process. Through extensive
experiments on the nuScenes dataset , we demonstrate
robust performance gains across varying training-set sizes
(100 vs. reduced data) and challenging geographical splits
(cross-location tests). Our results show particular strength
in out-of-distribution scenarios, where the combination of
uncertainty awareness and rule-based soft constraints provides
more reliable predictions than conventional training methods.
II. RELATED WORK
Trajectory Prediction Models: Existing models can be dif-
ferentiated based on whether they predict the future behavior
of all agents simultaneously (joint trajectory prediction) or a
single agent at a time (marginal trajectory prediction), with
most work focusing on the simpler latter problem [22, 54].
Deep learning approaches have achieved state-of-the-art per-
formance in in-distribution evaluation settings, surpassing tra-
ditional methods such as kinematic models , Kalman lters
and Bayesian Networks  . Building on the work of Hage-
dorn et al. , deep learning-based trajectory predictors can
be categorized based on their input representation, model back-
include birds-eye-view (BEV) image rasterizations , sparse
vector encodings , graphs , and parametric curves .
Model backbone encompass convolutional neural networks
(CNN) , recurrent neural networks , graph neural
networks  and more recently transformers [50, 57, 36].
Trajectory decoding method include regression , anchor
trajectory classication , trajectory renement  or end-
point completion . Our chosen baseline, CoverNet ,
utilizes a CNN backbone with BEV input rasterization and
frames the prediction problem as anchor trajectory classica-
tion. This makes it particularly suitable for our modications.
Informed Trajectory Prediction: Informed, or knowledge
or rule-guided learning integrates explicit prior knowledge
into the training process or architecture of deep learning
models . Most existing methods integrate physical knowl-
edge to eliminate physically implausible outcomes, thereby
simplifying the solution space [39, 12, 1, 27, 55]. In con-
as they can be violated in uncertain or atypical scenarios.
Their integration often relies on transfer learning  or multi-
task learning methods . A related probabilistic approach
learns informative priors sequentially from synthetic training
setup offers exibility, allowing sequences to be extended
while minimizing catastrophic forgetting [14, 51]. However,
unlike our method, it does not incorporate trafc rules and
local rule constraints. Additionally, a signicant challenge
remains in translating natural language descriptions of rules
into synthetic training labels. Recent efforts used retrieval
augmented generation (RAG)  and large language models
Trajectory Features
Scene Context
Epistemic Uncertainty
Aleatoric Uncertainty
Ground Truth Labels
Observed Trajectory
Input Processing
Historical Trajectories
HD Map Features
Stage 2: Multiclass classification
Spectral Normalization Refinement
Heteroscedastic GP Layer Refinement
Epistemic Uncertainty
Aleatoric Uncertainty
Categorical Cross-entropy Loss
Predicted Trajectory
Epistemic Uncertainty
Aleatoric Uncertainty
Stage 1: Multilabel Classification
Spectral Normalized Layers
Heteroscedastic GP Layer
Epistemic Uncertainty
Aleatoric Uncertainty
Binary Cross-entropy Loss
Regularization
Regularization
Traffic Rule Labels
LLM-Generated
Fig. 2: Concept of the SHIFT framework for uncertainty-aware trajectory prediction. Our model is based on the CoverNet
output layer is replaced by a heteroscedastic Gaussian process. Given the processed input, comprising trajectory histories and
map features, the model is trained in a multilabel classication on trafc rule labels generated from an LLM-based rule-ltering
approach (stage 1). The obtained informative prior is used to regularize the multi-class classication training on ground truth
labels from the observed data (stage 2). The model outputs consist of the predicted trajectories and disentangled epistemic and
aleatoric uncertainty estimates. Best viewed in color.
(LLMs) to convert language descriptions into formal logic
rules [21, 33, 34]. We employ these logic rules to lter input-
dependent synthetic training labels from predened anchor
trajectories.
Uncertainty Quantication: Mode-collapse arises when tra-
jectory prediction models fail to adequately capture the full
diversity of possible futures . This issue is commonly
addressed through design choices in trajectory decoding .
For instance, in anchor trajectory classication, the softmax-
normalized logits represent distributional information and
identify the most likely top-k anchors . However, deter-
ministic deep learning models often exhibit overcondence,
which hinders their ability to accurately reect the true vari-
ance and multiple modes of the underlying distribution. Their
calibration can be substantially improved using principled
uncertainty estimation techniques, such as those provided by
approximate Bayesian deep learning . Commonly used
sampling-based methods, including Deep Ensembles ,
Monte Carlo (MC) Dropout , Stochastic Weight Averaging
Gaussian (SWAG) , Laplace  or variational approx-
require multiple forward passes during prediction. Itkina and
Kochenderfer  demonstrated the signicance of estimating
epistemic uncertainty in trajectory prediction through eviden-
tial deep learning, utilizing a classication-based prediction
framework similar to ours. In contrast, Spectral-normalized
Gaussian Processes (SNGPs) , belonging to the family of
deterministic uncertainty estimators [9, 40], provide especially
compute-efcient last-layer approximation. The Heteroscedas-
tic Spectral-normalized Gaussian Process (HetSNGP)
leverages the heteroscedastic method for classication prob-
lems  to further improve calibration by disentangling the
uncertainty into distance-aware epistemic and input-dependent
aleatoric components . Thus, unlike homoscedastic mod-
factors such as the complexity of the driving scenario at hand,
instead of assuming a constant aleatoric uncertainty across all
input conditions. This, in turn, can benet the calibration of
the epistemic uncertainty , which we employ for our rule-
informed learning approach.
III. METHODOLOGY
We focus our discussion on the marginal trajectory predic-
tion. Here, we aim to predict the future possible trajectories
of a single agent at a time over some horizon, given the road
surrounding agents over some history, as well as the state of
any trafc guidance systems. Our goal is to inform the model
using trafc rules as explicit prior knowledge about the ex-
pected behavior of the agent. We propose SHIFT to approach
rule informed marginal trajectory prediction in a scalable
CoverNet as our baseline model for trajectory classication,
(B.) HetSNGP as uncertainty-aware extension to CoverNet,
that disentangles epistemic and aleatoric uncertainties, (C.) a
regularization method which enables the integration of priors
into our HetSNGP-CoverNet model, (D.) a sequential task
setup to learn these priors from synthetic training labels,
and (E.) an automated rule-ltering approach to encode these
synthetic training labels from natural language descriptions.
We describe these building blocks and (F.) our rule selection
A. Trajectory Classication with CoverNet
We adopt an anchor trajectory classication approach as
the foundation for our framework. This approach simplies
the trajectory prediction by generating a input-dependent or
input-independent set of anchor trajectories that capture all
plausible future motions [4, 8]. These anchors can be rep-
resented as classes in the output space Y  {1, 2, . . . , K}.
The training data {(xi, yi)}N
i1 consists of samples xi from
the d-dimensional input space X
Rd and the training
labels yi being the classes in Y that most closely reect
the observed ground truth trajectories as measured by the
Euclidean distance. The model can then be trained to classify
the most likely anchor using a categorical cross-entropy loss.
Anchor trajectory classication offers several advantages.
By appropriately sizing the anchor set, the prediction problem
can be simplied, and the softmax-normalized logits provide
probabilistic interpretations. Appropriately spaced anchors can
also guard against drastic forms of mode-collapse . How-
imation error, as the anchors may be arbitrarily distant from
the actual ground truth trajectory. Consequently, the selection
of anchor trajectories plays a critical role in the overall model
performance.
For our baseline, we build on CoverNet  as it employs
such an anchor classication framework and has been used
in related work for informed trajectory prediction [4, 44, 45].
CoverNet processes birds-eye-view (BEV), agent-centric ras-
terized input images using a ResNet-50 backbone. This ras-
terization preserves spatial relationships well. Next, CoverNet
generates a predened, input-independent anchor trajectory set
based on all ground truth trajectories in the training data.
Depending on a single distance parameter these trajectories
are clustered to an appropriate number of anchors. This set
of anchors is held xed across all inputs. Note, that CoverNet
also proposes an input-dependent anchor generation based on
a dynamical bicycle model which encodes physical knowledge
as hard constraint. Our proposed methodology is in principle
agnostic to the kind of anchor sets and can be combined with
such anchor generation for physical knowledge integration. For
simplicity we focus on the predened anchor set.
B. Uncertainty-Aware Trajectory Classication
CoverNet
heteroscedastic
Spectral-
Normalized Gaussian Process (HetSNGP) . The Spectral-
Normalized Gaussian Process (SNGP), originally introduced
by Liu et al. , combines the representative power of deep
neural networks with uncertainty-aware Gaussian Process
(GP) models . The architecture consists of a deterministic,
spectral-normalized feature extractor fNN : X H, and an
hierarchical GP output layer f L
Fig. 3: Components of a HetSNGP model include a spectral-
normalized feature extractor and a heteroscedastic Gaussian
process as output layer.
The feature extractor is a neural network parameterized
through NN that maps the high dimensional input space
X Rd into a low dimensional hidden space H Rm
with m d. The spectral-normalization enforces Lipschitz
the feature extractor  and prevent feature-collapse . It
is especially efcient to compute for the residual layers of a
CNN-based backbone , as used in our CoverNet baseline.
The resulting training data {(hi, yi)}N
i1 with its embedded
inputs hi  fNN(xi) is then fed into the GP output layer.
The GP output layer employs a radial basis function (RBF)
kernel  and maps from the hidden space H into the output
space Y. Its hierarchical structure enables to quantify both,
aleatoric and epistemic uncertainties. The GP latent variable
g is modeled with a zero-mean multivariate normal distribution
prior per-class c Y :
gc N(0, (h, h))
The posterior covariance matrix (h, h) of gc captures epis-
temic uncertainty. This uncertainty is class-independent, as the
GP prior is shared across all classes.
To model aleatoric uncertainty, the HetSNGP introduces a
hierarchical latent variable u with the following prior per-
sample i {1, . . . , N}:
ui N(gi, (h, h))
where (h, h) has a low-rank approximation based on a
factor covariance matrix modeled as linear neural network with
parameters u  to achieve scalability with the number of
classes K. The variable u captures input-dependent noise (e.g.,
sensor noise, agent intent ambiguity) and is processed through
a heteroscedastic layer to produce aleatoric uncertainty. Unlike
epistemic uncertainty, aleatoric uncertainty is sample-specic
and varies across inputs. The hierarchical setup using both
uncertainties correlates samples and classes in the posterior,
leading to a better calibration .
The above formulation is computationally intractable. To
achieve tractability the kernel is shared between all classes
and approximated as (h, h)  using m random Fourier
features (RFF) i
m cos(Whi  b), where W and b
are xed randomly sampled weights and biases respectively,
based on Bochners theorem [42, 29]. This RFF approxima-
tion reduces the computational complexity for inferring the
posterior from O(N 3) to O(Nm2) and allows us to write
the GP as neural network layer with logits gc(hi)  gci
and prior gc N(0, I). Since the posterior of gc is not of
closed form, we use a Laplace approximation, which yields a
Gaussian approximate posterior for the output weights gc,
p(gc{(xi, yi)}N
i1) N(gc;
gc is the maximum a posterior (MAP) estimate and
precision 1
gc is given by
where pi,c is the softmax output p(yi  cu
class per-sample u
gc. Overall, the trainable weights
in our output layer consist of GP  {u, gcc Y}. We
adopt this output layer for our CoverNet baseline by replacing
its dense output layer.
During inference, we approximate the marginal predictive
distribution p(yh)
p(yu)p(uh)du of u using Monte
Carlo (MC) sampling:
p(y  h)  Ep(GP{(hi,yi)}N
p(y  h, GP)
p(y  h, (s)
where S is the number of MC samples, and (s)
GP are samples
drawn from the approximate posterior distribution. Following
Fortuin et al. (2021), a temperature parameter  is introduced
to recalibrate uncertainty at test time. The MC sampling is
computationally efcient, as it involves sampling only from
the output layer and supports parallelization.
C. Regularization using Informative Priors
To enable the integration of trafc rule as informative priors,
we leverage the regularization method by Schlauch et al.
. The idea can be seen as an extension of online elastic
weight consolidation  for GPs with RFF-approximated
RBF-kernels. Assume we have some informative prior for
the GP output layer   N(gc; gc, gc) rather than a
standard Gaussian prior as before. We then regularize the MAP
estimate
gc through this prior. This leads to the penalized log-
likelihood
log pgc (yihi) GP
2 (gc gc)1
gc (gc gc).
The posterior variance of gc is then given by
This regularization scheme introduces two new hyperparame-
ters GP and GP, which we assume are the same for all classes.
The hyperparameter GP > 0 tempers the overall prior, while
the hyperparameter 0 < GP < 1 controls the learning decay
when the model is sequentially trained .
In addition, assume we have some informative prior for the
feature extractor   N(NN; NN, I) too. Regularizing the
feature extractor is then equivalent to L2-regularization for
the MAP estimates
log pNN(yihi) NN
2 (NN NN)2,
which introduces an additional hyperparameter NN control-
ling the strength of the parameter binding. Note, that we do
not specically place an informative prior on the parameters
u of the GP output layer as this head captures a property that
is purely related to the data itself.
D. Learning Informative Trafc Rule Priors
We encode prior knowledge about trafc rules by incor-
porating additional tasks with synthetic training labels .
Given the input samples xi X and the set of anchor
classes in Y that yield rule-compliant anchor trajectories. In
this multi-label classication setup, the model is trained to
predict which anchors satisfy the respective trafc rule using
a binary cross-entropy loss.
Combined Rules
Unified prior
Chained prior
Fig. 4: Illustration of the possible task setups in the rst
training stage. The uninformed prior 0 is either sequentially
updated using distinct trafc rule labels prl for each rule
l  1, . . . , R or updated once from a unied trafc rule
R as chained and unied priors, respectively. Both setups
can be mixed. The obtained nal prior is used to regularize
the training on the observed ground truth trajectories in the
next training stage.
Using an uncertainty-aware model in SHIFT, we can se-
quentially train on multiple tasks. Starting with an uninfor-
mative prior 0, the posterior distribution from a previous
task serves as informative prior l to regularize the sub-
sequent task. The probabilistic regularization as described
in Sec. III-C constitutes a key advantage over conventional
transfer learning, which does not guard against catastrophic
forgetting  or overly biasing subsequent training tasks
. Another important advantage lies in the exible task
task that encodes prior knowledge about multiple trafc rules
(right). Alternatively, we can dene multiple tasks that encode
prior knowledge about one trafc rule prl(yx) each, learning
a chained prior (left). This sequential task setup allows us to
re-use priors easily, constituting an advantage over multi-task
joint learning setups .
E. Synthetic Training Labels from Trafc Rules
To scale the integration of trafc rules, we employ a
semi-automated ltering approach that (a) translates natural
Template
Driving Rule as Text
Representation of Rules
def pedcross(a, b)
for a in b:
if a.crosses(b):
return False
Information
Retrieval
Vector DB
(nuScenes API)
Rule Compliant Trajectory
Selection
Fig. 5: High-Level LLM pipeline for synthetic training label
(2) the LLM generates Python functions of rules and human
experts review generated functions by running test cases using
a sample trafc scene, (3) based on rules synthetic labels for
training are generated.
language descriptions into executable Python functions using
a LLM-based human-in-the-loop setup and (b) uses these
functions to label each trajectory in an anchor set for rule
compliance within a given scene. The rst step (a) utilizes
the retrieval-augmented generation (RAG) technique for the
few-shot prompting of the LLM. We extended methodology
developed by Manas et al.  using Llama3.31 model for
this use case. When provided with a natural language rule
API. The LLM then integrates the relevant API calls to
generate an executable Python function grounded in dataset
compliance of the trajectory anchor set for all samples in the
training dataset. Fig. 5 illustrates this process for synthetic
label creation.
This pipeline accelerates rule integration, reducing the need
for extensive manual coding or rule formalization. Various nat-
ural language trafc rules can be translated depending on the
dataset APIs capabilities and the available semantic informa-
tion (e.g., lane types). However, even with sufcient semantic
distances based on the velocities of multiple agents over time)
remains challenging for the LLM. Further details on the LLM
prompts and limitations are provided in Appendix C.
F. Rule Selection
We select two sets of rules to allow comparisons to pre-
vious work and demonstrate the scalability of SHIFT. These
sets of rules model both globally-applicable and situational
behavioral constraints, while avoiding the limitations of our
LLM-powered rule-ltering approach. By integrating rules
of varying complexity, we can account for more nuanced
interactions with the trafc infrastructure.
Our rst set contains a globally-applicable trafc rule:
Stay within the road boundaries: This rule enforces the
fundamental constraint of staying within drivable areas,
reecting high-level driving behavior.
Our second set of rules are situational stop-related trafc
Stopping at red signals: A rule that prohibits crossing
trafc light zones when the red light is active, ensuring
compliance with trafc signals.
Respecting right-of-way: A rule that mandates yielding to
other participants at yield or stop signs, capturing critical
right-of-way interactions.
Prioritizing pedestrian safety: A rule that requires giving
way to pedestrians at active crossings, emphasizing safety
in shared spaces.
IV. EXPERIMENTAL DESIGN
We evaluate the models performance in various scenarios,
including standard full dataset training, low data regimes, and
out-of-distribution generalization across different geographical
locations within the nuScenes  dataset.
A. Dataset
We conduct our experiments on the nuScenes dataset .
We selected it for geographic diversity (Boston, USA, and Sin-
gapore), semantic information (including drivable areas, stop
geographic split enables controlled out-of-distribution (OoD)
Bostons grid-like roads with right-side driving) are tested
on another (e.g., Singapores curved intersections and left-
side driving). This mimics real-world deployment challenges
where models encounter unseen environmental semantics. For
our full dataset experiments, we utilize the trainvaltest splits
following Phan-Minh et al. . In terms of location diversity,
roughly 39 of the scenes are set in Singapore, while 61
take place in Boston. Refer to Appendix F for more details
about the dataset.
B. Evaluation Metrics
We evaluate our approach using three categories of met-
of predictions; (2) Distribution-aware Metrics, quantifying the
quality of predicted probability distributions; and (3) Rank-
ing Metrics, assessing the models ability to assign higher
probabilities to more likely trajectories. minADEk (Minimum
Average Displacement Error): Measures the average l2 dis-
tance in meters between the ground truth trajectory and the
closest predicted trajectory among the top-k predictions. We
report both minADE1 and minADE5 to evaluate single-mode
and multi-mode prediction accuracy respectively. minFDE
(Minimum Final Displacement Error): measures the minimum
l2 distance between the predicted nal position and the ground
truth nal position among the top-k predictions. Negative
Log-Likelihood (NLL): Quanties the quality of the pre-
dicted probability distribution by measuring the negative log-
likelihood of the ground truth trajectory under the models
predictions. A lower NLL indicates better alignment between
the predicted distribution and observed trajectories. Expected
Calibration Error (ECE): Measures the alignment between
predicted probabilities and empirical frequencies . ECE
TABLE I: Comparison of SHIFT and baselines on Full Dataset of nuScenes. Our approach is compared against other uncertainty-
aware classication baselines. Bold indicates best, and second best is underlined. Lower is better for metrics.
CoverNet2
GVCL-Det
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours With Chained Prior)
SHIFT (Ours With Unied Prior)
computes the weighted average of the absolute difference be-
tween predicted probabilities and observed frequencies across
predened probability bins. A lower ECE indicates better
correspond to observed frequencies. Rank (RNK): Measures
the rank of the ground truth trajectory among all predicted
trajectories when sorted by their predicted probabilities .
This metric also measures the calibration of the anchor classi-
A lower rank indicates the model assigns higher probabilities
to trajectories closer to the ground truth.
C. Baselines and Implementation
We compare SHIFT against CoverNet-based classication
baselines from previous work, as these best illustrate the
impact of the rule integration and last layer modications.
ual Learning (GVCL-Det)  with CoverNet, an uninformed
SNGP-CoverNet model (SNGPU) , and a rule-informed
SNGP-CoverNet model (SNGP with Rules) that integrates
sets of rules similar to the unied prior setup of SHIFT. The
SNGP baselines are uncertainty-aware but do not disentangle
uncertainty components, serving as The SNGP baselines are
uncertainty-aware but do not disentangle uncertainty compo-
only uses epistemic uncertainty.
Regarding the implementation, we use a ResNet50 back-
bone and a predened anchor set of 415 modes for all
evaluated models. The input is a rasterized BEV (480x480
pixel) image and color-coded drivability area, lanes, walkways,
stop lines and agent trajectories as described by Phan-Minh
et al. . We encode the agent trajectories with 1-second
observation window and 6-second prediction horizon. This
design aligns with evidence that short-term observations domi-
nate trajectory dynamics, contributing to 80 of the predictive
impact as noted in Scholler et al. . This mitigate tracking
error propagation and improve OoD generalization through
reduced temporal dependencies [35, 3].
Experiments
To ensure a reliable comparison with the baselines, each
experiment is conducted over ve independent runs and we
report the mean and standard deviation of the results. We
evaluate SHIFT with a unied prior integrating both sets of
trafc rules in the following experiments:
Full Data: We compare SHIFT against all baselines using
the full nuScenes train-val-test splits to evaluate general
performance. We also compare against a version with a
chained prior, integrating all trafc rules sequentially, to
investigate the exibility of the task setup in our rst
training stage.
Reduced Data: We compare SHIFT against the SNGP
baselines on training data subsets, using 50 and 10 of
training samples respectively, to assess the data efciency.
Geographic Generalization: We compare SHIFT with
a unied prior against the SNGP baselines on location
shifts to assess OoD generalization. These experiments
include an in-distribution (ID) trainingtesting in the
same location as baseline (Boston Boston and
Singapore Singapore) and an out-of-distribution
(OoD) trainingtesting across locations ( Boston
Singapore and Singapore Boston).
Rule Ablation: We compare SHIFT against variants that
incorporate only one of the rule sets, as well as a version
without any rules.
These experiments test data efciency and robustness to ge-
ographic distribution shifts and assess uncertainty calibration
within these contexts. We also report qualitative visualization
and prediction latency. In Appendix B we highlight results
related to the impact of the posterior GP.
V. RESULTS AND DISCUSSIONS
A. Full Dataset
Table I presents the performance comparison on the full
dataset. Among the SHIFT congurations, the version with
a unied prior achieves the best minFDE1 and RNK scores,
while the chained prior setup excels in minADE1 and
minADE5. More importantly, both variants of SHIFT sig-
nicantly outperform baseline methods across key metrics,
especially in distribution-aware metrics such as the ECE.
These results highlight two properties of SHIFT. First, the
task setup in the rst training stage is exible enough to ac-
commodate both chained and unied priors without substantial
performance trade-offs. Second, the improved calibration of
the heteroscedastic GP layer directly impacts the effectiveness
of the regularization-based approach.
2We use the result reported in Schlauch et al. , where they implemented
CoverNet with 1 s of history observation and 6 s prediction horizon along with
uncertainty integration.
TABLE II: Impact of Reduced Training Data on Performance Metrics. Bold values indicate the best results.
Train Data Used(in )
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours Unied Prior) 4.280.05 4.600.06
TABLE III: Comparison of SHIFT for Geographic Generalization. The best results are highlighted in bold. Both in-distribution (ID) and
out-of-distribution (OoD) performance are evaluated to assess the models ability to generalize to new driving scenarios and to quantify
performance degradation when encountering unseen environments.
Region Pair (Trained on Tested on)
Boston Boston (ID)
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours With Unied Prior)
Singapore Singapore (ID)
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours With Unied Prior)
Boston Singapore (OoD)
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours With Unied Prior)
Singapore Boston (OoD)
SNGPU (Without Rules)
SNGP (With Rules)
SHIFT (Ours With Unied Prior)
B. Reduced Datasets
Table II presents the model performance with reduced train-
ing data (50 and 10 of the original dataset). Even in data-
scarce environments, SHIFT consistently outperforms baseline
methods across all metrics, demonstrating its robustness in
low-data regimes. These results highlight the importance of
incorporating trafc rules as prior knowledge in autonomous
generalize effectively.
C. Geographic Generalization
Geographic generalization results in Table III further vali-
date the effectiveness of SHIFT. In in-distribution (ID) sce-
narios (Boston Boston and Singapore Singapore), our
model outperforms the baselines, demonstrating improved
prediction accuracy (minADE1, minADE5 and minFDE1) and
better-calibrated uncertainty quantication (NLL and RNK).
In out-of-distribution (OoD) scenarios (Boston Singapore
and Singapore Boston), SHIFT demonstrates enhanced
RNK compared to the baselines. The Singapore Boston
transfer exhibits a more pronounced improvement over the
data providing a stronger generalization capacity. In contrast,
the Boston Singapore transfer shows a more modest gain
over SNGP (with Rules), possibly due to limited exposure
to Singapore like complex trafc patterns. Nevertheless, the
results demonstrate that SHIFT mitigates performance degra-
dation under OoD conditions.
D. Rule Ablation
We analyze the role of trafc rules in the SHIFT model
by isolating two rule setsdrivability rules and stop-related
rulesas outlined in Sec. III-F. Since stop-related rules (e.g.,
red light, right-of-way, and pedestrian priority) apply only
to a subset of test cases, we group them together in this
ablation study to assess their combined impact. The results,
shown in Table IV, indicate that stop-related rules play a
more signicant role in improving top-k displacement metrics,
achieving the lowest minADE1 (4.10) and minADE5 (2.13).
improve calibration by lowering NLL from 3.19 (no rules) to
3.20. This suggests that while drivability constraints may not
strongly inuence top-k accuracy, they help rene predictive
uncertainty. Only Drivability Rule refers to global road
boundary rules, while Without Trafc Rules means neither
global nor local rules were used.
achieves the best overall performance, with improvements in
both accuracy (minADE5: 2.13, minFDE1: 9.23) and calibra-
tion (NLL: 3.15, RNK: 11.52). This highlights the complemen-
tary nature of global constraints (drivability) and situational
constraints (stop-related rules) in learning informative priors
across all test cases, especially in distribution-aware metrics.
Comparing SNGP with SHIFT, we observe that even without
rule priors, our model signicantly outperforms both the rule-
based and rule-free SNGP baselines.
E. Qualitative Results
In Fig. 6 we visually compare predictions of SHIFT and
SNGP with rules. At intersections, where agent behavior is
Fig. 6: Qualitative Comparison of Trajectory Predictions. The rst row displays predictions from our best baseline model,
while the second row presents results from SHIFT. Each column represents a different scene. The target agent, for which
predictions are made, is highlighted with a black rectangle. The past trajectory (shown only for SHIFT) is also included for
reference. Best viewed in color.
TABLE IV: Ablation Study: Trafc Rules Impact on SHIFT.
Conguration
SHIFT (Unied Prior)
SHIFT (Only Stop Rules)
SHIFT (Only Drivability Rule)
SHIFT (Without Trafc Rules)
SNGPU (Without Rules)
inherently stochastic, SHIFT more accurately predicts the cor-
rect turn direction. Similarly, in scenarios involving stopping
areas and multiple nearby vehicles, our models predictions
align more closely with the ground truth trajectory than those
of the baseline. Additional visualizations are available in the
Appendix E. Notably, in overtaking scenarios, both models
performed comparably.
F. Runtime Consideration
Computational efciency is crucial for autonomous driving
applications. Our benchmarks indicate that SHIFT achieves an
average prediction latency of 7.4 ms per sample on a single
NVIDIA RTX A5000, compared to 5.6 ms for CoverNet. This
demonstrates that SHIFT incurs only a minimal computational
overhead while enhancing trajectory prediction capabilities.
VI. CONCLUSION
We have introduced SHIFT, an uncertainty-aware trajectory
predictor that integrates prior knowledge of trafc rules into
deep learning models through a scalable probabilistic for-
mulation. By explicitly modeling disentangled heteroscedastic
behavior while reducing overcondence. Our empirical evalu-
ations on nuScenes demonstrate that SHIFT produces not only
more accurate predictions but also better-calibrated uncertainty
unied priors. The demonstrated exibility allows end users
to incrementally train and adapt the model by integrating new,
user-dened driving rules as needed. Such adaptability makes
SHIFT particularly well-suited for safety-critical applications
in autonomous driving, where agent behaviors are governed by
complex social norms and trafc regulations. In future work,
we aim to extend this framework to incorporate interactive
priors for multi-agent coordination and validate its efcacy
within closed-loop planning pipelines.
VII. LIMITATIONS
SHIFT demonstrates promising results by integrating trafc
rules into trajectory prediction. However, several open research
questions remain. First, SHIFT so far been demonstrated only
in a trajectory classication context, where compliance is
evaluated based on predened trajectory anchors. Adapting
it to other trajectory decoding strategies, such as regression,
remains an interesting topic. Second, SHIFT does not guaran-
tee that the integrated trafc rules are actually informative of
the behavior of the agents. Measuring the alignment between
informative priors and observations could help practitioners in
selecting suitable task setups. Third, our rule-ltering approach
for generating synthetic training labels is based on static trafc
rules. Natural language rules that require temporal evaluations
are considerably more difcult to formalize using the RAG-
LLM approach. Fourth, prior knowledge about trafc rules
must map to observable elements in the model input, such
as road geometry and trafc signs. Missing relevant inputs
(e.g., occluded trafc signs, unencoded lane types) prevent the
model from learning anything informative from the synthetic
training labels. This also applies, for example, to rasterized
image inputs, where distinguishing ner details such as solid
lines vs dashed lines might be challenging due to inherent
limitations in the pixel resolution. Despite these limitations,
the modular design of SHIFT reveals an exciting path for
future prior knowledge integration, as it can be scaled with the
development of both rule formalization and prediction models.
Acknowledgment. This work is partially funded by the
German Federal Ministry for Economic Affairs and Climate
Action within the project nxtAIM and Continental Automo-
tive. We also thank Yue Yao for listening to our ideas and
providing valuable feedback from a prediction perspective.
REFERENCES
Mohammadhossein Bahari, Ismail Nejjar, and Alexan-
dre Alahi.
Injecting knowledge in data-driven vehicle
trajectory predictors. Transportation Research Part C:
Emerging Technologies, 128:103010, 2021.
Carlos Barajas, Gianoberto Giampieri, Stefano Saba-
Addressing mode collapse in
trajectory prediction: A maneuver-oriented metric and
approach. In 2024 IEEE Intelligent Vehicles Symposium
(IV), Jeju-Island, Korea, 2024.
Stefan Becker, Ronny Hug, Wolfgang Hubner, and
Michael Arens. RED: A Simple but Effective Baseline
Predictor for the TrajNet Benchmark. In Computer Vision
ECCV 2018 Workshops, 2018.
Freddy A Boulton, Elena Corina Grigore, and Eric M
Motion prediction using trajectory sets and
self-driving
knowledge.
Mohamed-Khalil Bouzidi, Yue Yao, Daniel Goehring,
and Joerg Reichardt. Learning-aided warmstart of model
predictive control in uncertain fast-changing trafc. In
Proceedings of the 2024 IEEE International Conference
on Robotics and Automation (ICRA), Yokohama, Japan,
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh
Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes:
A multimodal dataset for autonomous driving. In Pro-
ceedings of the 2020 IEEECVF Conference on Computer
Vision and Pattern Recognition (CVPR), virtual, 2020.
Sergio Casas, Cole Gulino, Simon Suo, and Raquel
Urtasun. The importance of prior knowledge in precise
multimodal prediction.
In Proceedings of the 2020
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), Las Vegas, NV, USA, 2020.
Yuning Chai, Benjamin Sapp, Mayank Bansal, and
Dragomir Anguelov.
anchor trajectory hypotheses for behavior prediction. In
Proceedings of the 3rd Annual Conf
