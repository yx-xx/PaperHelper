=== PDF文件: FEAST A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization.pdf ===
=== 时间: 2025-07-22 16:01:07.042684 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Towards In-the-Wild Personalization
Rajat Kumar Jenamani1, Tom Silver1, Ben Dodson1, Shiqin Tong1, Anthony Song1,
Yuting Yang2, Ziang Liu1, Benjamin Howe3, Aimee Whitneck3, Tapomayukh Bhattacharjee1
1Cornell University, 2University of Michigan, 3Independent researcher
Fig. 1: Informed by a formative study with 19 care recipients and 2 community researchers, we propose FEAST: a flexible mealtime-assistance
system grounded in adaptability, transparency, and safety. Through in-home evaluations, we demonstrate that FEAST can personalize to the
needs of two care recipients, assisting each with three meals in diverse in-the-wild contexts.
AbstractPhysical caregiving robots hold promise for improv-
ing the quality of life of millions worldwide who require assistance
with feeding. However, in-home meal assistance remains chal-
lenging due to the diversity of activities (e.g., eating, drinking,
mouth wiping), contexts (e.g., socializing, watching TV), food
that can be personalized in-the-wild to meet the unique needs of
individual care recipients. Developed in collaboration with two
community researchers and informed by a formative study with a
diverse group of care recipients, our system is guided by three key
tenets for in-the-wild personalization: adaptability, transparency,
and safety. FEAST embodies these principles through: (i) modu-
lar hardware that enables switching between assisted feeding,
including a web interface, head gestures, and physical buttons,
to accommodate diverse functional abilities and preferences,
and (iii) parameterized behavior trees that can be safely and
transparently adapted using a large language model. We evaluate
our system based on the personalization requirements identified
in our formative study, demonstrating that FEAST offers a
wide range of transparent and safe adaptations and outperforms
a state-of-the-art baseline limited to fixed customizations. To
demonstrate real-world applicability, we conduct an in-home user
study with two care recipients (who are community researchers),
feeding them three meals each across three diverse scenarios. We
further assess FEASTs ecological validity by evaluating with an
Occupational Therapist previously unfamiliar with the system.
In all cases, users successfully personalize FEAST to meet their
individual needs and preferences. Supplementary materials and
videos can be found at: emprise.cs.cornell.edufeast.
Index TermsAssistive, Entertainment and Service Robots,
Human-Robot Interaction, Robot Learning: Foundation Models
I. INTRODUCTION
Eating is a fundamental part of human life, deeply inter-
twined with identity and social interaction . The inability
to self-feed has been associated with profound emotional
and heightened anxiety or fear [24]. Unfortunately, millions
worldwide require assistance with feeding due to spinal cord
conditions . For caregivers, feeding is one of the most time-
consuming Activities of Daily Living (ADLs) , contributing
significantly to their already substantial workload [7, 8].
Robot mealtime-assistance systems have the potential to
assist care recipients and improve their quality of life
while decreasing the physical workload on caregivers [10, 11].
Recent advancements have significantly improved various as-
pects of mealtime assistance, including food manipulation [12
19], skill sequencing [2022], and bite transfer [11, 2326].
Although further progress is needed before these systems can
operate long-term without expert supervision, these recent
works significantly enhance their robustness and autonomy.
Building upon these efforts, this work considers the need
for personalization in mealtime assistance. A one-size-fits-
all mealtime assistance system is not enough to address the
wide range of preferences, functional abilities, behaviors, and
environmental contexts that vary between care recipients and
between meals . For example, consider the two community
researchers (CRs) shown in Figure 1, who are both co-authors
on this paper. CR1, who has Multiple Sclerosis, prefers to
lean forward to take a bite. CR2, who has a C4-C6 Spinal
Cord Injury and very limited head and neck mobility, requires
inside-mouth bite transfer . In a social dining scenario,
CR1 prefers that the robot retract after bite transfer so they
can better see their companion. In another social scenario, CR2
prefers to control the robot using a custom long-continuous-
open-mouth gesture that they create themselves through the
user interface (a standard open-mouth gesture would be falsely
triggered in conversation). These are a few examples among
many underscoring the need for a mealtime assistance system
that can be personalized in-the-wild by the users themselves.
To better understand the nature of personalization in meal-
time assistance, we start our work with a formative user study
(Section III). We use speculative videos to guide in-depth
conversations with 21 care recipients who have diverse med-
ical conditions and mobility limitations. This study has two
outcomes. The first is a collection of specific personalization
requests that we use to develop our system (Section V-A). The
second is the realization of three key tenets crucial for per-
sonalization in mealtime assistance: adaptability, transparency,
and safety (Section V). Care recipients voice the need for
systems that can adapt to their needs and preferencesnot
just once, but from meal to meal, and over time as their
needs change. They also express that any adaptations should
be transparent so that they are able to understand and predict
system behavior. Finally, they underscore the importance of
With these key tenets in mind, we propose FEAST, a flexible
mealtime-assistance system towards in-the-wild personaliza-
tion (Section IV). FEAST is designed for personalization at
both the hardware and software level. The hardware features a
modular tool-change apparatus so that a single robot arm can
assist with feeding, drinking, and mouth wiping. The system
also features accessible buttons, status LEDs, cameras, micro-
interactions with the user. We also propose a novel feeding
utensil that increases the robots workspace and decreases
obstructions to the users view.
On the software side, to strike a balance between adapt-
together parameterized behavior-tree-based skills to achieve
user-specified goals. The parameterized behavior trees provide
a mechanism for personalization: user requests, formulated in
natural language, are translated with a large language model
(LLM) into structured updates to the behavior trees, which
can then be statically validated for safety. This structured skill
representation can also be analyzed by an LLM in response to
user transparency requests. Finally, using the code synthesis
capabilities of LLMs, we enable users to create their own
custom head gestures that can be added to the behavior trees
and used to interact with the robot. Users engage with the
robot through a flexible web-based user interface.
We develop FEAST using community-based participatory
research  in collaboration with two CRs (Figure 1). This
one or two CRs, is well-established in assistive technol-
ogy research [2931] and increasingly common in assistive
robotics [1, 3236]. Our collaborations began with introduc-
tory video calls in November 2022 and have continued through
regular meetingsboth virtual and in-person at the CRs
homes. These sessions have been instrumental in gathering
feedback on system design, brainstorming studies to identify
personalization needs (Section III), and piloting developments.
This iterative process led to a five-day in-home evaluation
in January 2025 (Section VI), where CRs fed themselves
six meals across three distinct contexts: personal, watching
fully completed meals with few researcher interventions and
personalized the system on the fly to their preferences and
abilities while reporting low cognitive workload, as indicated
by NASA-TLX surveys . CRs also rated FEAST highly for
real-world applicability, as reflected in Technology Acceptance
Model  survey results. Furthermore, both CRs reported
that FEAST provided greater control over their meals and
a stronger sense of independence compared to their human
easier with our system. Finally, to further assess ecological
unfamiliar with the system, who confirms its merits over a
no-personalization baseline (Section VII).
with community-based participatory research, tackling
in-the-wild scenarios by integrating diverse skills, custom
A user study involving 21 care recipients, which identifies
diverse personalization needs for mealtime assistance.
A personalization framework built on three key tenets:
(i) adaptability via LLM-based code synthesis, (ii) trans-
TABLE I: Comparison between FEAST and other mealtime assistance systems. Adaptability is assessed based on ability to handle open-ended
user requests, safety by self-reported adherence to ISO 13482 , and transparency by Levels 1-5 of the IEEE Transparency Standard .
ADAPTABILITY
TRANSPARENCY
INTERFACE
AUTONOMOUS BITE ACQUISITION
AUTONOMOUS BITE TRANSFER
OCCLUSION
WORKSPACE
REACHABILITY
PRE-ACQ.
Neater Eater
Park et al.
Bhattacharjee et al.
Feel the Bite
Nanavati et al.
FEAST (ours)
for commercial systems,
for academic systems
parency (Levels 1-5) through LLM summarization, and
(iii) safety aligned with ISOTS 13482 safety principles.
A five-day in-the-wild system evaluation with two
community researchers, spanning six realistic meals
across three distinct environmental contexts, showcasing
FEASTs real-world applicability.
An evaluation with an Occupational Therapist un-
familiar with our system, assessing ecological validity
and demonstrating improved performance over a non-
personalized baseline.
II. RELATED WORK
A. Mealtime Assistance
Research on mealtime-assistance systems dates back to the
Lifter . Over the years, several commercial systems have
been proposed, such as Winsford Feeder, My Spoon, Neater
reliance on manually programmed, fixed food acquisition and
transfer actions has limited user adoption, and as of now,
only Obi and Neater Eater remain commercially available.
To address these limitations, recent research focuses on using
and plan and execute autonomous motions.
Bite Acquisition and Transfer. Recent works have ex-
plored autonomous strategies for skewering solid bite-sized
foods [1215, 45], scooping soft food items [1618], twirling
and grouping noodle-like dishes , and cutting . Build-
ing on these works, FLAIR  introduces a bite acquisition
framework consisting of a library of vision-parameterized
food manipulation skills that use a fork-based utensil. This
framework leverages the commonsense reasoning and few-shot
learning capabilities of foundation models to appropriately
sequence these skills, enabling the feeding of complete meals
while adhering to bite ordering preferences of the user. Various
works also propose autonomous bite transfer methods [11, 23
Outside-mouth bite transfer methods [23, 24, 47] bring food
close to the care recipients mouth, requiring them to lean
forward to take a bite. For individuals with severe mobility
limitations who cannot lean forward, inside-mouth bite transfer
methods [11, 25, 26] place food directly inside their mouth.
Our system builds on FLAIR  for bite acquisition
and Gallenberger et al.  and Feel the Bite  for bite
mobility limitations. However, recent studies show that feeding
robots often exhibit obtrusive motion [1, 24], limiting their
use in social settings, and have restricted workspace ,
constraining plate and user placement. These limitations hinder
non-obtrusive motion and allow flexible user and plate posi-
tioning. Towards addressing these challenges, and based on
feedback from community researchers, FEAST adapts these
approaches to a novel feeding utensil (Section IV-A) that
improves the robots workspace and decreases obstruction.
Other Essential Mealtime Tasks. Several works have ex-
plored assistance with drinking [48, 49], focusing on grasping
a cup and having users drink from the rim. We instead focus on
drinking from a cup with a straw, aligning with our community
researchers routines. Unlike prior methods, which assume
specific cup colors  or pre-grasped cups , FEAST
emphasizes real-world flexibility, using an adaptable handle
that fits various cups without strict shape or color constraints.
Some works also propose methods for mouth wiping [50, 51].
Similar to these, FEAST can pick up a custom wiping tool
and position it near the users mouth, ready for wiping.
Mealtime-Assistance Systems. Most relevant to our work
are autonomous systems that demonstrate integration of vari-
ous components for feeding a complete meal [20, 23, 25, 36]
(see Table I). Unlike Park et al. , which focuses on feeding
which limit bite acquisition to skewering, our system employs
a range of skillsskewering, scooping, twirling, grouping, and
moreto pick up diverse food items and sequence them over
a long horizon for feeding an entire meal. Our system differs
from FLAIR  (which integrates with Feel the Bite )
by adapting it to a novel feeding utensil and a web interface
for user interaction. More significantly, our system is the
first to integrate feeding with other essential mealtime tasks,
including drinking and mouth wiping, while also automating
user personalization. No prior system has explored feeding
in diverse in-the-wild scenariosexcept Nanavati et al. ,
which offers customization of limited system parameters.
B. Personalization in Assistive Robotics
Various formative studies in assistive robotics emphasize
the importance of user personalization based on factors such
as mobility limitations, behavior, and context [1, 5254].
One approach to achieving personalization is through im-
plicit adaptation, where limited test-time examples guide the
Fig. 2: We use speculative videos recorded with community researchers (left) to conduct a formative study with 19 care recipients and the
2 community researchers (demographics shown on the right), to identify user personalization requirements.
system to a specific preference [5559]. FEAST aligns with
this by generating personalized gesture detectors using a
small number of examples from the user. Unlike prior black-
box approaches to gesture personalization [60, 61], FEAST
takes a more transparent approach by composing functions
from our perception library to generate customized detectors.
While some studies evaluate gesture detection for care recip-
ients [62, 63], FEAST is the first to consider personalized
gesture detection for those with mobility limitations.
Another approach involves explicitly identifying preferences
through direct user input. Canal et al.  use post-hoc scoring
for task planning, but this limits applicability in safety-critical
tasks. Nanavati et al.  provide predefined customizations
for a mealtime-assistance robot through a web interface,
while FLAIR  uniquely explores bite-ordering preferences
using language commands. FEAST leverages FLAIR for bite-
ordering preferences and significantly extends personalization
to other system components by enabling open-ended cus-
tomization through language inputs.
C. Large Language Models for Human-Robot Interaction
Recent advancements in large language models (LLMs) [65,
66] enable the use of language as an interface for human-
robot interaction. Some approaches utilize LLMs for high-
level planning, allowing systems to generate plans over a fixed
library of skills based on user language commands [6773].
Others focus on generating low-level skills by training on
large vision-language-action datasets [7477], synthesizing re-
wards [78, 79], or inferring skills via code generation [80, 81].
FEAST represents a recent category of approaches that enable
users to edit both high-level plans and parameters of low-level
skills using LLMs [8284]. However, given the physical nature
of human-robot interactions in mealtime assistance, FEAST
restricts edits to carefully selected operations to ensure safety.
III. IDENTIFYING PERSONALIZATION NEEDS FROM CARE
RECIPIENTS WITH DIVERSE MEDICAL CONDITIONS
To guide the development of our flexible mealtime-
assistance system, we conducted a formative user study with
21 care recipients (including 2 CRs) with diverse mobility
limitations. Our objectives in this study were two-fold: (1) to
collect specific personalization requests; and (2) to identify key
tenets of personalization in the context of mealtime assistance.
Participants. We recruited 19 care recipients with diverse
medical conditions such as Cerebral Palsy, Multiple Sclerosis,
Muscular Dystrophy, Spinal Cord Injury, etc. (Figure
There were 8 female and 11 male participants who ranged in
age from 20 to 62 years old and who were located throughout
the United States. The participants were also diverse in terms
of their prior experience with mealtime-assistance systems:
some had no experience; others had participated in academic
studies; and some had extensive experience (e.g., P6 noted I
had a Winsford Self-Feeder for over 20 years until the second
one I had stopped working 4 months ago).
Design Materials. Previous work  has shown that
caregiving scenarios vary depending upon (i) care recipient
ment. Working with our community researchers, we identified
realistic mealtime assistance scenarios where each of these
three components vary. We then recorded speculative videos
of the scenarios using our robot mealtime-assistance system
(see Section IV for system details). The videos were organized
into a slideshow and supplemented with conversation prompts
(Figure 2). For example, one video showed the robot success-
fully feeding a care recipient with outside-mouth bite transfer;
the next video showed the robot failing to feed a different care
recipient who has very limited neck mobility; a final video in
the sequence showed the robot successfully feeding the latter
care recipient with inside-mouth bite transfer. After these three
Fig. 3: We extract and categorize personalization requests from our formative study with 21 care recipients. See Appendix A for details.
own functional abilities pose for the robot? Other speculative
videos preceded discussion questions including: What unique
needs might your own behavioral preferences pose for the
robot? and In what scenarios do you typically eat? What
unique needs might these scenarios pose for the robot?
Study Design. To broaden participation in the study, we
conducted conversations virtually. Caregivers were also invited
to join and actively contribute to the discussion. Each session
consisted of the following steps: (1) introductions and agenda;
(2) a brief review of recent progress in robot mealtime-
assistance systems and an extension of gratitude to the study
participants for facilitating this progress; (3) speculative videos
and discussion questions; (4) final open-ended discussion and
conclusion. Care recipients were compensated 30 USD for
their participation in the 90-minute study.
Outcomes. This formative study successfully accomplished
its two objectives. First, we collected specific personalization
visualize in Figure 3. We detail each of these requests in
Appendix A. These diverse requests highlight the need for
personalization in a mealtime-assistance system and suggest
where we should focus our efforts to meet the needs of users.
Our second objective was to identify key tenets for person-
alization. From our conversations, we identified three themes:
1) Adaptability: The first clear tenet of personalization is
user requests. Study participants highlighted the need for
adaptability beyond one-time system setup. For exam-
depending on the feeding scenariowhen watching TV,
controlling the robot with a button may be preferable
to verbal commands; or when dining socially, the robot
should retract to a resting position immediately after bite
transfer. Study participants also described how their pre-
ferred interaction with the robot may change throughout
the day, depending on their energy level, and over time,
as their medical conditions change.
2) Transparency: Study participants also emphasized the
importance of being able to understand the behavior
of the robot, especially if that behavior may change.
For example, describing their ideal relationship with the
would like to understand the robot capabilities and the
robot would understand my requirements.
3) Safety: A final key tenet of personalization that emerged
from the formative study was safety. For example, P12
to look somewhere the robot should not try to feed me at
that position which might be unsafe.
These specific examples and key tenets informed the design
of our mealtime-assistance system, which we describe next.
IV. FEAST: A FLEXIBLE MEALTIME-ASSISTANCE
SYSTEM TOWARDS IN-THE-WILD PERSONALIZATION
In this section, we present FEAST, a mealtime-assistance
system that enables users to personalize to in-the-wild
eating scenarios commonly encountered in real-world set-
tings. All hardware and software components of FEAST are
open-sourced on our website. In the following subsections,
we describe our system hardware (Section IV-A), software
(Section IV-B), and user interface (Section IV-C), explaining
how each component can be personalized while adhering to
the tenets of adaptability, transparency, and safety.
A. System Hardware
FEAST (see Figure 4) uses a Kinova Gen3 7-DoF robot
arm  and a Robotiq 2F-85 gripper . It can be flexibly
mounted either on the users ROVI wheelchair , pow-
ered by the wheelchairs battery, or on a movable Vention
Tool-Change Apparatus. FEAST employs three custom
tools. First, a novel feeding utensil with integrated motors
provides wrist-like degrees of freedom, enabling tasks such
as twirling, scooping, and maintaining an upright orientation
when holding food. The utensils fork, made of compliant
sensor . This default fork is detachable, allowing users
to exchange it with a metal fork if desired. To power and
control the utensil without dangling wires, the robots gripper
fingertips are replaced with custom fingertips featuring mag-
netic electrical connections that engage with complementary
connectors on the utensil when grasped. Second, for drinking,
FEAST uses an adaptable mug handle inspired by adaptable
mug holders , which accommodates various cup shapes
without strict dimensional constraints and features an ArUco
marker  for autonomous grasping. Third, for mouth wip-
gentle cleaning. Each tool is mounted and dismounted by the
robot opening and closing its finger tips (see Figure 4).
Novel Utensil Orientation and Camera Mount. Previous
state-of-the-art feeding systems use forward-facing in-hand
cameras and similarly oriented utensils [11, 20, 23, 36].
large movements to transition between acquisition and transfer,
and can obstruct the users view during feeding. Based on
feedback from CRs, FEAST introduces a simple and effective
natural human wrist movements during eating. This change,
Intel RealSense Camera , oriented perpendicularly away.
Compute and Networking. FEAST employs custom con-
approximately 1 kHz. This necessitates a dedicated real-time
control system, for which we use an Intel NUC . The
primary computing platform is a Lenovo Legion Pro 7i laptop
equipped with a 16GB RTX 4090 GPU . Communication
between the main compute unit, the NUC, the robot, and the
web interface is managed via a Nighthawk RAX43 router .
When FEAST is mounted on a wheelchair, these components
can also be securely mounted to ensure they move along.
Accessible Buttons. FEAST utilizes three accessible but-
tons . Two are intended for the user: one customizable
through our personalization pipeline for interactions, and an-
other serving as an emergency stop. The third button functions
as an experimenter emergency stop.
Status LED. FEAST features a status LED within the
camera mount to alert users when their attention is needed,
based on feedback from CRs. This is especially useful for
users multitasking, such as watching TV while eating.
B. System Software
FEAST sequences together skills to accomplish goals given
by the user. For example, if the user requests a drink while
the robot is holding the utensil, the robot would invoke skills
to (1) dismount the utensil; (2) mount the drink; and (3)
transfer the drink to the user. We next describe how skills
are generally implemented, adapted, and sequenced together,
before detailing the specific skills used in this work.
Skills as Parameterized Behavior Trees. We imple-
ment skills as behavior trees . As an extension of
standard behavior trees, we expose node parameters that
can be adapted in response to user requests (see below).
For example, the behavior tree for bite acquisition includes
three parameters: Speed, TimeToWaitBeforeAutocontinue,
and AskUserForConfirmation. Every parameter is associated
with a domain of possible values. For example:
Speed {low, medium, high}
TimeToWaitBeforeAutocontinue [5, 100]
AskUserForConfirmation {true, false}.
All nodes and parameters are given human-readable names
and descriptions to facilitate LLM-based adaptations.
Personalizing Skills from Natural Language. FEAST
users can make personalization requests through spoken or
typed natural language. Our pipeline for processing these
requests is as follows:
1) The natural language request is converted into structured
behavior tree updates using an LLM. (adaptability)
2) Each potential update is checked for safety. If the up-
dates are deemed safe, the behavior trees are updated.
3) The updates and outcomes are briefly summarized with
an LLM and reported back to the user. (transparency)
We now describe these steps in more detail.
1) Language Structured Updates. We use an LLM (GPT-
4o ) to translate natural language personalization requests
into structured behavior tree updates. The LLM is prompted
with a brief explanation of the scenario, the personalization
response.1 The output of the LLM is statically checked to
ensure that the structured requests are in the expected form and
the names of behavior tree nodes and parameters are valid. If
any of these checks fail, the LLM is automatically re-prompted
(up to 3 times in experiments) with failure information. It is
important to note that one personalization request can elicit
multiple updates. For example, if the user requests for the
robot to always move as fast as possible, multiple behavior
trees with speed parameters would be updated.
2) Safety Checks. If the structured behavior tree requests
pass static checks, we pass them through another round
of safety checks. We consider two types of updates: node
additions and parameter changes. In this work, to guarantee
wait for gesture, and retract. We further restrict retract
node additions to ensure that the robot only retracts from a
limited set of configurations that have been empirically tested.
For parameter changes, we ensure that the proposed values lie
within the parameter domains. These domains are carefully
1We also found that prompting the LLM to rephrase the original request
into a specific setting that should be changed in the robots software helped
with certain requests such as The robot is too slow right now.
RealSense
Notification
Wireless
Magnetic Power
Connection
Utensil Motor
Control Module
(Encased)
Pitch Joint
Roll Joint
Interchangeable
Force-Torque
Robotiq 2F-85
Kinova Gen3
Utensil Design
Wiper Pickup
Utensil Pickup
Drink Pickup
Web Interface
Outside-Mouth
Bite Transfer
Inside-Mouth
Bite Transfer
Fig. 4: FEAST features diverse mealtime-assistance skills such as feeding, drinking and mouth wiping, custom tools, and a flexible web
chosen to ensure that safety is guaranteed, while also giving
the user enough flexibility to meet their personalization needs.
3) Update Results Language. Given the history of
structured behavior tree updates and the check outcomes, we
again use an LLM to generate a brief summary. The LLM is
prompted with a brief explanation of the context, the original
user request, the structured updates, and the outcomes, and
asked to generate a short summary that a non-technical end
user would understand. This summary is displayed on the user
interface (Section IV-C).
This processconverting natural language requests into
structured behavior tree updates that can be checked for
safety and summarized back to the useraddresses our three
key tenets of personalization. By exposing parameters and
allowing for certain node additions, the system is adaptable;
in summarizing the process, the system is transparent; and by
restricting and checking the adaptations, the system is safe.
Synthesizing New Gestures with LLMs. As an additional
step towards open-ended personalization, we allow the user
to synthesize new gestures that can then be integrated into
the behavior treefor example, if the user wishes to use a
custom head shaking motion to indicate that they are ready for
bite transfer. Through the interface (Section IV-C), the user is
guided through the process of collecting a few positive and
negative examples of their new gesture. The user also gives
the gesture a name (e.g., head shake) and a brief description
(e.g., shaking my head from right to left). FEAST then
uses LLM-based program synthesis to generate a program-
matic gesture detector that takes a head tracking module as
input. Our program synthesis pipeline is also able to propose
and optimize hyperparameters, e.g., NumberHeadShakes or
NoiseTolerance. See Appendix B for details.
Sequencing Skills with PDDL. Towards generating a
sequence of skills to achieve a users goal, we associate each
skill with a PDDL operator . For example:
(:action PickTool
:parameters (?tool - tool)
:precondition (and (GripperFree) (Reachable ?tool))
:effect (and (Holding ?tool) (not (GripperFree))
(not (Reachable ?tool)))
where ?tool can be grounded with utensil, mug, or wiper.
See McDermott et al.  for a formal introduction to
PDDL. We use an optimal task planner (FastDownward
with alias seq-opt-lmcut) to sequence the skills together
given a known initial propositional state and a proposi-
tional goal derived from the users input. For example, when
the user requests a bite, FastDownward generates a three-
PickTool(utensil),
AcquireBite(utensil),
TransferTool(utensil). The plan is executed open-loop and
the propositional state is updated using the operator effects.
If the user subsequently requests a drink after requesting
a bite, the second plan would be: PlaceTool(utensil),
PickTool(mug), TransferTool(mug).
Skills Included in FEAST. We now detail the specific skills
that are currently implemented in FEAST.
utensil and wiper are placed in a fixed tool mount relative
to the robots base, enabling a predefined pickup motion.
For the mug, the robot detects the ArUco marker on the
adaptable handle and executes an appropriate grasp.
utensils or wipers, this is a predefined position on the tool
mount; for the mug, it is the original pickup position.
builds on the bite acquisition framework proposed by
work includes a vision-parameterized food manipulation
skill library with four pickup actionsskewering, scooping,
ups. We incorporate force thresholds via the force-torque
sensor to ensure safe operation. For each new bite, FEAST
processes the plate image using vision-language foundation
models to identify plate contents and predict the required
skill sequence for each food type. These predictions, along
with the users bite order preferences and bite history, are
provided to an LLM, which determines the next bite while
balancing preference adherence and efficiency. The robot
then executes the predicted skills to pickup the bite.
moves to a predefined distance outside the mouth. Upon user
using a task-space compliant controller to continuously track
the users head pose and bring the tool inside the mouth.
After detecting that the user has finished taking a bite, sip,
or completed mouth wiping, it returns to its initial position.
Both transfer methods utilize the head perception pipeline
proposed by Feel the Bite .
in-front of the users mouth. Useful for recording and testing
new gestures as we detail in the next section.
an empty gripper.
C. User Interface
FEAST features a web-based interface accessible on per-
sonal devices, such as tablets or phones, enabling integration
with assistive technologies that care recipients already use.
For example, CR2 uses a tracker that follows a reflective dot
on their nose  (Figure 4). We refine the interface design
through multiple iterations with CRs to address their needs;
they highlight that the interface should have big buttons that
are easy to click, and should support speech to text.
implement
interface
open-source front-end JavaScript framework, while ROS
Noetic  facilitates communication with the mealtime-
assistance skills. FEASTs web interface displays pages on de-
mand from the robot, automatically updating to reflect system
behavior as users personalize it to meet their evolving needs.
The interface supports system adaptability and transparency
via a personalization webpage, as we detail in Section IV-D.
Beyond fully autonomous workflows that can continuously
feed users without intervention, FEASTs interface allows
on-demand user control. Users can override the robots au-
tonomous predictions, for instance, by manually selecting
the next bite or pinpointing key points on a displayed plate
image as parameters to bite acquisition skills. Such overrides
help users recover from autonomous errors, increasing the
robustness of the system in in-the-wild scenarios [104, 105].
D. Default Mealtime-Assistance Procedure
Mealtime Assistance. At the start of a new meal, the
robot begins in the retract configuration without holding
any tool. The user is presented with the New Meal Page
on the web interface, where they specify the food items on
the plate and their preferred bite order (e.g., Feed me all
of X first, then Y). The interface then transitions to the Task
Selection Page, allowing the user to choose from three primary
which sends TransferTool(utensil), TransferTool(mug),
and TransferTool(wipe) commands respectively. If the robot
is not holding the appropriate tool, the task planner directs it to
place the current tool and pick up the correct one. Moreover,
for a bite, the task planner invokes AcquireBite, which looks
at the plate contents and predicts the next bite in adherence
to the users ordering preferences. The interface subsequently
opens a Bite Acquisition Page, where the user can:
1) Switch to a different bite of the same or another food type.
2) Switch to a Manual Bite Acquisition Page to choose a
specific skill (e.g., skewering) and set keypoint parameters
(e.g., the exact skewering point on the plate image).
If the user clicks acquire bite or does not press any button
within 10 seconds, the robot proceeds with bite acquisition.
For TransferTool, the interface prompts the user to confirm
readiness for transfer. Once confirmed, the robot moves to a
ready-to-transfer configuration and announces, Please open
your mouth when ready. It waits for the user to open their
mouth and then, moves the fork, straw, or wiping tool tip near
the users mouth, saying, Ready for transfer. After detecting
a bite via the forcetorque sensor, or a completed sip or wipe
through a head nod, the robot retracts to the ready-to-transfer
position. It then returns to the Next Task Selection Page, with
a 10-second auto-continue countdown to repeat the last task if
it was a bite or sip. At any point during the meal, the user can
click a finish eating button that triggers the Retract skill.
Personalization. Before and during a meal, users can access
the Personalization Page, where they can send:
1) Adaptability Requests: Users can input commands to per-
sonalize components of the mealtime-assistance system to
meet their needs (details in Section V-A).
2) Transparency Requests: Users can ask about the robots
(details in Section V-B).
The page also allows users to transition to the Gestures Page,
which focuses on adding personalized gestures to the systems
library. To add a new gesture, users provide a language label
and description. The interface then triggers EmulateTransfer,
moving the robot to a position just outside the users mouth
without holding a utensil to record positive and negative
gesture examples. Once the recordings are complete, the robot
returns to the ready-to-transfer configuration, and the interface
displays the accuracy of the newly trained gesture detector.
Users can also test gesture detectors by having the robot
move to the position just outside their mouth (without holding
a utensil) while the interface displays whether the detector
ISO 13842: Safety Requirements for Personal Care Robots
IEEE Standard for Transparency of Autonomous Systems
Adaptability Requirements from Formative Study
Fig. 5: FEAST proposes a personalization framework for mealtime assistance built on three key tenets: (i) adaptability towards tackling the
various request types identified through our formative study, (ii) transparency towards Levels 1-5 on the IEEE Transparency Standard, and
(iii) safety aligned with ISOTS 13482 safety principles.
is triggered by their movements. When switching between
these personalization features enhance adaptability and align
with the IEEE Transparency framework (Section V-B).
V. FEAST ACHIEVES THE THREE TENETS OF
PERSONALIZATION
Our formative study affirmed the importance of adaptability,
is personalized in-the-wild. In this section, we detail how the
system design of FEAST addresses these three tenets.
A. Adaptability
In our formative user study, care recipients voiced 46
types of adaptability requests across 7 categories (Figure 3).
Requests of the same type often differed from user to user.
For example, P1 said: I would want the robot to be fast, as
fast as it could go maybe up until those very last moments of
moving towards my head, while P9 said, I have a trach and
have to eat slowly. P10 said: It would be useful to give the
robot more autonomy during those social situations, to pick the
bite or whatever, rather than me specifying something exactly,
and P5 said, This might be just a personal thing, but the idea
of hitting a switch and then waiting for something to happen
makes me then feel like Im no longer in control. Users also
voiced the importance of adapting over time: for example, P8
might change over time. So you might want to keep updating
the robot with the progression of the disease.
In Figure 5, we compare FEAST to two state-of-the-art
feeding systems, Nanavati et al.  and FLAIR , based
on their ability to address the 46 adaptability request types
identified in our study. FEAST covers 36, compared to 17
for Nanavati et al. and 15 for FLAIR. Appendix A provides
detailed justifications for each system. Appendix B presents an
ablation study demonstrating that our LLM-based personalized
gesture synthesis outperforms a no-personalization baseline.
B. Transparency
The IEEE Standard for Transparency of Autonomous Sys-
tems [40, 106], which explicitly includes care robots within its
from 1 to 5 (Figure 5). FEAST aims to meet all these levels.
Level 1 specifies that users must have access to information
such as example scenarios and general principles of operation.
FEAST satisfies this requirement with an instruction manual
provided to new users. Level 2 requires interactive training
the system in virtual scenarios. To address this, we provide
demonstration videos of the system across various use cases.
them directly on the real robot in isolation.
Levels 3 and 4 focus on providing user-initiated explana-
tions. Level 3 requires explanations of the systems most recent
about system behavior. FEAST uses LLM summarization to
fulfill these requirements. We define the following to encap-
sulate the current system state:
1) Current Behavior: Current behavior tree encodings for
mealtime-assistance skills, with text descriptions for each
node and its parameters.
2) Node Execution History: A real-time log of behavior tree
nodes in-execution and completed.
3) Perception Log: A real-time log of perceived data, such as
plate contents, drink pose, user head pose, and gestures.
4) Safety Log: A real-time log of safety checks, including any
invalidated specifications.
On the Personalization Page of the web interface, users can
submit language queries. The LLM processes these queries
along with the current system state, detailed descriptions of
system operations (as outlined in Section IV), and prior user
queries and the corresponding LLMs responsesto generate
Level 5 transparency requires the system to provide con-
tinuous explanations of its behavior. FEAST aims to achieve
this using LLM summarization. At fixed-time intervals, the
system checks for changes in the system state as defined above.
If a change is detected, the LLM is queried with detailed
descriptions of system operations, the previous state, and the
current state, prompting it to generate an explanation of what
changed during that period. This explanation is displayed on
the web interface whenever no other page is active.
C. Safety
FEAST fits within the scope of ISO 13842: Robots
and Robotic DevicesSafety Requirements for Personal Care
the following hardware and software checks (Figure 5):
Pose filtering. Our pose detection methods (food, drink, and
head pose) use predefined zones based on expected plate,
mealtime assistance scenario. Head pose limits are tailored to
the users range of motion . If detections occur outside
these zonessuch as mistakenly identifying a caregivers head
pose behind the user during feeding (a scenario that can arise
in real-world settings)the robot transitions to a safety state.
Active collision monitoring. While our robot verifies its
motion plans for collisions with known obstacles, it also uses
active collision monitoring to handle unexpected collisions
when not using compliant control. This is achieved by com-
paring torque feedback from the joint FT sensors with torque
predictions from a nominal model-based control law .
Compliant control. For inside-mouth transfer, the most phys-
ical human-robot interaction intensive aspect of mealtime as-
pliant controller that leverages Damped Least Squares .
Safety-centric hardware design. Within the feeding utensil,
the fork is attached to a custom-designed holder with a
mechanical weak point that yields under excessive force,
breaking the utensil into two pieces. This safety feature ensures
that if excessive force is applied while the utensil is in the
users mouth, only the fork tip remains, halting all physical
interaction with the robot. For safety, we restrict outside-
mouth bite transfers to the default silicone fork. Similarly, the
drinking utensil uses a silicone straw, and the mouth wiper
features a soft tip.
Emergency Stops. At all times, both the user and an experi-
menter have access to physical emergency stop buttons, which
immediately transition the robot into a safety state.
Watchdog. A watchdog continuously monitors the function-
ality of all robot sensors and emergency stops, transitioning
the robot to a safety state if an issue is detected.
We provide implementation details for the above and further
discuss alignment with ISO 13842 guidelines in Appendix C.
VI. FIVE-DAY IN-THE-WILD EVALUATION
To showcase the real-world applicability of FEAST, we
evaluate our system with two care recipients (CR1 and CR2)
in a five-day in-home study spanning six realistic meals across
three distinct environmental contexts:
1) Personal context where they focus solely on eating.
2) Watching television while they are eating.
3) Social context where they eat with another researcher.
This study asks the question: can end-users effectively
personalize FEAST to meet their requirements across
various mealtime scenarios?
A. Study Procedure
Introductory Meals and System Familiarization. Before
any evaluation meals, we conduct one introductory meal with
each care recipient on Day 1 of the study to familiarize them
with the system. During these training meals, we explain the
components of the mealtime-assistance system, demonstrate its
various personalization features, and answer their questions.
Evaluation Meals. Over the course of the next 4 days, we
feed 3 meals each to the two CRs, in their own homes, in the
following chronological order:
Meal ID 1 (on Day 2): CR1 Dinner in Personal Context
buffalo chicken bites, potato wedges, and ranch dressing
(home cooked meal), served with water.
Meal ID 2 (on Day 2): CR1 After-dinner dessert while
Watching TV - strawberries with whipped cream (store-
bought meal), served with water.
Meal ID 3 (on Day 3): CR2 Dinner in Personal Context
- chicken nuggets, apple slices, and ketchup purchased
from McDonalds, served with water.
Meal ID 4 (on Day 4): CR2 Lunch in Social Context -
protein shake (CR2s usual lunch). Mug is cleaned and
refilled with water after the shake is finished upon request.
Fig. 6: Meals fed to care recipients. The images, captured by the robots in-hand camera, highlight the variability in lighting conditions.
Each meal also included water. Meal ID 4 consisted only of a protein shake, which CR2 usually has for lunch.
Meal ID 5 (on Day 5): CR2 Dinner while Watching TV -
General Tsos Chicken and Broccoli (home cooked meal),
served with water.
Meal ID 6 (on Day 5): CR1 Dinner in Social Context
- chicken breast strips and hash browns (home cooked
meal), served with water.
We select the meals for the study based on the care recipi-
ents eating habits, the robots capabilities, and the need for
variation. While we validate our system with similar meal
types during study preparation, the exact meals naturally vary
based on how they are prepared and what is available. Many
adjustments were made on the fly; for example, hash browns
from CR1s fridge were added to Meal ID 6 at the last minute.
Environmental and User-Specific Variability. Care recip-
ients live in different homes and have unique preferences for
their environment setup. This leads to variations both across
and within the same context (see Figure 1), including:
mug for the last meal. CR2 uses a silicone fork (safer for
inside-mouth transfer) and a black mug for all meals.
Web Interface Interaction: Both use iPads. CR1, with
limited limb mobility, interacts using their left arm, while
tracker that follows a reflective dot on their nose.
Eating Tables: Different table types include multiple
overbed tables, a coffee table, and a dining table.
Seating Arrangements: While watching TV, both CR1
and CR2 face the screen. In a social setting, CR1s social
partner sits to their right, whereas CR2s social partner
sits directly in front.
Meal Setup and Scene Configuration. For each new scene,
experimenters update scene parameters used for safety checks,
such as table height (ensuring a lower limit for acquisition
actions) and the users head pose (used for filtering head
perception). They also adjust the fixed configurations that the
robot moves to before detecting the plate, drink, and users
head to ensure their visibility. After the CRs personalize the
robot for their personal contexts in their first evaluation meals,
we initialize their other contexts with those settings.
B. Evaluation Metrics
Before the study, participants rate their experiences with
human caregivers during mealtime on a 5-point Likert scale,
which is later compared to FEAST. To evaluate system per-
categorized into hardware, web interface, skills, and personal-
ization. We also track experimenter explanations, recording the
number of questions participants ask. Skill success rates are
measured for autonomous actions, which rely on perception,
such as bite acquisition, bite transfer, drink pickup, drink
utensil and mouth wipe pickup from the tool mount and
subsequent stow, always succeed. After each meal, participants
complete a NASA-TLX  survey (7-point Likert scale) to
assess cognitive workload. At the end of the study, a Technol-
ogy Acceptance Model (TAM)  survey evaluates overall
satisfaction and usability, along with additional questions
assessing key personalization tenets: adaptability, transparency,
and safety. Full questionnaires are in Appendix D.
C. Results
For each meal, CR1 and CR2 reported either fully clearing
their plate or eating until they were full. Table II summarizes
objective system performance per meal, whereas Figure 7
shows subjective metrics. Details on each of the interventions,
study can be found in Appendix E.
Interventions. A total of 18 experimenter interventions
occurred across six meals (3  2). Four hardware interventions
in the first meal were due to a damaged ArUco marker on
the mug handle, which was later replaced. All but one web
interface intervention resulted from a recurring network issue
that intermittently blocked commands, requiring a refresh.
Explanations. Users asked experimenters 20 questions
across six meals (3.33  2.69). Many could have been di-
rected to the robot, but the transparency page was only
accessible from the task selection page, underscoring the need
for transparency throughout the eating process.
Skill Success Rates. FEAST demonstrates robustness to
different food items, with a bite acquisition success rate
of 89.27  9.24 and a bite transfer success rate of
93.07  7.70 . Prior work suggests that an 80 acquisition
rate is sufficient for use . While drink acquisition success
was low for Meal ID 1 due to a damaged marker on the
drink handle during transit, it improved significantly after
TABLE II: Per-meal system performance, including experimenter interventions, explanations, and skill success rates.
INTERVENTIONS
EXPLANATIONS
SKILL SUCCESS RATE
HARDWARE
WEB INTERFACE
PERSONALIZATION
BITE ACQ.
BITE TRANSFER
DRINK ACQ.
DRINK TRANSFER
WIPE TRANSFER
Fig. 7: Left: Per-meal NASA-TLX survey results, with CR1 and CR2 meals shown in chronological order. Center: TAM results, including
questions on personalization tenets. Right: Comparison between human caregivers and FEAST.
5. Drink and wipe transfers were also highly successful at
94.44  13.61 and 100.0  0.0 respectively.
Cognitive Workload Per Meal. The average workload
imposed by FEAST was relatively low. On a 0100 NASA-
TLX scale (where lower scores indicate lower workload), CR1
had a mean of 22.22  10.02, while CR2 had a mean of
7.41  5.79, compared to a baseline of 37 [1, 110]. Both
care recipients rated their success in achieving mealtime goals
very positively: the inverted Performance metric (0100, lower
better) is 11.11  9.62 for CR1 and 5.56  9.62 for CR2.
Comparison with Human Caregiver. Both CRs rated
FEAST as providing greater control over their meals and a
greater sense of independence compared to their professional
caregivers. CR2 also reported less effort in conveying prefer-
ences to the robot than to their human caregiver.
TAMs and Personalization Tenets. Results from the TAM
survey indicate that both CRs rated FEAST highly across all
Ease of Use, Attitude, Intention, and Enjoyment. They also
rated the system highly for adaptability and safety but noted
that transparency could be improved. This may be because the
transparency page was only accessible from the task selection
page on the web interface, which was not always available,
requiring them to ask experimenters for explanations.
D. Lessons Learned
Lesson 1
Significant variability exists across in-home eating scenar-
adapt the system to these variations.
Across different meals, both CR1 and CR2 made a range
of personalization requests. These included adjusting robot
speed (Feed me as fast as you can, Meal ID 1), mod-
ifying skill parameters (Dip the strawberry deeper into
the whipped cream, Meal ID 2), changing web interface
workflows (Dont show continue pages on the web interface,
Meal ID 3), and customizing interactions between the robot
and user, such as muting the robot (Be quiet and do not talk
at all, Meal ID 5) or changing how transfers are confirmed
(Use the button to complete a transfer only when taking a
Even under similar contexts, the CRs had different person-
alization needs. For example, CR2 added a continuous mouth
open personalized gesture detector for initiating transfers in a
social setting, to differentiate from when they are talking. On
the other hand, CR1 realized the default mouth-open detector
required opening their mouth very wide. This design allowed
CR1 to talk without inadvertently triggering the robot, and
then open their mouth wide when ready to receive a bite.
The spatial arrangement of people and objects also influ-
enced personalization preferences. In CR2s social context,
the social partner sat across the table, so CR2 rarely felt
obstructed by the robot: I completely forgot about the robot
sometimes and had to remember that I had to take a drink.
seated to the rightdirectly across from the robotprompting
CR1 to request the robot retract after every bite (Move to
retract position after every bite) to avoid interference.
Lesson 2
Transparency helps users iteratively refine the system to
meet their preferences, even when adaptability commands
are not always effective.
FEASTs adaptability leverages an LLM to process lan-
guage commands and update the robots behavior. However,
LLMs can sometimes hallucinate or make mistakes. For ex-
with the command, Use button when completing a transfer
when taking a sip, the LLM mistakenly applied this change
to all tools, responding, The robot-assisted feeding system
has been updated to use a button to complete actions when
transferring drinks, utensils, and wipes. Despite the error, the
systems transparency features allowed the user to recover.
CR1 utilized transparency to verify the transfer completion
method by asking, What is the default action to complete a
transfer? and What other ways can I end a transfer besides
pushing the button? After several transparency-adaptability
sensors to end transactions with utensil use, the button to end
transactions when taking a sip, and the button to end transac-
tions for face wipes. Through these transparent interactions,
CR1 was able to iteratively refine the robots behavior to match
their preferences, even when sometimes the initial adaptability
command did not function as intended.
Lesson 3
Providing multiple interfaces is essential for transparency,
as users may not always be able to interact with a single
interface due to situational and environmental constraints.
Over multiple days of evaluating a flexible system, users of-
ten forgot the robots settings and expectations (which gesture
it is waiting to detect). FEAST provides transparency through
voice prompts and a web interface, with multiple options
proving crucial. In social settings, CR1 and CR2 switched off
voice prompts and relied on the web interface for guidance.
CR2 used a personalized gesture detector for keep head still
during mouth wipingthe wipers position right at their mouth
prevented them from checking the interface. As CR2 noted, A
small display on the gripper would help. I cant check the web
interface while the fork or another tool is at my mouth and
the robot expects something. Or maybe I could ask directly,
like Hey robot, what am I supposed to do right now?
Lesson 4
Cognitive workload generally decreases as users become
more familiar with the system, but it also depends on the
context and specific settings they choose.
A flexible system allows users to tinker and personalize
their experience, but it can also impose cognitive workload.
While NASA-TLX surveys indicated that our system did
not impose significant cognitive workload during evaluation
acclimate. Over three meals, cognitive workload generally
trended downward (Figure 7) as users became more familiar
with the system, except for Meal ID 6 (a social meal for
CR1). CR1 noted that the mental and
