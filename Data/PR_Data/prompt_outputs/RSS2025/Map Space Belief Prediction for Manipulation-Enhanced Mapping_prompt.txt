=== PDF文件: Map Space Belief Prediction for Manipulation-Enhanced Mapping.pdf ===
=== 时间: 2025-07-21 15:14:19.474305 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Map Space Belief Prediction for
Manipulation-Enhanced Mapping
Joao Marcos Correia Marques1
Nils Dengler2,3,4
Tobias Zaenker2,4
Jesper Mucke2
Shenlong Wang1
Maren Bennewitz2,3,4
Kris Hauser1
These authors contributed equally to this work
1. University of Illinois at Urbana-Champaign, IL, USA
2. Humanoid Robots Lab, University of Bonn, Germany
3. The Lamarr Institute, Bonn, Germany
4. The Center for Robotics, University of Bonn, Germany
Robots Belief
Robots Belief
Robots Belief
Scene after manipulation
New instances discovered
uncertainty reduced
New instance uncovered
uncertainty reduced
Time Step
Fig. 1: Example scenario with occlusions in a confined shelf environment. Given a current partial map of the environment (belief t), our planner decides
whether gathering another observation or manipulating the scene would be best to reduce map uncertainty. In this example, first a observation action would
increase environmental knowledge, followed by a push to unveil the hidden can behind the two boxes at time t  2. The predicted belief map is visualized
as a top-down projection of the shelf, ignoring the occluding top shelf board.
AbstractSearching for objects in cluttered environments
requires selecting efficient viewpoints and manipulation actions
to remove occlusions and reduce uncertainty in object locations,
of manipulation-enhanced semantic mapping, where a robot has
to efficiently identify all objects in a cluttered shelf. Although
Partially Observable Markov Decision Processes (POMDPs) are
standard for decision-making under uncertainty, representing
unstructured interactive worlds remains challenging in this
formalism. To tackle this, we define a POMDP whose belief is
summarized by a metric-semantic grid map and propose a novel
framework that uses neural networks to perform map-space be-
lief updates to reason efficiently and simultaneously about object
physics. Further, to enable accurate information gain analysis, the
learned belief updates should maintain calibrated estimates of un-
certainty. Therefore, we propose Calibrated Neural-Accelerated
Belief Updates (CNABUs) to learn a belief propagation model
that generalizes to novel scenarios and provides confidence-
calibrated predictions for unknown areas. Our experiments show
that our novel POMDP planner improves map completeness and
accuracy over existing methods in challenging simulations and
successfully transfers to real-world cluttered shelves in zero-shot
fashion.
I. INTRODUCTION
Active sensing has long been studied in robotics for tasks
such as exploring an unknown environment , complete 3D
object model acquisition , and searching for an unobserved
target object [26, 12, 47]. To build complete maps as efficiently
as possible, Next Best View (NBV) planning  is often
employed to reduce the uncertainty about the map as quickly
as possible. Although NBV planning offers an approach for
static scenes in which the robot simply moves the camera
passively through free space, there are many applications, such
as household and warehouse robotics, in which robots may
need to manipulate the environment in order to gain better
viewpoints [11, 33]. We refer to this problem as Manipulation-
Enhanced Mapping (MEM). MEM offers two significant new
challenges beyond standard NBV problems. First, in order to
decide when and where to manipulate objects, the robot should
reason about how object movement may affect previously
occluded regions. Second, it must anticipate the impact of
manipulations on observed objects and possibly partially-
observed or unobserved objects. For example, pushing boxes
in a grocery shelf backward will move all objects simultane-
ously until the furthest, occluded one, hits a wall.
MEM is related to the mechanical search problem
in which the robot manipulates clutter to reveal a target
object. Prior approaches in mechanical search tend to rely
on restrictive assumptions, such as a static viewpoint, which
ignores a robots ability to look around obstacles [18, 39].
Other studies assume full observability of object dynamics
and poses  or are limited to a fixed set of predefined
objects . These assumptions are too limiting for complex
cluttered scenes like shelves. The most closely-related work to
ours is Dengler et al. , who address these limitations by
training a reinforcement learning policy for viewpoint planning
and learn a push scoring network from human annotations to
derive manipulation actions, switching between manipulation
and manipulation when the information gain from obtaining
novel images of the environment saturates. However, their
approach to switching between action modes is inefficient,
since waiting for information gain saturation to perform a
push results in the agent sampling the environment several
times to reveal details that could have been revealed easily
through manipulation. Furthermore, the proposed method does
not update its environmental map after a push, leaving the
viewpoint planner, conditioned solely on this outdated map,
less capable of exploiting the newly revealed space.
This paper formulates the MEM problem as a Partially
Observable Markov Decision Process (POMDP) in the belief
space of semantic maps. By maintaining map-space beliefs,
our approach is applicable to unstructured cluttered environ-
ments with an arbitrary numbers of objects. The POMDP
computes the next best viewpoint or manipulation action that
maximizes the agents expected information gain over a short
horizon (Fig. 1) in a receding-horizon fashion. Our approach
leverages neural network methods for map-space belief prop-
literature to drastically improve map completion rates and offer
better guidance for object search [12, 47]. The key challenge in
belief propagation with manipulation actions is that they often
reduce certainty when the objects dynamics are unknown
or the robot interacts with unobserved objects. To address
this challenge, we introduce the Calibrated Neural-Accelerated
Belief Update (CNABU) technique to learn belief propagation
models for both observation (obtaining new images from the
environment) and manipulation actions. Confidence calibration
is especially important for belief propagation because over-
confidence in either object dynamics or map prediction can
result in ineffective exploration andor early termination. We
employ evidential deep learning to obtain better off-the-shelf
model calibration .
Our experiments in simulation environments demonstrate
that our proposed MEM planner outperforms prior work
and CNABU-enhanced baselines in terms of metric-semantic
accuracy. Furthermore, we perform hardware experiments with
a UR5 robot equipped with a gripper and an in-hand camera,
demonstrating zero-shot transfer of the learned models, and
showing the efficacy of our method in mapping of cluttered
shelves. An implementation of our method can be found on
Github1.
II. RELATED WORK
A. Next Best Viewpoint Planning
NBV planning is a well-researched approach in the area of
active vision that has been applied to both object reconstruc-
tion and large-scale scene mapping. Generally, NBV consists
of two steps: First sampling view candidates, then evaluating
which candidate is the best. For object reconstruction tasks
1 enhanced map prediction
the object. For large-scale scenes, sampling is more challeng-
ing. Monica and Aleotti  sample at the contour of the
explored scene. Other approaches sample at either predefined
or dynamically detected regions of interest. For the evaluation,
most approaches compute an estimated information gain to
determine the utility of a view. The information gain is often
based on the expected entropy reduction, e.g. by counting
unknown voxels in the field of view. Other approaches like
Hepp et al.  rely on a learned utility to predict the
best view. In this work, we build upon existing concepts of
NBV planing, but enhance them by incorporating manipulation
actions to interactively shape and explore the environment,
allowing the robot to gather richer information and adapt its
strategy based on both observation and interaction.
B. Mechanical Search in Shelves and Piles
Mechanical search algorithms [9, 18, 39] locate and extract
one or multiple target objects from a given scene, while
dealing with confined spaces, occlusion and object occurence
correlations. The task consists of multiple steps, i.e., visual
execution. For visual reasoning, current research demonstrates
that the scene can be effectively explored by interacting with
objects [3, 11, 26, 33] to actively reduce or overcome occlu-
Kim et al.  propose a method for locating and retrieving
a target object using both pushing and pick-and-place actions.
long-term map, and rebuilds environmental knowledge from
scratch with each observation. Therefore, the approach can
lead to unnecessary manipulation actions, as the target may
already be visible from other viewpoints. In the context of
planning for Manipulation Among Movable Objects (MAMO)
[36, 40], Saxena and Likhachev  introduced a method for
object retrieval in cluttered, confined spaces. Despite achieving
strong retrieval performance, their approach depends entirely
on prior knowledge of object shapes and dynamics. Pajarinen
et al.  propose a POMDP formulation for manipulating
objects under uncertain segmentation using a particle belief
prehensile manipulation to enable efficient belief propagation.
Muguira-Iturralde et al.  propose a planner based on two-
level hierarchical search to enable visibility-aware navigation
with movable objects. Their algorithm, Look and Manipulate
Backchaining (LAMB), however, relies on the assumptions
of deterministic action outcomes and on extremely simplified
environment dynamics (grasps always succeed, pushed objects
always slide along axis without rotation), having limited
applicability in the real world.
In this work, we do not focus on retrieving individual
environment. With our long-term occupancy and semantic map
on perfect model knowledge or single-shot scene understand-
complex manipulation behaviors.
C. Learned World Dynamics Models
Many model-based reinforcement learning algorithms learn
environment models from episodic environmental interaction,
often in latent spaces for improved evaluation speed .
These models, however, do not result in a human-interpretable
representation in contrast to our learned map-space dynamics.
Another recent trend leverages the popularity of conditional
video diffusion models to develop interactive pixel-space
simulators [45, 43]. These models focus on short-term visual
fidelity. We posit that explicit 3D maps provide long-term
temporal consistency and calibrated uncertainty measures that
are needed for robotic tasks that involve information gathering.
III. PROBLEM DEFINITION
We address the MEM problem as follows: Consider an
environment with a set of movable objects of varying sizes
and orientations, where some objects may be occluded and
not directly observable from any viewpoint. The arrange-
ment of these objects, along with the fixed support geometry
(e.g., a shelf or table), constitute the workspace configuration
space Cw . The environments initial configuration is un-
observable and denoted c0 Cw.
The robots objective is to create an accurate representation
of the workspace configuration ct after the execution of a
sequence  of actions [a0, . . . , an]. These actions include two
to a specific viewpoint vt V to capture an RGB-D image,
and manipulation actions, where the robot interacts with the
scene (e.g., via pushing or grasping). We address the eye-
in-hand RGB-D camera setting in which the robot does not
receive informative observations during manipulation and must
instead move to a retracted viewpoint to receive valid depth
data due to minimum depth restrictions. Moreover, we do
not integrate views during movement between locations, since
such images are subject to motion blur.
To formalize the problem, let the robots internal represen-
tation of the environment, a belief over metric-semantic maps
explaining object classes over the workspace, be denoted as
t. We assume a closed set of Nclasses semantic classes. Let
the most-likely map according to a belief t be denoted t.
When the robot executes a manipulation action at, it causes
a transition ct 7ct1 Cw according to the environments
dynamics ct1  Dyn(ct, at). Additionally, whenever the
robot takes any action at and gets an observation ot, drawn
according to the observation function Z(otat, ct1), its inter-
nal representation is updated through its belief update function
t1  BelUpdate(t, at, ot).
an optimal budgeted mapping problem. The robot is given
a maximum action budget T, and an initial environment
configuration c0, which is a priori unknown. The task is to
output the most informative sequence of actions  such that
the robots predicted map T , at the last step of the budget,
maximizes its mean Intersection over Union (mIoU) to the
ground truth map GT
which represents cT . So we have:
ToMap(cT )
ct1  Dyn(ct, at)t
t1  BelUpdate(tat, ot), ot Z(otat, ct1), t
where BelUpdate() represents the robots belief update, Z()
is the observation function, and ToMap() yields the metric-
semantic map that corresponds to a known configuration of
the environment. In deployment, the robot cannot accurately
predict GT
ration nor the dynamics of the environment. It may not even
know the number of objects, their shapes, or semantic labels.
IV. METHOD
A. Overview
We model the MEM problem as a Partially Observable
Markov Decision Process (POMDP) in metric-semantic map-
space . To solve this POMDP, the agent should perform a
belief update about the state of the map after both manipu-
lation and observation actions. For manipulation actions, the
belief update propagates through a map transition function
T(t1t, at) that is a priori unknown. For observation
while reasoning over hidden object shapes and arrangements
to reduce uncertainty.
sible maps makes traditional belief updates computationally
infeasible . The approximation that map cells are inde-
leads to compact belief representations but reduces precision
in the belief update. Our CNABU method captures prior
knowledge about the world, such as object contiguity and usual
object sizes and arrangements, in its update. We also leverage
uncertainty-aware deep learning models to predict factorized
belief updates that are better aligned with plausible map con-
figurations and yield actionable quantification of uncertainty.
These models are trained using simulated ground truth to ap-
proximate occlusion reasoning and interaction dynamics, i.e.,
Dyn. Object sizes, classes, occlusion levels, and manipulation
effects should be roughly representative of real scenes, but
our method is tolerant to differences in configuration, number,
class distribution, and moderate shape changes.
tive with the Volumetric Information Gain (VIG) metric ,
since directly modeling the mIoU metric is not feasible, as it
depends on the unobservable workspace configuration. It uses
a 2-step greedy approach that obtains good performance by
exploiting near-submodularity of the VIG function. Moreover,
the 2-step approach obviates the need to sample from the
observation distribution Z.
B. Neural Map Belief Dynamics
Following grid mapping literature , we represent a belief
t over the semantic-metric map using a Bernoulli distribution
Manipulation
Occlusion Aware
Information Gain
Observation
Partial observation
Predicted push dynamics
Executed
manipulation
Belief at time t
Push beliefs
Selection
If observation action selected
Best manipulation action
If manipulation action selected
Executed
observation
Manipulation actions
View actions
Belief at time t1
after manipulation
Belief at time t1
after observation
Best observation
Fig. 2: From a prior map belief, our pipeline predicts a map belief resulting from a set of candidate pushes. It then weighs the information gain from taking
two consecutive independent views given the current belief (orange arrows) or taking a single observation given any of the predicted beliefs after pushing
(blue arrows), selecting the path of highest cumulative information gain and taking its respective first action  either taking the next best view or executing
the best push. IGVt represents the best information gain obtainable from taking two distinct observation actions, whereas IGMt is the best information gain
obtainable through a manipulation action followed by an observation action.
for a cells occupancy and a categorical distribution for a
cells semantic class. Each cell is assumed independent. We
represent the occupancy map as a 3D voxel belief O
RHW D and the semantic map as a 2D birds-eye belief
t RHW Nclasses. The semantic map is 2D for simplicity
because objects are roughly prismatic and stacking is not
allowed in our problem, while the occupancy is 3D because
object heights affect visibility determination. We consider
observations ot O consisting of an RGB-D image with
added semantic labels.
efficient
introduce
Calibrated-Neural Accelerated Belief Update (CNABU) tech-
nique that uses separate neural networks to represent the
posteriors of the viewpoint and manipulation actions in the
factored representation. The first, called observation CNABU,
computes a map belief update after a observation action
o(t, ot, at). The second, called manipulation
action t1 m(t, at), where we drop the observation
because no new observation is generated.
Let t()  P(t) denote the probability density of a
map  under belief t. For any action at and observation ot,
the standard POMDP belief update equation  gives the
posterior belief as:
Z(otat, )
T(, at)t(),
where  ranges over all possible maps, with  being a
normalizing constant. If we wished to project the belief state
into a marginalized occupancy probability of a cell, we would
need to compute:
t1[i, j, k]  Et1[O[i, j, k]  1],
where E is the expected value. A similar equation would hold
for semantic updates. Regardless, the space of possible maps is
far too large  and there is no current method for assessing
map densities t() that accurately accounts for inter-cell
correlations (e.g., object shapes, arrangements, and visibility).
any scene that could produce a belief t via its observations is
inherently a sample of the distribution induced by this belief,
the training process leverages a neural networks averaging
tendency to create an implicit Monte Carlo estimate of Eq. 3.
We postpone the network and training details to Section. IV-G
and proceed to describe their use in MEM.
C. Solving the POMDP
We propose to solve the map-space POMDP by using a
k-step receding horizon greedy planner, as shown in Fig. 2,
which uses Volumetric Information Gain (VIG)  as an
approximation of the true reward. VIG correlates to infor-
mation gain and is a submodular optimization objective in
static scenes , having been used to efficiently solve NBV
planning and sensor placement problems. Due to VIGs sub-
bounded suboptimality, justifying the greedy receding-horizon
strategy. While VIGs submodularity does not hold in general
for dynamic scenes, we assume that manipulation actions do
not increase entropy at a rate that justifies the significant
expense of long lookaheads. However, because a manipula-
tion action does not produce any observation (and hence no
immediate information gain) at least one further observation
must be considered in scoring. Hence, the minimum viable
search horizon at k  2, which yields a balanced tradeoff
between computational efficiency and action quality.
To perform an observation action, the robot chooses from
vi V possible views in a fixed array of camera positions
V to which the robot can move. Furthermore, let t t
be a sampled manipulation action from a set of feasible
manipulation actions. In our two-step greedy search, we only
Semantic
Occupancy
Semantic
2D Occ. Beta t1
Semantic Map t
Changes Beta t1
Occ. UNet Inputs
Occ. UNet Inputs
Manipulation CNABU ()
Observation CNABU ()
Semantic Dirichlet t1
3D Occ. Beta t1
Semantic Map t
Semantic Obs.
Occ. Cell Obs.
Free Cell Obs.
3D Occ. Beta t
Occupancy
Projection
Semantic UNet Inputs
Semantic Map t
Semantic Obs.
2D Occ. Beta t1
Semantic Map t
End Point Map
Start Point Map
Swept Volume
3D Occ. Beta t
3D Occ. Beta t1
Changes Beta t1
Projection
Semantic Dirichlet t1
Learned Modules
Intermediate Results
Static Processes
Semantic UNet Inputs
Fig. 3: Architecture overview of our observation and manipulation CNABU networks. The observation prediction network uses the occupancy posterior beta
and semantic posterior Dirichlet for loss computation, while the manipulation prediction network additionally takes the 2D map of occupancy changes after
the push for loss calculations.
consider two possible kinds of action sequences: taking two
observation actions (vt, vt1) or performing a manipulation
action followed by an observation (t, vt1). This is because
(t, t1) would result in no observation and therefore no
information gain and the information gain of (vt, t1) is
strictly smaller than the VIG of any (vt, vt1), vt1  vt.
Letting IG(vi,    , vin1O) denote the Volumetric
Occlusion-aware Information Gain  for voxels intersected
by the rays from n views (vi    vin1) given belief O ,
the two most informative consecutive views (v
t1), are:
t1)  arg max
IG(vt, vt1O
For manipulation, t
t1  m(t, t) denotes the predicted
belief from the manipulation CNABU when given action
t t as input. We can define the most informative 1-step
t and its associated most informative view v
represent
semantic
H(t, t1)  H(t) H(t1).
IGVt  IG(v
information
gain obtained from two viewpoints, also called ViewPoint
Planning (VPP), while IGMt  IG(v
t1) represents
the best information gain from a push followed by a
selection.
Regt  H(t,
captures
difference
between the current semantic map and the map after the best
Our policy decides the action at to take according to:
if IGVt > IGMt  Regt
otherwise
Where  is a discount factor to account for different
magnitudes between camera array VIGs and whole-map se-
mantic entropy values. Regt serves as a regularization on
the aggressiveness of the selected pushes, as more radical
environment manipulations could potentially reveal more of
the environment, but would cause unnecessary disturbances to
the scene and introduce a lot more uncertainty to the post-
push representations. If at
get the observation ot and use o to obtain the new belief
t1  o(t, at, ot). This search is then repeated in a loop
until the maximum number of actions has been performed, or
a threshold for full map completion has been reached, which
we set to 95 of the semantic voxels with greater than 85
certainty in the belief . When this threshold is reached, the
planner no longer pushes, but still collects novel views.
D. Push Sampling
We consider pushing as our manipulation action. To com-
pute valid push candidates using O
high-confidence frontier points from the shelf entrance and
sample k of them uniformly at random as start points for
the pushes. We test the start points of the k sampled pushes
for collisions against other high confidence voxels in O
After randomly sampling these k unique frontier points, we
determine for each point whether this starting position of a
push will lead to a feasible and valid motion plan. For each
valid start point, we sample a likely occupied point in O
near it to obtain the push direction and sample a push distance
uniformly at random between 50 and 150 mm. We then obtain
a valid motion plan using a sampling based motion planner
[13, 15] and parametrize this plan with .
E. Evidential Posterior Learning
Although we could learn to predict beliefs as functions
evidential posterior networks , as evidential learning is
known improve the calibration of uncertain predictions. An
evidential map belief  consists of belief prior parameters
for each cell of the map belief . Specifically, we store
O RHW D2, a 3D grid of Beta distribution parameters
for each voxel in the map, and S RHW Nclasses, a
grid of Dirichlet distribution parameters for each cell in the
2D map. Let Beta() and Dir() denote the Dirichlet and
Beta distributions, respectively. Therefore, the occupancy and
semantic map beliefs are related to evidential parameters via
O[]  E[Beta(O[])] and S[]  E[Dir(S[])] . We
assume that the initial states O
0 are uninformed and
set to 1, a unit tensor with appropriate dimensions. Note: evi-
dential parameters are maintained for each map belief  in our
algorithm and CNABUs operate by propagating the evidential
parameters (t1 o(t, ot, at) and t1 m(t, at))
followed by an update to the standard belief parameters.
F. Dataset Generation
To train the CNABU models o and m, we collect datasets
on maps, viewpoints, and sampled pushes in simulation.
A total of 14 different object categories from the YCB
dataset  are used and sampled in a shelf board of size
(0.8  0.4  0.4)m. We sample object configurations on
the shelf following a stochastic method that considers class
dependencies and efficient free space coverage for placing
objects. This method allows for the sampling of varied object
detail in Appendix A.
To train the viewpoint belief prediction model o, we
assemble a dataset D  {d1, d2, . . .} where each scene di has
the form (GT, o1,    , on). Here, GT represents the ground
truth 3D metric-semantic voxel map of a shelf environment
with randomly placed objects, and o1,    , on are the depth
and semantically segmented images captured from V, the set
of discrete viewpoints in the environment. The ground truth
semantic labels are used in the rendered images.
To train the manipulation belief prediction model m, the
simulated robot executes a randomly sampled action in synthe-
sized scenes. This produces a dataset Da  {da
each sequence da
post are the ground truth maps before and after manipu-
observations from V as before, taken before the manipulation
is executed.
G. Training CNABU Networks
We now outline the procedure for training the two CNABUs,
whose network architectures are shown in Fig. 3, with further
details given in Appendix B.
We train o from the dataset D by sampling, without
l1), from every scene di at every epoch. This
sampling diversifies the beliefs encountered during training by
varying the emulated observation sequences for each scene.
The loss over the sequence d is computed as follows. We
recursively predict the evidential beliefs t  o(t1, o
up to time l. Also, let yi {0, 1}dim indicate a one-hot
encoded tensor of the ground truth value for a given voxel
i according to GT, where dim is either Nclasses for S
t . For each voxel i in t, define, for its predicted
distribution parameter i
t  yi  (1 yi) i
where denotes element-wise multiplication. We employ the
evidential uncertainty-aware cross-entropy from Sensoy et al.
as the loss, which, for each time step, is given by:
t)Dir(1)
Calibration
L515 Camera
Fig. 4: Real-world environment showing a shelf scenario. The UR5 is
equipped with an Robotiq parallel-jaw gripper and a Realsense L515 RGB-D
camera to create a calibrated representation of the scene.
, where L(i
t) log(ij
tween distributions a and b, 1 is a vector of all ones, type is
o if it is an occupancy loss (dim  2) and s if it is a semantic
loss (dim  Nclasses), and  is an annealing parameter, set
according to Sensoy et al. . The total loss for the sample
d is the sum of the semantic and occupancy losses Lo
over the l observations.
The manipulation CNABU is defined similarly to the obser-
vation CNABU, except that it has an auxiliary output, which
predicts a Beta distribution over a voxel grid modeling the
probability of a given voxel being changed in GT after the
manipulation is executed, which we call diff. Therefore, we
t1  m(t, at)
Training epochs iterate over sequences in Da. For each
without replacement as above. We then recursively obtain
the beliefs from the observation CNABU, t1  o(t, o
for t  0, . . . , l 1. Next, we predict the post-manipulation
belief S
t1  m(l, a) and use GT
post as the
training target. The random sampling of a different number
of observations prior to pushing ensures the CNABU sees
different belief stages during training. We derive the ground
truth for diff, GT
consistency loss, Lcon which is the Mean Squared Error
between l1 and l. This loss serves as a regularization
to encourage the alpha parameters of the distributions in the
unchanged areas of the map to have a similar magnitude.
As before, the network heads are trained using the loss in
Eq. 7 . The total loss for the manipulation sequence is
given by L  LO
l1  Ldiff
l1  Lcon.
V. EXPERIMENTS
We perform four core experiments to evaluate our approach.
provements in map completeness and accuracy compared to
state-of-the-art . Next, we present a series of ablations
of our method and evaluate several interactive baselines. We
Ours wo Pushing
Random  Observation CNABU
Dengler et al.
Action Taken
Occupancy mIoU
Low Occlusion - Occupancy
Action Taken
High Occlusion - Occupancy
Action Taken
Semantic mIoU
Low Occlusion - Semantics
Action Taken
High Occlusion - Semantics
Fig. 5: Simulation results for Manipulation Enhanced Mapping against SOTA and non-pushing baselinesshowing both occupancy and semantic IoUs over
time for each method. Our method outperforms all baselines in highly occluded scenes, while not having degraded performance in low occlusion scenes.
Standard deviation of performance of random baselines over random seeds is represented as shading around each plot.
then present some experiments on the generalization of the
trained CNABUs to within-class shape variations. Finally, we
study the robustness of our system in terms of its zero-shot
transferability to a physical setup. Further evaluations, which
validate the individual CNABUs performance and the use of
VIG as a reward proxy, are provided in Appendices C and D.
A. Experimental Setup
Our task setup consists of a shelf scene with a UR5 arm for
observation and action execution (Fig. 4). The robot is fixed to
a table facing an occluded shelf and equipped with a Robotiq
parallel-jaw gripper for manipulation and an RGB-D camera
for observations. In simulation, the ground truth observations
and segmentations are provided by rendering.
The real-world setup is similar, but with a few notable
differences. For action execution, ROS and MoveIt  are
used. The depth image is obtained from an intel Realsense
L515 camera and the semantic segmentation in the real world
is performed using segment anything 2 (SAM2)  and a
strategy similar to LSeg [25, 19]. We take detected masks
from SAM2 and crop the original image around them. Next,
we extract their CLIP  embeddings and compute their
cosine similarity to the language embeddings of our target
we normalize the similarity scores to classify each mask.
B. Simulation Experiments
These experiments consider both low and high occlusion
scenarios for manipulation-enhanced mapping. We generate
100 low occlusion scenarios via rejection sampling, using our
sampling method described in Appendix A, but keeping only
scenarios for which at least one object cannot be seen from
any of the 300 viewpoints. We then crafted 25 high occlusion
scenarios by hand to be challengingly crowded and with many
objects occluded. We provide examples of each category in
Figs. 10 and 11 in the Appendix. The robot begins with a
naive uniform map prior.
We compare our work (Ours) with the following baselines.
and fine-tuned the network weights provided by the authors
for only 5,000 action steps. Although their experimental setup
closely aligns with ours, our tests introduce a lower upper
shelf board height and more densely sampled object con-
figurations. Second, the Random baseline randomly samples
a set of unique views [vr
n] V and uses standard
metric-semantic occupancy mapping . We also combine
random view selection with our observation belief predictor
ablation of our pipeline that does not use manipulation, Ours
wo pushing.
Metric and semantic mIoU compared to the ground truth
map at time t are plotted in Fig. 5. We observe that the
previous S.O.T.A. for unstructured MEM, Dengler et al. ,
explores efficiently at early stages. However, with more of the
scene uncovered and new areas being harder to observe, its
performance degrades to comparable or worse than random,
particularly in terms of semantic mIoU. We attribute this
performance degradation to two key challenges: the use of
heuristic-based action switching and the lack of map updates
after manipulation. The heuristic switching mechanism relies
on hand-crafted rules to alternate between observation and
manipulation actions, and may not always select the most
informative action, occasionally choosing to push too early,
too often, or to continue observing when manipulation would
be more beneficial. Our POMDP-based action selection over-
comes this problem and does not select manipulation actions
when they are not beneficial for higher information gain. Fur-
update its belief after a push, it requires multiple subsequent
observations to reconcile inconsistencies between the actual
scene and the previously assumed map representation. This
delay in belief correction leads to inefficient re-exploration
and degraded semantic accuracy, as the agent lacks a reliable
signal for where to focus its attention.
scenes even without pushing. In highly occluded scenarios,
pushing is required to make progress after the visible surfaces
Random Push Every 5th
Saturation Pushing
Random Push Every 5th, No Manip. CNABU
Random View, Random Push Every 5th
Action Taken
Occupancy mIoU
Low Occlusion - Occupancy
Action Taken
Occupancy mIoU
High Occlusion - Occupancy
Action Taken
Semantic mIoU
Low Occlusion - Semantics
Action Taken
Semantic mIoU
High Occlusion - Semantics
Fig. 6: Simulation tests of push selection alternatives. Note how our method not only achieves better mIoUs than any of the other methods, it does so
consistently across all the steps, avoiding uninformative or overly aggressive manipulation actions. Also, next-best-view with the observation belief prediction
but without push belief prediction (purple) leads to degraded performance. Standard deviation of performance of random baselines is represented as shading
around each plot.
are observed. Our method uses pushing to achieve significantly
higher mIoUs. Note that its IoU growth is slower early
viewpoint step is taken in the following action.
C. Push Selection Alternatives
Unless otherwise noted, the same belief prediction networks
are provided to each method. Three of the strategies push
regularly at a five-step interval, which is a typical rate at
which observation actions provide diminishing returns. The
its belief using m. The second, Random Push Every 5th, No
Manip. CNABU is the same, but does not use m to update its
belief after pushing. The third, Random View Random Push
Every 5th, chooses random views and randomly pushes every
five steps without using m. Finally, we consider a heuristic
that performs a random push when VIG seems to saturate,
Saturation Pushing. The saturation threshold is when two
consecutive estimates of VIG during NBV differ by less than
2. Saturation pushing uses m after each push to update its
belief. A comparison of selection strategies is given in Sec. F
in the Appendix.
Results are shown in Fig. 6. Each baseline that has a
random component is run three times with different random
seeds. We observe that push belief prediction is beneficial to
manipulation-enhanced mapping, even when random pushes
are being executed. The blue and purple curves show methods
that are not informed by m. Further, we see that the satura-
tion pushing (green) does not observe post-push performance
pushes and our method. Overall, our method still achieves the
best performance, most strikingly in highly occluded scenes.
D. Out of Distribution Shape Experiments
In order to evaluate the robustness of the proposed pipeline
to shape variations within a given class, we modify the
simulation setup. For each individual instance of an object
in a given scene, we randomly and independently rescale
the objects mesh on each of its principal axis by a factor
chosen uniformly at random within [0.8, 1.2], and re-run
our simulation experiments in the high-occlusion scenes with
these out-of-distribution shapes for the strongest baselines in
simulation. As seen in Fig. 8, we observe a decrease in the
agents performance when out of distribution, but note that
in the long run the performance remains consistent with our
ID experiments. This is notable since we do not perform
any sort of shape augmentation during the training of the
observed maps helps them overcome some level of distribution
shift. Naturally, we presume that adding shape augmentation
and more diverse object geometries of the same class during
training should further help reduce this OOD performance gap.
E. Hardware Experiments
We also performed 10 real-world experimental runs on
a UR5 as described in Sec. V-A. All results are collected
in a zero-shot fashion, i.e., no fine-tuning on real data was
performed. We set the budget to 20 steps and sampled a fixed
set of 75 reachable camera poses in front of the shelf for V.
We handcrafted 10 challenging scenes, each with an average
of 18 objects from the YCB dataset , where pushing is
required to reveal other objects. We collect the ground truths
by removing the top of the shelf at the end of each episode
to manually score the final maps. We score each scenario
according to the status of all of the objects present in the
map. Each object in the map is classified in four categories:
1) Correctly Found if the majority of the object is correctly
represented in the map with the right class; 2) Misclassified
But Found if the majority of the object is present in the
occupancy map but is mislabeled; 3) Not Found - if the
majority of the object is absent from the occupancy map and
4) Hallucinated if an object that is not present in the scene is
present in the map. Due to the complexity of precisely resetting
a scene multiple times for different baselines and the fact that
cornflakes
ohio cookies
chewing gum
pasta box
pringles can
baking powder
tomato can
gelatin box
cheez-it box
glass cleaner
coffee can
milk carton
mustard bottle
background
Fig. 7: Qualitative real-world experiment results. Note how VPP and Random baselines are unable to fully explore the environment due to its occlusions,
while our method is able to better explore it via reasonable manipulations. In Step 16, Ours, we highlight in yellow one of the scene objects revealed by
manipulation. We highlight in red a persistent hallucination across all 3 methods, likely due to unreliable semantic segmentation and significant camera noise
at that corner of the shelf .
Ours OoD
Ours wo Pushing OoD
Saturation Pushing OoD
Action Taken
Occupancy mIoU
(a) Occupancy Results (IoU)
Action Taken
Semantic mIoU
(b) Semantic Results (mIoU)
Fig. 8: OoD Results in High Occlusion scenes.
both random  observation CNABU and our approach without
pushing are about as strong as other pushing baselines (see
Figs. 5 and 6), we only compare against them in the physical
experiments. For each model, we report the total quantity of
each detection at time step 20 summed over all 10 trials.
Results in Tab. I show that with zero-shot transfer from sim-
over the compared baselines. Note that all methods compared
use calibrated belief prediction. However, little difference is
seen between viewpoint planning (Ours wo Pushing) and
random viewpoint choices. We expect that this is due to a
domain gap caused by camera noise from the realsense L515
leading to some strong artifacting in the depth images and the
inaccuracies of the open-set semantic segmentation pipeline.
We found the SAM2 segmenter performed particularly poorly
for oblique angles or partial object views, resulting in many
missed instance detections and misclassifications. For instance,
there is a consistent artifact on the segmentation pipeline,
which consistently classifies the bottom right corner of the
shelf as a gelatin box, as seen in all methods in Fig. 7.
and without pushing) greatly reduce the number of hallucina-
tions and improve the number of correctly identified objects.
were previously unseen by the non-interactive baselines, per-
formance consistent with the simulation experiments, despite
the significant sim-to-real gap, particularly in segmentation
incorrectly classified.
TABLE I: Comparing our method to the strongest baselines in zero-shot
transfer to real-world shelves. Total counts of each type of detection are
reported across our 10 trials.
Correctly
Missclassified
But Found
Not Found
Hallucinated
Ours wo Pushing
VI. LIMITATIONS
Limitations of our method include the need for represen-
tative simulation training data or ground truth segmented
maps. It also relies on high-quality semantic segmentation,
and although the computer vision field is making significant
progress on segmentation, segmentation accuracy is still too
low for many robotics applications in occluded, poorly lit and
partial views, especially in open-set scenarios.
Computation times for our POMDP solver vary but take
on the order of several seconds, due to the need for in-
formation gain calculations and belief propagation for many
actions. Our current framework navely samples manipulation
actions during action selection, and more intelligent action
sampling could improve computational efficiency. This will be
especially important when including additional manipulation
There is currently a significant sim-to-real gap that could be
addressed by fine-tuning on real data or performing domain
randomization of the object dynamics during data collection in
simulation to help improve real-world performance. Further,
our maps are defined over dense voxel grids, which poses
scalability challenges when applied to larger spaces. Moreover,
the mapped region is a fixed volume and we assume prior
knowledge about the fixed parts of the environment (e.g., shelf)
for motion planning. These assumptions should be relaxed
to address fully unstructured and unknown worlds. Finally,
our maps use a closed-world semantic labeling, and extending
belief propagation to open-world segmentation would be an
interesting frontier to explore.
VII. CONCLUSION
presented
approach
manipulation-enhanced mapping problem in which the solver
decides between changing a camera view and manipulating
objects to map an area cluttered with objects. It relies on the
novel Calibrated Neural-Accelerated Belief Update map-space
belief propagation approach, which allows a unified treatment
of both viewpoint change and manipulation actions. Using
well-calibrated beliefs allows the POMDP solver to make
decisions between different action types according to the most
informative outcome. Experimental results in both simulation
and on a real robot show that our approach outperforms non-
manipulating and heuristic manipulation baselines in terms
of occupancy and semantic map accuracy. Overall, this work
offers a promising new framework for robots navigating and
manipulating real-world cluttered environments.
VIII. ACKNOWLEDGMENTS
This work used the NCSA Delta GPU cluster through
allocation CIS240410 from the Advanced Cyberinfrastruc-
ture Coordination Ecosystem: Services  Support (ACCESS)
Foundation grants 2138259, 2138286, 2138307, 2137603,
and 2138296. This work has partly been supported by the Eu-
ropean Commission under grant agreement numbers 964854
(RePAIR) and by the BMBF within the Robotics Institute
supported by NIFAUSDA Awards 2020-67021-32799 and
REFERENCES
Andreas Bircher, Mina Kamel, Kostas Alexis, Michael
erage path planning via viewpoint resampling and tour
optimization for aerial robots. Autonomous Robots, 40
Timothy J. Boerner, Stephen Deems, Thomas R. Furlani,
Shelley L. Knuth, and John Towns.
vancing innovation: NSFs advanced cyberinfrastructure
coordination ecosystem: Services  support. In Practice
and Experience in Advanced Research Computing 2023:
Computing for the Common Good, PEARC 23, page
Computing Machinery.
Jeannette Bohg, Karol Hausman, Bharath Sankaran,
Oliver Brock, Danica Kragic, Stefan Schaal, and Gau-
rav S Sukhatme.
Interactive perception: Leveraging
action in perception and perception in action.
Transactions on Robotics, 2017. doi: 10.1109TRO.20
Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha
marking in manipulation research: The YCB object and
model set and benchmarking protocols. IEEE Robotics
and Automation Magazine, 2015.
J Chase Kew, Brian Ichter, Maryam Bandari, Tsang-
Wei Edward Lee, and Aleksandra Faust. Neural Colli-
sion Clearance Estimator for Batched Motion Planning.
In Steven M LaValle, Ming Lin, Timo Ojala, Dylan
of Robotics XIV, pages 7389, Cham, 2021. Springer
International Publishing. ISBN 978-3-030-66723-8.
Sachin Chitta, Ioan Sucan, and Steve Cousins. MoveIt!
[ROS Topics]. IEEE Robotics  Automation Magazine,
cument6174325.
Sanjiban Choudhury, Ashish Kapoor, Gireeja Ranade,
and Debadeepta Dey. Learning to gather information via
imitation.
In 2017 IEEE International Conference on
Robotics and Automation (ICRA), pages 908915, 2017.
Erwin Coumans and Yunfei Bai.
module for physics simulation for games, robotics and
machine learning.  20162021.
Michael Danielczuk, Andrey Kurenkov, Ashwin Bal-
berg. Mechanical search: Multi-step retrieval of a tar-
get object occluded by clutter.
In Proc. of the IEEE
Intl. Conf. on Robotics  Automation (ICRA), 2019.
URL https:
ieeexplore.ieee.orgdocument8794143.
Jeffrey Delmerico, Stefan Isler, Reza Sabzevari, and
Davide Scaramuzza. A comparison of volumetric infor-
mation gain metrics for active 3D object reconstruction.
Autonomous Robots, 42(2):197208, 2018. ISSN 1573-
Nils Dengler, Sicong Pan, Vamsi Kalagaturu, Rohit
point push planning for mapping of unknown confined
spaces. In 2023 IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 11781184.
Georgios Georgakis, Bernadette Bucher, Karl Schmeck-
ing to Map for Active Semantic Goal Navigation.
International Conference on Learning Representations,
Sanchez Gildardo and Jean-Claude Latombe. A single-
query bi-directional probabilistic roadmap planner with
lazy collision checking. In Jarvis Raymond Austin and
Alexander Zelinsky, editors, Robotics Research, pages
delberg. ISBN 978-3-540-36460-3.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo-
hammad Norouzi. Dream to control: Learning behaviors
by latent imagination. arXiv preprint arXiv:1912.01603,
K. Hauser. Robust contact generation for robot simula-
tion with unstructured meshes. In Proc. of the Intl. Sym-
posium on Robotic Research (ISRR), 2013.
Benjamin Hepp, Debadeepta Dey, Sudipta N Sinha,
Ashish Kapoor, Neel Joshi, and Otmar Hilliges. Learn-to-
utility. In Proceedings of the European conference on
computer vision (ECCV), pages 437452, 2018.
H. Hu, S. Pan, L. Jin, M. Popovic, and M. Bennewitz.
Active implicit reconstruction using one-shot view plan-
ning. In Proc. of the IEEE Intl. Conf. on Robotics
Automation (ICRA), 2024. doi: 10.1109ICRA57147.20
Huang Huang, Marcus Dominguez-Kuhne, Vishal Satish,
Michael Danielczuk, Kate Sanders, Jeffrey Ichnowski,
Andrew Lee, Anelia Angelova, Vincent Vanhoucke, and
Ken Goldberg.
Mechanical search on shelves using
lateral access x-ray.
In 2021 IEEERSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 20452052. IEEE, 2021. doi: 10.1109IROS5116
ment9636629.
Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala,
Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh
Joshua B Tenenbaum, Celso Miguel de Melo, Madhava
ralba. ConceptFusion: Open-set multimodal 3D mapping,
Leslie Pack Kaelbling, Michael L Littman, and An-
thony R Cassandra.
Planning and acting in partially
observable stochastic domains.
Artificial Intelligence,
Seungyeon Kim, Young Hun Kim, Yonghyeon Lee, and
Frank C Park.
Leveraging 3d reconstruction for me-
chanical search on cluttered shelves.
In 7th Annual
Conference on Robot Learning, 2023.
URL https:
proceedings.mlr.pressv229kim23akim23a.pdf.
Michael Krainin, Brian Curless, and Dieter Fox.
tonomous generation of complete 3d object models using
next best view manipulation planning.
In 2011 IEEE
international conference on robotics and automation,
pages 50315037. IEEE, 2011.
Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-
optimal sensor placements in Gaussian processes: The-
of Machine Learning Research, 9(2), 2008.
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing
Encoders for Object Detection From Point Clouds. In
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition (CVPR), 6 2019.
Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Language-driven semantic
segmentation. arXiv preprint arXiv:2201.03546, 2022.
Jue Kun Li, David Hsu, and Wee Sun Lee. Act to see and
see to act: Pomdp planning for objects search in clutter.
In Proc. of the IEEERSJ Intl. Conf. on Intelligent Robots
and Systems (IROS). IEEE, 2016. doi: 10.1109IROS.2
Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu
and Zhi Tang. BEVFusion: A simple and robust lidar-
camera fusion framework. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
Advances in Neural Information Processing Systems,
volume 35, pages 1042110434. Curran Associates, Inc.,
2022. URL  filesp
aper2022file43d2b7fbee8431f7cef0d0afed51c691-Pap
er-Conference.pdf.
Joao Marcos Correia Marques, Albert J Zhai, Shenlong
On the overconfidence prob-
lem in semantic 3d mapping.
In Proc. of the IEEE
Intl. Conf. on Robotics  Automation (ICRA). IEEE,
Riccardo Monica and Jacopo Aleotti.
Contour-based
next-best view planning from point cloud segmentation
of unknown objects. Autonomous Robots, 42(2):443458,
2018. URL
Lozano-Perez.
Visibility-aware navigation among movable obstacles. In
2023 IEEE International Conference on Robotics and
Automation (ICRA), pages 1008310089, 2023.
Joni Pajarinen, Jens Lundell, and Ville Kyrki. Pomdp
planning under object composition uncertainty: Appli-
cation to robotic manipulation.
IEEE Transactions on
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Des-
Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-
tala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library.
In H Wallach, H Larochelle,
A Beygelzimer, F dAlche Buc, E Fox, and R Gar-
Systems 32, pages 80248035. Curran Associates, Inc.,
2019. URL
-an-imperative-style-high-performance-deep-learning-l
ibrary.pdf.
Thomas Pitcher, Julian Forster, and Jen Jen Chung.
Reinforcement learning for active search and grasp in
clutter. In Proc. of the IEEERSJ Intl. Conf. on Intel-
ligent Robots and Systems (IROS). IEEE, 2024.
URL https:
ieeexplore.ieee.orgdocument10801366.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
models from natural language supervision, 2021.
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang
man Radle, Chloe Rolland, Laura Gustafson, et al. SAM
Dhruv Saxena and Maxim Likhachev.
Planning for
manipulation among movable objects: Deciding which
objects go where, in what order, and how. In Proc. of
the Int. Conf. on Automated Planning and Scheduling
(ICAPS), 2023.
Dhruv Mauria Saxena and Maxim Likhachev.
proved M4M: Faster and richer planning for manipula-
tion among movable objects in cluttered 3d workspaces.
In Proc. of the IEEE Intl. Conf. on Robotics  Automa-
tion (ICRA). IEEE, 2024. doi: 10.1109ICRA57147.20
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Ev-
idential deep learning to quantify classification uncer-
tainty. In S Bengio, H Wallach, H Larochelle, K Grau-
in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018. URL
neurips.ccpaper filespaper2018filea981f2b708044d6f
b4a71a1463242520-Paper.pdf.
Satvik Sharma, Kaushik Shivakumar, Huang Huang,
Lawrence Yunliang Chen, Ryan Hoque, Brian Ichter,
and Ken Goldberg.
Open-world semantic mechanical
search with large vision and language models. In 7th
Annual Conference on Robot Learning, 2023.
Mike Stilman, Jan-Ullrich Schamburek, James Kuffner,
and Tamim Asfour. Manipulation planning among mov-
able obstacles.
In Proc. of the IEEE Intl. Conf. on
Robotics  Automation (ICRA). IEEE, 2007.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox.
Probabilistic robotics. 2005. Massachusetts Institute of
Dennis Ulmer, Christian Hardmeier, and Jes Frellsen.
Prior and posterior networks: A survey on evidential deep
learning methods for uncertainty estimation. Proceedings
of Machine Learning Research, 2023.
Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi
Fruchter. Diffusion models are real-time game engines,
2024. URL
Yuchen Xiao, Sammie Katt, Andreas ten Pas, Shengjian
object search in clutter under partial observability.
Proc. of the IEEE Intl. Conf. on Robotics  Automation
(ICRA). IEEE, 2019. doi: 10.1109ICRA.2019.8793494.
Dale Schuurmans, and Pieter Abbeel. Learning interac-
tive real-world simulators. In The Twelfth International
Conference on Learning Representations, 2024.
Rui Zeng, Yuhui Wen, Wang Zhao, and Yong-Jin Liu.
View planning in robot active vision: A survey of sys-
Albert J Zhai and Shenlong Wang. PEANUT: Predicting
and navigating to unseen targets. In Proceedings of the
IEEECVF International Conference on Computer Vision
(ICCV), pages 1092610935, October 2023. doi: 10.110
orgdocument10378364.
APPENDIX
A. Dataset Generation Details
The simulation environment used for data generation and
simulation testing is PyBullet  (Fig. 9). The object ar-
rangements for the CNABU training datasets were created
according to the following procedure: First, we sample the
desired occupancy fraction of the shelf floor plan - which we
set to be between 30 and 45. We define two parameters for
each object class: its affinity to other classes and its radius of
influence. Classes within an objects affinity class and radius
of influence have their probability of appearance increased.
from 0 to 1. When this parameter is zero, there is no enforced
alignment in the shelf and when it is one, there is a higher
probability for objects to be placed directly in line with the
centroid of previously placed objects, emulating more orderly
arrangements by increasing the probability of sampling areas
directly in front or behind already placed objects.
We begin sampling by placing a fine grid over the floor
space of the scene. Then, until the desired shelf occupancy
is reached (or a total number of iterations is reached), we
sample a point in this grid that is not yet occupied by another
object - taking into consideration the altered probabilities of
occupancy by . Through the use of Minkowski differences
between the object shapes and the free space, we determine
the placeable area for the centroid of each object class within
the current arrangement and, for each object that is placeable
in the selected point, we compute its sampling probability
conditioned on the affinities of the objects whose area of
influence contains the sampled point. We then sample an object
class according to that distribution and randomly sample an
angle for the object, between 0and 180and place the object
in that orientation. To stimulate the presence of occlusions, we
set the base probability of larger objects to be slightly higher
- and set large objects to have affinity for smaller objects. For
dataset generation, we leave  at 0. The ground truth map for
each scenario is collected by removing the top shelf, taking
a series of dense top-down images of the scene and mapping
them with traditional metric-semantic grid mapping . For
the pushing dataset, the scenario sampling is identical, except
we also sample a random push following Sec. IV-D.
Fig. 9: Simulation environment configuration example of different YCB
objects in a confined shelf scenario. The UR5 is equipped with an Robotiq
parallel-jaw gripper
(a) High Occupancy. sample - frontal
(b) High Occupancy sample - top
Fig. 10: Frontal and (privileged) top-down views of a scene from the highly
occluded set
(a) Low Occupancy sample - frontal
(b) Low Occupancy sample - top
Fig. 11: Frontal and (privileged) top-down views of a scene from the Slightly
Occluded set (right).
Figures 10 and 11 show example scenes from the High Oc-
cupancy and Low Occupancy sets. Note how both scenes have
occluded objects which are hard to see from any viewpoint -
but the HOS scene has more occluded objects - and a much
more challenging manipulation environment to uncover the
occlusions.
B. CNABU Implementation Details
Each CNABU implements a preprocessing step to encode
actions and observations in a representation aligned to the
map grid. For the observation CNABU o(, at, ot), we first
project the observation occupancies and semantics into an
aligned map space using the chosen viewpoint (Free Cell Obs.,
Occ. Cell Obs., and Semantic Obs. in Fig. 3). We then learn
o o(, Project(at, ot)). For the manipulation CNABU,
the robot trajectory at is projected into an aligned map space
that approximates the robots swept volume. To calculate
the robots swept volume, we approximate the robots end-
effector and last 2 links with simple convex shapes (triangular
prism and boxes) and subdivide the robots path, marking
all voxels within those primitives as being part of the swept
(swept volume in Fig. 3). Additionally, the trajectory start
and end points are encoded in 2D binary masks (Start Point
and End Point maps in Fig. 3). Ultimately, we learn m
m(, RobotOccupancy(at(ts)), RobotOccupancy(at(te))).
architectures
Georgakis et al. , with the exception that the output
heads are set to be posterior networks. Our network structure
can be seen in Fig. 3 The occupancy head in both architectures
is a 1x1 convolution, while the differences head in the push
TABLE II: Summary of features of all considered baselines
Baseline Name
Action Selection
Sec. IV-C
Ours wo Pushing
Random  Observation CNABU
Random Push Every 5th
Random every 5 steps
Random Push Every 5th, No Push CNABU
Random every 5 Steps
Random View Random Push Every 5th
Random every 5 steps
Saturation Pushing
Random upon VPP saturation
Dengler et al.
Push classifier network upon saturation
prediction network is a series of Convolutional  ReLU
BatchNorm layers followed by a 1x1 convolution. The
semantic head is similar to the differences head. The 2D
projection block merely takes a horizontal slice of the
occupancy tensor at a fixed height, in our case, 3cm above the
shelf. Their losses and training are described in Sec. IV-G.
We used BEVFusions approach of feeding the voxel heights
as additional channels to 2D Res-UNets for processing the
voxel grids as inputs [27, 24].
The networks are trained using backpropagation in PyTorch
, with grid search-optimized learning rates and ADAM
loss. The dataset for training o consists of 30.000 randomly
sampled scenes, while the dataset for training m consists of
11.700 pushes. Both datasets were split into train, validation
and test splits at a ratio of 0.8:0.1:0.1. Dataset generation
details are discussed in Sec. A of the appendix. For added
robustness in real-world scenarios, we augment the simulation
data with salt-and-pepper noise, random rotations and transla-
tions and add Gaussian noise to the depth images.
C. Individual CNABU Performance Evaluation
To evaluate the performance of the trained CNABUs, we
use the unseen test set of the dataset used for their training.
To evaluate the observation CNABU o, we select, for each of
the scenes, 10 viewpoints to observe at random and obtain the
beliefs at each time step, comparing them against the ground
truth map. For evaluating the performance of the manipulation
CNABU. m, we also choose 10 viewpoints at random and
obtain the pre-manipulation beliefs at every time step, and then
obtain the predicted belief after the manipulation is executed.
This evaluates the performance of the manipulation CNABU
at different reconstruction steps. For each network, we report
their mean Intersection over Union and their mean Expected
Calibration Error (mECE)  for both the semantic and occu-
pancy beliefs in Fig. 12. The mIoU serves as a measure of the
correctness of the predicitons, while the mECE measures the
confidence calibration of these predictions, i.e., how well the
predicted confidences align with actual network performance.
Note how after few observations, both networks achieve off-
the-shelf reasonable calibration and accuracy, which improves
as more views are obtained. It is worth noting that the calibra-
tion of the manipulation CNABU with very few observations
is low because there is no guarantee that one or two random
views would have captured the area being manipulated when
predicting the belief after the push.
Number of Observations
(a) mECEs
Number of Observations
Semantic (Manipulation CNABU)
Occupancy (Manipulation CNABU)
Semantic (Observation CNABU)
Occupancy (Observation CNABU)
(b) mIoUs
Fig. 12: mECEs and mIoUs for the CNABUs on their respective test sets.
D. Validating Visual Information Gain
Our proposed planner relies on the assumption that the
Volumetric Occlusion-aware Information Gain heuristic, which
was developed to estimate information gain in traditional
occupancy grid maps, translates well to maps that are no longer
fully independent, but predicted via CNABUs. We validate this
assumption with the following experiment. Consider the pure
Viewpoint Planning task, i.e., we must survey the environment
without manipulating it, which is a submodular optimization.
Consider now a greedy clairvoyant oracle policy, which, at
every time step, has access to all possible observations that
could be taken and selects the one that leads to the largest
information gain. More formally, this policy is defined by:
vclairvoyant
where H denotes the entropy of the maps. We compare our
agent without pushing to this privileged information agent
in the high-occlusion set of scenes and report the resulting
mean map occupancy entropies in Fig. 13. Note how our
method which relies on the Occlusion-aware information gain
heuristic closely tracks the performance of the privileged
clairvoyant oracle in terms of map entropy reduction. While
more extensive studies are encouraged, this suggests that this
heuristic still performs well, even when the maps are being
predicted via CNABUs.
Observations
Mean Voxel Entropies (Nats)
clairvoyant
Fig. 13: Evaluation of the occlusion-aware information gain heuristic against
a privileged clairvoyant view selection policy
E. Prompts used for Semantic Segmentation
To perform semantic segmentation of the SAM2 masks, we
use the embeddings of the following text prompts to classify
the segments:
tomato can -tomato or kidney beans round tin can
chewing gum -small round chewing gum box
ohio cookies - ohio cookie box in purple cardboard
carton cookies,
mustard bottle - yellow frenchys mustard bottle,
coffee can - maxwell house coffee can with blue wrap-
gelatin box - light pink gelatin box,
cheez-it box - cheezeit cracker box in dark red color,
pringles can - pringles chips tube cyllinder red or green,
in red color or green color bottles with transparent lid,
glass cleaner - glass cleaner spray plastic bottle,
baking powder - koop mans baking powder box,
pasta box - Big blue carton of pasta collezione
Cornflakes - kolln schoko cronflakes in yellow brown
cardboard box cereal
milk carton - milk box tetrapak blue label voll milch,
shelf - wooden shelf board,cream colored wooden
shelf of light cream color
black - just black
In this case, both shelf and black were used as syn-
onymous of the background class, capturing different failure
cases of SAM2 segmentation.
F. Summary of baseline features
We summarize the considered baselines in Tab. II to facili-
tate comparing their features.
G. Physical Experiments Object Set
Fig. 14 shows the objects which were used to create the
test scenes during our real world experiments. Note how
there are multiple shapes for objects of the same class, like
Fig. 14: The set of objects used during the real world experiments
pringles cans, milk cartons and cans. They also differ from
the geometries used during training.
