=== PDF文件: Users and Wizards in Conversations How WoZ Interface Choices Define Human-Robot Interactions.pdf ===
=== 时间: 2025-07-22 10:00:41.167495 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Users and Wizards in Conversations:
How WoZ Interface Choices Define Human-Robot
Interactions
Ekaterina Torubarova
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
ekatorkth.se
Jura Miniota
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
jurakth.se
Andre Pereira
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
atapkth.se
AbstractIn this paper, we investigated how the choice of
a Wizard-of-Oz (WoZ) interface affects communication with a
robot from both the users and the wizards perspective. In
a conversational setting, we used three WoZ interfaces with
varying levels of dialogue input and output restrictions: a) a
restricted perception GUI that showed fixed-view video and ASR
transcripts and let the wizard trigger pre-scripted utterances and
gestures; b) an unrestricted perception GUI that added real-time
audio from the participant and the robot c) a VR telepresence
interface that streamed immersive stereo video and audio to the
wizard and forwarded the wizards spontaneous speech, gaze and
facial expressions to the robot. We found that the interaction
mediated by the VR interface was preferred by users in terms of
robot features and perceived social presence. For the wizards, the
VR condition turned out to be the most demanding but elicited a
higher social connection with the users. VR interface also induced
the most connected interaction in terms of inter-speaker gaps and
and the largest silences. Given these results, we argue for more
WoZ studies using telepresence interfaces. These studies better
reflect the robots of tomorrow and offer a promising path to
automation based on naturalistic contextualized verbal and non-
verbal behavioral data.
Index TermsVR, Wizard-of-Oz, teleoperation, social robotics
I. INTRODUCTION
In the field of Human-Robot Interaction (HRI), Wizard-of-
Oz (WoZ) interfaces where a human operator replaces part
or the entire decision process of a system, are commonplace
, especially in scenarios involving complex social decision-
making or language exchange. Many HRI studies employ
WoZ settings, with humans controlling one or more robot
functions . Yet, the wizards data from these studies, rich
in potential, is often discarded post-study, however, this data
can be invaluable for different applications. Immersive WoZ
interfaces offer a unique opportunity in this regard as they
allow for the collection of rich, context-specific data that can
be used in refining behaviors and decision-making processes.
In this paper, we delve into a comparative analysis of
three different WoZ interfaces, each offering varying degrees
of input and output capabilities that will result in varying
levels of flexibility and control: a) a restricted perception
An example of the Meta Quest Pro tracking the operators facial
expression and displaying them on the Furhat robot in real-time.
graphical user interface (GUI) receiving a fixed-view video
stream and automatic speech recognition (ASR) feedback
from the participants, with pre-written utterances and gestures
delivered via synthetic robot voice; b) an unrestricted per-
ception GUI receiving fixed-view video, and real-time audio
from the participant, offering the same output; and c) a VR
interface receiving immersive video and real-time audio, with
unrestricted spontaneous natural speech and gestures. Our
experiment involved 25 users and 5 novice wizards engaging in
5-minute human-robot conversations centered around ethical
dilemmas. Each wizard, controlling the robot via each of the
three interfaces, engaged in three conversations with 5 unique
users in a within-subjects design, resulting in 15 interactions
per wizard (75 in total).
We hypothesize that these interface variations will influence
both the communication efficiency and users preferences
for the robot. In particular, we expect that the interaction
mediated by the immersive VR interface will foster deeper
engagement and perceived social connection, enabling more
natural conversations due to enhanced turn-taking and non-
verbal expression capabilities. Additionally, by assessing the
workload and habituation process across the three interfaces of
the wizards who do not have prior experience operating robots,
we hypothesize the VR interface to provide the most intuitive
control resembling real-life human-human interaction.
The unique contributions of this paper include 1) testing and
sharing a novel VR WoZ interface compared to more common
GUI interfaces; 2) analysis of both users and wizards data,
lacking in prior studies on WoZ methodologies; 3) in-depth
qualitative assessment of novice wizards experience of ac-
quiring different interfaces. Our findings aim to guide future
decisions on WoZ interface design, whether for targeted HRI
studies or paving the path for automation.
II. RELATED WORK
A. WoZ in Conversational Systems and HRI
Multiple studies investigated the intricacies of the WoZ
approach in tasks requiring verbal communication with the
user due to its complexity and sensitivity . A foundational
paper delved into the potential of the WoZ technique to
emulate interactive technologies, with a special emphasis on
speech input and output systems . More than 30 years later,
implementing believable verbal communication still remains a
Scarecrows - quick and capable, but requiring cautiousness
and transparency .
In verbal communication, there is a great demand for the
wizard to rapidly select and input open-domain content. The
study by  tackled the problem of a single wizard load
by proposing a cooperative multi-wizard platform Wizundry,
where the wizards could split speech-based tasks in real-
by  proposed to address this challenge through voice
puppetry. Their system leverages a neural TTS system in
near real-time, allowing natural speech input to determine
the output for a specific synthetic target voice. With a high
demand on wizard, in an extended conversation, turn-taking
timing when producing a behavior is particularly important:
response latency in everyday communication is reported to be
universally as short as 200 ms . While a significant line of
research is dedicated to modeling turn-taking in conversational
study by  compared their prototypical WoZ dialogue corpus
to face-to-face (FTF) dialogue. They found longer speaker
switch time in WoZ dialogues and more overlapped speech
in FTF dialogues, suggesting tighter turn coordination, but no
difference in speech duration between speakers. Delayed turn-
taking has also been consistently linked to lower engagement
. Thus, we consider it valuable to investigate both the
turn-taking dynamics of WoZ dialogues and the subsequent
impressions of end users and wizards.
Another early study argued that computer dialogue inter-
faces shouldnt merely replicate human-human interactions.
inherent to human-computer interactions . Building on
this foundation, a more recent exploration into the dynamics
between humans and various communicative agents, such as
unique challenges each agent presents . This underscores
the significance of robust WoZ simulations specialized for
HRI. A preliminary study with a humanoid robot  observed
that wizards experience difficulties in four different sub-
exploration into WoZ dialogues  further scrutinized the
challenges of emulating natural human dialogues using a
Graphical User Interface (GUI) WoZ approach. While the
interactions were enjoyable for the users, this study highlights
the difficulties of fully incorporating an operators intricate
interaction tactics into dialogue systems with limited dialogue
options. A recent study  examined how the balance
between robotic features and wizards features affects the
perception of teleoperated robots in a supermarket setting.
This study examined degrees of social presence by varying
the robots interaction modes: with or without a wizards
face and with or without conversion to a robotic voice. The
in-between condition, with the wizards face and converted
robotic voice, was perceived as more trustworthy and was
able to maintain longer interactions with customers. This study
highlights the delicate balance between anthropomorphizing
robots and maintaining their inherent robotic nature for opti-
mal user engagement in social interaction scenarios.
B. Virtual Reality (VR) WoZ for Immersive Robot Telepresence
In the domain of Immersive Robot Telepresence (IRT),
the design and efficiency of remote control interfaces play a
crucial role in determining operator performance for remotely
controlled robots. Multiple studies showed that VR-mediated
interfaces can allow high immersivity and efficient, intuitive
task execution. An insightful study compared three remote
control interfaces for mobile inspection robots, particularly
emphasizing the potential of VR devices . The primary
control interface incorporated a VR headset, data gloves for
enhanced gripper control, joystick-based movement controls,
and a motion tracking system to gauge head orientation and
hand positions. The results showed that VR-enhanced inter-
faces significantly boosted operator productivity, heightened
spatial presence, and improved distance evaluation. Notably,
such VR interfaces also reduced the adaptation time for oper-
and comfort levels. In , an operator wore a VR headset and
used handheld controllers to control a Pepper robot by simply
moving their body, allowing for intuitive embodied acting
and efficient robot control with first-person view. The authors
claimed their system could reduce wizard response times
and produce more contextually apt robot behaviors. However,
the system did face challenges such as motion sickness and
lagging movements caused by the slow servos present in
the Pepper robot. They also underscored the need for future
integration of eye-tracking technology, which would elevate
the realism and responsiveness of the robot interaction in social
contexts. Similarly,  utilized a VR headset connected to
the robots camera perspective, coupled with a Leap Motion
Controller that offered the operator a hands-free interaction
medium to control the Pepper robot. An augmented reality
Interface
Input Type
Output Type
Restricted Perception GUI
fixed-view video
ASR transcriptions
pre-written speech
(synthesized voice)
built-in gestures
Unrestricted Perception GUI  fixed-view video
real-time audio
pre-written speech
(synthesized voice)
built-in gestures
VR Telepresence Interface
immersive video
real-time audio
unrestricted speech
(natural voice)
spontaneous gestures
COMPARISON OF THE INPUT AND OUTPUT OF EACH INTERFACE.
interface was introduced to facilitate the control of a robots
verbal and nonverbal behaviors in . The results showed
that when a single operator controlled both types of behavior
that require tight coordination, it yielded better task results and
provided a heightened sense of presence compared to when
the task was split between two operators (one for verbal and
another for nonverbal behavior). The study also suggested that
this interface, which harnesses full-body motion control for
the robot while also enabling verbal interactions, presented a
powerful tool for WoZ interactions in human-robot scenarios,
as well as for data collection to inform the development of
autonomous multimodal dialogue systems.
C. WoZ for Data Collection or Automation
WoZ interfaces provide valuable data for learning models of
social behavior and have been used to automate robot actions.
where robots learn a behavior policy from WoZ demonstra-
tions. This study showed that robots that learned from prior
interactions successfully engaged children in play. In ,
the authors used WoZ interactions with constrained wizard
laborative robot tutor. Supervised Progressively Autonomous
Robot Competencies was proposed in , , enabling
robots to learn online from wizard guidance and become
more autonomous. Another study  used an LSTM model
on WoZ data to identify conversation shift moments, finding
that model performance varied across contexts, highlighting
the importance of tailoring WoZ data collection. The authors
of  explored human-to-robot conversation transition for
language learning, showing high accuracy in wizard response
automation. While our paper doesnt focus on automation, the
proposed telepresence system can facilitate the collection of
more naturalistic data for future automation endeavors.
III. SYSTEM
A. Social Robot
In this study, we used the humanoid robotic head Furhat
2.7.0 , capable of advanced speech synthesis and speech
recognition (Fig. 2). The robots face is back-projected on
the plastic mask, allowing for different appearances, be-
lievable eye-gaze and realistic facial movements (for lip-
synchronization and timely expressions that reveal emotional
Fig. 2. Furhat robot: 1) built-in static camera; 2) built-in speaker; 3) stereo
camera; 4) external speaker.
states such as sadness and excitement). The motors in the
robots neck allow for head movements such as nods or
headshakes. Additionally, it is equipped with a large voice
B. Graphical User Interfaces
For the GUI-based interfaces, we used the built-in web
interface for wizarding the Furhat robot (see Fig. 3). By
the fixed-view robots camera (Fig. 3.1) and a dialogue history
component that displays the utterances of both interlocutors,
with the user speech recognized by the built-in ASR (Fig.
3.2). We employed this default interface as the Restricted GUI
condition. For the Unrestricted GUI condition, we used the
same system but added a real-time audio stream from the
We also created two sections of buttons corresponding to
the robots utterances: dynamic (Fig. 3.4) and static (Fig. 3.5).
The dynamic section, in which the options changed based
on the conversation state, had a maximum of four buttons
displayed at any given time. In contrast, the static buttons
remained consistently visible on the screen, with ten buttons in
their statement, 3) ask the participant to repeat, 4) mention
another viewpoint, 5) agree, 6) disagree, 7) give an uncertain
Each static button had multiple utterance options to ensure
variety. This design ensured that pressing the same button
repeatedly would not produce repetitive utterances. However,
we maintained a sequential order for the different utterances
linked to each static button to ensure a more consistent
experience across users. In addition to the dialogue buttons,
by default, the Furhat interface includes a typing box for
producing a custom utterance (Fig. 3.3). The wizards were
informed that they could use this option during the interaction
but they should avoid long delays caused by typing.
C. VR Telepresence Wizard Interface
To implement the VR Interface condition, we created a
telepresence platform for the Furhat robot using the Meta
Quest Pro1, a VR headset that has built-in eye and face
Fig. 3. WoZ GUI in the middle of the conversation: 1) robots camera view;
2) dialogue history; 3) typing box; 4) dynamic buttons; 5) static buttons.
tracking. The facial expressions, head, and eye movements of
the wizard wearing the headset were collected from the headset
and displayed on the Furhat robots mask in real-time (Fig.
1). We used Meta platforms Movement SDK for Unity2 and
a Furhat CSharp interface inside the Unity3 editor to integrate
the VR headset with the Furhat robot to create a VR-mediated
WoZ interface. The full open-source version of our code can
be found in Github repository4.
While Furhat is equipped with a built-in camera at its base,
it was not suitable for the VR setup. It was the fixed-view
camera used in the initial two conditions, thus not allowing
for a full immersion for the wizard. Instead, we used a stereo
camera inserted into the robots fur hat (Fig. 2.3). This camera
was located slightly above the eye level of the robot but
oriented in a way that the wizard could keep the head straight
and maintain eye contact with the user. It also allowed the
wizard to move their head freely, maintaining a first-person
view. The camera was directly connected to the computer to
reduce lag and displayed the video to the operator in real time.
(Fig. 2.4) instead of the built-in speaker (Fig. 2.2) which can
only be used for Furhats speech synthesis.
D. Ethical Dilemmas Conversation Scenario
As a conversational scenario, we chose three ethical dilem-
mas that we expected to provide an engaging 5-minute con-
versation. The dilemmas chosen for this study were shared by
the Stockholm Museum of Technology holding an exhibition
on AI and society prior to running this study. These dilemmas
were chosen for their relevance and ease of scripting, enabling
participants to quickly memorize and navigate dialogues. The
dilemmas were the following: 1) Should we have robot judges
that use machine learning to make decisions? 2) If we invent a
pill that stops aging but only 5 of people can use it, should
it be allowed? 3) Would you take a DNA test before a date if
the other person asks you?
We developed a dialogue script for each of the topics which
was used to generate the dynamic buttons in the GUIs (Fig.
4). In the VR Interface condition, the wizards were instructed
to follow a similar structure of the dialogue but were not
restricted in what they could say. The scenario for each topic
included three issues (e.g. for the dilemma about robot judges:
bias and fairness; accountability; and emotions). Each issue
contained three pros and three cons. Apart from that, the
wizards had the option to introduce the next issue or to
reintroduce the current one again. To script the text of the
1) Imagine an ethical dilemma: [Dilemma Text]. Produce
five main issues of [Question of the Dilemma].
2) Now provide three pros and three cons of [Issue] in this
context.
3) Now write them in a more conversational way in one
short paragraph.
4) Reintroduce [Issue] in three different ways as if the other
person forgot what we were talking about, and we want
to get back on track.
Each paragraph was further tuned to follow the topic more
closely or provide more concrete examples and detailed de-
scriptions. While refining the script, we selected three out of
five issues raised about each topic to keep the conversation
within the intended 5 minutes. To create a fairer comparison
between our conditions we also added facial expressions
available in Furhats gesture library that were synchronized
with the robots speech whenever a button was pressed in
both of our GUIs. For this, we prompted Gemini6 with the
There is a script for what a social robot should say during
a dialogue: [Script]. There is a list of facial expressions and
gestures the robot can perform: BigSmile, Blink, BrowFrown,
the gestures across the script where appropriate.
The result was refined in cases if gestures were too frequent
or too sparse, or if they significantly interrupted the speech
flow. We selected the best-performing language model for each
of these tasks. Although the results were largely satisfactory,
both the text of the script and the gestures were manually
revised.
IV. METHOD
A. Participants
We aimed at a sample size of 5 wizards, each interacting
with 5 unique users across three conditions, resulting in a
total of 75 interactions. The data of one wizard and all
corresponding users was excluded from the analysis due to
technical issues with the VR headset that occurred with 4
users. To reach our intended sample size, we recruited another
wizard and 5 new users. All were recruited through university
channels and a local experiment recruitment portal.
Fig. 4. Dialogue script flow.
In the final analysis, 5 wizards were included (1 male, 4
with robots was medium to low (M  2.2 out of 5, SD  1.09),
as well as average familiarity with VR (M  2.6 out of 5, SD
1.14). 4 out of 5 wizards reported no prior experience with
social robots (one wizard has seen or interacted with Furhat
and Pepper). All wizards had experience in performing arts
(dance, stand-up comedy, theater, public speaking) and were
native or fluent in English. The performing experience criterion
was added to make sure the wizards were comfortable in the
task of performing the robots role. Prior to the experiment,
they were interviewed and informed about the tasks they would
have to do during the experiment. The wizards were rewarded
with a gift card equal to 91 USD for their participation.
As for the users, we analyzed data from 25 participants (11
28.5, SD  11.27). They had a low average familiarity with
robots (M  1.92 out of 5, SD  0.79), and 65.3 reported no
prior experience with social robots. All users were fluent or
native in English. They were rewarded with either a gift card
equal to 9 USD or a cinema ticket for their participation. All
of the participants gave written consent to participate in the
study. According to the Swedish national regulations, ethical
approval for the study was not required because we did not
collect sensitive personal data or conduct physical intervention.
B. Measures
To evaluate the users perception of the robot, we used the
Godspeed questionnaire . To evaluate the social aspects
of the interaction, we used the social presence questionnaire
described in  evaluated in other social HRI settings. The
questionnaire consists of 18 questions assessing six dimen-
fect interdependence. To evaluate the wizards proficiency and
progression experience, we used the NASA-TLX questionnaire
, a common tool for assessing task load; and the System
Usability Scale (SUS) . In the SUS questionnaire, we
omitted questions 1 and 4 because they were not relevant for
the current experiment, since they refer to future recurrent use
of the systems, which we did not intend.
C. Experimental Setup
The experiment was conducted in two adjacent rooms: the
control room and the experimental room. The experimental
room was soundproof with no visual connection to the control
room. The robot was installed in the experimental room, and
users sat in front of the robot at a comfortable distance and at
the same eye level. In the the Restricted GUI and Unrestricted
GUI conditions, the robot used its built-in static camera (Fig.
2.1) and provided tracking software to autonomously attend
to the user with eye and head movements, and had micro-
expressions enabled (e.g. blinking, small head movements). In
the VR Interface condition, both attention and facial expres-
sions were captured from the wizards natural behavior that
was displayed on the robot. The robots appearance and voice
were changed between conditions. Six robot mask textures
were selected, three male and three female, and two male
and female Amazon Polly voices from the built-in voice
library. The order of the topics, interfaces, and robot features
(corresponding to the wizards gender) was randomized for
each user. Video and audio data from the interaction were
recorded with two cameras: one facing the user and another
facing the robot.
D. Procedure
Before starting the experiment, the wizards filled in a
questionnaire that reported on their familiarity with social
robots and VR. Then they had a 1-hour practice to get used
to the interfaces and the topics. They were trained on the
functionalities of each interface and practiced using them with
the experimenter. They were informed that the user would not
know about them controlling the robot and that their task was
to make the robot have a naturalistic and smooth conversation
with the user. They were also asked to get familiar with the
script for each topic.
The wizard then left the room while a user entered and re-
entered when the user was sitting in the experimental room.
This procedure ensured that the wizard and the user did
not see each other before the experiment. Before starting
the interaction, the users also answered how familiar they
were with social robots. Then they could read the topics
they were supposed to discuss and think about their opinion.
the robot about the topic that the robot would suggest for 5
minutes. After 5 minutes, the robot informed the participants
that it was the time to wrap up the discussion. After the
Presence questionnaires regarding that interaction. After that,
they were instructed to have another conversation with the
robot. They were told that the robot would be powered by
a different system which would not have any recollection of
their previous conversation. The same procedure was repeated
for each of the three topics. Before and after each interaction,
the robots face projector was turned off and the user was
told that the robot system was being changed. At the end of
their experiment, the users were asked to answer two open-
ended questions: (1) Did you feel any differences between the
systems? If so, what kind of differences? (2) Did you feel that
any of the systems could be controlled by a human? If so,
which aspects of which system?
For each interaction, the wizards were reminded about the
topic and the interface they would use. They had time to check
the script if they needed to before the interaction. They were
instructed to have a conversation for 5 minutes and finalize
it by saying goodbye to the user when appropriate once the
experimenter signaled to them by a gentle tap on the back that
the time had run out. After each interaction, the wizards filled
in the NASA-TLX questionnaire.
After discussing all three topics and filling out the ques-
fact controlled by a human operator and they could meet the
wizard. At the end of the experiment day, the wizards filled in
a System Usability Scale (SUS) for each of the systems, and
left their open-ended comments about their experience. After
they were done filling in all questionnaires they were asked
to sum up their experience in a semi-structured manner.
V. RESULTS
A. User Questionnaires
1) Godspeed: To examine whether Godspeed scores were
affected by experimental condition, we first calculated a mean
score for each section of the questionnaire as well as the mean
total score for each participant. Then, we fitted a linear mixed-
effects model (LMM) with condition as a fixed effect and
wizard ID and participant ID as random effects to account for
potential variability between wizards and individual users. For
each section as w
