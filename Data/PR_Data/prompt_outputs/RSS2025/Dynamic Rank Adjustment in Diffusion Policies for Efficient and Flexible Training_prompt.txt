=== PDF文件: Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training.pdf ===
=== 时间: 2025-07-22 09:41:49.290284 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Dynamic Rank Adjustment in Diffusion Policies for
Efcient and Flexible Training
Xiatao Sun1, Shuo Yang2, Yinxing Chen1, Francis Fan1, Yiyan (Edgar) Liang2, Daniel Rakita1,
1Yale University
2University of Pennsylvania
Corresponding author
{xiatao.sun, daniel.rakita}yale.edu
AbstractDiffusion policies trained via ofine behavioral
cloning have recently gained traction in robotic motion gener-
ation. While effective, these policies typically require a large
number of trainable parameters. This model size affords pow-
erful representations but also incurs high computational cost
during training. Ideally, it would be benecial to dynamically
adjust the trainable portion as needed, balancing representational
power with computational efciency. For example, while over-
parameterization enables diffusion policies to capture complex
robotic behaviors via ofine behavioral cloning, the increased
computational demand makes online interactive imitation learn-
ing impractical due to longer training time. To address this
the Singular Value Decomposition to enable dynamic rank
adjustment during diffusion policy training. We implement and
demonstrate the benets of this framework in DRIFT-DAgger, an
imitation learning algorithm that can seamlessly slide between
an ofine bootstrapping phase and an online interactive phase.
We perform extensive experiments to better understand the pro-
posed framework, and demonstrate that DRIFT-DAgger achieves
improved sample efciency and faster training with minimal
impact on model performance. The project website is available
I. INTRODUCTION
Diffusion policies have recently emerged as a powerful
paradigm for robotic motion generation, demonstrating im-
pressive performance across various manipulation tasks .
Their strong performance is attributed to overparameterization,
which has been shown to enhance both the generalization and
representational capacity of neural networks . However, this
advantage comes with a signicant drawback, which is the
inefciency of training overparameterized diffusion models
. While the machine learning community has made strides
in addressing this inefciency by leveraging the intrinsic low-
rank structure of neural networks [25, 26], these approaches
primarily focus on ne-tuning pre-trained models [13, 6]. In
making these ne-tuning methods unsuitable.
To exploit the intrinsic low-rank structure, training from
scratch in robotics necessitates a dynamic approach to bal-
ancing the trade-off between overparameterization, which
provides strong representational power, and the efciency
gained from reduced-rank training. Ideally, at the beginning of
capture the general patterns of desired behaviors. As training
improve training efciency, as the policy shifts to incremental
Fig. 1: This paper explores balancing overparameterization
and training efciency in diffusion policies by dynamically
adjusting the frozen and trainable portions of weight matrices.
In the top section of the gure, the learner, trained ofine
via behavior cloning with full-rank training, attempts to insert
the upper drawer box into the container but fails due to
collisions with both the container and the lower drawer box.
In the bottom section, after efcient online adaptation with
reduced trainable ranks, the learner efciently improves its
renement. This capability is particularly valuable in scenarios
like interactive imitation learning (IL) with diffusion policies.
Before the adoption of diffusion policies, interactive IL
methods typically used simple and compact network archi-
tectures as policy backbones. These methods were developed
to address the sample inefciency of behavior cloning (BC)
[41, 49], and often involve an ofine bootstrapping stage
for initial training, followed by an online adaptation stage
where experts provide corrective interventions to rene the
policy. However, directly extending these methods to diffusion
policies is impractical due to their signicantly larger number
of trainable parameters, which is often an order of magnitude
greater than those of compact networks and results in substan-
tially increased training times. This challenge undermines the
feasibility of online interactive IL with diffusion policies.
where a large dataset of demonstrations is collected, and
training occurs in isolation. However, when these policies
tions targeting the challenging trajectories, provide corrective
manner. This process is both inefcient and unintuitive, as the
expert often has limited insight into the trajectories where the
policy struggles and may nd it difcult to reproduce such
challenging scenarios.
To address these limitations, we propose Dynamic Rank-
adjustable DIFfusion Policy Training (DRIFT), a framework
designed to enable dynamic adjustments of the number of
trainable parameters in diffusion policies through reduced-
rank training. The framework introduces two key components,
which are rank modulation that leverages Singular Value De-
composition (SVD)  to adjust the proportion of trainable
and frozen ranks while maintaining the total rank constant,
and rank scheduler that dynamically modulates the number of
trainable ranks during training using a decay function.
To demonstrate and evaluate the effectiveness of DRIFT, we
instantiate and implement it into DRIFT-DAgger, an expert-
gated interactive IL method that incorporates reduced-rank
training. As shown in Fig. 2, DRIFT-DAgger uses low-rank
component to speed up training of diffusion policies. By
freezing a signicant portion of the ranks during online adap-
interactive IL with diffusion policies more practical.
Despite being inspired by existing parameter-efcient ne-
tuning methods, DRIFT-DAgger with rank modulation and
rank scheduler is specically designed for training diffusion
policies from scratch and dynamically adjusting the trainable
components during training. This design enhances stability and
reduces the time for forward passes during each training batch
(VI-D). Additionally, we perform extensive ablation studies
on different variants of rank schedulers (VI-A), and mini-
mum trainable ranks (VI-B). By combining diffusion policies
with online interactive IL, DRIFT-DAgger improves sample
efciency compared to training diffusion policies with BC
(VI-C). We also validate our methods in real-world scenarios
(VII). Finally, we discuss the limitations and implications of
our work (VIII). Our contributions are as follows:
We propose DRIFT, a framework for diffusion policies
that includes rank modulation and rank scheduler as novel
components that exploit the intrinsic low-rank structure of
overparameterized models, balancing training efciency
Expert Action
Learner Action
Learner Policy for
Online Adaptation
Bootstrapped
Learner Policy
Offline Demonstrations
Trainable Rank
Reduction
Observation
Environment
Action to Environment
Gating Function
Fig. 2: DRIFT-DAgger combines ofine policy bootstrapping
with online adaptation. The gating function, following the
nomenclature of HG-DAgger , refers to expert intervention
and demonstration when the learner reaches undesirable states
during online adaptation. Compared to BC, DRIFT-DAgger
reduces the need for expert labeling while maintaining high
performance. The trainable rank reduction accelerates batch
adaptation for large models without sacricing performance.
and model performance.
We instantiate DRIFT into DRIFT-DAgger, an interactive
IL algorithm that combines ofine bootstrapping with
efcient online adaptation, enabling effective integration
of expert feedback during the novice policy training.
We perform extensive experiments to demonstrate that
DRIFT-DAgger improves sample efciency and reduces
training time while achieving comparable performance to
diffusion policies trained with full rank.
We provide open-source implementations in Pytorch for
the DRIFT framework and DRIFT-DAgger algorithm.1
II. BACKGROUND
A. Diffusion Policy Primer
A denoising diffusion probabilistic model (DDPM) [10, 36]
consists of a forward process and a reverse process. In the for-
ward process, Gaussian noise is gradually added to the training
governed by a predened noise schedule, t, which controls
how much noise is added at each step. Mathematically, the
forward process is dened as:
q(x1:T  x0) :
q(xt  xt1),
q(xt  xt1) : N(xt;
1 txt1, tI),
1 dagger
where q(xt  xt1) is a Gaussian distribution with a mean of
1 txt1 and variance t. Intuitively, this step progres-
sively adds noise to x0, such that by the nal step xT , the
data is almost entirely noise.
The reverse process aims to undo this noise, step by step, to
recover the original data x0. This is parameterized by a neural
t. Using this prediction, the reverse process reconstructs the
data from the noisy input xt:
xt1 p(xt1  xt) : N(xt1; k(xt, (xt, t)), 2
where k() computes the mean for the denoised data at step
t 1, and 2
t is a xed variance term.
In the context of robotics, a diffusion policy  adapts
the DDPM framework for visuomotor control by treating
robot actions as x and conditioning the denoising process
on robot observations, such as camera images or sensor data.
noisy action representation xt and the observations as inputs
and predicts the noise to remove. Architectures like U-Nets
or transformers  are commonly used for . Diffusion
policies are typically trained ofine using BC, where the model
learns to mimic expert demonstrations.
B. Ranks in Diffusion Models
The rank of a matrix, dened as the maximum number
of linearly independent rows or columns , is closely tied
to the expressiveness and representational power of a model.
For example, in linear models, the rank of the weight matrix
determines the dimensionality of the feature space that the
model can effectively capture. Weight matrices with low ranks
often correspond to models with limited capacity but faster
slower training .
In the context of a diffusion policy that employs a U-
Net with one-dimensional convolutional blocks as its network
Rmn can be created by reshaping a corresponding wei
