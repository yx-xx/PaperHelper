=== PDF文件: Action Flow Matching for Lifelong Learning.pdf ===
=== 时间: 2025-07-22 09:42:12.749872 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Action Flow Matching
for Continual Robot Learning
Alejandro Murillo-Gonzalez and Lantao Liu
Indiana UniversityBloomington
{almuri, lantao}iu.edu
AbstractContinual learning in robotics seeks systems that can
constantly adapt to changing environments and tasks, mirroring
human adaptability. A key challenge is refining dynamics models,
essential for planning and control, while addressing issues such
as safe adaptation, catastrophic forgetting, outlier management,
data efficiency, and balancing exploration with exploitation all
within task and onboard resource constraints. Towards this goal,
we introduce a generative framework leveraging flow matching
for online robot dynamics model alignment. Rather than execut-
ing actions based on a misaligned model, our approach refines
planned actions to better match with those the robot would take
if its model was well aligned. We find that by transforming
the actions themselves rather than exploring with a misaligned
model as is traditionally done the robot collects informative
data more efficiently, thereby accelerating learning. Moreover,
we validate that the method can handle an evolving and possibly
imperfect model while reducing, if desired, the dependency
on replay buffers or legacy model snapshots. We validate our
approach using two platforms: an unmanned ground vehicle and
a quadrotor. The results highlight the methods adaptability and
strating its potential towards enabling continual robot learning.
I. INTRODUCTION
Humans exhibit a remarkable ability to adapt in dynamic
and uncertain environments, excelling at compensating for
mismatches between expectations and actual outcomes. For
they instinctively adjust their steering and braking actions to
align with the anticipated altered dynamics caused by the
changed road condition. This ability to plan effectively in
the face of discrepancies inspires the robotics community to
tackle a pressing challenge: enabling robots to operate robustly
with potentially misaligned or limited dynamics models, where
misalignment can emerge from simplifying assumptions, new
or unaccounted factors, incomplete or inaccurate data and
parameter uncertainty. This capability is crucial for long-
term autonomy, where unanticipated changes in environmental
can render pre-designed, pre-learned or pre-calibrated models
ineffective.
The dynamics model of a robot encapsulates how the robot
predicts the outcome of its actions based on its state and
control inputs. Accurate dynamics models form the foundation
for planning and control, dictating how a robot predicts the
outcomes of its actions to achieve desired goals. Techniques
such as Linear Quadratic Regulator (LQR) [18, 1] and Model
Predictive Control (MPC) [33, 8, 36] rely critically on the
fidelity of these models to generate and evaluate control
policies. Without a robust and adaptable dynamics model,
even advanced controllers falter, particularly in realistic sce-
narios where the robot must operate in diverse or evolving
circumstances. For instance, modeling the effects of reduced
traction on a slippery surface or compensating for actuator
wear are essential for maintaining safe and effective robot op-
eration. Dynamics models thus represent the link between the
robots perception, prediction, and control, enabling informed
decision-making under uncertainty.
Although the potential for robots to improve their dynamics
models through experiential learning is compelling, achieving
this safely and effectively remains a significant challenge. Fre-
quent model updates risk overfitting to transient phenomena or
acquired knowledge . Additionally, outliers and spurious
correlations in data can compromise the model, undermining
performance and safety. Conversely, overly conservative up-
dating may fail to capture legitimate shifts in dynamics .
Lifelong learning [37, 43, 19] introduces additional complex-
context of ongoing task execution, rather than solely focusing
on model improvement. The need to reconcile these trade-
offs is further complicated by the diversity of environments
a robot may encounter, each presenting unique and evolving
dynamics.
In this paper, we propose a novel method for continual
learning of robot dynamics models that efficiently leverages
the robots latest experiences. Our approach departs from the
conventional approach in current Model-Based Reinforcement
Learning (MBRL)  where exploration is mostly guided via
plans obtained with the latest version of the model [7, 15].
transform planned actions into those that more closely reflect
the robots actual intentdefined as the action the robot would
have selected if an aligned dynamics model was available.
are often suboptimal or even totally wrong. This happens
because the model fails to precisely account for the actual
environmental and dynamical factors influencing the robot.
planned or desirable future states. AFM leverages such signals
and learns a transformation that adjusts the initially planned
Adjust via AFM
Fig. 1: Action Flow Matching for efficient continual learning. The planner uses the latest version of the dynamics model and the desired goal
to select the next action a0
t. AFM transforms a0
t into a1
t whenever the misalignment between the model and the environment is unacceptable.
model inaccuracies, thus increasing the likelihood of reaching
the planned state. In other words, the realigned actions can
be viewed as the output of a calibration process correcting
for the imperfect understanding of system dynamics, ensuring
that the executed actions are more effective and informative
for subsequent learning and control. Figure 1 illustrates AFMs
role while updating a robots dynamics model in an online and
non-episodic manner.
Our key contributions can be summarized as follows:
Efficient Dynamics Space Reduction: The initial model
can be leveraged to reduce the state-action space to a
dynamically feasible region, which would otherwise be
inefficient to capture through random trajectory gener-
ation. Since at this stage we are not assuming access
to a ground-truth model, we can only obtain from the
initial model the reduction of the space to an approximate
subset of feasible regions. The intuition behind this idea
is demonstrated in Figure 3.
Representing the Dynamics Regime without Ground-
Truth Models: By focusing on state evolution patterns,
we can learn to represent different dynamics regimes
without needing an accurate ground-truth model. Here it
is important to highlight a crucial distinctionlearning
state evolution patterns means merely identifying re-
curring patterns of state changes. This is unrelated to
learning the underlying system dynamics which are
unavailable as long as the misalignment persists: we will
capture the dynamics model via continual model updates.
Intent Mapping for Accelerated Learning: Leveraging
flow matching within the reduced dynamics space and
the dynamics regime representations allows us to produce
guiding signals that the agent can exploit. This, in turn,
accelerates learning thanks to the collection of more in-
formative transition information via transformed actions.
II. RELATED WORK
This section reviews related work, starting with continual
dynamics learning for real-time system modeling, followed by
online methods for robotic adaptation. We also briefly touch on
batch-based approaches for robot adaptation, which, although
not continual, provide valuable insights for enhancing robot
performance and versatility.
A. Continual Robot Dynamics Learning
Learning a robots dynamics online to fine tune or correct
previous models has usually relied on strong inductive biases,
which helps reduce the learning complexity but leads to task-
or embodiment-specific methods. For example, Jiahao et al.
learn online the residual dynamics of a drone following a
reference trajectory. In this setting, the drones mass changes
as it operates, for instance because it needs to carry a load not
considered in its original dynamics. Their work builds upon
differential equation methods, used to augment a model
obtained from first principles . To turn KNODE-MPC
into an online method in , the authors relied on a set of
architecturally-equivalent models fine tuned sequentially and
in parallel to the drones operation. The robot then switches
to the most up-to-date weights. However, to train the models,
they require a replay buffer that relies on a window of past
transition information, which requires additional memory re-
quirements and selecting an additional hyperparameter (replay
buffer window size).
Meier et al.  learns the inverse dynamics error for a
robotic arm. They focus on getting the residual terms for
the model of a KUKA lightweight arm in simulation and
real-world evaluation. They leverage Gaussian processes with
an smoothed update rule to achieve controlled parameter
for robustness. Nguyen-Tuong and Peters  find a sparse
representation of the data using a test of linear independence
to get a fixed-size subset of the data that will be used for
training. Then, the subset of data is used with an online kernel
regression method to obtain the robots model. However, note
that this approach requires keeping both a database of the
observed data and the varying subset of training data, which
might be impractical for lifelong learning. The method is
shown to work with a 7 degree-of-freedom Barrett WAM arm.
Sun et al.  focus on dynamics learning in the con-
text of legged locomotion. They conduct evaluations using a
simulated biped and a real-world quadruped. Their approach
involves learning a time-varying, locally linear residual model
along the robots current trajectory, aiming to correct the
prediction errors of the controllers model. In particular, they
use ridge regression to find the residual weights and biases
of the nominal model. Initially, they rely on the nominal
model and only start learning the residual parameter after k
decide when enough data has been collected to be able to
learn a good set of coe
