=== PDF文件: Sense and Sensibility What makes an social robot convincing to high-school students.pdf ===
=== 时间: 2025-07-21 14:45:29.290372 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Sense and Sensibility: What makes a social robot
convincing to high-school students?
Pablo Gonzalez-Oliveras, Olov Engwall, Ali Reza Majlesi
Dept. of Intelligent Systems, KTH Royal Institute of Technology, Stockholm, Sweden
Dept. of Education, Stockholm University, Stockholm, Sweden
pabloolkth.se, engwallkth.se, ali.reza.majlesiedu.su.se
AbstractThis study with 40 high-school students demon-
strates the high influence of a social educational robot on
students decision-making for a set of eight true-false questions
on electric circuits, for which the theory had been covered in the
students courses. The robot argued for the correct answer on
six questions and the wrong on two, and 75 of the students
were persuaded by the robot to perform beyond their expected
when it was wrong. Students with more experience of using large
language models were even more likely to be influenced by the
robots stance  in particular for the two easiest questions on
which the robot was wrong  suggesting that familiarity with AI
can increase susceptibility to misinformation by AI.
We further examined how three different levels of portrayed
robot certainty, displayed using semantics, prosody and facial
on specific questions and how convincing they perceived the robot
to be on these questions. The students aligned with the robots
answers in 94.4 of the cases when the robot was portrayed
as Certain, 82.6 when it was Neutral and 71.4 when it
was Uncertain. The alignment was thus high for all conditions,
highlighting students general susceptibility to accept the robots
lower than in the Certain. Post-test questionnaire answers further
show that students found the robot most convincing when it
was portrayed as Certain. These findings highlight the need for
educational robots to adjust their display of certainty based
on the reliability of the information they convey, to promote
students critical thinking and reduce undue influence.
Index TermsSocial educational robots, AI Trust, Persuasion,
Certainty
I. INTRODUCTION
Educational robots are becoming more common and they
have significant potential in, e.g., STEM (science, technology,
engineering and mathematics) education [46, 69, 17], offering
students realistic and natural interactions, not the least by
employing Large Language Models (LLMs), as demonstrated
in several recent studies [41, 68, 67]. However, it is also well-
known that while the LLMs linguistic proficiency is often
Since robots can exert high informational social influence
[38, 24, 25, 55, 56] and students will align with the robots
views to large extents , the positive as well as negative
effects of learning with a social robot need to be considered:
Students need to use critical thinking to decide if they should
accept the robots propositions . Educators need to un-
derstand which students are more at risk of being misled by
a robot presenting incorrect STEM facts, to provide in-time
support. Developers need to find ways to signal how certain
the robot is about the presented facts to avoid overtrust .
In a prior exploratory study (currently submitted), we found
that Persuasion, i.e., robot arguments for an incorrect solution
to a maths problem, and Prejudice, i.e., students positive
attitudes towards robots and more experience of using LLMs,
influenced a large majority of students to conform with the
robots incorrect solution. On the other hand, taking Pride
in being a strong maths student increased resistance against
incorrect arguments.
The present study follows up on these findings, by systemat-
ically investigating Sense, i.e., the students reliable ability to
judge and decide with soundness, prudence, and intelligence
[Merriam-Webster] if the robots arguments are correct or
incorrect; and Sensibility, i.e., the students awareness of and
responsiveness toward [...] emotion in another [Merriam-
Webster] regarding their responses to the robots arguments
depending on the robots multimodal display of certainty
presented using semantics, prosody and facial signals.
A. Study objectives
To guide this study, we posed the following research questions:
by a social robots correct or incorrect arguments during
a series of truefalse questions?
align with its answers?
(being right or wrong; certain or uncertain) influence
student alignment in later interactions?
self-perception as a learner, or prior experience with
AIaffect the likelihood of students aligning with the
Our expectations, based on our previous exploratory study,
are that (H1) a majority of students will align with the robots
answer even when it is incorrect; (H2) students will be more
prone to follow the robot when it is portrayed as being certain.
(H3) robot argumentation on preceding questions will, to some
students with greater AI experience will align more frequently
with the robots answers, even when they are incorrect.
II. BACKGROUND  RELATED WORK
Like human actors, social robots impact their interaction
partners through informational and normative social influ-
ence [70, 6, 10]. These processes can lead to conformity
or persuasion, which differ in how the target perceives the
sources intent. In conformity, individuals adjust their stance
after being exposed to others opinions, without perceiving
an active attempt to change their minds . In persuasion,
the target typically perceives an intentional effort to influence
their stance, which can involve explicit argumentation and
This study focuses on how informational trust (RQ1) and
the robots persuasive certainty cues (RQ2) affect students
willingness to align with the robots answers, while also
exploring whether prior interactions influence subsequent de-
cisions (RQ3) and whether individual traits like AI experience
moderate these effects (RQ4). The following subsections elab-
orate these constructs.
A. Informational Trust in HRI
Informational trust plays a key role in informational social in-
making decisions under uncertainty . Cognitive dissonance
theory suggests that individuals weigh new information against
existing beliefs, with higher uncertainty leading to increased
reliance on external sources such as robots . This dynamic
is especially relevant in educational HRI, where students often
perceive robots as authoritative sources of knowledge . As
a result, overtrust can lead students to adopt information even
when the robot is clearly wrong [11, 23].
A meta-analysis  found that factors such as reliability,
false alarm rates, and failure rates significantly influence trust
development in HRI. Robots that adapt to a students learning
perceived as more trustworthy [44, 49]. Peer-like, anthropo-
morphic robots, such as Furhat used in this study, further
enhance trust through human-like gestures and responsiveness
[12, 52], particularly when their information is consistent and
accurate .
In this study, students ability to critically assess the robots
answers is shaped by their prior knowledge of the topic. Their
evaluation of the robots correctness, along with their percep-
tion of its confidence, determines the level of informational
trust they place in it, ultimately influencing how susceptible
they are to its argument-based persuasion.
B. Persuasion in HRI
Argument-based persuasion is explained by the Elaboration-
Likelihood Model, which describes how persuasion can in-
fluence attitudes via two distinct routes: the central route,
relying on detailed reasoning, and the peripheral route, de-
pendent on superficial cues like authority or likability . A
2022 systematic review confirmed that while persuasion has
been extensively explored in HRI , most studies focus
on peripheral strategies such as compliance, assertiveness,
and emotional cues [48, 2], with limited attention to logical
argumentation. Among 54 identified studies, only one partially
addressed structured argumentation  and this trend has
persisted in recent years [59, 58]. Studies on persuasion
by educational robots are even scarcer. A systematic review
of 89 studies found only two studies addressing real-world
educational tasks . These studies demonstrated positive
effects on compliance  and conformity , but their de-
signs lacked generalizability beyond experimental academical
contexts. Moreover, the review reaffirmed the heavy focus
on the peripheral route and revealed that approximately two-
thirds of the reviewed studies focused on affective rather than
cognitive outcomes.
C. Challenges to Students Critical Thinking in HRI
Learning gains in educational HRI are often measured
via pre- and post-tests , neglecting students academic
uncertainty and vulnerability of their existing knowledge.
robot confidence cues and learner certainty. This highlights
a gap in studies addressing how robots and students interact
with uncertain or conflicting prior knowledge.
Recent studies have shown the potential of tailored persua-
sive strategies, such as personalized storytelling to align with
users traits , or adjusting assertiveness to suit social con-
texts . However, they also highlight risks like persuasive
assertiveness can undermine trust in robots . These risks
have become more prominent with the recent development of
LLM-driven educational robots that have transformed inter-
actions with learners. They improve feedback and guidance
during problem-solving processes [68, 61, 60], but since these
robots tend to increase students trust in their outputs  and
the factual correctness of LLMs in STEM contexts has been
questioned [34, 3], there is a need for students to critically
evaluate AI-generated information, which they often fail to do
. Techniques such as adaptive feedback, enhanced multi-
modal communication, and transparency in decision-making
can improve improve students assessment of their interaction
with potentially unreliable robots [45, 66, 56, 27].
This study addresses research gaps by investigating how
variations in a robots displayed confidence influence the
outcome of the robots argument-based persuasion. Based
on these constructs, we hypothesize that (H1) informational
trust will lead students to align even when the robot is
persuasive cues, (H3) prior robot behavior might affect future
alignment via informational updating, and (H4) students AI
familiarity and personality traits will moderate these effects.
Fig. 1. Methodological flowchart outlining study phases: diagnostic test, robot
III. METHODOLOGY
We designed an experiment in which Swedish secondary
school students interacted one-on-one with a fully autonomous
educational robot, discussing eight electric circuits to deter-
mine whether statements about each were true or false (see
Fig. 2). The overall procedure is summarized in the method-
ological flowchart (Figure 1), which outlines the sequence
from diagnostic testing to robot interaction and post-session
assessments.
The recruitment of students and the questions were planned
together with two secondary school teachers to ensure that
topics corresponded to material that had been covered in
the students classes. One month before the experiment the
teachers distributed a diagnostic test with 19 three-choice
questions about electric circuits in their classes, respectively
in grade 10, 11 and 12 of a practically oriented electrical
engineering program, and grade 11 and 12 of a theoretical
natural sciences program. The students were unaware that the
diagnostic test was linked to the upcoming experiment.
The diagnostic test had two main objectives: 1) selecting
the eight questions for the experiment and 2) assessing student
subject knowledge, to balance group distribution and evaluate
their performance in the main experiment. An Item Response
Theory analysis of the diagnostic test answers (DA) was then
conducted to assess both student ability and question difficulty,
as input to the experimental design.
A. Item Response Theory
The item response theory (IRT)1 refers to a family of mathe-
matical models that attempt to explain the relationship between
latent traits (unobservable characteristic) and their observed
outcomes. We opted for the 3-Parameter Logistic (3PL) model,
includes a guessing parameter (c 0.25 to prevent over-
fitting), making it particularly appropriate for three-option
multiple-choice tests to account for guessing by low-ability
methodsitem-response-theory
Fig. 2. The eight electric circuit statements with ratio of correct diagnostic
answers (EEasy, MMedium, DDifficult). Robot certainty levels indicated
as Certain (C, pale blue background), Neutral (N, light gray background) and
Uncertain (U, soft yellow background), along with correctness (correct or
wrong). Cohort frames are coloured dark gray (N), yellow (U), and blue (C).
students. The following IRT assumptions were applied: mono-
tonicity (the probability of a correct response increases with
increased knowledge of electric circuits); unidimensionality
(the dominant latent trait, ability, is the driving force for the
observed DA); local independence (separate DA are mutually
independent given a certain level of ability) and invariance
(parameters can be estimated from DA for any sub-group
meeting the conditions).
The analysis was implemented in a two-stage process in
a Python-driven Jupyter Notes environment. The first stage
involved iteratively estimating the Item Characteristic Curves
(ICCs) for each question. This provided metrics for item
difficulty (a), discrimination (b), and guessing (c). Initial
parameters and bounds were set and refined through multiple
runs of the algorithm to achieve optimal values. Akaike In-
formation Criterion (AIC) and Bayesian Information Criterion
(BIC) were used as indicators of model fit, guiding the iterative
parameter adjustment to identify the best model configuration.
In the second stage, the calibrated ICC parameters were
used to estimate each students latent ability and the proba-
bility of a correct response for both DA (for verification) and
experiment (for analysis). In the 3PL model, the probability
that a student with ability  will correctly answer a specific
question is p()  c
1ea(b) . This probability can be used
to predict the students base performance in the experiment
and thus assess the robots influence. DA was lacking for four
students and their ability and probable correctness per question
was instead estimated through a second IRT analysis using the
40 students preliminary answers (PA) in the interaction with
the robot. The validity of these metrics was confirmed through
a correlation analysis (r0.43, p.009) between DA and PA.
B. Robot conditions
As outlined in the research questions, this study investigated
how robot correctness and certainty influence participants
experimental conditions. This section describes these robot
conditions and their validation.
The robot was portrayed as having three levels of certainty,
Uncertain U, Neutral N, and Certain C. These portrayals
were achieved through semantic, prosodic, and facial cues:
adjustments of arguments (Sec III-E), speech rate  (-10
for U and 10 for C relative to N), insertion of pauses
for U, and facial expressions . In the U condition,
the robot used filled and silent pauses, subtle smiles, slow
gaze shifts, head tilts, half-closed eyes, and pursed lips. In
the C condition, it exhibited open smiles, wider eyes, raised
An online validation survey was conducted to ensure that
the robots U, N and C portrayals were perceived as intended.
Fifteen short video clips (less than 8 s each) were created,
representing five distinct contexts (c1c5) combined with the
three certainty levels (U, N, C), showing the robot saying
the utterances in Table I in randomized order. The contexts
consisted of the robot disclosing its answer before knowing
the students response (c1), disclosing while disagreeing (c2)
or agreeing (c3) with the student, reminding the student of
its position (c4), and presenting an argument to support its
answer (c5). The utterances, which also occurred during the
main experiment, were delivered by the robot looking directly
at the camera against a black backdrop, as shown in Fig. 3.
Invitations were sent to 323 MSc, 166 BSc and 57 PhD stu-
dents at a technical university. The survey began by informing
about the purpose and procedures and the participants then
self-assessed their proficiency in Swedish on a 6-point Likert
scale. For the validation, we filtered the 125 responses to only
include subjects reporting medium to high proficiency (the top
three levels). This resulted in 76 participants, of which 15 were
excluded for incomplete responses and 2 for failing to watch
all videos. 59 participants thus rated the robots certainty on a
7-point Likert scale, where 1 indicated Highly Uncertain, 4
in 885 ratings (295 per condition).
Statistical analysis confirmed that the U condition (
2.86,   1.30) was perceived as less certain than the N
A one-way ANOVA revealed significant differences between
conditions (F  285.31, p < 0.001), and post-hoc Tukey tests
confirmed that all pairwise differences were statistically sig-
nificant (p < 0.001), as shown in Figure 3 (right). Cronbachs
0.74 indicated acceptable internal consistency, supporting
the reliability of participants assessments. It can be noted that,
in qualitative terms, the U condition was perceived as only
slightly uncertain, the N condition as slightly more certain
than neutral and the N condition as moderately certain.
C. Subjects, Groups  Cohorts
A modified mixed factorial design was used , with robot
correctness (within subjects, RQ1) and robot certainty (be-
tween and within subjects, RQ23) as factors. The robots
certainty levels were varied across three groups of participants
during the experiment. Group 1 (baseline) always experienced
the robot with neutral certainty (N condition), while Groups
2 and 3 alternately experienced robot uncertainty (U) or
certainty (C) after giving their preliminary answer.
47 subjects registered for the experiment and they were
distributed between groups by means of stratified assignment,
using the characteristics educational program, gender, grade
level and ability as criteria, categorised in canonical combi-
nations to assign students to groups. 40 subjects (31 male, 8
showed up. 28 were from the practical Electrical Engineering
(E) and 12 from the theoretical Natural Sciences (Na) program
with 9, 15 and 16 students respectively from grades 1012.
Due to the no-shows, the distribution became unbalanced:
Group 1: n14 (11 Male, 3 Female; 10 E, 4 Na), DA
performance (completed by 13 subjects): mDA:137.41.9.
Group 2: n11 (8M, 3F; 7E, 4Na), mDA:98.40.97.
Group 3: n15 (12M, 2F, 1NA; 11E, 4Na), mDA:148.91.7.
As the diagnostic score was similar for Groups 2 and 3 but
substantially lower for Group 1, a data analysis is required
controlling for abilities calculated through IRT as a covariate.
Three student cohorts were created, as illustrated in Fig. 2.
Cohort N corresponds to Group 1, exposed to Neutral condi-
tion throughout. Cohort U consists of Group 2 for questions
Distribution of certainty ratings for the three robot conditions (U, N, C),
with significant differences between conditions.
EXPERIMENT-USED ROBOT UTTERANCES CHOSEN FOR THE VALIDATION SURVEY, COVERING FIVE DISTINCT SITUATIONS (c1c5) FOR CONDITIONS N
(NEUTRAL), U (UNCERTAIN) AND C (CERTAIN). . . .  DENOTES PAUSES.
Q14 and Group 3 for Q58, interacting with an Uncertain
robot. Conversely, Cohort C includes Group 3 for questions
Q14 and Group 2 for Q58, interacting with a Certain
robot. Alternating Groups 2 and 3 between Cohorts U and C
served two purposes: first, to create a more natural interaction,
preventing individual students from experiencing a robot that
was always certain or always uncertain; and second, to enable
the investigation of how robot certainty levels on preceding
questions influenced students subsequent responses (RQ3).
D. Electric circuit problems
Based on the IRT analysis, the eight true or false problems
shown in Fig. 2 were selected so that the two halves of the test
would have similar questions (Q15, Q26, Q37, Q48).
The problems were four Conditioned questions of Medium
difficulty (Q1,2,5,6) with varying robot certainty levels and
correct robot arguments, two Easy Deception questions where
the robot provided incorrect answers (Q37) with neutral
tion and with neutral certainty (Q48). Using the diagnostic
answers (DA), we aimed for equidistant difficulty levels, but
as Q48 turned out to be notably more challenging for these
of more similar level than the Difficult (z: 1.67).
E. Robot Arguments for its Answer
For each question, the robot first asked what preliminary
answer the student had chosen (e.g., c1 in Table I) and agreed
or disagreed with this choice (e.g., c2  c3) and then presented
a set of four arguments (e.g., c5). The same four arguments
were used for a given question, but differed in presentation,
as shown in Table II, depending on if the student and robot
disagreed (arguments presented one by one), if they already
were in agreement (arguments grouped in pairs together with
expressions confirming the agreement), and the robot condition
(U, N, C). After presenting the four arguments, the robot
asked the student to give a final answer (e.g., c4 in Table I).
F. Pre- and post-test questionnaires
When signing up for the experiment, the students filled in a
pre-test-questionnaire (cf. the supplementary material) at home
to gather demographic data (gender, age, country of origin,
preferred language of communication, educational program
and grade); and answers to 10 questions based on the student
characteristics questionnaire  focused on the students self-
perception (using a four-point Likert scale) about their ability
to learn in different circumstances; and, on five-point Likert
8 questions from the Big Five Inventory  focused on
in teachers and in persons they like ; 3 questions about
AI attitudes (including frequency of interactions with LLMs,
attitudes towards educational robots in school and expectations
regarding collaborating with a robot on school problems).
After the session, the subjects were guided to an adjacent
room to fill in the post-session questionnaire (cf. the supple-
mentary material) on a tablet. The 11 questions focused on
how much confidence the students had in the robot as an
exercise partner, if the student or the robot prevailed in dissent
electric circuits provided), how the students thought before the
robot convinced them or they convinced it, the extent to which
the robot influenced their thinking and made them change their
mind and on which questions the robot was more and less
convincing (with pictures of the circuits provided).
G. Procedure  Robot Programming
The experiment was carried out in a study room at the stu-
dents school, with each student interacting individually with a
female Furhat robot  (using the Isabel face and the Elin-
Neural voice from Amazon Polly), placed on a table (Fig. 4).
To the right of the robot, a monitor presented the questions
in a web-based GUI with which the students interacted using
a mouse. The system connected the robot and the GUI via
Fig. 4. Experiment setup: Furhat robot (left), microphone (centre), screen with
GUI and mouse (right) and tripod with camera for close-up face recording.
TABLE II
EXAMPLES OF ROBOT ARGUMENTS FOR CONDITIONS N (NEUTRAL), U (UNCERTAIN) AND C (CERTAIN). . . .  DENOTES PAUSES.
a local web server using HTTP protocols, where events and
commands were exchanged in JSON format. This allowed the
robot to be contextually aware of student actions in the GUI,
such as submitting a response, but to foster a natural dialogue,
the robot acted as if it was unaware of the GUI interaction.
The session was recorded using a floor-standing video
camera (corresponding to the view in Fig. 4) and a table-
standing cam (see Fig. 4). The speech recognition results and
GUI interaction data were synchronized and logged to create a
multimodal interaction dataset. One experimenter prepared the
robot and the data logging for each student, gave instructions
on how to initiate the interaction, and then left the room.
After a brief welcome by the robot, the GUI showed the first
question and instructed to think silently before selecting TRUE
or FALSE. The robot then presented its stance (Sec. III-E).
After discussion, the robot prompted students to register their
final answer, with no feedback provided, before proceeding
to the next question. The robots responses to students input
were managed using an intent-based approach of the Furhat
such as adapting to perceived student (dis)agreement captured
via speech recognition and mapped to predefined user intents.
These intents were linked to corresponding states in the
interaction flow. Video sequences of the robots part of the
interaction are available in the supplementary material.
H. Statistical Analysis
Given the likely non-normality of behavioral data, the un-
balanced group sizes, and the correlated nature of repeated
student observations, our primary analyses used Generalized
Linear Mixed Models (GLMMs) to flexibly account for non-
tions of normal residuals. As covered in Sec III-A, IRT
modeling was used to estimate students baseline abilities
and question difficulties, providing a principled way to derive
expected performances without the need for traditional mixed-
effects random intercepts. In peripheral analyses where data
independence could be safely assumedsuch as group com-
parisons based on single aggregate measureswe employed
ANOVA models for their computational efficiency and easier
interpretation. This analytical strategy allowed us to match
model complexity to the nature of each analysis while ensuring
that critical statistical assumptions were respected throughout.
IV. RESULTS
With 40 students interacting with the robot across 8 ques-
the students preliminary answers (PA) and final answers
(FA). The eight questions (see Fig.2 and Sec.III-D) included
two Deception questions (Easy), four Conditioned questions
(Medium), and two Contrast questions (Difficult), correspond-
ing to robot correctness and question difficulty levels as
previously outlined. The main dependent variable, Alignment,
FA resisting the robots influence (A-1) or changing to align
with the robot (A1).
A. General findings on student alignment
Students could agree with the robot either by maintaining the
same PA as the robot (nA0  181) or by changing for their
FA after hearing the robots arguments (nA0  139). Of the
181 instances on which the student and robot agreed on the PA,
they were correct on 146 and wrong on 35. For these 35 events,
the students did not change their answer, despite hearing the
robots incorrect arguments for this answer, which provided
opportunities to detect errors and induce scepticism. Dissent
(A0) occurred 139 times: 45 for the Deception questions
(56.2 out of 80), 55 for the Conditioned (34.38 of 160),
and 39 for the Contrast (48.75 of 80). A logistic regression
model was fitted to assess the influence of question type
on dissent, using Deception (Easy) as reference. The model
revealed that dissent was significantly less likely for Condi-
tioned (Medium) questions than for Deception (  0.898,
p  .001), while no significant difference was found between
Contrast (Difficult) and Deception (  0.301, p  .343).
(Left) Alignment matrix between students (sorted by ability) and
questions. Blue and red cells (border or filled) indicate, respectively, students
aligning and resisting after PA dissent. Cell numbers describe the question-
student pairs contribution to beyond expectation results. (Right) z-score values
per student, showing deviation from expectations for PA (light bars) and FA
(dark), and which students performed beyond expectations (red line and dots).
(Bottom) z-score values per question, indicating deviation from expectations
for PA and FA, and for which questions the students answered beyond
expectations (red line and dots). Bars above zero indicate more correct answers
than expected, below zero less, based on the diagnostic test.
Student alignment as a function of question difficulty: Easy (E)
Students aligned with the robot after 117 of the PA dissents
and resisted in only 22 (11 for Deception, 9 for Conditioned,
and 2 for Contrast questions), as shown in blue and red
(S40), whose PA always agreed with the robots, all other
students aligned at least once (for 25 on at least one Deception
question). 13 students resisted the robots persuasion at least
once (10 for Deception, with one student resisting on both
Q37). An analysis of variance on alignment rates found no
relationship with ability (F0.16, p.696, R20), showing that
the robot influenced students across all ability levels.
The linear regression analysis shown in Fig. 6 indicated
that question difficulty significantly predicts alignment (
1.874, p  .026). A Mixed Linear Model analysis on alignment
across difficulty levels (Easy, Medium, Difficult), controlling
for ability, revealed a significant increase in alignment from
Easy to Difficult questions (-0.174, p.020). There was
no significant difference for Medium questions (-0.100, p
.161), and ability had no significant effect (-0.070, p.865).
It should be noted that E questions are identical to Deception,
and thus also differ in robot correctness.
A Generalized Linear Mixed Model (GLMM) was used
to assess the impact of Deception vs. non-Deception on
and condition as fixed covariates. The model showed no
significant effect (-0.055, p.469) and students did hence
not respond statistically differently when the robot presented
incorrect arguments. We implemented another model exploring
interactions between these factors and Deception, finding that
question difficulty has a substantial effect size (134.91,
SE101.91, p.186) on alignment in Deception, as will be
further explored in Sec. V.
B. Robot Influence on Student Performance
Following this broad overview of student alignment, we ex-
plore to what extent the changes between students preliminary
answers (PA) and final answers (FA) were driven by the
robots influence. Specifically, we measure how much the
deviation from expected outcomes shifted between PA and FA,
taking into account each students ability. The methodology
consisted of several sequential steps: 1. Data compilation into
a unified dataset of correctness probabilities (cp), PA and
FA per student. 2. Expected performance per question was
Comparison of Diagnostic (x-axis) and In-Experiment Performance
(y-axis) on Preliminary (light dots) and Final Answers (dark dots).
calculated using cp values and dispersion metrics, assessing
deviations in PA and FA with p-values. Thus, an 8-item vector
per student captured IRT-derived probabilities of correctness
for each question. 3. Segmented analysis of Deception and
non-Deception to separate negative and positive robot-induced
deviations. 4. Monte Carlo simulations estimated the probabil-
ity of observed PA and FA under the null hypothesis of random
chance. Per student and question, 10,000 simulations were run
based on cp values to generate p-value distributions. 5. Fishers
method for combined p-values synthesized Deception and non-
Deception questions to capture negative and positive perfor-
mance shifts, enhancing statistical power . 6. Deviation
factors were calculated as the difference between PA and FA
p-values from steps 2 and 5, for cases where PA performance
was within expectations (p0.05) and FA performance was
beyond (p<0.05), or where both deviated but FA performance
was more extreme.
The students and questions that contributed the most to
the results being beyond expectations (p<0.05) are indicated
in Fig. 5 (right and bottom). An analysis revealed that 98
out of 117 alignments directly contributed to the deviation
from expected results. While 35 students performed within
expectations in their PA, only seven did so in their FA. Of the
40 students, 36 deviated more in their FA than in their PA,
the exceptions being S12, S17, S18, and S40.
For students S12 and S17, FA deviated less from expec-
tations than PA, but both results were beyond expectations.
Only student S18 performed according to expectations in both
PA and FA and S40s performance remained unchanged (but
beyond expectations) since there was no dissent with the robot.
tween diagnostic and preliminary answers, establishing that the
student group performed within their expected ability shown
by the diagnostic test before hearing the robots input, there
was no significant correlation between diagnostic and final
FA. A thorough and conservative investigation revealed that
75 of the students performed beyond their expected capacity,
above expectations for non-Deception and below expectations
for Deception questions. This should be interpreted as a direct
result of robots influence.
Fig. 8. Alignment for cohorts U, N, C for (Left) Conditioned questions and
(Right) All questions, with significance levels for differences between cohorts.
C. Influence of robot certainty
We performed a series of GLMM analyses to examine the
impact of the robots displayed certainty on student alignment.
We controlled for covariates student Ability and question
sequence of robot condition.
Conditioned questions: The GLMM showed that with
cohort U as reference, cohort C demonstrated significantly
higher alignment (p  0.001), but not cohort N (p.083), as
shown in Fig. 8. This highlights the substantial impact of the
robots expressed certainty on alignment, beyond what could
be attributed to question difficulty.
Deception questions: The model showed no significant ef-
fects of cohorts (U, N, C) or ability on alignment. Coefficients
for cohort and ability are close to zero (p0.05) and group
variance is minimal, indicating low variability between groups.
Since the robot portrayed neutral certainty for all cohorts in
preceding questions did not have a significant effect.
All questions: A Mixed Linear Model regression analysis
was conducted using Alignment as dependent variable for all
139 dissents with Cohort C as reference. Cohort U showed
a marginal effect (-0.131, p.059), suggesting a possible,
although not statistically significant, reduced alignment com-
pared to Cohort C over all questions (i.e., not only the ones
on which the robot was displaying certainty). The effects of
Cohort N were not significant, and as for the co-variates,
alignment was significantly influenced by question difficulty
(  1.794, p  .020) but not student ability.
First half vs. last: A Mixed Linear Model regression anal-
ysis was conducted to examine the effects of robot condition,
question sequence, difficulty, and ability on alignment for the
139 instances of dissent. The question sequence (first vs. last
four questions: p  .907) had no impact on alignment and
interaction terms between condition and question sequence
were non-significant. This suggests that students were not
influenced by the fact that the robot had been uncertain (Group
2) or certain (Group 3) on questions in the first half when they
interacted on questions in the second half.
D. Pre-test questionnaire responses
The analysis of the pre-test questionnaire targeted finding
student characteristics that made them more prone to accept
the robots correct or incorrect arguments. We first performed
single-factor ANOVA on Deception and non-Deception ques-
an above-mean response for each characteristic and those
who had a below-mean. The most striking difference is that
students with more experience of using LLMs aligned 38
more (p0.029) over all questions than students with below-
mean experience. More importantly, when considering only the
Deception questions, students with more experience of LLMs
were significantly more aligning with the robots incorrect
solution (see Table III). The results in Table III further suggest
that extroversion and engagement may play a role in student
alignment with the robot. For non-Deception questions, being
full of energy and considering that efforts lead to success
(which could be interpreted as being open to constructively
interact with the robot on problem-solving) had a positive
supported by non-significant results for being social, 39,
p.065; talking a lot, 37, p.075; and Trying, I can learn
as being positive in difficult situations and being persuasive
(and thus presumably having higher confidence in ones own
stance) had a marginally significant negative effect, as this
lead to, respectively, 31 and 28 less alignment (p.050;
p.086). For Deception questions, being reserved and quiet
(and presumably less prone to follow others lead) had a
positive effect, as these students aligned less with the incorrect
answer (supported by results for being shy -38, p.072).
E. Post-test questionnaire responses
We focus on the students perception of how correct and
convincing the robot was, per questions and robot conditions.
correct to 78 (3.95, 0.87), compared to the true value
of 75. Groups 2 and 3, who interacted with the certain
uncertain robot, rated the robot correctness slightly (and
non-significantly) higher than the control Group 1 (13.75,
24.1, 33.93). The students further responded that the
robot had convinced them in 84 of the dissents (4.225,
0.79), compared to the true ratio of 81 (95 out of
117 cases), with Group 1 perceiving that they had been
convinced slightly more often (14.33, 24.13, 34.14).
The students rating of the extent to which the robot made
them change their mind (3.43, 1.26) and influenced their
TABLE III
CHARACTERISTICS IN THE PRE-TEST RESPONSES FOR WHICH THERE WAS
A SIGNIFICANT DIFFERENCE IN ALIGNMENT BETWEEN STUDENTS WITH
ABOVE- AND BELOW-MEAN RESPONSES IN THE PRETEST FOR EITHER THE
Deception OR NON-Deception QUESTIONS.
Deception
non-Deception
Characteristic
LLM usage
Being reserved
Being quiet
Efforts lead to success
Being full of energy
thinking (3.6, 1.27) was lower, but until a thorough
ethnomethodology conversation analysis is performed, it is not
possible to assess whether this discrepancy should be attributed
to the students not actually being convinced by the robot when
questionnaire indicating that the robot had not always been
correct. The students were, on average, neither certain nor
or they convinced the robot (2.92, 1.51).
Regarding on which questions there had been a dissent
and the robot had been more convincing on, two qualitative
observations can be made (as the measures are the number
and ratio of students mentioning each question, no statistical
test is applicable). Firstly, the ratios of questionmentions
indicate that the students assessed post-test that there had
been a higher degree of dissent on the Deception (r0.38)
than on non-Deception questions (r0.28). Secondly, by
calculating the difference in ratio of mentions between more
convincing and less convincing we find that the robot was
perceived as more convincing when it was
correct (r0.37) than wrong (r-0.19);
certain or neutral (r0.70; 0.60) than uncertain (r0.07).
V. DISCUSSION
Starting from the concept of informational trust (Sec. II-A),
we interpret the results at the event level and longitudinally.
At the event level, each dissent and each robot argument,
constitutes an independent opportunity for alignment or resis-
tance. A correct or incorrect robot argument allows the stu-
dents to evaluate their own position and the robots credibility,
potentially reinforcing their decision to align, strengthening
their resistance or even change their previous alignment. In
117 out of 139 instances where students preliminary answers
(PA) disagreed with the robot, they changed their final answers
(FA) to align with the robot, including 34 times when it was
there was a dissent, confirming H1. This high alignment rate
suggests that uncertainty in their own knowledge led students
to seek guidance from the robot, consistent with informational
trust concepts. The experimental setting, the robots social
contributed to the students uncertainty. Question difficulty
significantly influenced alignment, with greater conformity on
more difficult questions, aligning with prior research . A
stringent analysis demonstrated the robots influence, since
75 of students performed beyond their expected capacity.
Consistent with H2, students were more inclined to follow
the robot when it was portrayed as being certain. This effect
reflects the role of source reliability . When the robot
expressed certainty, students likely perceived it as a knowl-
edgeable and trustworthy source, increasing their willingness
to align. Conversely, expressions of uncertainty reduced align-
students continued to align at a high rate.
Confirming H4, we found that students with more expe-
rience of using LLMs like ChatGPT were more likely to
align with the robots answers, even when the robot was
incorrect. This indicates that prior experience with AI can
enhance the perceived reliability of AI sources, potentially
leading to over-reliance also in AI that are not actually
driven by LLMs [8, 9, 21]. Personality traits may have also
influenced alignment, in that students self-identifying as more
outgoing and energetic were more inclined to align with the
robot. Extrovert individuals may have a higher propensity for
social engagement and be more susceptible to social influence,
affecting their trust in the robot .
Alignment did not differ significantly between Deception
and non-Deception questions. While the flawed robot argu-
ments on Q37 might have provided students with oppor-
tunities to detect inconsistencies, this was not reflected in
statistically significant differences.
As these questions were the easiest on a topic that the
students should master, they should have sounded incorrect
or strange, prompting the students to critically evaluate and
potentially resist the robots influence. The rate of non-
alignment in Deception was in fact doubled compared to
non-Deception (24 vs. 12), and though not statistically
more resistant when the robots arguments are incorrect. The
rate of non-alignment in Deception was numerically higher
than in non-Deception questions (24 vs. 12), though this
difference was not statistically significant. This observation
may warrant further exploration in future studies examining
how prior knowledge interacts with flawed arguments.
This aligns with the Elaboration Likelihood Model ,
which posits that individuals are less likely to be persuaded
by weak or flawed arguments when they are motivated and
able to process the information carefully.
als may adjust their perceptions based on prior experiences
with the source [18, 29, 36]. However, our results did not
support H3, as no significant effects of preceding questions
on student alignment were found. Statistical analyses showed
that students were neither influenced by the robots certainty
on the previous two questions when confronted with the
Deception questions, nor by experiences on the first half when
coming to the second. This suggests students treated each
interaction independently, without adjusting their perceptions
of the robots reliability based on prior behaviour.
mean change between preliminary and final answers on the
Deception questions was the largest for cohort C (-0.58)
and clearly smaller for cohort U (-0.29), with cohort N in
between (-0.40), thus suggesting that it may be worth inves-
tigating carry-over effects on Deception from preceding robot
certainty in future work. Second, we explored the behaviour
of the 39 students who dissented at least once, categorizing
them into Aligners (the 32 who aligned with the robot at their
first dissent) and Non-aligners (the seven, S10, S12, S16, S18,
rate after the first dissent and conducted an independent
samples t-test. The results showed a statistically significant
difference between the groups, t(37)  3.73, p.001. Aligners
(n  32, 0.94, 0.21) had a higher Alignment rate than
Non-aligners (n7, 0.58, 0.30). These findings suggest
that when students resist the robot at the first dissent, they
may perceive the robot as less reliable and increase their self-
ence . The pattern indicates that over time, the students
interaction experiences can influence their susceptibility to
informational influence.
VI. LIMITATIONS  FUTURE WORK
A number of limitations should be considered to accurately
interpret the findings of this study. These can be grouped into
two main areas: limitations related to the condition validation
and those of the main experiment.
Robot condition validation: The use of virtual robot video
clips for condition validation may not fully replicate interac-
tions with the physical robot. Additionally, the initial valida-
tion survey used decontextualized clips, potentially affecting
participants perception of the robots behaviour. To address
these concerns, we conducted a supplementary survey with
HRI experts, evaluating the certainty conveyed by the physical
robot in real interaction settings. The survey used three 8090
second video clips presented in randomized order, each repre-
senting one certainty condition: Uncertain (U), Neutral (N),
and Certain (C). The clips were extracted from experiment
recordings made with the floor-standing video camera (view
corresponding to Fig. 4). Each included three key moments:
the robots answer disclosure, a claim, and a re-check of the
students stance. To focus on the robots behaviour, the video
only showed the robot, with the students speech muted and
shortened. The videos are available as supplementary material.
We invited 47 professors and researchers with practical
experience using the Furhat robot at a technical university
to participate in the survey. Participants self-reported their
Swedish proficiency (6-point Likert scale) and then rated the
robots certainty in the three clips using a 5-point Likert
scale (1  Highly Uncertain, 5  Highly Certain). After
data cleaning (removing 16 incomplete responses and 8 cases
where videos were not fully watched), 23 valid participants
confirmed that U (  2.17,   0.81) was assessed as
significantly less certain than N (  3.91,   0.91) and
C (  4.39,   0.67) (p < 0.001). The difference between
N and C (p  0.183) was not significant.
These results support that the validation of certainty displays
using the virtual robot in short video clips is valid. The
expert evaluations also relieves concerns about demographic
differences between validation and experiment participants.
Caution is nevertheless warranted when interpreting nuanced
differences between conditions.
Main experiment: Some limitations arise from the design
choices of the experiment. The study relies on Furhats abil-
ity to produce realistic facial expressions, which may limit
reproducibility with less expressive robots. In the light of the
finding on the influence of LLM experience, it should be noted
that the present study (out of necessity to be able to control
the Deception questions) did not employ an LLM to drive the
robot. A natural direction for future work would be to test how
student alignment and informational trust is affected if LLMs
are in fact used as the dialogue engine for an educational robot
within STEM. Although our analysis found signs that some
carry-over effects emerged during the interaction (see Sec. V),
the experiments short time frame, set to avoid cognitive over-
robot and longer-term effects remain unexplored. A follow-up
assessment could improve future work by determining whether
the robot had a lasting impact on students knowledge and
whether informational social influence led to internalization
of the new stance .
Other limitations arose from the experiments implemen-
tation. Since the experiment took place at the end of the
academic year, motivation levels may have varied, affecting
overall engagement and effort and reducing variation in ability
levels. Future work should account for potential academic
fatigue.
The Deception questions were, intentionally, easier than the
a large effect size ( 135), future work should either vary
difficulty level for Deception question or compare them with
similar non-Deception questions to better contrast question
difficulty and deception.
No-shows led to an imbalance in group sizes and, more
comparability of conditions. Nonetheless, covariates were con-
trolled for in all analyses.
The gender distribution of participants was uneven (8 fe-
gender could have influenced interactions with the robot .
Although the sample size (n  40) should be adequate for the
performed analyses, increasing it to 80 subjects could better
capture variation in student ability and help mitigate the issue
of gender imbalance.
VII. CONCLUSIONS
Referring back to the title, we found that 30 out of 40 students
showed little sense in assessing the robots arguments for the
easy Deception questions, aligning with the robots wrong
answer on both question, and 9 other students aligned on one
of them. Only one student resisted on both. Further, more
experience of LLMs reduced students critical assessment of
robot arguments. On the other hand, the robot also influenced
the students to perform significantly better than their ability
on non-Deception questions. The students did demonstrate
sensibility in responding to the robots displays of certainty,
with significantly higher alignment with the robots  correct
answer when it was portrayed as being certain than when it
was portrayed as uncertain.
The implications of this study are that educators must
be aware of the high informational trust that students have
in general in educational AI and that influence of AI ex-
perience increases this trust. Developers must also enable
social educational robots with means to signal certainty or
uncertainty in presented facts. As LLMs begin to provide
reliability metrics , robots should make use of both the
voice  and familiar human facial expressions  to
express less certainty for potentially unreliable information to
reduce student susceptibility to misinformation and enhance
critical thinking.
VIII. ETHICAL APPROVAL
The research has undergone ethical review by the Swedish
Ethical Review Authority
IX. FUNDING
This work is supported by the Marcus and Amalia Wal-
lenberg Foundation under grant MAW 2020.0052 and the
Swedish Research Council (VR) under grant 2022-03265.
REFERENCES
Samer Al Moubayed, Jonas Beskow, Gabriel Skantze,
and Bjorn Granstrom. Furhat: a back-projected human-
like robot head for multiparty human-machine interac-
tion. In Cognitive Behavioural Systems: COST 2102 In-
ternational Training School, Dresden, Germany, Febru-
ary 21-26, 2011, Revised Selected Papers, pages 114
130. Springer, 2012.
Sidra Alam, Benjamin Johnston, Jonathan Vitale, and
Mary-Anne Williams.
Would you trust a robot with
your mental health? the interaction of emotion and
logic in persuasive backfiring.
In 2021 30th IEEE
International Conference on Robot  Human Interactive
Communication (RO-MAN), pages 384391, 2021. doi:
Karina Avila, Steffen Steinert, Stefan Ruzika, Jochen
Using ChatGPT for
teaching physics. The Physics Teacher, 62, 09 2024. doi:
Wilma A Bainbridge, Justin W Hart, Elizabeth S Kim,
and Brian Scassellati. The benefits of interactions with
physically present robots over video-displayed agents.
International Journal of Social Robotics, 3:4152, 2011.
Robert Baron, Joseph Vandello, and Bethany Brunsman.
The forgotten variable in conformity research: Impact
of task importance on social influence.
Journal of
Personality and Social Psychology, 71:915927, 11 1996.
Clay Beckner, Peter Racz, Jennifer Hay, Jurgen Brand-
humans but not to humanoid robots in an english past
tense formation task. Journal of Language and Social
Tony Belpaeme, James Kennedy, Aditi Ramachandran,
Brian Scassellati, and Fumihide Tanaka. Social robots
for education: A review.
Science robotics, 3(21):
1126scirobotics.aat5954.
Emily M. Bender, Timnit Gebru, Angelina McMillan-
On the dangers of
stochastic parrots: Can language models be too big? In
Proceedings of the 2021 ACM Conference on Fairness,
Rishi Bommasani et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258,
Jurgen Brandstetter, Peter Racz, Clay Beckner, Ed-
uardo B Sandoval, Jennifer Hay, and Christoph Bartneck.
A peer pressure experiment: Recreation of the asch
conformity experiment with robots. In 2014 IEEERSJ
International Conference on Intelligent Robots and Sys-
Ivar Braten, Helge I Strms, and M Anne Britt. Trust
students construction of meaning within and across
multiple texts. Reading Research Quarterly, 44(1):628,
Cynthia L Breazeal, Anastasia K Ostrowski, Nikhita
older adults. Natl. Acad. Eng. Bridge, 49:2231, 2019.
Vijay Chidambaram, Yueh-Hsuan Chiang, and Bilge
Mutlu. Designing persuasive robots: how robots might
persuade people using vocal and nonverbal cues. In Pro-
ceedings of the seventh annual ACMIEEE international
conference on Human-Robot Interaction, pages 293300,
Robert B Cialdini and Robert B Cialdini. Influence: The
psychology of persuasion, volume 55. Collins New York,
New York, NY, 2007.
Robert B. Cialdini and Noah J. Goldstein.
Social in-
Morton Deutsch and Harold B Gerard.
A study of
normative and informational social influences upon in-
dividual judgment. The journal of abnormal and social
Melissa Donnermann, Philipp Schaper, and Birgit Lu-
Social robots in applied settings: A long-
adaptive
education.
Frontiers
Robotics
robotics-and-aiarticles10.3389frobt.2022.831633.
Leon Festinger.
A theory of social comparison pro-
Human Relations, 7(2):117140, 1954.
Ronald Aylmer Fisher. Statistical Methods for Research
Workers. Oliver and Boyd, Edinburgh, UK, 1925.
Furhat Robotics. Furhat SDK: Developers Guide, 2023.
Available at:
Kate Goddard, Abdul Roudsari, and Jeremy C. Wyatt.
Automation bias: A systematic review of frequency,
effect mediators, and mitigators. Journal of the American
Medical Informatics Association, 19(1):121127, 2012.
Peter A Hancock, Deborah R Billings, Kristin E Schae-
Parasuraman. A meta-analysis of factors affecting trust in
human-robot interaction. Human factors, 53(5):517527,
William Hare.
Credibility and credulity: Monitoring
teachers for trustworthiness.
Journal of Philosophy of
Nicholas Hertz and Eva Wiese. Influence of agent type
and task ambiguity on conformity in social decision
In Proceedings of the human factors and
ergonomics society annual meeting, volume 60, pages
313317. SAGE Publications Sage CA: Los Angeles,
Nicholas Hertz and Eva Wiese. Under pressure: Exam-
ining social conformity with computer and robot groups.
Human factors, 60(8):12071218, 2018.
Oliver P. John, E. M. Donahue, and R. L. Kentle. The
big five inventory  versions 4a and 54.
Technical
Institute of Personality and Social Research., 1991.
Alireza Kamelabad, Olov Engwall, and Gabriel Skantze.
Conformity and trust in multi-party vs. individual human-
robot interaction. In ACM International Conference on
Intelligent Virtual Agents (IVA 24), pages 12, 2024.
Maurits Kaptein, Panos Markopoulos, Boris de Ruyter,
and Emile Aarts.
Can you be persuaded? individual
differences in susceptibility to persuasion.
In Human-
Computer InteractionINTERACT 2009: 12th IFIP TC
13 International Conference, Uppsala, Sweden, August
Harold H. Kelley. Attribution theory in social psychol-
ogy. In David Levine, editor, Nebraska Symposium on
Nebraska Press, Lincoln, NE, 1967.
Herbert C. Kelman. Compliance, identification, and in-
of Conflict Resolution, 2(1):5160, 1958.
James Kennedy, Paul Baxter, and Tony Belpaeme. Com-
paring robot embodiments in a guided discovery learning
interaction with children. International Journal of Social
Ambika Kirkland, Joakim Gustafson, and Eva Szekely.
Pardon my disfluency: The impact of disfluency effects
on the perception of speaker competence and confidence.
In Proceedings of INTERSPEECH, pages 52175221, 08
2023. doi: 10.21437Interspeech.2023-887.
Maximilian
reflected acceptanceinvestigating the negative conse-
quences of chatgpt-assisted problem solving in physics
education. In HHAI 2024: Hybrid Human AI Systems for
the Social Good, pages 199212. IOS Press, 2024.
venga Lozano, Matthias Schweinberger, Yavuz Dinc,
Karina Avila, and Jochen Kuhn. Can ChatGPT support
prospective teachers in physics task development? Phys-
ical Review Physics Education Research, 19, 09 2023.
Harm Lameris, Jaokim Gustafson, and Eva Szekely.
Beyond style: Synthesizing speech with pragmatic func-
tions. In Proceedings of INTERSPEECH, pages 3382
Bibb Latane. The psychology of social impact. Amer-
ican Psychologist, 36(4):343356, 1981. doi: 10.1037
Jieun Lee and Lillie R Albert. Students personality and
susceptibility to persuasion during mathematics group-
in Education, 2(6):1022, 2021.
Baisong Liu, Daniel Tetteroo, and Panos Markopoulos.
A systematic review of experimental work on persuasive
social robots. International Journal of Social Robotics,
Potsawee Manakul, Adian Liusie, and Mark Gales. Self-
tion for generative large language models.
In Houda
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing, pages 90049017, Sin-
Linguistics.
Jura Miniota, Siyang Wang, Jonas Beskow, Joakim
its not what you say, its how you say it. In 2023 32nd
IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN), pages 307314,
Chinmaya Mishra, Rinus Verdonschot, Peter Hagoort,
and Gabriel Skantze.
Real-time emotion generation
in human-robot dialogue using large language models.
Frontiers in Robotics and AI, 10, 2023.
Douglas C. Montgomery. Design and Analysis of Exper-
iments. John Wiley  Sons, Hoboken, NJ, 2017.
Andrzej Nowak, Mikolaj Biesaga, Karolina Ziembowicz,
Tomasz Baran, and Piotr Winkielman. Subjective con-
sistency increases trust.
Scientific reports, 13(1):5657,
Kazuo Okamura and Seiji Yamada.
Adaptive trust
calibration for human-ai collaboration. Plos one, 15(2):
Atte Oksanen, Nina Savela, Rita Latikka, and Aki
Koivula.
Trust toward robots and artificial intelli-
interactions online.
Frontiers in Psychology, 11,
psychologyarticles10.3389fpsyg.2020.568256.
Fan Ouyang and Weiqi Xu. The effects of educational
robotics in stem education: a multilevel meta-analysis.
International Journal of STEM Education, 11, 02 2024.
Raul Benites Paradeda, Maria Jose Ferreira, Carlos Mar-
assertiveness in persuasive human-robot interactions. In
Social Robotics: 12th International Conference, ICSR
ceedings 12, pages 516528. Springer, 2020.
Raul Benites Paradeda, Carlos Martinho, and Ana Paiva.
Persuasion strategies using a social robot in an interactive
storytelling scenario.
In Proceedings of the 8th Inter-
national Conference on Human-Agent Interaction, HAI
for Computing Machinery. ISBN 9781450380546. doi:
Jin-Hwa Park and Eun Kyung Lee.
Influence of pro-
fessor trust, self-directed learning and self-esteem on
satisfaction with major study in nursing students. The
Korean Data  Information Science Society, 29(1):167
Richard E. Petty and John T. Cacioppo.
Communica-
tion and Persuasion: Central and Peripheral Routes to
Attitude Change. Springer-Verlag, New York, NY, 1986.
Richard E Petty, John T Cacioppo, Richard E Petty, and
John T Cacioppo. The elaboration likelihood model of
persuasion. Springer, 1986.
Irene Rae, Leila Takayama, and Bilge Mutlu. In-body
mediated communication. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
Paul Robinette, Wenchen Li, Robert Allen, Ayanna
gency evacuation scenarios. In ACMIEEE international
conference on human-robot interaction, pages 101108,
Melodie Rowbotham and Gerdamarie Schmitz. Devel-
opment and validation of a student self-efficacy scale.
Journal of Nursing  Care, 02, 01 2013. doi: 10.4172
Strohkorb Sebo, and Brian Scassellati. Humans conform
to robots: Disambiguating trust, truth, and conformity.
In Proceedings of the 2018 ACMIEEE International
Conference on Human-Robot Interaction, HRI 18, page
Computing Machinery.
Nicole Salomons, Sarah Strohkorb Sebo, Meiying Qin,
and Brian Scassellati.
A minority of one against a
majority of robots: Robots cause normative and informa-
tional conformity. ACM Transactions on Human-Robot
Interaction (THRI), 10(2):122, 2021.
Shane Saunderson and Goldie Nejat.
It would make
me happy if you used my guess: Comparing robot
persuasive strategies in social humanrobot interaction.
IEEE Robotics and Automation Letters, 4(2):17071714,
Shane Saunderson and Goldie Nejat.
Investigating
strategies for robot persuasion in social humanrobot
interaction.
IEEE Transactions on Cybernetics, 52(1):
Stephen P. Saunderson and Goldie Nejat.
Persuasive
robots should avoid authority: The effects of formal
and real authority on persuasion in human-robot inter-
Science Robotics, 6(58):eabd5186, 2021.
10.1126scirobotics.abd5186. URL
orgdoifull10.1126scirobotics.abd5186.
Steffen Steinert, Karina E Avila, Stefan Ruzika, Jochen
Harnessing large lan-
guage models to enhance self-regulated learning via
formative feedback.
arXiv preprint arXiv:2311.13984,
Francesco Stella, Cosimo Della Santina, and Josie
How can LLMs transform the robotic de-
sign process?
Nature Machine Intelligence, 5(6):
Zakary L. Tormala and Richard E. Petty. What doesnt
kill me makes me stronger: The effects of resisting
persuasion on attitude certainty. Journal of Personality
and Social Psychology, 83(6):12981313, 2002.
Lisa van der Werff, Alison Legood, Finian Buckley, An-
toinette Weibel, and David de Cremer. Trust motivation:
The self-regulatory processes underlying trust decisions.
Organizational Psychology Review, 9(2-3):99123, 2019.
Laura Vincze and Isabella Poggi. I am definitely certain
of this! towards a multimodal repertoire of signals com-
municating a high degree of certainty. In European and
7th Nordic Symposium on Multimodal Communication,
David Gray Widder.
Gender and robots: A literature
review. arXiv preprint arXiv:2206.04716, 2022.
Katie Winkle, Severin Lemaignan, Praminda Caleb-
fective persuasion strategies for socially assistive robots.
In Proceedings of 14th ACMIEEE International Confer-
ence on Human-Robot Interaction, pages 277285, 03
Yang Ye, Hengxu You, and Jing Du. Improved trust in
human-robot collaboration with ChatGPT. IEEE Access,
Ceng Zhang, Junxin Chen, Jiatong Li, Yanhong Peng,
and Zebing Mao.
Large language models for human-
robot interaction: A review. Biomimetic Intelligence and
Baichang Zhong and Liying Xia. A systematic review
on exploring the potential of educational robotics in
mathematics education.
International Journal of Sci-
ence and Mathematics Education, 18, 11 2018.
Joshua Zonca, Anna Fols, and Alessandra Sciutti.
Social influence under uncertainty in interaction with
International Journal of
Social Robotics, 15:249268, 2021.
semanticscholar.orgCorpusID:254876990.
