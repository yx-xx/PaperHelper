=== PDF文件: Learning Getting-Up Policies for Real-World Humanoid Robots.pdf ===
=== 时间: 2025-07-22 09:41:52.857725 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Learning Getting-Up Policies for
Real-World Humanoid Robots
Xialin He1
Runpei Dong1
Zixuan Chen2
Saurabh Gupta1
1University of Illinois Urbana-Champaign
2Simon Fraser University
(a) Getting Up from Supine (i.e., Lying Face Up) Poses
(b) Rolling Over from Prone (i.e., Lying Face Down) Poses
(c) Getting up on Different Terrains
Fig. 1: HUMANUP provides a simple and general two-stage training method for humanoid getting-up tasks, which can be
directly deployed on Unitree G1 humanoid robots . Our policies showcase robust and smooth behavior that can get up from
diverse lying postures (both supine and prone) on varied terrains such as grass slopes and stone tiles.
AbstractAutomatic fall recovery is a crucial prerequisite
before humanoid robots can be reliably deployed. Hand-designing
controllers for getting up is difficult because of the varied
configurations a humanoid can end up in after a fall and the
challenging terrains humanoid robots are expected to operate on.
This paper develops a learning framework to produce controllers
that enable humanoid robots to get up from varying configurations
on varying terrains. Unlike previous successful applications of
learning to humanoid locomotion, the getting-up task involves
complex contact patterns (which necessitates accurately modeling
of the collision geometry) and sparser rewards. We address
these challenges through a two-phase approach that induces a
curriculum. The first stage focuses on discovering a good getting-
up trajectory under minimal constraints on smoothness or speed
torque limits. The second stage then refines the discovered
motions into deployable (i.e. smooth and slow) motions that are
Equal contributions.
robust to variations in initial configuration and terrains. We find
these innovations enable a real-world G1 humanoid robot to get
up from two main situations that we considered: a) lying face up
and b) lying face down, both tested on flat, deformable, slippery
surfaces and slopes (e.g., sloppy grass and snowfield). This is
one of the first successful demonstrations of learned getting-up
policies for human-sized humanoid robots in the real world.
Project page:
I. INTRODUCTION
This paper develops learned controllers that enable a hu-
manoid robot to get up from varied fall configurations on varied
terrains. Humanoid robots are susceptible to falls, and their
reliance on humans for fall recovery hinders their deployment.
environments involving complex terrains and tight workspaces
(i.e. challenging scenarios that are too difficult for wheeled
robots), a humanoid robot may end up in an unpredictable
configuration upon a fall, or may be on an unknown terrain. 26
of the 46 trials at the DARPA Robotics Challenge (DRC) had
a fall, and 25 of these falls required human intervention for
recovery . The DRC identified fall prevention and recovery
as a major topic needing more research. This paper pursues
it and proposes a learning-based framework for learning fall
recovery policies for humanoid robots under varying conditions.
The need for recovering from varied initial conditions makes
it hard to design a fall recovery controller by hand and motivates
the need for learning via trial and error in simulation. Such
learning has produced exciting results in recent years for
locomotion problems involving quadrupeds and humanoids,
e.g. [47, 64]. Motivated by these exciting results, we started
with simply applying the Sim-to-Real (Sim2Real) paradigm
for the getting-up problem. However, we quickly realized that
the getting-up problem is different from typical locomotion
problems in the following three significant ways that made a
naive adaptation of previous work inadequate:
a) Non-periodic behavior. In locomotion, contacts with the
environment happen in structured ways: cyclic left-right
stepping pattern. The getting-up problem doesnt have
such a periodic behavior. The contact sequence necessary
for getting up itself needs to be figured out. This makes
optimization harder and may render phase coupling of left
and right feet commonly used in locomotion ineffective.
b) Richness in contact. Different from locomotion, contacts
necessary for getting up are not limited to just the feet.
Many other parts of the robot are likely already in touch
with the terrain. But more importantly, the robot may find
it useful to employ its body, outside of the feet, to exert
forces upon the environment, in order to get up. Freezing
decoupling the upper body, only coarsely modeling the
upper body for collisions, and using a larger simulation
step size: the typical design choices made in locomotion,
are no longer applicable for the getting up task.
c) Reward sparsity. Designing rewards for getting up is
harder than other locomotion tasks. Velocity tracking
offers a dense reward and feedback on whether the robot
is meaningfully walking forward is available within a few
tens of simulation steps. In contrast, many parts of the
body make negative progress, e.g., the torso first needs to
tilt down for seconds before tilting up to finally get up.
We present HUMANUP, a two-stage reinforcement learning
(RL) training framework that circumvents these issues. Stage I
targets solving the task in easier settings (sparse task rewards
with weak regularization), while Stage II makes the learned
motion deployable (i.e., control should be smooth; velocities
and executed torques should be small; etc). Discovering the
getting-up motion is hard because of sparse and underspecified
rewards. Stage I tackles this hard problem without being limited
by smoothness in motion or speed  torque limits. Tracking a
trajectory is easier as it offers dense rewards. Stage II tackles
this easier problem but does it under strict Sim2Real control
regularization and randomization of terrains and initial poses.
curriculum that progresses from simplified full collision
strong control regularization, and domain randomization.
This amounts to a hard-to-easy curriculum on task difficulty
(Stage I: getting-up task; Stage II: motion tracking), and an
easy-to-hard curriculum on regularization and variability (Stage
We conduct experiments in simulation and the real world
with the G1 platform from Unitree. In the real world, we find
our framework enables the G1 robot to get up from two different
poses (supine, i.e. lying face up, and prone, i.e. lying face down)
across six different terrains. This expands the capability of
the G1 robot: the manufacturer-provided hand-crafted getting-
up controller only successfully gets up from supine poses on
a flat surface without bumps. In simulated experiments, our
framework can successfully learn getting-up policies that work
on varied terrains and varied starting poses.
II. RELATED WORK
We review related works on humanoid control, learning for
humanoid control, and work specifically targeted toward fall
recovery for legged robots.
A. Humanoid Control
Controlling a high degree of freedom humanoid robots has
fascinated researchers for the last several decades. Model-based
(ZMP) principle [35, 65, 74, 76], optimization [4, 14, 45],
and Model Predictive Control (MPC) [12, 15, 22, 79], have
demonstrated remarkable success in fundamental locomotion
tasks like walking, running and jumping. However, these
approaches often struggle to generalize or adapt to novel
environments. In contrast, learning-based approaches have
recently made significant strides, continuously expanding the
generalization capabilities of humanoid locomotion controllers.
1) Learning for humanoid control: Learning in simulation
via reinforcement followed by a sim-to-real transfer has led to
many successful locomotion results for quadrupeds [46, 47]
and humanoids [2, 8, 29, 6264]. This has enabled locomotion
on challenging in-the-wild terrain [28, 62], agile motions
like jumping [48, 81], and even locomotion driven by visual
inputs [50, 83]. Researchers have also expanded the repertoire
of humanoid motions to skillful movements like dancing and
naturalistic walking gaits through use of human mocap or
video data [9, 34, 38, 57]. Some works address locomotion and
manipulation problems for humanoids simultaneously to enable
loco-manipulation controllers in an end-to-end fashion facili-
tated by teleportation [20, 32, 52]. Notably, these tasks mostly
involve contact between the feet and the environment, thus
requiring only limited contact reasoning. How to effectively
develop controllers for more contact-rich tasks like crawling,
and unpredictable contacts between the whole body and the
environment remains under-explored.
Discovery
(a) Stage I: Discovery Policy (RL-based Task Learning)
(b) Stage II: Deployable Policy (RL-based Tracking)
Tracking
Real-World Deployment
Discovered
Trajectory
(c) Sim-to-Real Curriculum
Deployable
Curriculum 1: Collision Mesh
(Stage II)
Curriculum 2: Posture Randomization
Curriculum 3: Control Regularization
Regularization
(Stage I)
(Stage II)
(Stage I)
Regularization
Simplified
(Stage II)
(Stage I)
Canonical
(Stage II)
(Stage I)
(Stage II)
(Stage I)
Environment  Domain
constraints
Fig. 2: HUMANUP system overview. Our getting-up policy (Sec. III-A) is trained in simulation using two-stage RL training,
after which it is directly deployed in the real world. (a) Stage I (Sec. III-B1) learns a discovery policy f that figures out a
getting-up trajectory with minimal deployment constraints. (b) Stage II (Sec. III-B2) converts the trajectory discovered by Stage
I into a policy  that is deployable, robust, and generalizable. This policy  is trained by learning to track a slowed down
version of the discovered trajectory under strong control regularization on varied terrains and from varied initial poses. (c)
The two-stage training induces a curriculum (Sec. III-C). Stage I targets motion discovery in easier settings (simpler collision
learned motion deployable and generalizable.
B. Legged robots fall recovery
Humanoid robots are vulnerable to falls due to under-actuated
control dynamics, high-dimensional states, and unst
