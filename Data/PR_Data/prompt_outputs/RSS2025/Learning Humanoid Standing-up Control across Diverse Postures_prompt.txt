=== PDF文件: Learning Humanoid Standing-up Control across Diverse Postures.pdf ===
=== 时间: 2025-07-22 09:42:22.651064 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Learning Humanoid Standing-up Control across
Diverse Postures
Tao Huang2,1
Junli Ren1,3
Huayi Wang1,2
Zirui Wang1,4
Qingwei Ben1,5
Muning Wen1,2
Xiao Chen1,5
Jianan Li5
Jiangmiao Pang1
1Shanghai AI Laboratory
2Shanghai Jiao Tong University
3The University of Hong Kong
4Zhejiang University
5The Chinese University of Hong Kong
Fig. 1: Overview. (a) Our proposed framework HOST enables the humanoid robot to learn standing-up control via reinforcement learning without prior data,
where the robot can successfully stand up across diverse postures in both laboratory and outdoor environments. (b) HOST also demonstrates strong robustness
to many environmental disturbances, including external forces, stumbling blocks, 12kg payload, and challenging initial postures.
AbstractStanding-up control is crucial for humanoid robots,
with the potential for integration into current locomotion and
loco-manipulation systems, such as fall recovery. Existing ap-
proaches are either limited to simulations that overlook hard-
ware constraints or rely on predefined ground-specific motion
world scenes. To bridge this gap, we present HOST (Humanoid
Standing-up Control), a reinforcement learning framework that
learns standing-up control from scratch, enabling robust sim-
to-real transfer across diverse postures. HOST effectively learns
posture-adaptive motions by leveraging a multi-critic architecture
and curriculum-based training on diverse simulated terrains. To
ensure successful real-world deployment, we constrain the motion
with smoothness regularization and implicit motion speed bound
to alleviate oscillatory and violent motions on physical hardware,
respectively. After simulation-based training, the learned control
policies are directly deployed on the Unitree G1 humanoid robot.
Our experimental results demonstrate that the controllers achieve
range of laboratory and outdoor environments (Fig. 1). Videos
and code are available on our project page.
I. INTRODUCTION
Can humanoid robots stand up from a sofa, walk to a
recent advancements in humanoid robot hardware and control
have enabled significant progress in bipedal locomotion [38,
robots to navigate environment and interact with objects
effectively. However, the fundamental capabilitystanding-
up control [43, 17]remains underexplored. Most existing
systems assume the robots start from a pre-standing posture,
limiting their applicability to many scenes, such as transition-
ing from a seated position or recovering after a loss of balance.
We envision that unlocking this standing-up capability would
broaden the real-world applications of humanoid robots. To
this end, we investigate how humanoid robots can learn to
stand up across diverse postures in real environments.
A classical approach for this control task involves tracking
handcrafted motion trajectories through model-based motion
planning or trajectory optimization [17, 18, 22, 43]. Although
effective in generating motions, these methods require exten-
sive tuning of analytical models and often perform subopti-
mally in real-world settings with external disturbances [29,
23] or inaccurate actuator modeling . Besides, real-time
optimization on the robot makes these methods computa-
tionally intensive, prompting workarounds such as reduced
optimization precision or offload computations to external
machines [34, 8], though both are with practical limitations.
Reinforcement learning (RL) offers an alternative effective
framework for humanoid locomotion and whole-body con-
trol [36, 13, 4, 54], benefiting from minimal modeling assump-
tions. However, compared to these tasks that partially decou-
ple upper- and lower-body dynamics, RL-based standing-up
control involves a highly dynamic and synergistic maneuver
on both halves of the body. This complex maneuver features
time-varying contact points , multi-stage motor skills ,
and precise angular momentum control , making RL ex-
ploration challenging. Although predefined motion trajectories
can guide RL exploration, they are typically limited to ground-
specific postures [35, 36, 52, 12], leaving the scalability to
other postures unclear. Conversely, training RL agents from
scratch with wide explorative strategies on the ground can
lead to violent and abrupt motions that hinder real-world
and wide joint limits. In summary, learning posture-adaptive,
real-world deployable standing-up control with RL remains an
open problem (see Appendix B).
In this work, we address this problem by proposing HOST,
an RL-based framework that learns humanoid standing-up con-
trol across diverse postures from scratch. To enable posture-
adaptive motion beyond the ground, we introduce multiple
terrains for training and a vertical pull force during the initial
stages to facilitate exploration. Given the multiple stages of the
groups independently for a better reward balance. To ensure
real-world deployment, we apply smoothness regularization
and motion speed constraints to mitigate oscillatory and vio-
lent motions. Our control policies, trained in simulation
with domain randomization , can be directly deployed on
the Unitree G1 humanoid robot. The resulting motions, tested
in both laboratory and outdoor environments, demonstrate high
including forces, stumbling blocks, and heavy payloads.
We overview the real-world performance of our controllers
in Fig. 1 and summarize our core contributions as follows:
TABLE I: Comparison with existing methods on standing-up control.
wo Prior
Trajectory
Training
Peng et al.
Yang et al.
Tao et al.
Haarnoja et al.
Gaspard et al.
HOST (ours)
Real-world posture-adaptive motions are well achieved
through our proposed RL-based method, without relying on
predefined trajectories or sim-to-real adaptation techniques.
demonstrated by our learned control policies, even under
challenging external disturbances.
Evaluation protocols are elaborately designed to analyze
standing-up control comprehensively, aiming to guide fu-
ture research and development in this control task.
II. RELATED WORK
A. Learning Humanoid Standing-up Control
Classical approaches to standing-up control rely on tracking
handcrafted motion trajectories through model-based opti-
mization [17, 18, 22, 43]. While effective, these methods are
computationally intensive, sensitive to disturbances [29, 23],
and require precise actuator modeling , limiting their
real-world applicability. In contrast, RL-based methods learn
control policies with minimal modeling assumptions, either
by leveraging predefined motion trajectories to guide explo-
ration [35, 36, 52, 12] or employing exploratory strategies to
learn from scratch . However, none of these methods have
demonstrated real-world standing-up motion across diverse
postures. Our proposed RL framework addresses these limita-
tions by achieving posture adaptivity and real-world deploy-
ability without predefined motions, enabling smooth, stable,
and robust standing-up across a wide range of laboratory and
outdoor environments.
B. Reinforcement Learning for Humanoid Control
Reinforcement learning (RL) has been effectively applied
to various humanoid control tasks, showcasing its versatility
and effectiveness. For example, RL has enabled humanoid
robots to achieve robust locomotion on diverse terrains [38,
motions [35, 36, 13, 14, 4], versatile jumping , and
loco-manipulation [7, 27, 50]. Building on these advances,
we address humanoid standing-up control, a parallel problem
presenting unique challenges due to its dynamic nature and
the need for precise coordination of multi-stage motor skills
and time-varying contact points [17, 29]. We propose a novel
approach that integrates a multi-critic framework, motion
Environment
Multiple
RL Agent
Pulling Force
PD Controller
Rescaler
PD Target
(a) Training HoST in simulation
(b) Real-world deployment
PD Controller
Rescaler
PD Target
Encoders
Constraints
Fig. 2: Framework overview. (a) We train policies in simulation from scratch
with multiple critics and motion constraints operationalized by rewards,
smoothness regularization, and action bound (rescaler). (b) The trained polices
can be directly deployed in the real robot to produce standing-up motions.
C. Learning Quadrupedal Robot Standing-up Control
Standing-up control for quadrupedal robots shares similari-
ties with humanoid robots but faces distinct challenges due to
morphological differences, such as quadrupedal designs. Clas-
sical approaches for quadrupedal robots often rely on model-
based optimization and predefined motion primitives [3, 40],
which work well in controlled environments but struggle with
adaptability to diverse postures and real-world uncertainties.
Recent RL-based methods have enabled quadrupedal robots to
recover from falls and transition between poses [23, 30, 52],
using exploratory learning to manage complex dynamics and
environmental interactions. Our work draws inspiration from
these advances, extending them to humanoid robots by ad-
dressing the unique challenges of bipedal standing-up control.
By incorporating posture adaptivity, motion constraints, and a
structured training curriculum, our framework bridges the gap
between quadrupedal and humanoid robot control, enabling
robust standing-up motions across diverse environments.
III. PROBLEM FORMULATION
We formulate the problem of humanoid standing up as a
Markov decision process (MDP; ) with finite horizon,
which is defined by the tuple M  S, A, T , R, . At each
timestep t, the agent (i.e., the robot) perceives the state st S
from the environment and executes an action at A produced
by its policy (st). The agent then observes a successor
state st1 T (st, at) following the environment transition
function T and receives a reward signal rt R. To solve the
goal learn an optimal policy  that maximizes the expected
cumulative reward (return) E[PT 1
t0 trt] the agent receives
during the whole T-length episode, where  [0, 1] is the
discount factor. The expected return is estimated by a value
function (critic) V. In t
