=== PDF文件: Gain Tuning Is Not What You Need Reward Gain Adaptation for Constrained Locomotion Learning.pdf ===
=== 时间: 2025-07-21 13:48:26.894083 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Gain Tuning Is Not What You Need: Reward Gain
Adaptation for Constrained Locomotion Learning
Arthicha Srisuchinnawong1 and Poramate Manoonpong1,2
1Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand.
2The University of Southern Denmark, Odense, Denmark.
Fig. 1: (a) Parameter trajectories from (white) RL, (black) constrained RL, and (brown) ROGER on a simulated reward
landscape with their (transparent) explorations. Brighter regions indicate higher rewards, while darker regions indicate lower
rewards. The red areas highlight violations with a red dashed line indicating the constraint threshold. RL and constrained RL
consistently violate constraints, possibly during exploration, while ROGER effectively avoids violations. Physical quadruped
locomotion learning from scratch (b) with dynamic load, (c) on slippery terrain, (d) on a loose gravel field, and (e) on a random
step field. A video of this experiment is available at  Piw?sijtzJCpRubbFHx06w.
AbstractExisting robot locomotion learning techniques rely
heavily on the offline selection of proper reward weighting gains
and cannot guarantee constraint satisfaction (i.e., constraint
violation) during training. Thus, this work aims to address
both issues by proposing Reward-Oriented Gains via Embodied
Regulation (ROGER), which adapts reward-weighting gains
online based on penalties received throughout the embodied
interaction process. The ratio between the positive reward
(primary
negative
(penalty)
automatically reduced as the learning approaches the constraint
thresholds to avoid violation. Conversely, the ratio is increased
when learning is in safe states to prioritize performance. With a
60-kg quadruped robot, ROGER achieved near-zero constraint
violation throughout multiple learning trials. It also achieved up
to 50 more primary reward than the equivalent state-of-the-
art techniques. In MuJoCo continuous locomotion benchmarks,
including a single-leg hopper, ROGER exhibited comparable or
up to 100 higher performance and 60 less torque usage
and orientation deviation compared to those trained with the
default reward function. Finally, real-world locomotion learning
of a physical quadruped robot was achieved from scratch within
one hour without any falls. Therefore, this work contributes
to constraint-satisfying real-world continual robot locomotion
learning and simplifies reward weighting gain tuning, potentially
facilitating the development of physical robots and those that
learn in the real world.
I. INTRODUCTION
Robot locomotion is a challenging task involving the
embodied interaction between the robot and the environment
properly
embodied
reinforcement
learning
employed
promising framework, enabling robots to discover effective
policies
technique has demonstrated remarkable success [4, 5, 6].
random exploration and are prone to constraint violation.
Desired characteristics, such as being stable without falling
and staying within physical limits, cannot be guaranteed either
during  or after [6, 8] the learning process (white path
in Figure 1), leading to instability, danger, and degraded
performance .
To deal with this challenge, constrained RL emerges as
a technique to enforce the constraints through mathematical
formulations describing the desired characteristics of robot
behaviors . This technique can be divided into model-
based and model-free approaches. The model-based approach
relies on accurate models for learning, making model-free
more practical. Without an environment model, model-free
constrained RL represents undesired behaviors using penalty
terms (Rit), resulting in the following reward function (Rt):
Rt  0tR0t
where Rt, R0t, and Rit denote the total reward, primary
reward (i.e., the weighted summation of all positive reward
terms), and ith semi-positive definite constraint penalty terms
at time t, while 0t and it denote the corresponding
weighting gains. This approach can be categorized further into
two groups: fixed-weighting, where the gains are pre-tuned and
fixed; and adaptive weighting, where the gains are adapted or
learned.
A. Fixed-Weighting Constrained RL
fixed-weighting
constrained
carefully tuned offline, either empirically or by hyperparameter
employ fixed weighting gains with error-based penalties, e.g.,
a squared error relative to the desired states or a highly
negative penalty given at undesired states to enforce constraint
satisfaction and the desired results. Others model the penalty
terms as control barrier functions (CBFs) [6, 10], providing
high penalties near safety boundaries only to obtain improved
performance. These techniques could include as many as 16
terms with 11 distinct properly selected values , where
adjusting a single hyperparameter may require four additional
training repetitions . Choosing improper values may result
in unnatural motion, undesired gaits, or poor performance ,
making the selection process crucial and time-consuming.
B. Adaptive Weighting Constrained RL
To tackle the issue of fixed-weighting constrained RL,
in adaptive weighting constrained RL, 0t and it are
continuously adjusted throughout the learning. Adaptive
weighting constrained RL can be further divided into two
optimize the network parameters in the primal update while
numerically adjusting the penalty regularization gains in the
dual update, and (2) primal-only approaches [16, 17], which
explicitly compute the penalty regularization gains rather than
using iterative optimization.
One early technique, primal-dual optimization (PDO) ,
maximizes the total reward in the primal update while
simultaneously adjusting the penalty gains in the dual update,
as summarized in Appendix A. This method provides a
foundation for later works like constrained policy optimization
(CPO) , which resets the penalty gains and incorporates
performance with greater computational complexity. Although
a recent quadruped robot trained with CPO managed to
progressively reduce its orientation deviation during learning,
its exploration trials still experienced constraint violations
. Building upon CPO, inertia-point policy optimization
(IPO)  replaces penalty terms with logarithmic CBF;
constraint
violations
according to .
One possible reason for constraint violation during learning
is the delay in adjusting the regularization gains in primal-
dual approaches . In other words, multiple updates
are required to increase a penalty gain from nearly zero
to a high proper value to ensure constraint satisfaction.
To investigate this, constraint-rectified policy optimization
(CRPO) , a primal-only approach, introduces a reward-
switching mechanism, which performs policy updates solely
with the constraint penalties after a certain tolerance is
reported that CRPO outperformed previous techniques, like
CPO and IPO, a quadruped robot trained with CRPO
still experienced around 960 violations per 1,000 episodes;
i.e., tolerance parameter. Following that, according to ,
a variant of CRPO incorporates a distortion-based risk
measure to modify reward distribution, highlighting high-
penalty actions; however, a quadruped robot trained with this
technique still experienced around 300 constraint violations
per 1,000 episodes. Despite simplifying much of the tuning
in Figure 1).
Recent techniques proposed in early 2025 are based on these
approaches. For instance, QRSAC-Lagrangian  performs
dual updates as in PDO with Adam optimizer to handle
shift in value functions and improve stability, Constraint-
Rectified Multi-Objective Policy Optimization (CR-MOPO)
modifies CRPO for multiple constraint objectives along
with incorporating a conflict-averse technique optimized with
natural policy to deal with conflicting objectives, while
uses neural network-based CBFs trained to adjust CBFs for
specific conditions.
Recent techniques proposed in early 2025 build on these
approaches. For instance, QRSAC-Lagrangian  performs
a dual update similar to PDO, using the Adam optimizer
to handle shifts in value functions and improve stability.
Constraint-Rectified
Multi-Objective
Optimization
(CR-MOPO)  extends CRPO to multiple constraint
objectives
incorporates
conflict-averse
technique
optimized with a natural policy gradient to handle conflicting
objectives. Meanwhile,  employs neural network-based
CBFs that are trained to adapt to specific conditions.
C. Current Stage of Real-World Locomotion Learning
Although several constrained RL techniques have been
especially during learning, has yet to be achieved. Only
two robots have demonstrated locomotion learning in the
real world by using simple reward functions with some
workarounds [7, 21]. In one work, a physical hexapod robot
achieved locomotion learning within 20 minutes using a
single-term speed reward function without falling, thanks to
the stability provided by leg redundancy . In another
similar time scale using a reward function combining speed
and orientation plus a fall recovery policy for resetting the
robot . Therefore, when foot contact redundancy cannot be
obtained or the consequence of falling is destructive, safety
constraints defined using certain robot state variables, such
as orientation, must be predefined, included in the reward
Fig. 2: Illustration of how embodied interaction between the
robot and the environment can be used to train a control policy.
The traditional RL loop is shown in gray, and the additional
ROGER loop is shown in blue.
To this end, this work hypothesizes that robot-environment
interaction can be leveraged for both policy updates (RL loop)
and dynamic reward gain adjustments (online weighting gain
adaptation loop), as illustrated in Figure 2, without introducing
additional hyperparameters that require tuning. Building
on this idea, this work proposes Reward-Oriented Gains
via Embodied Regulation (ROGER), dynamically balancing
reward weightings based on proximity to the constraint
thresholds. These weightings are continuously refined through
embodied interaction, where insufficient penalty gains drive
the robot toward unsafe states, prompting stronger penalties
and reducing the primary gain to prevent violations. In
1) Ensuring constraint satisfaction throughout learning
(brown path in Figure 1).
2) Elimination
extensive
utilizing
intuitive hyperparameters, e.g., constraint thresholds.
This paper is organized as follows: Section II describes
ROGER in more detail; Section III presents the experimental
Section IV discusses the results and future potential; and
II. REWARD-ORIENTED GAINS VIA EMBODIED
REGULATION (ROGER)
The core idea of Reward-Oriented Gains via Embodied
Regulation (ROGER) is to ensure safe operation while
maximizing performance by dynamically adjusting the reward
structure through real-time interaction with the environment,
thereby eliminating the need for additional non-intuitive
hyperparameters. When the robot is far from the constraint
gain with low penalty weighting gains to encourage task-
specific optimization. As the robot approaches the constraint
automatically
increases
weighting gains and reduces the primary reward weighting
This process continues iteratively: when penalties reach a
critical point, they steer the robot back into safe regions,
after which the penalty weighting gains decrease, allowing for
improved performance.
multiple-reward-
channel setup , where rewards and penalties are stored
independently before being combined into total advantage
scores for policy updates. This setup enables ROGER to
dynamically adjust the reward and penalty weighting gains
(Equation 1) across different learning episodes and timesteps.
At each timestep, the weighting gains are computed according
it  ritt,
where t denotes the clipped summation of constraint
and rit denotes the constraint contribution ratio of the ith
constraint penalty term. t and rit are computed from
the ratio of the estimated constraint penalties ( Rit) to the
constraint thresholds (i) as:
( Rjtj)2, 1.0
( Riti)2
j( Rjtj)2 ,
where i and j are selected intuitively based on the physical
properties of the system, e.g., maximum hardware limits,
together forming a safe region into which the robot should
be bounded, as shown by the dashed lines in Figure 3, while
Rit is a statistical estimate of the ith penalty at time t:
Rit  average[Rit] kstd[it]. A preliminary result when
varying k is presented in Figure S8b in the Appendix.
This adaptation strategy is also proven to be partially stable
in key conditions (i.e., near the constraint thresholds and
convergence), while the expected primary reward is guaranteed
to increase, as detailed in Appendix B . As a result, if an
optimal solution exists far from the constraint thresholds, such
as body orientation in robot locomotion, ROGER will converge
to that solution; otherwise, it chooses a safe alternative.
since all penalty terms and their gains are zero, i.e., t  0
approaches a constraint threshold, the Rit terms increase,
causing t to grow, reducing 0t and increasing it, thereby
distributing more and more attention to the penalty terms.
This setup prevents the robot from approaching the constraint
thresholds. However, when near a constraint threshold, i.e.,
some Rit i, t saturates at 1.0, making 0t
Fig. 3: This adaptation strategy is also proven to be partially
stable in key conditions (i.e., near the constraint thresholds and
convergence), while the expected primary reward is guaranteed
to increase, as detailed in Appendix B. As a result, if an
optimal solution exists far from the constraint thresholds, such
as body orientation in robot locomotion, ROGER will converge
to that solution; otherwise, it chooses a safe alternative.
and Rt Rit. This setup neglects the primary reward,
immediately enforcing the reduction of the penalty terms and
ensuring constraint satisfaction.
according to the ratio of Rit, as illustrated in Figure 3b.
As the first penalty term (R1t) increases, it receives more
weight and higher optimization priority. However, as other
lowering the priority of the first penalty term to increase
those of the others. Thus, this setup automatically balances the
contributions between different penalty terms online during
locomotion learning. In total, ROGER1 takes an additional
computation time of 0.460.09 ms on an Intel i7 CPU with
an Nvidia GTX1050 GPU, or approximately 0.03 of data
collectionexploration time.
III. EXPERIMENTS AND RESULTS
Two experiments were conducted to investigate and evaluate
ROGER. The first compared ROGER with state-of-the-art
techniques on a 60-kg quadruped robot in simulation, followed
by the demonstration of real-world locomotion learning. The
second further tested ROGER with a MuJoCo hopper, which
violates the assumptions presented in Appendix B: zero-
penalty optimality and gentle system and transition dynamics.
Under this condition, the reward weightings adapted using
ROGER were compared with the default reward weightings
provided by OpenAI Gymnasium and CRPO. The results of
other locomotion learning tasks are provided in Appendix D.
A. Quadruped Locomotion Learning
Unlike previous studies that used small, less sensitive
quadruped robots, the first experiment performs a comparison
platform
prone to instability and falling if improper policies are
learned. To ensure safe and efficient locomotion learning,
1Code is available at  ROGER public.
Fig. 4: Locomotion learning framework for the Unitree B1
quadruped with ROGER. An adaptive neural control produces
joint position targets that are used by low-level controllers.
After execution, the robot receives rewards and penalties,
which are then combined using ROGER and subsequently used
to train the neural control.
a state-of-the-art adaptive neural control framework, called
Sequential
Executor-Adaptive
Gradient-weighting
Online Learning (SME-AGOL) , was adopted, as shown in
Figure 4. This control framework has previously demonstrated
real-world locomotion learning on a physical hexapod robot.
performing exploration in parameter space, e.g., avoiding
noisy exploration as in PPO, and providing interpretability,
e.g., mapping weights directly to a series of corresponding
robot trajectoriesconfigurations.
The SME network is a central pattern generator-based
network. It consists of three layers: (1) a central pattern
generator layer producing rhythmic activity, (2) a triangular
basis layer forming locomotion bases, and (3) an output
layer mapping the bases to eight joint position commands
for the quadruped robot. It should be noted that the hip
abductionadduction joints were fixed, reserving them for
future turning control. The mapping weights were initialized
to zero before being optimized using the AGOL learning rule
as follows:
where  and  denote the updates to the parameters ()
and the exploration standard deviations (), respectively.
and  denote the learning rates. n denotes the length of the
stored trajectories, i.e., 8 episodes  70 timesteps (3 gait
cycles per episode). ai denotes the gradient of the explored
actions.  denotes the explored parameter re-randomized at the
beginning of every episode.
Gi and Gi denote the average
and standard deviation of the return Gi, respectively, serving as
the baseline and normalization gains for advantage estimation.
Taking advantage of the rhythmic nature of locomotion, the
Fig. 5: (a) Final primary reward values obtained from the last training episode and (b-c) roll and pitch angles recorded
throughout the locomotion learning of the simulated Unitree B1 quadruped robot. The robot was trained using six techniques:
two fixed-weighting techniques in red (fixed-gain penalty and fixed-gain CBF), three adaptive weighting techniques in gray
(PDO, CRPO, and OL-AUX), and ROGER in blue. All conditions are presented along with their kernel density estimation.
In (b-c), red dashed lines indicate constraint thresholds at  0.2 rads, or approximately  10; therefore, the data points
exceeding these lines indicate violations. A video of this experiment is available at  T4.
return Gi is computed from the average of the total reward
performed over a horizon of 20 timesteps, or approximately a
gait cycle. Here, ROGER was applied as an add-on mechanism
to the AGOL learning mechanism by adapting its reward
weighting gains.
The total reward function in Equation 1 includes a forward
velocity reward (R0t  vfwd, encouraging forward movement)
and orientation penalties for roll (R1t  t, penalizing
sideways oscillation) and pitch (R2t
longitudinal oscillation). The constraint thresholds, e.g., j in
Equations 4 and 5, are selected as 0.2 rad, or approximately
10. To include 99.9 of the exploration uncertainty in the
k  3. In other state-of-the-art methods, Rit is used as
summarized in Appendix A.
1) Simulated Robot Experiment: The comparison between
ROGER and state-of-the-art constrained RL techniques was
conducted using a Mujoco-based simulation of a 60-kg Unitree
B1 quadruped robot. The state-of-the-art techniques include
two fixed-weighting techniques: fixed-gain penalty (Penalty)
and fixed-gain CBF (qCBF: quadratic CBF and lnCBF:
logarithmic CBF); three adaptive-weighting techniques: PDO,
included here as an additional baseline because it dynamically
adapts the auxiliary weighting gains (e.g., penalty), with the
aim of exploiting the auxiliary terms to maximize the primary
reward. The summarized details of all techniques are provided
in Appendix A.
The key metrics for the evaluation are the final primary
reward (i.e., robot forward speed), the constraint violation
rate throughout the learning process obtained from kernel
density estimation, and the maximum roll and pitch deviations.
For each technique, hyperparameters were selected through a
grid search , with those yielding high rewards and low
constraint violations being selected for comparison. Some of
the preliminary selection results are presented in Appendix C.
The robot was subjected to 500 training episodes under each
Figures 5b and 5c present that, in this case, the fixed
gain penalty is the only fixed-weighting approach capable of
satisfying the constraints throughout learning. When properly
orientation deviation with an estimated violation probability of
training the quadruped robot solely with the primary reward
did not satisfy any constraints: the robot experienced over 30
orientation deviations with an estimated violation probability
of 0.85, both of which were significantly higher than those
of the fixed-gain penalty (p-value 0.01, two-proportion
test). Similarly, CBF-based approaches, whether logarithmic
or quadratic, reduced the estimated violation probability to
0.600.16, with most of the deviation being below 20; yet,
the numbers were still significantly higher than those of the
fixed gain penalty (p-value 0.01, two-proportion test).
Fig. 6: Evolution of the main reward term across 500 learning
episodes from the locomotion learning of the simulated
Unitree B1 quadruped robot trained with (dark red) only the
primary reward term, (light red) fixed gain penalty, (blue)
Figures 5b and 5c also reveal that CRPO and ROGER,
Fig. 7: (Left) Evolution of the primary weighting gain (0t) and body roll penalty weighting gain (1t) across 50 timesteps
and 500 episodes, obtained from the locomotion learning of a simulated quadruped robot. (Top right) Graphical illustration
showing the robot body from the back view, with the values amplified 30 times for visualization purposes. (Middle right) Body
roll values collected over eight previous episodes (in red) along with their 99 confidence interval (in gray). (Bottom right)
Evolution of the weighting gains: 0t in blue and 1t in red.
which recompute weighting gains for each iteration, are
the only two adaptive weighting approaches capable of
satisfying constraints throughout learning. CRPO had zero
violations in 25,000 timesteps, while ROGER merely exhibited
one in 50,000 timesteps (without any falls; the violation
was caused by exploration, as no violation was observed
during reruns without exploration). This corresponds to an
estimated violation probability of around 2  1012. In
and an estimated violation probability of 0.02, significantly
higher than those from CRPO and ROGER (p-value
0.01, two-proportion test). Interestingly, OL-AUX, designed to
maximize the increase in the primary reward, exhibited similar
20deviations with a lower estimated violation probability
of 0.00061.6 compared to PDO (p-value 0.01, two-
proportion test). Nevertheless, the deviation and estimated
violation probability of OL-AUX were still significantly
greater than those of CRPO and ROGER (p-value 0.01,
two-proportion test).
Among the three techniques with fewer than three violations
in over 25,000 training timesteps presented in Figure 5,
ROGER achieved the highest final primary reward. The
primary rewards reached 0.1 ms and 0.3 ms after 50 and
250 learning episodes, respectively, before the final value of
0.6 ms at 500 episodes, as shown in Figure 6. This final value
of 0.6 ms was 50 greater than those of the fixed gain penalty
and CRPO, which were 0.3 ms (p-value 0.01, t-test).
that achieved when using the primary reward solely (p-value
0.54, t-test), but with near-zero constraint violation. This
highlights that ROGER can autonomously and properly adapt
and balance different reward weighting gains while ensuring
constraint satisfaction.
ROGER in more detail, revealing that the primary weighting
gain 0t and roll penalty weighting gain 1t are dynamically
adjusted based on the estimated body roll (gray lines), both
across and within episodes. When the robot tilts to one side,
0t decreases and 1t increases to prevent tilting at that
specific stage. Conversely, when the robot is stable and its
body roll exhibits minor deviations, 0t increases and 1t
decreases to prioritize the primary reward at that stage. Given
the rhythmic nature of robot locomotion in this case, where the
robot walks for two gait cycles in 50 timesteps, the weighting
gains exhibit two periodic repetitions, offering insights into
the learning objectives across different stages.
2) Physical Robot Experiment: According to the results
of the simulation experiment, ROGER was then tested on a
60-kg physical quadruped robot. The robot was trained for
300 learning episodes with all mapping weights initialized as
zero and the reward and penalty estimated from the outputs
of an Intel RealSense T265 camera installed at the front.
Training started from a standing posturei.e., the home
configurationwhich has a high center of mass and is prone
to falling. A wheeled 15-kg support structure with slack ropes
was employed as a safety mechanism. The support was set
up such that it did not assist in maintaining the stability of
the robot during learning. However, it added complexity to
the task, as the robot had to drag the structure while learning
from an imperfect pose estimation. Due to the limited testing
allow the experimenter to pause the main program and reorient
the robot after it went near the testing space boundaries. In
lasted almost one hour.
Figure 8 presents the results of physical locomotion
learning. The robot successfully completed all five trials
without falling, demonstrating the capability of ROGER to
train a heavy quadruped robot in real-world conditions. By
episode 50 (10 minutes), the robot had achieved a forward
Fig. 8: (a) Snapshots capturing the locomotion learning of a physical robot after 50 and 250 episodes. (b, top) Forward speed
(i.e., primary reward), robot roll, robot pitch, (b, bottom) the reward weighting gains, and their min-max ranges, recorded
throughout 300 learning episodes. The gray solid line indicates zero forward speed, while the red dashed lines indicate the
constraint thresholds. A video of this experiment is available at
speed of 0.1 ms, with roll and pitch angles remaining below
3. By episode 250 (40 minutes), the average forward
speed had increased to 0.3 ms, matching the performance
observed in simulation experiments and potentially suggesting
a minimal sim-to-real learning performance gap.
robot effectively maintained its roll and pitch angles below
the constraint threshold of 10throughout 300 episodes, as
shown in the top row of Figure 8b. This stability was achieved
through the dynamic adjustment of reward weighting gains.
In general, the orientation deviation increased slightly as
the robot learned, reducing the primary weighting gain and
increasing the roll and pitch penalty weighting gains, as shown
in the bottom row of Figure 8b. As shown in the video (https:
youtu.beF1olq7W6J9g), the robot learned relatively short
steps with low foot lift while walking on flat terrain, a strategy
that helps maintain stability under continuous exploration.
Longer steps and higher foot lifts, while potentially more
such stochastic movements, as shown in
cZ5qOw0i T4. Nevertheless, Figure 1 illustrates that a variety
of complex gaits emerged under more challenging conditions,
supporting that physical interaction with the environment
shapes the resulting behavior .
Apart from the long-term trend, specific adjustments in the
weighting gains were also observed throughout the learning
in Figure 8b. For example, an interesting event occurred
around episode 220 (2:36 minutes in the video; https:
youtu.beF1olq7W6J9g), when the robot started oscillating in
both roll and pitch angles. In response, ROGER lowered the
primary weighting gain and increased the penalty weighting
spent the next five episodes adjusting its locomotion pattern to
reduce the oscillation. This is reflected in a slight decrease in
forward speed, accompanied by roll and pitch values changing
toward zero shortly after 220 episodes. After 3:00 minutes, the
robot had successfully reached a stable gait, demonstrating
the autonomous balancing process of the primary reward and
constraint penalty terms in real time.
To further investigate this mechanism, the robot was trained
under four challenging conditions, as shown in Figure 1(be):
(1) a step field with step heights varying between 4 and 10
cm; (2) a gravel field with gravel diameters ranging from 2
to 7 cm; (3) a slippery surface, created by applying machine
lubricant to a whiteboard (static friction coefficient 0.25);
and (4) dynamic loading, involving a 1.5-kg bottle of water
placed at the front, two 1.5-kg bottles at the rear, and a football
sacktogether comprising nearly 10 of the robots weight.
Under dynamic loading, the robot successfully maintained
balance while walking forward. On the slippery surface, initial
difficulty in maintaining ground contact was observed, but
the robot eventually learned to slide forward while preserving
body posture. On the loose gravel terrain, the robot leaned
slightly forward and learned to kick the top-layer gravel to
clear a path. In the step field, the terrains rigidity restricted
both foot placement and terrain manipulation, initially leading
to unsuccessful attempts. To deal with this, training was
initialized with a trotting-in-place gait featuring increased
foot lifting. As a result, the robot developed a hopping-
like gait to overcome the steps. Notably, all these behaviors
emerged without the use of any exteroceptive terrain sensing,
suggesting that feasible control policies should be reachable
within the exploration while learning should satisfy constraints
throughout.
B. Hopper Locomotion Learning
This experiment extends the evaluation of ROGER to further
investigate a condition of assumption violations. A simulated
MuJoCo hopper was used, which requires a highly dynamic
hopping gait and is more sensitive than a quadruped robot.
Due to being feedback-dependent, which cannot currently be
achieved with the SME architecture , the hopper was
instead controlled by a three-hidden-layer neural network with
256 hyperbolic tangent neurons per layer, trained using a
standard PPO algorithm , as shown in Figure 4. The
experiment followed the default OpenAI Gymnasium setup,
except that the reward was decomposed into multiple channels
as required by ROGER. The rewardpenalty terms included
forward velocity plus a healthy reward as the primary reward,
an absolute torque penalty to penalize torque usage, and an
absolute body orientation penalty to encourage an upright
body. The torque penalty represents a non-zero constraint in
the optimal solution (i.e., zero-penalty optimality violated),
while the orientation penalty represents a sensitive constraint
(i.e., gentle system and transition dynamics violated). The
penalty thresholds were set at 1  1.0 (maximum value) for
torque and 2 10for orientation. Other hyperparameters,
e.g., those of the neural network and learning algorithm, were
empirically selected such that the robot could complete the
task with the default reward function.
Three testing conditions were evaluated: (1) the baseline
reward functions with fixed weighting gains obtained directly
from the OpenAI Gymnasium, i.e., default; (2) CRPO; and (3)
ROGER. Each condition was repeated for over 20 trials, with
each trial lasting one million timesteps and 2048 timesteps per
episode. Four performance metrics were employed: hopping
distance (i.e., related to the main reward), absolute joint
violations.
Figure 9 demonstrates that overall, ROGER significantly
outperformed
achieving
deviations. Specifically, ROGER achieved a hopping distance
of 6.6 m, representing a 20 improvement over the default
reward function (p-value < 0.01, t-test), which was similar
to that of CRPO. With ROGER, the average torque usage
was 0.33 Nm, representing a 57 reduction from the default
reward function (p-value < 0.01, t-test) and a 58 reduction
from CRPO (p-value < 0.01, t-test). Similarly, the average
orientation deviation was 2, representing a 59 decrease
compared to the default reward function (p-value < 0.01,
t-test) and 53 decrease compared to CRPO. In total, the
percentages of constraint violation obtained with ROGER
were lower than those with the default reward function and
CRPO both throughout the learning process and in the last
10 episodes, as summarized in Appendix D. However, unlike
the quadruped experiment, while ROGER ensures that the
expected value of constraints is satisfied, the absence of falls
was not guaranteed due to assumption violations (i.e., having
sensitive system dynamics). Nevertheless, it offers improved
Fig. 9: Snapshots, hopping distance, joint torque usage,
and absolute orientation deviation, obtained from a MuJoCo
hopper trained using the default reward function from OpenAI
experiment is available at
performance and a simpler tuning process.
Figure 9 further reveals the relationship between the
three rewardpenalty channels and the underlying mechanism
of ROGER. During the first 50,000 timesteps, the hopper
focused on reducing torque usage and orientation penalties, as
indicated by their steady decline. Once these terms had been
sufficiently minimized, the hopper shifted its focus toward
maximizing the primary reward, resulting in a slight delay
in primary reward maximization. Subsequently, the hopper
showed a continuous performance increase while maintaining
low torque usage and minimal orientation deviation.
Figure S10 in Appendix D suggests that, since ROGER
utilizes
embodied
interaction
environment to adapt reward weighting gains, the results
may be environment-specific. The torque usage obtained
from ROGER, i.e., with only the primary reward and
torque penalty, mirrored that obtained from the three-channel
obtained from the default reward function. Conversely, the
orientation deviation obtained from ROGER, i.e., with only
the primary reward and orientation penalty, matched that
of the three-channel ROGER, with the torque usage being
similar to that obtained from the default reward function.
stricter constraints, with the constraint thresholds being
halved to (1, 2)  (0.5, 5), as shown in Figure S11 in
Appendix D. In this scenario, the hopper initially prioritized
the reduction of constraint penalties before maximizing the
primary reward. Although this stricter configuration delayed
reward optimization, the hopper maintained forward hopping
without moving backward, i.e., exhibiting a negative hopping
distance. These results reveal that given sufficient learning,
ROGER can achieve comparable performance to finely tuned
reward functions with constraint satisfaction.
IV. CONCLUSION
locomotion
learning
commonplace in the real world due to certain requirements,
such as avoiding falling while learning in the case of
quadruped robots . Current state-of-the-art RL techniques
often experience constraint violations in quadruped robots,
especially during learning [8, 14], while selecting proper
reward weighting gains is time-consuming  and risky,
as shown in Appendix C. Therefore, this work introduces
Reward-Oriented Gains via Embodied Regulation (ROGER),
a simple rule that adjusts reward weighting gains and balances
different objectives through the dynamic interaction between
the robot and the environment. ROGER increases the penalty
gains and decreases the primary weighting gain as the robot
approaches the constraint thresholds, continuously enforcing
constraint satisfaction. Conversely, it increasingly relaxes the
constraints to prioritize primary reward maximization as the
robot moves away from the constraint thresholds. Unlike most
state-of-the-art techniques, which heavily rely on simulation-
based training [10, 12, 13, 16], ROGER strictly enforces
conditions (e.g., Figure S11 in Appendix D) but is crucial
for real-world learning where violations are catastrophic.
constraint thresholds (i; acceptable values) and uncertainty
levels (k, e.g., k  3 for 99.9 confidence), making the
setting of unrealistic constraints, e.g., 1body deviation,
unlikely in practice.
ROGER demonstrates its effectiveness in both theory and
practice. Theoretically, constraint satisfaction is achieved
through partial stability under key conditions: near the
constraint thresholds and when converged, while ensuring an
increase in the primary reward, as detailed in Appendix B.
Although ROGER is designed around three key assumptions
zero-penalty optimality, gentle system dynamics, and gentle
learning dynamicsit still performs well even when these
assumptions are violated. First, despite the torque constraint
are less stable than the quadruped robot, ROGER outperforms
state-of-the-art methods under sensitive system dynamics,
although fall prevention isnt guaranteed (see Appendix D).
quadruped robot, which has gentle system dynamics compared
to the hopper and walker2D, reveals that sensitive learning
dynamics can lead to failures, as constraint satisfaction cannot
be guaranteed (Figure S8a in Appendix C).
exhibited
near-zero
violation
throughout the multiple-seed locomotion learning of a heavy
quadruped robot (i.e., five times heavier than Unitree A1
), which is comparable to carefully tuned fixed weighting
gains and state-of-the-art CRPO , but with a higher
performance. Furthermore, the technique can be applied to
MuJoCo continuous locomotion learning tasks both in terms of
the primary reward (e.g., distance), constraints where optimal
values are expected to be non-zero (e.g., torque usage), and
constraints where optimal values are expected to be near zero
(e.g., orientation stability). Finally, ROGER offers stable real-
world locomotion learning on a quadruped robot, both on
regular terrain and under various challenging conditions, in
less than an hour, with the robot neither falling nor requiring
extensive tuning, even with limited training data (i.e., 500
timesteps per update, or rather 0.5 of that in ) and
without exteroceptive terrain sensing.
In summary, this work highlights how embodied interaction
can dynamically adapt reward weighting gains in real time,
achieving near-optimal performance across all tests presented
here while minimizing the need for reward weighting gain
tuning. By enabling continuous improvement and simplifying
development in terms of efficient continual robot learning in
the real world, especially when parameter selection is high-
stakes or resource-intensive. Thus, if constraint violations are
acceptable during learning, simulation-based methods may be
preferable; for real-world continual learning or fine-tuning,
ROGER may be a better choice.
V. LIMITATION
within robot systems that satisfy zero-penalty optimality as
well as gentle system and learning dynamics. It does not
account for exteroceptive constraints like global positioning,
which can accumulate over time. These limitations, including
real-world quadruped locomotion learning on complex terrain
with exteroceptive terrain sensing , will be addressed in
future works.
ACKNOWLEDGMENTS
This work is intended for research and academic purposes.
The authors would like to thank Chaicharn Akkawutvanich
for valuable better-late-than-never post-submission discussions
and comments, Run Janna for assisting in fine-tuning the
low-level control, and Kanut Tarapongnivat for suggesting the
dynamic load experiment. We also appreciate the reviewers
and area chair for their thoughtful feedback and constructive
suggestions.
REFERENCES
Riccardo Manzotti.
Embodied AI beyond embodied
cognition and enactivism. Philosophies, 4(3):39, 2019.
Rolf Pfeifer and Josh Bongard. How the body shapes
the way we think: a new view of intelligence. MIT press,
Richard S Sutton, Andrew G Barto, et al. Reinforcement
learning. Journal of Cognitive Neuroscience, 11(1):126
David Hoeller, Nikita Rudin, Dhionis Sako, and Marco
Hutter. Anymal parkour: Learning agile navigation for
quadrupedal robots.
Science Robotics, 9(88):eadi7566,
Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen,
and Pulkit Agrawal. Rapid locomotion via reinforcement
learning. The International Journal of Robotics Research,
Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok
and Jemin Hwangbo.
Not only rewards but also
IEEE Transactions on Robotics, 2024.
Demonstrating a walk in the park: Learning to walk
in 20 minutes with model-free reinforcement learning.
Joonho Lee, Lukas Schroth, Victor Klemm, Marko
Alexander
Exploring constrained reinforcement learning algorithms
quadrupedal
locomotion.
International Conference on Intelligent Robots and
Systems (IROS 2024), 2024.
Shangding Gu, Long Yang, Yali Du, Guang Chen,
Florian Walter, Jun Wang, and Alois Knoll. A review
of safe reinforcement learning: Methods, theories and
applications. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2024.
Thanh Long Vu, Sayak Mukherjee, Renke Huang,
function-based
reinforcement learning for emergency control of power
systems. In 2021 60th IEEE Conference on Decision and
Control (CDC), pages 36523657. IEEE, 2021.
Theresa Eimer, Marius Lindauer, and Roberta Raileanu.
Hyperparameters in reinforcement learning and how to
tune them.
In International Conference on Machine
Santiago Paternain, Miguel Calvo-Fullana, Luiz FO
Safe policies for
reinforcement learning via primal-dual methods. IEEE
Transactions on Automatic Control, 68(3):13211336,
Joshua Achiam, David Held, Aviv Tamar, and Pieter
Constrained
optimization.
International Conference on Machine Learning, pages
Dohyeong Kim, Kyungjae Lee, and Songhwai Oh. Trust
region-based safe distributional reinforcement learning
for multiple constraints. Advances in Neural Information
Processing Systems, 36, 2024.
Yongshuai Liu, Jiaxin Ding, and Xin Liu.
Interior-point
optimization
constraints.
In Proceedings of the AAAI conference on Artificial
Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo:
A new approach for safe reinforcement learning with
convergence guarantee. In International Conference on
Machine Learning, pages 1148011491. PMLR, 2021.
James Queeney and Mouhacine Benosman.
averse model uncertainty for distributionally robust safe
reinforcement learning. Advances in Neural Information
Processing Systems, 36, 2024.
Masaya Kinoshita, and Kazumi Aoyama.
Constraints
as rewards: Reinforcement learning for robots without
reward functions.
arXiv preprint arXiv:2501.04228,
Shangding Gu, Bilgehan Sel, Yuhao Ding, Lu Wang,
Qingwei Lin, Alois Knoll, and Ming Jin.
Safe and
reinforcement learning.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2025.
Weishu Zhan, Zheng Liang, Hongyu Song, and Wei
Safe distributed learning-enhanced predictive
control for multiple quadrupedal robots. arXiv preprint
Arthicha Srisuchinnawong and Poramate Manoonpong.
An interpretable neural control network with adaptable
online learning for sample efficient robot locomotion
learning.
IEEE Transactions on Neural Networks and
Learning Systems, pages 113, 2025.
Harm Van Seijen, Mehdi Fatemi, Joshua Romoff,
Romain Laroche, Tavian Barnes, and Jeffrey Tsang.
Hybrid reward architecture for reinforcement learning.
Advances in Neural Information Processing Systems, 30,
Aleksandr Mikhailovich Lyapunov. The general problem
of the stability of motion.
International Journal of
Xingyu Lin, Harjatin Baweja, George Kantor, and
David Held.
Adaptive auxiliary task weighting for
reinforcement learning. Advances in Neural Information
Processing Systems, 32, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Nikita Rudin, David Hoeller, Philipp Reist, and Marco
Learning to walk in minutes using massively
parallel deep reinforcement learning. In Conference on
Robot Learning, pages 91100. PMLR, 2022.
Arthicha Srisuchinnawong and Poramate Manoonpong.
Growable and interpretable neural control with online
continual learning for autonomous lifelong locomotion
learning machines. The International Journal of Robotics
APPENDIX
A. Comparison Table
Table S1 presents the fundamental concept underlying
different model-free constrained RL techniques, where xi
denotes an observationstate variable, Rit denotes a penalty
denotes the constraint threshold, i denotes the tolerance, [x]
denotes the positive clipping function that yields an output in
the range [0, ],  denotes the learning rate of the penalty
weighting gains, and E[Rit] denotes the gradient of the
expected values of Rit with respect to the network parameters.
B. Mathematical Analysis
This study assumes that there exists only a pre-normalized
constraint penalty term, that the optimal reward occurs when
R1t 0 (i.e., zero-penalty optimality), and that R1t represents
the estimated constraint penalty that is mostly in conflict with
the primary reward (0 Rit 1; 1  1). The total reward
can now be expressed as:
Rt  R0t R0tR2
1) Stability Analysis: To present the stability of ROGER,
this work adopts a Lyapunov-based approach . Consider
the Lyapunov candidate function (R1t), which measures the
systems deviation from equilibrium:
Given that R1t is a squared term, V (R1t) is positive semi-
derivative is:
V (R1t)  R1t R1t.
Since the learning rule maximizes Rt, it follows that Rt
R0t (2R0t  3R1t) R1t R1t,
and thus,
V (R1t)  R0t,
where   (1 R2
1t)(2R0t  3R1t).
This analysis can be divided into four key conditions.
1) Near the constraint boundary: When R1t 1,  0
and V (R1t) 0, this indicates that ROGER remains
stable near the constraint boundary. As the reward gains
are clipped according to Equation 4, they remain the
same even when the constraint boundary is violated;
2) During the convergence: When R0t 0, V (R1t) 0,
this indicates that ROGER maintains stability as R0t
converges.
3) Positive reward with trade-off: When
R0t 0 and
R0t 0, V (R1t) 0, this indicates that ROGER can
trade off the primary reward for stability as long as the
main reward remains non-negative.
4) Other cases: When  R0t 0, stability is not strictly
guaranteed. However, as long as R0t and R1t do not
change abruptlye.g., with an appropriate learning
rateROGER remains bounded according to other
conditions.
In summary, when the system does not change abruptly
(i.e., gentle system and learning dynamics), ROGER achieves
partial stability under key conditions: near the constraint
boundary (condition 1) and during convergence (condition 2),
further supported by the ability to trade off R0t for stability
(condition 3).
2) Learning Analysis: This study also proves that, while
being stable (i.e., involving gentle system and learning
dynamics), adaptive adjustment of reward-weighting gains, as
implemented in ROGER, ensures an increase in the primary
reward R0t. Specifically, the cumulative change in R0t over
time is always greater than or equal to its initial reward R0t0,
dR0t R0t0, t,
Since the learning rule maximizes Rt, its change satisfies:
dR0t 3R2
1t  2R1tR0t
Considering the equivalent differential equation, where x
R1t and y  R0t:
dy  3x2  2xy
This equation can be written using the standard form dydx
factor (x)  exp
After multiplying by the integrating factor, the equation
Integrating both sides then yields:
y  x3  c
where c is a constant.
Given that 0 R1t < 1, t, the inequality becomes:
R0t0 R0t0.
Figure S1 presents the trajectories of R0t and R1t obtained
from locomotion learning experiments on the quadruped robot
and MuJoCo hopper. The trajectories illustrate the continuous
increase in the primary reward described by inequality S4,
demonstrating successful learning.
Technique
Primary Gain
Penalty Gain
Penalty Term
Penalty [4, 5]
1.0 (fixed)
tuned and fixed
Logarithmic CBF
1.0 (fixed)
tuned and fixed
Quadratic CBF
1.0 (fixed)
tuned and fixed
i (i i)2]
1.0 (fixed)
[it  ( Rit (i i))]
0.0 if exist Rit > i i
1.0 if Rit i i
1.0 if Rit > i i
0.0 if Rit i i
1.0 (fixed)
it  E[R0t]E[ Rit]
ROGER (ours)
(as in Equation 2)
(as in Equation 3)
TABLE S1: Comparison of the fundamental concepts behind various state-of-the-art model-free constrained RL techniques.
Fig. S1: Trajectories of R0tR1t obtained from a simulated
quadruped robot (blue), a physical quadruped robot (purple),
and a simulated MuJoCo hopper (orange). Color intensity
corresponds to learning time: bright dots indicate early
C. Hyperparameter Selection
1) Fixed-Gain
fixed-gain
traditional technique still employed in most works. However,
as shown in Figure S2, the tuning process is relatively
complex. On the one hand, low penalty weighting gains
produce higher rewards, but the robot often falls during
training. On the other hand, the use of higher penalty
weighting gains typically results in lower rewards and
suboptimal performance.
Fig. S2: (left) Final primary reward term and (right) maximum
body rollpitch angles, i.e., 99.9th percentile, obtained from
the locomotion learning of a Unitree B1 quadruped robot
with fixed gain penalties. The results were obtained from a
grid search performed over different combinations of penalty
weighting gains: roll penalty weighting gain (1t) and pitch
penalty weighting gain (2t), with the primary weighting
gain (0t) fixed at 1.0. The figure presents the average and
maximum values obtained from over ten repetitions in each
condition. It should be noted that X indicates failure conditions
where the robot falls during learning, defined as roll or pitch
angles 45, while circles indicate the conditions presented
in Figure 5.
2) Control Barrier Function (CBF): The CBF technique is
also employed. However, as shown in Figure S3, although the
CBF receives higher rewards than the fixed-gain penalty in
during learning, while the tuning process is as complex as that
for the fixed-gain penalty.
Fig. S3: (left) Final primary reward term and (right) maximum
body rollpitch angles, i.e., 99.9th percentile, obtained from
the locomotion learning of a Unitree B1 quadruped robot with
CBFs. The results were obtained from a grid search performed
over different combinations of penalty weighting gains, CBF
fixed at 1.0. The figure presents the average and maximum
values obtained from over ten repetitions in each condition. It
should be noted that X indicates failure conditions where the
robot falls during learning, defined as roll or pitch angles
3) Primal-Dual Optimization (PDO): Figure S4 shows that
PDO can prevent the robot from falling during training in
some cases; however, it fails to meet the 10constraint during
learning in all cases and still requires careful hyperparameter
Fig. S4: (left) Final primary reward term and (right) maximum
body rollpitch angles, i.e., 99.9th percentile, obtained from
locomotion learning of a Unitree B1 quadruped robot with
PDO. The results were obtained from a grid search performed
over different tolerance values. The figure presents the average
and maximum values obtained from over ten repetitions in
each condition. It should be noted that X indicates failure
conditions where the robot falls during learning, defined as
roll or pitch angles 45, while circles indicate the conditions
presented in Figure 5.
4) Online Learning with Auxiliary Loss (OL-AUX):
Figure S5 shows that, although OL-AUX can prevent the robot
from falling during training in some cases, it fails to meet the
10constraint and still requires careful hyperparameter tuning.
Fig. S5: (left) Final primary reward term and (right) maximum
body rollpitch angles, i.e., 99.9th percentile, obtained from
locomotion learning of a Unitree B1 quadruped robot with OL-
AUX. The figure presents the average and maximum values
obtained from over ten repetitions in each condition. It should
be noted that X indicates failure conditions where the robot
falls during learning, defined as roll or pitch angles 45,
while circles indicate the conditions presented in Figure 5.
5) Constraint-Rectified Parameter Optimization (CRPO):
Figure S6 shows that, although CRPO can prevent the robot
from falling during training in this case, it still requires careful
selection of hyperparameter values to meet the constraints.
Fig. S6: (left) Final primary reward term and (right) maximum
body rollpitch angles, i.e., 99.9th percentile, obtained from
locomotion learning of a Unitree B1 quadruped robot with
CRPO. The results were obtained from a grid search
performed over different tolerance values. The figure presents
the average and maximum values obtained from over ten
repetitions in each condition. It should be noted that a circle
indicates the condition presented in Figure 5.
6) Reward-Oriented
Embodied
Regulation
(ROGER): Given that the key hyperparameter of ROGER is
the constraint threshold (i), Figure S7 presents the results
after varying 1 (roll constraint threshold) and 2 (pitch
constraint threshold) between 5and 17(i.e., 0.1, 0.2,
and 0.3 radians). The figure shows that ROGER is less
sensitive to the hyperparameter choices than other state-of-
the-art methods since none of the tested cases cause the robot
to fall. The maximum body pitch and roll angles appear to
be environment-dependent, with the maximum rollpitch being
around 10and the final values of the primary reward equal
to those of the relaxed fixed-gain penalty shown in the bottom
left of Figure S2.
Fig. S7: (left) Final primary reward term, (middle) maximum
body roll angle, and (right) maximum body pitch angles,
i.e., 99.9th percentile, obtained from locomotion learning of
a Unitree B1 quadruped robot with ROGER. The results
were obtained from a grid search performed over different
combinations of constraint thresholds. The figure presents
the average and maximum values obtained from over ten
repetitions in each condition.
Fig. S8: (left) Final primary reward term and (right) maximum
body orientation (i.e., both roll and pitch angles) obtained from
locomotion learning of a Unitree B1 quadruped robot with
and 4 and (b) the parameter k used for computing statistical
estimates of penalties is varied between 2 and 4. The figures
present the average and maximum values obtained from over
ten repetitions in each condition.
D. MuJoCo Continuous Locomotion Learning
An additional experiment is included here to present
generalization
multiple
continuous locomotion learning: Ant, Half Cheetah, Hopper,
and Walker2D, as shown in Figure S9.
In Figure S10, four testing conditions were investigated:
(1) the baseline reward function with fixed weighting gains
obtained directly from the OpenAI Gymnasium, i.e., default;
(2) ROGER with all three reward channels, i.e., ROGER; (3)
ROGER with the primary reward and torque penalty channels,
i.e., ROGER; and (4) ROGER with the primary reward and
orientation penalty channels, i.e., ROGER.
Figure S11 presents the results with excessively low
constraint thresholds (i). The result shows that selecting
constraint thresholds that are too low causes the robot to
Snapshots
different
continuous
locomotion
learning
and Walker2D, trained with ROGER, CRPO , and the
default reward function provided by OpenAI Gymnasium.
High body oscillation indicates a greater orientation penalty
and less stable locomotion, while higher joint activation
indicates a higher action penalty, more torque usage, and a
less energy-efficient policy. A video of this experiment is
available at
prioritize reducing the constraint penalties before improving
the primary reward.
The overall experimental setup is presented in Section III-B,
showing the comparison performed between the adaptive
reward with ROGER, CRPO , and the default reward
function provided by OpenAI Gymnasium (i.e., a properly
tuned reward function). It should be noted that the humanoid
is omitted because it was unable to learn proper locomotion
Hyperparameter
learning rate
entropy coefficient
value coefficient
PPO clip rage
GAE discounted rate (GAE)
GAE bootstrapping (GAE)
trajectory length
batch size
training epoch
maximum gradient norm
TABLE S2: Hyperparameters for MuJoCo locomotion learning
Under the implementation of ROGER, multiple reward
channels were employed in all locomotion tasks, with
additional channels added as penaltiesconstraints according
to the termination criteria. For two-dimensional locomotion
channels consisted of the primary reward (healthy forward
speed reward plus remaining terms), torque usage penalty, and
Fig. S10: Snapshots, hopping distance, joint torque usage,
and absolute orientation deviation, obtained from a MuJoCo
hopper trained using the default reward function from OpenAI
Gymnasium and three different ROGER variants: ROGER
with the primary reward, torque penalty, and orientation
penalty; ROGER with the primary reward and torque penalty,
i.e., ROGER; and ROGER with the primary reward and
orientation penalty, i.e., ROGER. A video of the experiment
is available at
pitch deviation penalty. For three-dimensional tasks, i.e., Ant,
the channels comprised the primary reward (healthy forward
speed reward plus remaining terms), torque usage penalty, and
pitch deviation penalty. Intuitively, for all tasks, the torque
usage constraint threshold was set to 1 Nm (i.e., the maximum
limit); the orientation deviation constraint threshold was 45
for Walker2D to allow for forward-backward hip swing, 45
for Ant to maintain the body, and 10for Half Cheetah and
Hopper to encourage upright body segments; and the height
deviation constraint threshold for Ant was 0.2 m from the
starting height.
outperformed the default reward functions provided by
OpenAI Gymnasium and CRPO  in most cases, with the
improvement of up to 100 in performance (i.e., walking
distance) and lower constraint violations with up to 60
less torque usage and orientation deviation. In all tasks,
constrained behaviors were observed progressively during
learning.
Fig. S11: (Top) Hopping distance, (middle) absolute joint
hopper trained with default fixed weighting gains, ROGER
with 1  1.0 Nm and 2 10, and ROGER with 1  0.5
Nm and 2 5. It should be noted that, due to the highly
dynamic nature of the hopper, a stable hopping gait cannot be
achieved without the robot falling. A video of the experiment
is available at
RobotEnvironment
throughout learning  last 10 episodes
Torque ()
Height ()
Pitch ()
Half Cheetah
throughout learning  last 10 episodes
Torque ()
Pitch ()
throughout learning  last 10 episodes
Torque ()
Pitch ()
throughout learning  last 10 episodes
Torque ()
Pitch ()
TABLE S3: Percentages of constraint violations in four
continuous locomotion tasks obtained with default reward
functions provided by OpenAI Gymnasium, representing a
properly tuned reward function, CRPO , and ROGER after
1m training timesteps. A video of the experiment is available
RobotEnvironment
Change (p-value)
Distance (m), reward
Torque (Nm),   1.00
Height (m),   0.20
Half Cheetah
Distance (m), reward
Torque (Nm),   1.00
Distance (m), reward
Torque (Nm),   1.00
Walker2D
Distance (m), reward
Torque (Nm),   1.00
performance
obtained
continuous locomotion tasks trained with default reward
functions provided by OpenAI Gymnasium, representing a
properly tuned reward function, and ROGER after 1m training
timesteps. A video of this experiment is available at https:
youtu.beSpL4awVgDZM.
RobotEnvironment
Change (p-value)
Distance (m), reward
Torque (Nm),   1.00
Height (m),   0.20
Half Cheetah
Distance (m), reward
Torque (Nm),   1.00
Distance (m), reward
Torque (Nm),   1.00
Walker2D
Distance (m), reward
Torque (Nm),   1.00
performance
obtained
continuous locomotion tasks trained with CRPO  and
ROGER after 1m training timesteps. A video of the experiment
is available at
