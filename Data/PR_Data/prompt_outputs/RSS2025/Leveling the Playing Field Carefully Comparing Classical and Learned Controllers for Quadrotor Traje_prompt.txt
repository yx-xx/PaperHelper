=== PDF文件: Leveling the Playing Field Carefully Comparing Classical and Learned Controllers for Quadrotor Traje.pdf ===
=== 时间: 2025-07-22 15:46:17.247043 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Leveling the Playing Field: Carefully Comparing
Classical and Learned Controllers for Quadrotor
Trajectory Tracking
Pratik Kunapuli
University of Pennsylvania
pratikkseas.upenn.edu
Jake Welde
University of Pennsylvania
jweldeseas.upenn.edu
Dinesh Jayaraman
University of Pennsylvania
dineshjseas.upenn.edu
Vijay Kumar
University of Pennsylvania
kumarseas.upenn.edu
AbstractLearning-based control approaches like reinforce-
ment learning (RL) have recently produced a slew of impressive
results for tasks like quadrotor trajectory tracking and drone
racing. Naturally, it is common to demonstrate the advantages of
these new controllers against established methods like analytical
controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more
complicated than might appear at first sight. As a case study, we
take up the problem of agile tracking of an end-effector for a
quadrotor with a fixed arm. We develop a set of best practices
for synthesizing the best-in-class RL and geometric controllers
(GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric
access to: (1) the task definition, in the form of an objective
and (3) feedforward information, describing the desired future
trajectory. The resulting contributions are the following: our im-
provements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above
asymmetries can yield misleading conclusions. Prior works have
implied that RL outperforms GC, but we find the gaps between
the two controller classes are much smaller than previously pub-
lished when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL
has better transient performance, resulting in GC performing
better in relatively slow or less agile tasks, but RL performing
better when greater agility is required. Finally, we open-source
implementations of geometric and RL controllers for these aerial
I. INTRODUCTION
The capabilities of aerial robots have seen explosive growth
in recent years, with many exciting results in fast quadrotor
advances have involved data-driven techniques, and this has
spawned careful studies of the design choices in learning-
based controller synthesis, such as policy architectures, train-
ing procedures, and modeling choices. It is now largely
accepted wisdom that data-driven controller synthesis ap-
proaches outperform more classical model-based methods for
aerial robot control tasks.
pared these two very different classes of controllers. This
may be partly be attributed to the small overlap in research
Fig. 1: Trajectory Tracking for a Quadrotor. Rollouts of tra-
jectory tracking from an initial perturbation of a reinforcement
learning controller (blue) and a geometric controller (orange)
on a quadrotor. Robots are visualized at t  1 and t  5 to
highlight transient and steady-state performance relative to the
reference trajectory (grey). RL Controller has better transient
performance (t  1), but worse steady-state error (t  5)
compared to GC.
communities engaged in the development and application of
model-based versus learned controllers. In any case, existing
empirical studies largely focus on careful comparisons within
each broad model class: such as Sun et al.  who compare
model-based controllers, or Dionigi et al. , Kaufmann et al.
who benchmark learning-based controllers.
As we will discuss in detail in Sec II and IV, the few prior
studies that do compare learned and model-based controllers
suffer from some shortcomings. While they aim to reproduce
in good faith the respective common practices of the model-
based and learned control communities, this inadvertently
confounds the comparison, because these standard practices
are very different. It is standard in data-driven controller
synthesis research to optimize parameters with a carefully
designed objective function on a dataset of experiences that
exactly match the target task. On the other hand, the tuning
of a model-based controller is often a much more heuristic
manual procedure to achieve a good enough configura-
tion. Inheriting such practices when comparing learned and
model-based controllers can produce misleading conclusions:
the model-based controller may be sub-optimally tuned and
perform worse only on that account. In other words, correct
comparisons across learned and model-based controllers is
more complicated than might appear at first sight.
In this paper, our first contribution is to identify and fix
three key asymmetries that can produce misleading gains
for learning versus model-based methods on standard aerial
vehicle tasks like trajectory tracking: objective functions, task-
relevant data, and feedforward inputs specifying upcoming
trajectory waypoints. Our second contribution is to apply
these proposed improvements in experimental protocol to
throughly benchmark reinforcement learned controllers (RL)
and geometric controllers (GC) for agile trajectory tracking
in quadrotors. Having leveled the playing field, our findings
substantially erode the gains of RL versus GC controllers:
the two controller classes perform about on par in most
of our evaluations, with small gains for GC in steady-state
(Fig 1). These findings add nuance to todays prevailing
wisdom about the relative merits of these controllers. We show
further how our improved recipe for empirical comparison
can be applied towards validating various hypotheses on new
aerial robot classes and tasks. Finally, we provide re-useable
implementations of best practices for both RL and geometric
controller synthesis for aerial vehicle trajectory tracking tasks
including task implementations in a highly parallel RL-ready
future practitioners and researchers.
II. RELATED WORKS
In this section, we elaborate on the deficiencies highlighted
in Sec I in the state of current scientific evidence regarding the
efficacy of learned and model-based controllers for quadrotor
control.
Comparative studies for quadrotor control have been per-
formed in the past, but a majority of them consider compar-
isons within the same class of controllers. Among learning-
based methods in quadrotor trajectory tracking tasks, Kauf-
mann et al.  evaluate the effect of control abstraction on
tracking performance in hardware experiments, Dionigi et al.
evaluate the effect of observation on policy performance,
and Welde et al.  evaluate the sample efficiency gains
afforded by a symmetry-informed approach to tracking control
via RL. While comparisons across model-based analytical con-
trollers are fairly straightforward [24, 25], comparisons involv-
ing learning-based methods must contend with a data-driven
approach and compare to a non-data-driven approach, making
it difficult to perform a correct experimental comparison. As
the communities are largely disjoint, it is reasonable that these
papers are preoccupied comparing against other works of the
same controller class, and thus there are few works that even
consider the comparison between RL and analytical controllers
like geometric controllers (GC) for trajectory tracking tasks.
RL-based control is the state-of-the-art for agile trajectory
tracking in quadrotors, owing to better computational tractabil-
ity than Model Predictive Control (MPC) and outperforming
GC in tracking error. To evaluate such claims, we summarize
the few most relevant prior comparison studies in Table I.
Based on our close reading of these papers, supplementary
presents the access to task objective functions, task-aligned
experiences or datasets, and feedforward waypoint tracking
specifications made available to each method considered in
these works. Significant asymmetries are apparent in the access
granted to RL and GC methods respectively, which we believe
(and show evidence in Sec V) might confound these prior
findings. Among the recent works presenting a model-free
RL method, there are few that train directly on the tracking
static task like hovering [7, 8, 19], making the upper bound
of tracking performance unclear. Even those few that directly
optimize the tracking task either omit a comparison against
an analytical controller [5, 12, 28] or compare against an im-
plementation lacking essential components proposed in prior
work on analytical control , resulting in an asymmetrical
comparison. Often this is a result of inheriting a baseline
controller which may not be optimized for the task at hand,
such as an existing module of the firmware of a commercially-
available platform [8, 19]. These observations motivate the
key contribution of our work: to identify and fix oversights
in these prior experimental comparisons (Sec IV), ultimately
enabling us to arrive at more reliable and nuanced conclusions
regarding the efficacy of RL and GC controllers for various
aerial vehicles and tasks (Sec V).
III. STATE-OF-THE-ART CONTROLLERS
In this section we overview two popular classes of con-
(RL), that have widely been used for trajectory tracking in
quadrotors. Our empirical evaluations and comparisons in later
sections will focus on these two classes of controllers.
A. Geometric Control
Seminal works in the quadrotor control literature [14, 17]
showed that quadrotors are differentially flat systems and
analytically developed a hierarchical controller for this system.
This control paradigm uses an explicit control law, constructed
from separate position and attitude control loops that are
connected via a backstepping-like approach to yield a cascade-
like control structure. In this paper, we refer to this controller
simply as the Geometric Controller (GC) for brevity (but
it is sometimes also referred to as SE(3) Control  or
Differential-Flatness Based Control (DFBC) ).
The controller exploits the coupled dynamics between the
attitude and position of the system, and controls them with two
geometric PD control loops separately. First, a desired linear
Geometric Control (GC)
Reinforcement Learning (RL)
Tuned for Obj.?
Traj Data?
Feed Forward?
Tuned for Obj.?
Traj Data?
Feed Forward?
Benchmark of Learned Policies
SimpleFlight
Leveraging Symmetry
Sim-2-Multi-Real
Power of Input
Learning to Fly in Seconds
Geometric SE(3) Control
NMPC vs. DFBC
PID AutoTune
NonLinear PID
TABLE I: Trajectory Tracking Controller Comparisons in Recent Literature. Partial survey of works proposing trajectory
tracking methods for quadrotors based on GC andor RL. We measure these works on whether (to the best of our knowledge)
the methods a) optimized a task objective, b) were optimized for trajectory tracking, and c) incorporate future reference
information. represents a controller had this component, represents a suboptimal implementation (details in Sec IV), and
represents a method that did not have this component. Asymmetries occur when comparing across model classes whose
implementation was granted unequal access to task, data, or feedforward information.
acceleration is computed from the position PD controller.
desired orientation, which constitutes the reference tracked by
the attitude PD controller. The final output of the controller
is the collective thrust and moment applied at the center-of-
mass (COM) of the vehicle. A detailed implementation of this
controller is presented in Appendix B.
B. Reinforcement Learning
Reinforcement learning (RL) seeks to train a policy
parameterized by , represented by a neural network. This
allows the controller to express a wide range of policies at the
expense of requiring data to learn parameters . As is standard
in RL, the action at is produced by passing an observation
ot through the policy according to: at (ot), and the
policy is optimized by maximizing rewards rt  R(ot, at).
Policy optimization is performed by collecting experience and
optimizing based on the rewards accumulated along the expe-
riences over many episodes. Following previous work [7, 28],
we define the observation as a body-frame error representation
concatenating position error
Bep R3, orientation error
BeR R33, the gravity vector Bg R3, velocity error
Bev R3, and angular velocity error Be R3:
This representation allows us to use the state of any arbi-
trary body B in the observation to the RL policy , given
some desired position specified by WpB
d and yaw orientation
specified by WRB
The observation is flattened into a vector in R21. The action
produced by the policy is at R4, clipped to lie within [1, 1]
and then scaled to the collective thrust fT and body moment
M limits of the platform. Further implementation details are
provided in Appendix C.
IV. METHODOLOGY
A proper comparative study levels the playing field between
signal-to-noise ratio from experiments, so that the conclusions
drawn offer insights to guide future research and practice.
In this section, we identify commonly-overlooked details that
yield misleading conclusions in comparative studies for tra-
jectory tracking controllers for aerial vehicles and propose
mechanisms to correct them. We finish by summarizing the
necessary techniques to perform a proper, fair, and unbiased
comparison between learning-based and analytical controllers.
A. Task Definition
The goal of a trajectory tracking task is to drive an output
of the system (e.g., the COM for quadrotors) asymptotically
towards a specified trajectory despite some initial disturbance.
To quantitatively compare solutions, we must choose some
objective which describes the optimal performance in the task.
1) Trajectories: Formally, we can define our trajectory as
a time-varying goal g(t) R4 describing the desired position
and yaw of some body in the system. We sample g(t) at a
fixed rate to discretize it into desired waypoints of position
and yaw.
If comparing two methods, they should be optimized on the
same tasks, otherwise the training task distribution may be
the dominant source of performance differences, confounding
the results between the controller classes. Unfortunately, many
State Component
Tracking Lissajous
Position (m)
Velocity (ms)
Yaw (rad)
Angular Velocity (rads)
TABLE II: Initial Conditions. Randomization ranges for
initialization in Hover and Tracking Lissajous tasks by state
component.
prior works inadvertently make this error, as seen in Table I. In
hardware experiments, for example, it is common to tune gains
for low-level controllers on simple, near-hover tasks but then
evaluate the controller with the same gains on any trajectory.
This is done primarily for ease of tuning, interpretability, and
because of the risk of damaging hardware in case of crashes
while tuning on more dynamic trajectories. However, in order
to hold controllers equal, all controllers must be optimized or
tuned on the same class of tasks for the results to be insightful.
In this study, we present two versions of trajectory tracking
for the quadrotor: Hovering at a specified pose (i.e., tracking
a constant reference), and Tracking Lissajous curves, where
the desired position and yaw is specified as a Lissajous curve.
Axsin(x  t  x)  x
Aysin(y  t  y)  y
Azsin(z  t  z)  z
Asin(  t  )
Hovering is simply a special case of the Lissajous curve,
where the amplitudes Ax,y,z, are set to 0 and the desired
end-effector position and yaw is defined by x,y,z,. Details
regarding ranges used for randomization in each task is in
Appendix A.
2) Initial Conditions: The distribution from which the
initial conditions are sampled is also a core component of the
task definition, since even for the same trajectory, varied initial
conditions describe vastly different tasked behaviorfor ex-
task than first recovering from large initial perturbations and
then hovering at the goal. In order to hold methods equal, we
must explicitly specify the family of initial conditions. Initial
conditions for the tasks considered are listed in Table II.
3) Objective: The task objective is crucial to rigorously
encode optimality. Intuitively, this objective describes how well
a method accomplishes the task, and serves to distinguish
performance during evaluation as well as provide a structured
objective during optimization and tuning. In RL, this objective
is optimized directly, and as such, the gradients may not be
very informative during the initial stages of training. Correct-
ing this often requires the addition of extra terms to help guide
the policy gradients, resulting in a proxy objective [10, 23].
While designing an effective reward function for some tasks
may be quite difficult, for trajectory tracking, the reward
Parameter
TABLE III: Objective Hyperparameters. Parameters which
define the form of the reward function (objective) for trajectory
tracking tasks.
objective commonly mirrors an optimal control objective, like
the linear quadratic regulator-style tracking costs used in some
works [12, 19].
We define the objective for the tasks considered following
previous work  to measure deviation of the end-effector
from the desired trajectory:
r(t)  p(p(t) pd(t), p)  R((t) d(t))
v(v(t) vd(t))  ((t) d(t)).
The desired position pd(t) represents the x, y, and z coordi-
nates of the desired end-effector waypoint in the world frame.
vd(t) represents the desired velocity, and d(t) represents
the desired angular velocity of the body being tracked in
the world frame, both obtained from the derivative of the
trajectory g(t). This form follows previous work , with
, allowing the position tolerance to be tuned
with . The hyperparameters used for the reward function are
listed in Table III.
B. Optimizing Controllers
the respective tunable parameters for each controller class.
1) Training the Reinforcement Learning (RL) Policy: The
RL policy is parameterized by a 3-layer MLP with 256 units
per layer for the network (details in Appendix C). Optimizing
the network parameters is done by applying the popular model-
free RL method, proximal policy optimization (PPO) .
This approach is standard in the field, where methods optimize
a reward function (3) over many million simulated environ-
ment steps. We found that annealing the position tolerance p
was key to enabling agile behavior while still converging to
the goal without large steady-state error. We reduced p by
half every 50M timesteps, going from 0.8 at the beginning of
training and ending at 0.1 as seen in Table III.
2) Tuning the Geometric Controller (GC): The tunable
parameters of the Geometric Controller take the form of PD
gains for both the position and attitude loops. Considering each
axis separately, this amounts to 12 gains. However, we elect
to match the gains for the xV and yV axes, owing to the sym-
metrical control authority over pitch and roll, which is greater
than that over yaw (zV). This reduces the number of tunable
parameters to 8. While hand-tuning this controller is common
via Optuna , which uses Bayesian Optimization to more
systematically tune the parameters according to a specified
objective function. We use the same reward function as the
reinforcement learning controller (Eqn. 3), in order to produce
the geometric controller tuned with the best performance.
Tuning gains for the PD controller via optimization is not
novel. Specific to quadrotor control, Zhu et al.  and Cheng
et al.  presented an approach to tune PD gains using gradient
can be used to tune PID gains, Berkenkamp et al.  used
Gaussian Processes to tune gains, and recently, Zhang et al.
used RL to tune PID gains directly. In fact, Loquercio
et al.  presented an auto-tuning procedure for an MPC
controller and found that optimal gains varied even along a
single trajectory depending on the motion in various segments.
In this work, we specifically tune the controller for each task,
in the same way that RL is trained on each task, facilitating
comparisons between best-in-class controllers. To the best of
our knowledge, all of the prior methods comparing RL and
GC have used static gains that are specific per-platform for all
C. Feedforward Terms
A core component of differential flatness-based controllers
in trajectory tracking is the use of higher-order derivatives of
the reference flat outputs (i.e., desired COM position and yaw
). The addition of these terms allows the SE(3) controller
to choose actions informed by how the reference will move
in the future, leading to asymptotic stability even in agile
trajectory tracking scenarios. These feedforward terms are a
vital component of the GC control laws, and depend on up to
4th order derivatives of position and 2nd order derivatives of
yaw [14, 27]. Feed forward reference acceleration, pd, appears
directly in the computation of the desired acceleration pdes
in the position PD controller along with the position error,
velocity error, and gravitational acceleration. Using standard
pdes  Kp(p pd) Kv(v vd) mgzW  pd.
and angular acceleration d from derivatives of the desired
orientation Rdes (which is determined in part by the desired
acceleration pdes) and uses them in the attitude PD controller:
des  KR(eR) K( d)
(RT Rdesd RT Rdes d).
mentations as seen in Table I. Feedforward information in
the prior work is often replaced by an integral loop to close
steady-state errors  in the position control (4), or simply
omitted [8, 11, 19] by setting the feedforward angular velocity
and acceleration to 0 in the attitude loop (5), perhaps due to
ease of implementation. This information is crucial to encode
the future reference information, and contributes a significant
performance gain, as we will verify in Sec V.
In fact, the benefit of leveraging future information is also
seen in many recent reinforcement learning controllers for
trajectory tracking [11, 12]. Often, this information appears
as a horizon of future waypoints or goals appended to the
observation. In order to give both controllers access to the
same information and to avoid privileging one controller over
to the observation for both the GC and RL controllers. The
horizon length used is H  10, corresponding to samples from
t  dt to t  10dt of the reference trajectory. This horizon
is sufficiently large for the GC controller to approximate
high-order derivatives of the reference via finite differencing,
while the RL controller directly observes the horizon without
imposing any structure a priori.
D. Summing Up: Best Practices For Benchmarking
In summary, there are a number of advantages that could be
afforded to methods of either class, but which have often been
inadvertently only presented to RL in standard implementa-
tions. As RL is a data-driven approach, it must use some data
and objective function, and the fair practice to advantage GC
equally is to use a tuning algorithm on the same objective and
data. Furthermore, when specifically considering trajectory
of the geometric controller, and both RL and GC should get
the same access to future reference information in the form
of a horizon of waypoints. Holding equal the optimization
to properly evaluate controller differences.
Prior works have compared RL controllers against hand-
tuned GC implementations (asymmetric optimization), which
may have been tuned for hovering (asymmetric data access).
Even trajectory tracking works like  compare against a
controller using PID (asymmetric access to future reference
information), and do not use feedforward information equally
between the RL and GC method compared.
V. EXPERIMENTAL RESULTS
In this section, we seek to first validate the methodology
proposed for fairly comparing learning-based and analytical
tries leads to strengthened baseline models. Next, armed with
a powerful experimental protocol to compare methods fairly,
we seek to answer specific questions about trajectory tracking
of aerial vehicles:
1) How do the best-in-class RL and GC controllers perform
for trajectory tracking?
2) Does an offset between the COM and the reference point
(e.g., an end effector) affect tracking performance?
3) In which scenarios does each controller perform better
than the other?
A. Simulation Environment
We perform the experiments in simulation, giving us the best
ability to hold methods equal in data access, initial conditions,
and optimization, and to evaluate over large numbers of trials.
To simulate the aerial vehicles during training and evaluation
of policies, we implement the dynamics in IsaacLab ,
using the IsaacSim  physics engine. Following prior work
leveraging massively parallelized simulation , the envi-
ronment is implemented with GPU-level parallelism, allowing
for a large number of simultaneous simulations. We ensure
RL-?-Liss.-FF
GC-?-Liss.-FF
Lissajous Tracking
Normalized Gap to Max Reward
<Manual> vs <Optimized>
RL-Opt-?-FF
GC-Opt-?-FF
Hover Liss.
Hover Liss.
<Hover> vs <Lissajous>
RL-Opt-Liss.-?
GC-Opt-Liss.-?
<None> vs <Feed Forward> vs <PID>
RL-?-Hover-FF
GC-?-Hover-FF
Normalized Gap to Max Reward
RL-Opt-?-FF
GC-Opt-?-FF
Hover Liss.
Hover Liss.
RL-Opt-Hover-?
GC-Opt-Hover-?
Fig. 2: The Impact of Each Type of Asymmetry in RL vs. GC Comparisons. Model comparisons in both Lissajous Tracking
(top row) and Hover (bottom row) tasks for the aerial manipulator vehicle. Results are shown with median and inter-quartile
range across 1000 evaluations per task. Controllers are measured on normalized gap to maximum reward to highlight improved
performance by correcting asymmetries. Hatched bars represent controller variants that are a suboptimal choice within each
category. Comparisons are isolated into asymmetric access to optimization (left), asymmetric access to data (middle), and
asymmetric access to future reference information (right). All controllers improve by conducting the protocols presented for
Tracking task.
the same goal configurations are randomly sampled across
evaluated methods, regardless of control strategy selected.
Following prior work , we simulate the dynamics at
performed on 4096 simultaneous environments over 200 mil-
lion total timesteps in approximately 30 minutes (175, 000
stepssec) on an NVIDIA RTX A5000 GPU. The code to
run this environment (as well as training scripts) is open-
sourced in order to accelerate the development of agile aerial
manipulation.
In this study, we consider a quadrotor platform with a
rigidly attached arm. This aerial manipulator morphology is
a generalization of the quadrotor, and serves as a platform to
evaluate new hypotheses regarding control of aerial vehicles.
We expect that the GC will suffer in this morphology since
it is principally designed to operate on the COM as opposed
to controlling the end-effector directly, whereas RL can be
applied to any body in the system. Together with the quadrotor,
this aerial manipulator robot allows us to answer the questions
posed above.
B. Validating the Experimental Protocol
In this section, we show how previous methods may have
drawn conclusions between RL-based methods and GC-based
implementations that bias towards better results for the RL-
based controller. Specifically, we show how correcting the
asymmetries laid out in the methodology (access to the
optimization objective, access to the dataset, and use of
future reference information) lead to significantly improved
performance of both controllers, closing the the perceived gap
in controller performance in recent literature. We construct
models from the controller type: {RL, GC}, optimization
(Hov.), Lissajous (Liss.)}, and feed forward strategy: {None,
Feedforward (FF), Integral (PID)}. We optimize (train) every
method according to the dataset and optimization procedure
Lissajous Tracking and Hover tasks, highlighting results be-
tween choices of optimization, dataset, and feed forward
information in Fig 2 for the aerial manipulator morphology.
A full presentation of the achieved reward, position tracking
Quadrotor
Aerial Manipulator
Controller
Avg. Reward
Position RMSE (m)
Yaw RMSE (rad)
Avg. Reward
Position RMSE (m)
Yaw RMSE (rad)
RL-Opt.-Liss.-FF
GC-Opt.-Liss.-FF
TABLE IV: Trajectory Tracking for Quadrotor and Aerial Manipulator. Comparison of the best RL controller against the
best GC controller in trajectory tracking for a Quadrotor and an Aerial Manipulator (Quadrotor with fixed-arm) over 1000 trials
in the Lissajous Tracking task. Rewards are averaged over time per rollout, then presented as average and standard deviation
over the trials. Maximum reward is 15.0. RMSE of position and yaw are shown as averages with standard deviation over 1000
trials. Contrary to many literature claims, performance is very similar between best-in-class methods.
the supplementary material. We measure the performance of
the controllers in Fig 2 by the difference between maximum
reward and the average reward achieved over the rollout to
highlight the improved reward by correcting for asymmetries.
1) Optimizing Controllers: First, we consider the effect of
optimizing the controller in the left column of Fig 2. In these
mizing based on some objective. Note, that it is not feasible
to hand-tune the neural network RL controllers we study
in this work, and thus we omit this variant. Comparatively,
we can observe how tuning for an objective affects the GC,
and we see that tuning the controller for the objective lowers
the gap to maximum reward, improving performance. In this
from the measured task (Lissajous tracking in top row, Hover
in bottom row), and utilize feedforward information so this
represents the best-performing model.
2) Data Access: In the middle column of Fig 2, we com-
pare the effect of task-aligned experience during optimization
on the controller performance. We see that models optimized
on another task (Hover for the Lissajous Tracking evaluation
and vice versa) perform worse than those optimized for the
same task as that on which they are evaluated. This result is
expected for RL (since it is a data-driven method), but note
that GC also benefits from this practice, which should thus
be adopted. Commonly, GC gains are hand-tuned in quasi-
static motions and set for all downstream tasks like trajectory
disadvantaging the GC.
3) Future Reference Information: Finally, in the last col-
umn (Fig 2) we investigate the role of feedforward information
for both Lissajous Tracking and Hover. In the top row we
can clearly see that for Lissajous tracking both RL and GC
benefit from the addition of future reference information.
information in GC performs worse than if no feedforward
information was provided at all, perhaps due to the integral
terms being sensitive to the gains which are auto-tuned. In the
bottom row, we find that utilizing feedforward information
presents no benefit due to the static nature of the reference in
the Hover task, and all models are nearly perfect in this task,
except for the GC controller using PID.
4) Conclusions: From these results, it is evident the best
performing GC controller is the one which optimizes a reward,
has access to the same dataset being evaluated on, and uses
feedforward terms (GC-Opt.-Liss.-FF). This policy adopts the
strategies proposed in this work to reduce asymmetries in the
three areas, and not performing these corrective measures in
any one category increases the gap to optimal reward, creating
a suboptimal policy. Indeed, we can examine how previous RL
methods had implemented the analytical baseline, and show
why their claims of superior performance over the baseline
may have been misguided or overstated. For trajectory track-
(GC-Man.-Hover-None) , optimizing for the wrong dataset
(GC-Opt.-Hover-None), or using PID instead of feedforward
terms (GC-Opt.-Liss.-PID or GC-Opt.-Hover-PID) [7, 8, 11]
all result in misguided comparisons since the baseline method
is handicapped. This ultimately results in an over-estimate
of the performance gap between RL and GC in trajectory
previously thought. We open-source the implementation and
optimization code for the strengthened controllers, hoping to
accelerate research in agile aerial vehicles. We hope to impress
the importance of these corrections of model-based controllers
to the research community to improve baselines and draw more
robust conclusions.
C. Best-of-the-Best Comparison
By using this protocol to obtain best-in-class methods for
both RL and GC controllers, we can now perform experiments
to develop controllers for trajectory tracking of the quadrotor
and the aerial manipulator. We seek to properly evaluate the
claim that RL outperforms GC for trajectory tracking. We
develop the best-in-class controllers for both morphologies
using both RL and GC, and evaluate the robots on Lissajous
Tracking in 1000 evaluations, showing the median error over
time with inter-quartile range for both position and yaw
tracking in Fig. 3. Root-mean-square-error (RMSE) for these
experiments are recorded in Table IV.
Both controllers work well to quickly reduce the error in
position and yaw of large initial perturbations. Although the
RMSE metrics seem to indicate the RL controller has a slight
edge over the GC controller, we see in Fig. 3 that the GC error
converges to 0 while the RL controller has a slight steady-
state offset. While RMSE is a standard metric to report in
Position
Error (m)
Quadrotor
Aerial Manipulator
Time (s)
Error (rad)
Time (s)
RL-Opt.-Liss.-FF
GC-Opt.-Liss.-FF
Fig. 3: Trajectory Tracking Errors. Position and yaw errors
over time for both the best-in-class RL and GC controllers
evaluated on the Quadrotor and Aerial Manipulator morpholo-
gies. Results are shown as the median with inter-quartile range
shaded from 1000 evaluations per morphology in Lissajous
Tracking.
trajectory tracking, we notice it is sensitive to the scale of
the inital perturbation and the duration of the episode (where
long episodes emphasize steady-state error and short episodes
emphasize transient performance), and as such may not reflect
the overall or asymptotic performance of the controller for
some downstream task. In summary, although the best-in-class
RL controller achieves lower RMSE in trajectory tracking for
both a quadrotor and aerial manipulator compared to the best-
in-class GC controller, the steady-state error does not converge
to zero. This suggests that the RL controller is better at
achieving transient performance, while the GC achieves better
asymptotic performance at the cost of near-term error, echoing
results shown in . Thus, we answer question 1, showing that
GC outperforms RL on the reward objective and RL only has
a slight advantage in position RMSE from large perturbations
due to better initial transient performance, reversing claims of
prior literature.
work well to control the desired reference point, whether it
is the COM location in the quadrotor or the end-effector in
the aerial manipulator. This shows impressive results for the
excellent performance in the end-effector tracking, answering
question 2.
Time-To-Catch (s)
Controller
TABLE V: Ball Catching success rate. Percentage of catches
made by the aerial manipulator with varied initial velocities by
each controller, denoted by the time-to-catch in each setting.
Results are shown as mean catch rate over 100 trials.
D. Does RL beat GC?
In order to evaluate if and when RL outperforms GC,
we choose to evaluate on the Hover task, which isolates
the transient component of the trajectory tracking task (since
the reference trajectory is stationary). In this setting, we can
evaluate how the controllers can reduce their position and yaw
errors as a function of time from a large initial perturbation.
As seen in Fig 2, for the Hover task, the feed-forward terms
do not yield any significant benefit in performance due to the
stationary trajectory. Thus, we use the RL-Opt.-Hover-None
as the RL controller, and GC-Opt.-Hover-None as the GC
controller. Additionally, since we evaluate this on a quadrotor
with a fixed arm (that is, a 0-DOF Aerial Manipulator ), we
can directly compare controller assumptions by presenting an
RL controller observing the end-effector state (RL-EE), a GC
controller which can only by design observe the COM state,
and an RL controller observing the same COM state as the GC
(RL-COM). This comparison allows us to directly investigate
if observing only the COM state is inherently limiting for
control of the end-effector. The insights from this experiment
are only possible due to the experimental protocol optimizing
the controllers in the same way, isolating resultant differences
in performance to the inherent controllers.
We evaluate this for 1000 random goal positions, and
present the results in Fig. 4. This evaluation is also motivated
by the real-world task of catching projectiles (Ball-Catching),
for which agility and transient performance are essential. Thus,
we present an evaluation scenario in which a ball is thrown
with some random initial velocity, and the quadrotor aims
to catch the ball at a given height off the ground. Time-to-
Catch is a metric used to roughly define difficulty of the
times correspond to more difficult tasks. This is modulated
by affecting the vertical component of the initial velocity
which moves the catch location as well as the time to reach
the catch plane. We present the catch success percentages in
Table V for 100 evaluations of 5 catch opportunities each,
with varied initial velocities corresponding to different time
allocations for the catching. Please view videos of this task in
the supplementary materials.
Here we can see that there is a manifest difference between
the controllers, where RL-EE is shown to have the best tran-
sient performance in reducing position error compared to the
Position Error (m)
Time (s)
Yaw Error (rad)
Fig. 4: Hover Errors. Position and yaw errors over time
for the RL controller observing the end-effector (RL-EE), RL
controller observing the COM (RL-COM), and the geometric
controller (GC). Errors are shown as the median with inter-
quartile ranges across 1000 evaluations in the Hover task.
other controllers, resulting in the highest success percentages
of balls caught. The GC controller, although guaranteed to
converge to 0 steady-state error, has the worst transient per-
formance among the controllers, resulting in poor performance
in the ball catching task. The RL-COM performance indicates
that the COM state observation is not the impeding factor
for the GC transient performance, as the RL-COM has better
GC may be limiting for very agile behavior, especially of an
end-effector.
E. Data Asymmetry under Domain Randomization
Thus far, weve considered data asymmetry as access in
simulation to rollout data under a particular task. Another
use of simulation data is to perform domain randomization, a
common technique used to bridge the simulation to real-world
gap by simulating a distribution of parameters that may be
unk
