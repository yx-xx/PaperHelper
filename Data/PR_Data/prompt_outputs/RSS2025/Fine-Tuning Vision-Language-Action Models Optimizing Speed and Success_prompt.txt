=== PDF文件: Fine-Tuning Vision-Language-Action Models Optimizing Speed and Success.pdf ===
=== 时间: 2025-07-21 15:47:48.981146 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Fine-Tuning Vision-Language-Action Models:
Optimizing Speed and Success
Moo Jin Kim1
Chelsea Finn1
Percy Liang1
AbstractRecent vision-language-action models (VLAs) build
upon pretrained vision-language models and leverage diverse
robot datasets to demonstrate strong task execution, language
following ability, and semantic generalization. Despite these
tuning to achieve good performance, yet how to most effectively
fine-tune them is unclear given many possible strategies. In
this work, we study key VLA adaptation design choices such
as different action decoding schemes, action representations,
and learning objectives for fine-tuning, using OpenVLA as our
representative base model. Our empirical analysis informs an
Optimized Fine-Tuning (OFT) recipe that integrates parallel
and a simple L1 regression-based learning objective to altogether
improve inference efficiency, policy performance, and flexibility
in the models input-output specifications. We propose OpenVLA-
art on the LIBERO simulation benchmark, significantly boosting
OpenVLAs average success rate across four task suites from
76.5 to 97.1 while increasing action generation throughput
by 26. In real-world evaluations, our fine-tuning recipe enables
OpenVLA to successfully execute dexterous, high-frequency con-
trol tasks on a bimanual ALOHA robot and outperform other
VLAs (0 and RDT-1B) fine-tuned using their default recipes,
as well as strong imitation learning policies trained from scratch
(Diffusion Policy and ACT) by up to 15 (absolute) in average
success rate. We release code for OFT and pretrained model
checkpoints at
I. INTRODUCTION
Recent vision-language-action models (VLAs)robot poli-
cies built by fine-tuning pretrained vision-language models on
large-scale robot datasets for low-level robotic controlhave
demonstrated strong task performance, semantic generaliza-
tasks [4, 34, 24, 23, 7, 15, 8, 53, 58, 3]. Despite their strengths,
fine-tuning is crucial for satisfactory deployment of VLAs on
novel robots and tasks, yet it is unclear what the most effective
approach for adaptation is given the large design space. A
robotics practitioner who wishes to fine-tune a VLA to a new
robot setup and task may default to using the same training
recipe used for pretraining (or a parameter-efficient variant),
but it is not obvious whether this would yield the best policy,
and there is limited empirical analysis of alternative fine-tuning
approaches in the literature.
1Stanford University. Correspondence to: Moo Jin Kim <moojinkcs.
stanford.edu>, Chelsea Finn <cbfinncs.stanford.edu>, Percy
Liang <pliangcs.stanford.edu>.
Prior work has begun exploring VLA adaptation strategies,
with Kim et al.  proposing parameter-efficient fine-tuning
via LoRA. However, their autoregressive action generation
remains too slow (3-5 Hz) for high-frequency control (25-
50 Hz), and both LoRA and full fine-tuning of autoregressive
VLAs often yield unsatisfactory performance in bimanual ma-
nipulation tasks [54, 27, 3]. While recent approaches improve
efficiency through better action tokenization schemes [2, 38],
achieving 2 to 13 speedups, significant latency between
action chunks (e.g., 750 ms for the recent FAST approach )
still limits real-time deployment on high-frequency bimanual
robots. Exploring alternative VLA adaptation approaches that
achieve both satisfactory speed and quality remains an under-
explored area of research.
In this work, we study key design decisions for adapt-
ing VLAs to novel robots and tasks using OpenVLA, a
representative autoregressive VLA, as our base model. We
examine three key design choices: action decoding scheme
(autoregressive vs. parallel generation), action representation
(discrete vs. continuous), and learning objective (next-token
prediction vs. L1 regression vs. diffusion). Our study reveals
several key insights that build on each other: (1) parallel
decoding with action chunking not only boosts inference
efficiency but also improves success rates on downstream tasks
while enabling greater flexibility in the models input-output
specifications; (2) continuous action representations further
improve model quality compared to discrete representations;
and (3) fine-tuning the VLA with an L1 regression objective
yields comparable performance to diffusion-based fine-tuning
while offering faster training convergence and inference speed.
Building on these insights, we introduce OpenVLA-OFT:
an instantiation of an Optimized Fine-Tuning (OFT) recipe that
integrates parallel decoding and action chunking, continuous
action representations, and an L1 regression objective to en-
hance inference efficiency, task performance, and model input-
output flexibility while maintaining algorithmic simplicity.
We conduct experiments on both the standardized LIBERO
simulation benchmark and dexterous tasks on a real bimanual
ALOHA robot. In LIBERO, OpenVLA-OFT establishes a new
state of the art by achieving 97.1 average success rate across
four task suites, outperforming both fine-tuned OpenVLA
policies  (76.5) and 0 policies  (94.2) while
achieving a 26 speedup in action generation with 8-step
action chunks. For real-world ALOHA tasks , we augment
our recipe with FiLM  for enhanced language grounding,
Fig. 1: OpenVLA-OFT on the bimanual ALOHA robot. Our Optimized Fine-Tuning (OFT) recipe enhances fine-tuned OpenVLA policies through
improved inference efficiency, model quality, and input-output flexibility. The resulting OpenVLA-OFT policies execute diverse dexterous manipulation tasks
on a real-world bimanual robot at high control frequencies (25 Hz). The  suffix indicates the integration of feature-wise linear modulation (FiLM) ,
which strengthens language grounding in tasks where accurate language understanding is critical for success.
denoting the augmented recipe as OFT. OpenVLA-OFT
successfully executes dexterous bimanual tasks like folding
clothes and manipulating target food items based on the users
prompt (see Figure 1), outperforming both fine-tuned VLAs
(0 and RDT-1B) and prominent imitation learning policies
trained from scratch (Diffusion Policy and ACT) by up to 15
(absolute) in average success rate. With 25-timestep action
base OpenVLA, demonstrating that our new fine-tuning recipe
enables real-time robot control with strong task performance
and language following ability.
II. RELATED WORK
Prior works have leveraged language and vision founda-
tion models to enhance robotic capabilities, using them as
pretrained visual representations that accelerate robotic policy
learning [30, 33, 31, 20, 32], for object localization in robotics
tasks [9, 47], and for high-level planning and reasoning [1, 17,
fine-tuning vision-language models (VLMs) to directly predict
low-level robotic control actions, producing vision-language-
and generalization to out-of-distribution test conditions
and unseen semantic concepts. These works focus primarily on
model development, while we focus on developing a recipe for
fine-tuning such models, justifying individual design decisions
with insights that we gain from our empirical analysis.
Despite the importance of fine-tuning for real-world VLA
remains limited. While Kim et al.  study various parameter
update strategies and from their findings show that LoRA fine-
tuning enables effective adaptation to single-arm robots oper-
ating at low control frequencies (< 10 Hz), their analysis does
not extend to bimanual robots with high control frequencies
(25-50 Hz), a more complex control scenario. We address
this gap by exploring VLA adaptation design decisions for fast
inference and reliable task execution on a real-world bimanual
manipulator with a 25 Hz controller.
Recent works by Belkhale and Sadigh  and Pertsch
et al.  improve VLA efficiency through new action to-
kenization schemes, using vector quantization or discrete co-
sine transform-based compression to represent action chunks
(sequences of actions) with fewer tokens than simple per-
dimension binning (as used in RT-2  and OpenVLA ).
While these approaches achieve 2 to 13 speedups for
autoregressive VLAs, we explore design decisions beyond
autoregressive modeling, which remains inherently limited by
iterative generation. Our parallel decoding approach, when
paired with action chunking, achieves significantly greater
(0.07 ms for single-arm tasks with one input image and 0.321
ms for bimanual tasks with three input images).
Another line of research [54, 27, 3] demonstrates effective
VLA fine-tuning for high-frequency, bimanual manipulation
using generative approaches like diffusion or flow match-
Fig. 2: Key design decisions for VLA fine-tuning. Left: Comparison between autoregressive decoding, which generates actions sequentially, and parallel
next-token prediction and continuous action values with L1 regression or diffusion modeling objectives. The original OpenVLA training scheme includes
autoregressive decoding, discrete actions, and next-token prediction.
ing. While these diffusion-based VLAs achieve higher action
throughput than autoregressive VLAs by generating multi-
timestep action chunks simultaneously, they introduce compu-
tational trade-offs through slower training and multiple denois-
ing or integration steps at inference time. Furthermore, these
diffusion VLAs vary considerably in architecture, learning
specificationsand which design elements most significantly
impact performance remains unclear. Through controlled ex-
regression objective can match more complex approaches in
task performance while achieving significantly greater infer-
ence efficiency.
low-level control policy in addition to a slower VLA ,
in this work we fine-tune the base VLA end-to-end with
techniques that enable high efficiency without needing a
separate controller. Further, unlike other works that collect
online interaction data to continually update specialist policies
via reinforcement learning , here we focus on the sim-
pler imitation learning paradigm in which we develop high-
performing policies just by training once offline on a fixed
dataset of expert task demonstrations.
III. PRELIMINARIES
Original OpenVLA formulation. We use OpenVLA
as our representative base VLA, a 7B-parameter manipulation
policy created by fine-tuning the Prismatic VLM  on
1M episodes from the Open X-Embodiment dataset . See
Appendix A for architecture details. OpenVLAs original train-
ing formulation uses autoregressive prediction of 7 discrete
robot action tokens per timestep: 3 for position control, 3
for orientation control, and 1 for gripper control. It employs
next-token prediction with cross-entropy loss as its learning
formulations including parallel decoding, continuous action
diffusion modeling in the next few sections.
Action chunking. Prior works have shown that action
chunkingi.e., predicting and executing a sequence of future
actions without intermediate replanningimproves policy suc-
cess rates across many manipulation tasks [56, 5, 28]. How-
action chunking impractical, as generating even a single-
timestep action takes 0.33 seconds on an NVIDIA A100 GPU.
For a chunk size of K timesteps and action dimensionality
versus just D passes without chunking. This K-fold increase in
latency makes action chunking impractical for high-frequency
robots under the original formulation. In the next section,
we present a parallel generation scheme that enables efficient
action chunking.
IV. STUDYING KEY VLA FINE-TUNING DESIGN
DECISIONS
In this section, we first outline key design decisions for
adapting VLAs to novel robot setups and tasks and provide
details on their implementation.
A. VLA Fine-Tuning Design Decisions
Existing approaches that fine-tune VLAs using the base
models autoregressive training recipe face two key limitations:
slow inference speed (3-5 Hz) unsuitable for high-frequency
To address these challenges, we investigate three key design
components for VLA fine-tuning:
(a) Action generation strategy (Figure 2, left): We com-
pare autoregressive generation, which requires sequential
token-by-token processing, with parallel decoding, which
generates all actions simultaneously and enables efficient
action chunking.
(b) Action representation (Figure 2, right): We examine
discrete actions (256-bin discretization of normalized ac-
tions) processed through softmax-based token prediction,
versus continuous actions directly generated by an MLP
action head. For discrete actions, the final hidden states
of the language model decoder are linearly projected into
form the probability distribution over action tokens. For
continuous actions, the final hidden states are instead
mapped directly to normalized continuous actions by a
separate action head MLP.
(c) Learning objective (Figure 2, right): We compare
policies fine-tuned with next-token prediction for discrete
conditional denoising diffusion  for continous actions
(similar to Chi et al. ).
We conduct our study using OpenVLA  as the base
relatively small training datasets (500 demonstrations versus
1M demonstrations for pretraining).
B. Implementing Alternative Design Components
The base OpenVLA model originally employs autoregres-
sive generation of discrete action tokens optimized via next-
token prediction. We implement alternative design decisions
for fine-tuning while keeping the original pretraining un-
changed. We describe the key implementation aspects below,
with further details explained in Appendix B.
Parallel decoding and action chunking. Unlike autore-
gressive generation which requires sequential token prediction,
parallel decoding enables the model to map input embeddings
to the predicted output sequence in a single forward pass. We
modify the model to receive empty action embeddings as input
and replace the causal attention mask with bidirectional atten-
This reduces action generation from D sequential passes to a
single pass, where D is the action dimensionality.
Parallel decoding naturally extends to action chunking: to
predict actions for multiple future timesteps, we simply insert
additional empty action embeddings in the decoders inputs,
which are then mapped to a chunk of future actions. For chunk
size K, the model predicts KD actions in one forward pass,
increasing throughput K-fold with minimal latency impact.
While parallel decoding may theoretically be less expressive
than autoregressive approaches, our experiments show no
performance degradation across diverse tasks.
Continuous action representations. OpenVLA originally
uses discrete action tokens where each action dimension is
normalized to [1, 1] and uniformly discretized into 256
bins. While this approach is convenient since it requires
no architectural modifications to the underlying VLM, the
discretization process can sacrifice fine-grained action de-
tails. We study continuous action representations with two
learning objectives drawn from prominent imitation learning
gression by replacing the decoders output embedding layer
with an MLP action head that directly maps final decoder
layer hidden states to continuous action values. The model is
trained to minimize the mean L1 difference between predicted
and ground-truth actions, maintaining the efficiency benefits of
parallel decoding while potentially improving action precision.
denoising diffusion modeling where the model learns to pre-
dict noise added to action samples during forward diffusion.
During inference, the policy gradually denoises noisy action
samples via reverse diffusion to produce real actions. While
this approach offers potentially more expressive action mod-
diffusion steps in our implementation), impacting deployment
latency even with parallel decoding.
Additional model inputs and outputs. While the original
OpenVLA processes a single camera view, some robot setups
include multiple viewpoints and additional robot state informa-
tion. We implement a flexible input processing pipeline: For
camera images, we use OpenVLAs dual vision encoder to
extract 256 patch embeddings per view, which are projected
into the language embedding space with a shared projector
network. For low-dimensional robot state inputs (e.g., joint
angles and gripper state), we employ a separate projection
network to map these into the same embedding space as one
additional input embedding.
All input embeddingsvisual features, robot state, and
language tokensare concatenated along the sequence dimen-
sion before being passed to the decoder. This unified latent
representation enables the model to attend to all available
information when generating actions. Combined with parallel
decoding and action chunking, this architecture can efficiently
process rich multimodal inputs while generating multiple
timesteps of actions, as illustrated in Figure 1.
C. Augmenting OpenVLA-OFT with FiLM for Enhanced Lan-
guage Grounding.
Challenges with language following. When deploying on
the ALOHA robot setup with multiple viewpoints including
from wrist-mounted cameras, we observe that policies can
struggle with language following due to spurious correlations
in visual inputs. During training, policies may learn to latch
onto such spurious correlations when predicting actions, rather
than properly attending to the language instructions, resulting
in poor adherence to the users commands at test time. Further-
in a taskfor example, after grasping the spoon and deciding
which ingredient to scoop in the scoop X into bowl task
discussed in Section VI. Therefore, without special techniques,
training the model to appropriately focus on language inputs
can be particularly challenging.
FiLM. To enhance language following, we employ feature-
wise linear modulation (FiLM) , which infuses language
embeddings into the visual representations so that the model
pays more attention to the language inputs. We compute
the average of the language embeddings x from the task
description and project it to obtain scaling and shifting vectors
and . These vectors modulate the visual features F through
an affine transformation:
FiLM(F, )  F  (1  ) F
A crucial implementation detail is the choice of what
represents a feature for modulation in vision transformers.
While one might naturally consider treating individual patch
embeddings as features to be modulated, we find that this
approach results in poor language following. Instead, drawing
from how FiLM operates in convolutional networks, where
modulation applies spatially-agnostically by scaling and shift-
ing entire feature maps, we apply each element of  and
to the corresponding hidden unit across all visual patch
Fig. 3: LIBERO simulation benchmark  task suites. We study VLA
fine-tuning design decisions using four representative task suites. Here we
depict two of ten tasks per suite.
embeddings so that  and  influence all patch embeddings.
where DV iT is the number of hidden dimensions (i.e., the
number of elements in each of the patch embeddings in the
vision transformers latent representations).
We apply FiLM after the self-attention layer and before
the feedforward layer in each vision transformer block, with
separate projectors for each block (see Figure 8). Additional
implementation details are provided in Appendix C. We only
use FiLM for the ALOHA experiments discussed in Section
of spurious correlations in visual inputs.
V. EXPERIMENTS: EVALUATING VLA FINE-TUNING
DESIGN DECISIONS
In this section, we evaluate the effects of our proposed VLA
adaptation design decisions through controlled experiments
aimed at answering three key questions:
1) How does each design decision affect the fine-tuned
policys success rate on downstream tasks?
2) How does each design decision affect model inference
efficiency (action generation throughput and latency)?
3) How do the alternative fine-tuning formulations affect
flexibility in model input-output specifications?
A. LIBERO Experimental Setup
We evaluate on the LIBERO simulation benchmark ,
which features a Franka Emika Panda arm in simulation
with demonstrations containing camera images, robot state,
task annotations, and delta end-effector pose actions. We use
four task suitesLIBERO-Spatial, LIBERO-Object, LIBERO-
strations across 10 tasks to assess policy generalization to
different spatial layouts, objects, goals, and long-horizon tasks.
Following Kim et al. , we filter unsuccessful demon-
strations and fine-tune OpenVLA via LoRA  on each
task suite independently. We train for 50-150K gradient steps
for non-diffusion methods and 100-250K steps for diffusion
methods (which converge slower), using a batch size of 64-128
across 8 A100H100 GPUs. We test checkpoints every 50K
steps and report the best performance for each run. Unless
specified otherwise, policies receive one third-person image
and language instruction as input. For methods using action
Policy baseline , and execute full chunks before replanning,
which we find improves both speed and performance. See
Appendix D for hyperparameter details.
Our primary baseline in this study is the base OpenVLA
model fine-tuned using the original fine-tuning recipe. How-
from prior state-of-the-art imitation learning methods, such as
Diffusion Policy , Octo , DiT Policy , Seer ,
90 pretraining data.
B. LIBERO Task Performance Comparisons
For satisfactory deployment, robot policies must demon-
strate reliable task execution. We first assess how different
VLA fine-tuning design decisions affect success rates on the
LIBERO benchmark.
Our efficiency analysis (which we discuss later) reveals that
parallel decoding (PD) and action chunking (AC) together
are necessary for high-frequency control (25-50 Hz), espe-
cially for bimanual robots with double the amount of action
dimensions. We therefore evaluate OpenVLA policies with
both techniques used jointly, comparing variants using discrete
actions with diffusion.
Results in Table I show that parallel decoding and ac-
tion chunking not only increase throughput but also improve
performance significantly, raising average success rates by
14 (absolute) over autoregressive OpenVLA policies. This
improvement is particularly pronounced in LIBERO-Long,
suggesting that action chunking helps capture temporal de-
pendencies  and reduce compounding errors , which
ultimately leads to smoother and more reliable task execution.
In addition, we find that using continuous action variants
further improves success rates by 5 (absolute) over the
discrete action variant, likely due to higher precision in the
action predictions. L1 regression and diffusion variants achieve
comparable performance, indicating that the high-capacity
OpenVLA model can effectively model the multi-task action
distribution even with simple L1 regression.
C. LIBERO Inference Efficiency Comparisons
Efficient inference is crucial for deploying VLAs on high-
frequency control robots. We now evaluate how parallel de-
coding (PD), action chunking (AC), and continuous action
representations affect model inference speed. We measure
average latency (time to generate one robot action or action
chunk) and throughput (total actions generated per second)
by querying each model variant 100 times on an NVIDIA
A100 GPU. Each query processes a 224 x 224 px image and
TABLE I: LIBERO task performance results. Success rates (SR) across LIBERO benchmark task suites . Results include policies fine-tuned from
pretrained base models (Octo, DiT Policy, Seer, 0), models trained from scratch (Diffusion Policy, Seer (scratch), MDT), and OpenVLA variants using
different fine-tuning design decisions: parallel decoding (PD), action chunking (AC, chunk size K  8 timesteps), and continuous actions with L1 regression
(Cont-L1) or with diffusion (Cont-Diffusion). Unless otherwise specified, OpenVLA diffusion policies use 50 diffusion steps at both train and test time.
OpenVLA results from this work are averaged over 500 trials for each task suite (10 tasks  50 episodes). Our full OpenVLA-OFT method with additional
inputs achieves state-of-the-art 97.1 average success rate. Baseline results are from the original papers, except for Diffusion Policy, Octo, and the original
OpenVLA policies, whose results are reported by Kim et al. . Bold and underlined values indicate best and second-best performance. We separate the
comparisons based on which inputs the policies receive as well as whether or not they were trained using modified datasets following Kim et al.  (in
the modified versions, actions with near-zero magnitude and unsuccessful demonstrations are filtered out).
: MDT results with 100 language annotations
were obtained through direct correspondence with the authors.
Policy inputs: third-person image, language instruction
(Modified training dataset; unsuccessful demonstrations filtered out)
Diffusion Policy (scratch)
Octo (fine-tuned)
DiT Policy (fine-tuned)
OpenVLA (fine-tuned)
OpenVLA (fine-tuned)  PDAC
OpenVLA (fine-tuned)  PDAC, Cont-Diffusion
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
Policy inputs: third-person image, wrist camera image, robot proprioceptive state (optional), language instruction
(Original unfiltered training dataset)
MDT (scratch; with 2 language annotations)
MDT (scratch; with 100 language annotations)
Seer (scratch)
Seer (pretrained on LIBERO-90, then fine-tuned)
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
Policy inputs: third-person image, wrist camera image, robot proprioceptive state (optional), language instruction
(Modified training dataset; unsuccessful demonstrations filtered out)
0  FAST (fine-tuned)
0 (fine-tuned)
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
TABLE II: LIBERO inference efficiency results. Action generation through-
put and latency for 7-dimensional actions, averaged over 100 queries on an
NVIDIA A100 GPU. Each query processes a 224  224 px image and a
LIBERO task command (pick up the alphabet soup and place it in the
basket). We compare OpenVLA variants with parallel decoding (PD), action
chunking (K  8 timesteps), and continuous actions using L1 regression or
diffusion objectives. For the diffusion variants, we report inference efficiency
measurements with varying numbers of denoising steps at test time using
the DDIM sampler , along with the corresponding success rates in the
LIBERO-Long task suite. In the final row, we report the inference efficiency
of our proposed OpenVLA-OFT formulation with additional inputs, such as
wrist camera image and robot state.
Throughput (Hz)
Latency (Sec)
LIBERO-Long SR ()
Ttrain  50, Ttest  50
Ttrain  50, Ttest  10
Ttrain  50, Ttest  5
Ttrain  50, Ttest  2
Ttrain  50, Ttest  1
Additional Inputs (wrist img, proprio)
a sample LIBERO language instruction (pick up the alphabet
soup and place it in the basket).
Results in Table II show that parallel decoding reduces
latency and increases throughput by 4 by replacing 7 se-
quential forward passes through the decoder portion of the
policy with a single pass. Adding action chunking (K  8)
increases latency by 17 due to longer attention sequences
in the decoder, but when combined with parallel decoding, it
dramatically improves throughput, achieving a 26 speedup
over baseline OpenVLA. The continuous actions variant with
L1 regression shows negligible difference in efficiency since
the additional MLP action head adds minimal computational
cost compared to the base model. The primary diffusion
variant requires 50 denoising steps (since this is the num-
ber of steps specified at train time) and thus suffers from
high latency. However, it still achieves the same effective
throughput as baseline OpenVLA due to parallel decoding
and action chunking. This means that despite longer pauses
between action chunks, the 50-step diffusion variant still
completes robot episodes at the same speed as the original
autoregressive variant. We also test the OpenVLA diffusion
variant using less than 50 denoising steps at inference time
(enabled by the DDIM sampler ), matching state-of-the-
art diffusion and flow matching methods that use 5-10 steps at
inference time [40, 27, 3]. These configurations result in much
greater inference efficiency, as shown in Table II. However,
the success rates decrease with fewer denoising steps due to
reduced model quality.
D. Model Input-Output Flexibility
As explained in Section IV-B and validated by our efficiency
evaluations in the prior section, parallel decoding enables
OpenVLA to generate action chunks with minimal latency
significant speedup realized by parallel decoding and action
chunking creates headroom for processing additional model
inputs as well. We demonstrate this by fine-tuning OpenVLA
with additional inputs such as robot proprioceptive state and a
robot wrist-mounted camera image, which doubles the number
of visual patch embeddings being passed into the language
model decoder, from 256 to 512. Despite this substantial
increase in input sequence length, the fine-tuned OpenVLA
policy maintains high throughput (71.4 Hz) and low latency
(0.112 sec), as shown in Table II.
Evaluating these policies with additional inputs on the
LIBERO benchmark reveals further improvements in average
success rate across all task suites (Table I). Notably, our
enhanced fine-tuned OpenVLA policies outperform even the
best fine-tuned 0 policies [3, 38]which benefit from a
base model with larger-scale pretraining and a more sophis-
ticated learning objective (flow matching )as well as
Multimodal Diffusion Transformer (MDT)  and Seer
policies. Even with a simpler base model pretrained on less
data than more recent VLAs, we find that our alternative VLA
adaptation design decisions empower fine-tuned OpenVLA
policies to establish a new state of the art on the LIBERO
benchmark.
E. Optimized Fine-Tuning Recipe
Based on the demonstrated improvements in task perfor-
we propose an Optimized Fine-Tuning (OFT) recipe for
VLA adaptation that combines three key components:
1) parallel decoding with action chunking
2) continuous action representation
3) L1 regression objective
These design choices work together to produce strong
policies that can be deployed at high frequencies while main-
taining algorithmic simplicity. We denote policies fine-tuned
from the OpenVLA base model using our OFT recipe as
OpenVLA-OFT. In Section VI, we evaluate OpenVLA-OFTs
capabilities on dexterous, bimanual manipulation tasks in the
real world using a high-frequency control robot.
F. Additional Experiments
Given that the alternative fine-tuning formulation, along
with additional model inputs and outputs, induces a distri-
bution shift between the base VLAs pretraining and fine-
pretrained representations are helpful and have any influence
on the results we have reported. We conduct an ablation
study in Appendix G2 to address this question, ablating
the OpenVLA pretraining phase and directly fine-tuning the
underlying pretrained VLM with the OFT recipe. As shown
in Table XV, the base OpenVLA pretrained representations are
indeed still beneficial for robotic policy learning, as removing
them leads to a 5.2 drop in average success rate (absolute)
in our LIBERO evaluation suite.
(See Appendix G for other additional experiments, including
scaling OpenVLA-OFT up to larger datasetsin both the
LIBERO simulation setting as well as a real-world single-arm
robot manipulation setting with over 50K demonstrations from
the BridgeData V2 dataset .)
VI. EXPERIMENTS: ADAPTING OPENVLA TO A
REAL-WORLD ALOHA ROBOT
While our experimental results in the prior section demon-
strate OpenVLA-OFTs effectiveness in simulation, successful
deployment in the real world, on robot platforms that differ
substantially from those seen during pretraining, is crucial
for showing broad applicability. We thus assess the efficacy
of our optimized fine-tuning recipe on the ALOHA robot
at a high control frequency. We evaluate on novel dexterous
manipulation tasks that have never been encountered before
during OpenVLAs pretraining (which only involves single-
arm robot data).
Prior works [54, 27, 3] have shown that vanilla LoRA fine-
tuning with autoregressive VLAs  is impractical for such
even lower for bimanual tasks) falls well below the 25-50 Hz
required for real-time deployment. We therefore exclude this
baseline from our experiments and compare more effective
methods that we discuss shortly.
In this section, we use an augmented version of our VLA
fine-tuning recipe (OFT) that additionally includes feature-
wise linear modulation (FiLM) for enhanced language ground-
policy instantiated through this augmented fine-tuning recipe
as OpenVLA-OFT.
A. ALOHA Experimental Setup
The ALOHA platform comprises two ViperX 300 S arms,
three camera viewpoints (one top-down, two wrist-mounted),
and robot state inputs (14-dimensional joint angles). It operates
at 25 Hz (reduced from the original 50 Hz to enable faster
training while still maintaining smooth robotic control), with
actions representing target absolute joint angles. This setup dif-
fers significantly from OpenVLAs pretraining, which includes
single-arm robot data only, a single camera viewpoint from
a third-person camera, no robot state inputs, low-frequency
control (3-10 Hz), and relative end-effector pose actions. The
distribution shift poses a challenge to the adaptation of this
We design four representative tasks testing deformable
object manipulation, long-horizon skills, tool usage, and
language-driven control:
1) fold shorts: Fold white shorts on a table with two
consecutive bimanual folds. Training: 20 demonstrations.
Fig. 4: ALOHA task performance results. Comparison between policies trained from scratch (ACT, Diffusion Policy) and fine-tuned VLAs (RDT-1B, 0,
OpenVLA-OFT) across four ALOHA manipulation tasks. OpenVLA-OFT enhances the base model with parallel decoding, action chunking, continuous
achieving the highest average performance. Scores represent average percent completion of each task (see rubrics and detailed results in Appendix F2).
2) fold shirt: Fold white T-shirt through multiple synchro-
nized bimanual folds, testing contact-rich, long-horizon
manipulation. Training: 30 demonstrations. Evaluation:
10 trials.
3) scoop X into bowl: Move bowl to center of table with
left arm, scoop specified ingredient (raisins, almonds
and green MMs, or pretzels) with right arm using
metal spoon. Training: 45 demonstrations (15 per ingre-
dient). Evaluation: 12 trials (4 per ingredient).
4) put X into pot: Open pot with left arm, place specified
item (green pepper, red pepper, or yellow corn)
with right arm, close pot. Training: 300 demonstrations
(100 per object). Evaluation: 24 trials (12 in-distribution,
12 out-of-distribution).
We fine-tune OpenVLA using OFT on each task indepen-
dently for 50-150K gradient steps (total batch size 32 with
8 A100H100-80GB GPUs) with action chunk size K  25.
At inference time, we execute the full action chunk before
requerying the model for the next chunk.
B. Methods in Comparison
The ALOHA tasks present a significant adaptation chal-
lenge for OpenVLA as the base model, given the substantial
differences from its pretraining platforms in terms of con-
trol frequency, action space, and input modalities. For this
VLAsRDT-1B  and 0 that were pretrained on
bimanual manipulation data and might reasonably be expected
to perform better on these downstream tasks. We evaluate
these models after fine-tuning them using their authors rec-
ommended recipes, and these methods serve as important
points of comparison. Additionally, to provide comparisons
with computationally efficient alternatives, we evaluate two
popular imitation learning baselines: ACT  and Diffusion
To enable language following in these baseline methods,
we use language-conditioned implementations. For ACT, we
modify EfficientNet-B0  to process CLIP  language
embeddings via FiLM [37, 43]. For Diffusion Policy, we use
the DROID dataset  implementation that conditions action
denoising on DistilBERT  language embeddings, modified
to support bimanual control and multiple image inputs.
C. ALOHA Task Performance Results
We evaluate all methodsACT, Diffusion Policy, RDT-1B,
vide fine-grained assessment, we use a predetermined rubric
that assigns scores for partial task completion (see Appendix
F for details). Figure 4 shows aggregate performance scores,
while Figure 5 specifically tracks language following ability
for the language-dependent tasks.
Performance of non-VLA baselines. The baseline methods
trained from scratch show varying levels of success. ACT,
while able to complete basic tasks, produces less precise
actions and achieves the lowest overall performance. Diffu-
sion Policy demonstrates stronger capabilities, matching or
exceeding RDT-1Bs reliability on the clothes folding and
scooping tasks. However, it struggles with the put X into pot
task which has a larger training dataset, suggesting limited
scalability compared to VLA-based approaches.
Performance of fine-tuned VLAs. Fine-tuned VLA poli-
cies generally outperform the from-scratch baselines in both
task execution and language following, consistent with prior
findings [27, 3]. Among VLAs, we observe distinct charac-
its Alternating Condition Injection scheme , but shows a
limitation in handling closed-loop feedback. As visualized in
Figure 6, it often fails to correct mistakes in the scoop X into
We use this FiLM-EfficientNet implementation only for language-
dependent tasks (scoop X into bowl and put X into pot). For clothes
folding tasks, we use the original ResNet-18  backbone as in .
Fig. 5: ALOHA language following results. Success rates in approaching
language-specified target objects for language-dependent tasks. Fine-tuned
VLAs follow the users command more frequently than policies trained from
scratch. OpenVLA-OFT shows the strongest language grounding, though
removing FiLM  reduces success to chance level.
Fig. 6: Sample rollouts contrasting RDT-1B and 0 error handling in
ALOHA tasks. Top: In some cases, RDT-1B fails to respond to missed
bowl placement, continuing to pour ingredients into empty space. Bottom:
0 demonstrates adaptive behavior by reattempting green pepper grasp after
initial failure, highlighting better visual feedback integration. See our website
for videos.
bowl taskfor instance, continuing to pour ingredients into
an imaginary bowl after missing the actual bowl, suggesting
over-reliance on proprioceptive state over visual feedback.
On the other hand, 0 demonstrates more robust execution
with smoother motions and better reactivity to feedback, often
successfully recovering from initial failures (as shown in
Figure 6). While its language following slightly trails RDT-
the strongest baseline. Finally, OpenVLA-OFT achieves the
highest performance across both task execution and language
following (see Figure 7 for examples of successful task
rollouts). This is particularly noteworthy given that the base
OpenVLA model was pretrained only on single-arm data,
while RDT-1B and 0 were pretrained on substantial bimanual
Fig. 7: Sample successful OpenVLA-OFT rollouts on the ALOHA robot.
OpenVLA-OFT can fold clothes, use a metal spoon to scoop and pour
targeted trail mix ingredients into a bowl, and place targeted objects into
a pot. See our website for videos.
datasets (6K episodes and 8K hours of bimanual data, respec-
tively). This suggests that the fine-tuning technique can be
more crucial than pretraining data coverage for downstream
performance.
Ablation study of FiLM. We evaluate the importance of
FiLM in our OpenVLA-OFT approach by ablating it and
assessing the policies language following ability on the last
two tasks, which require good language grounding for suc-
cessful execution. As shown in Figure 5, language following
drops to 33 in both tasksequal to randomly choosing the
correct instruction. This demonstrates that FiLM is essential
for preventing the model from overfitting to spurious visual
features and ensuring proper attention to language inputs.
Please see the project website for ALOHA robot rollout
videos and an in-depth qualitative analysis of all methods:
D. ALOHA Inference Efficiency Comparisons
We evaluate inference efficiency by measuring action gen-
eration throughput and latency across 100 queries for each
method. We report the results in Table III. The original
OpenVLA formulation, even with just the additional wrist
camera inputs, shows poor efficiency with 1.8 Hz throughput
and 0.543 sec latency. In contrast, OpenVLA-OFT achieves
77.9 Hz throughput, though its latency is higher compared to
the policies in the prior LIBERO experiments since it must
process two additional input images.
demonstrate
throughput
OpenVLA-OFT due to their smaller architectures: ACT
(84M parameters), Diffusion Policy (157M), RDT-1B (1.2B),
TABLE III: ALOHA robot inference efficiency results. Throughput and
latency measurements averaged over 100 queries on an NVIDIA A100 GPU.
Each query processes three 224  224 px images, 14-D robot state, and a
task command (scoop raisins into bowl). All methods use action chunk
size K25, except Diffusion Policy (K24, as a multiple of 4 is required)
and the original OpenVLA (K1, no chunking). RDT-1B predicts 64 actions,
but we execute the first 25 only for fair comparison. All implementations
use PyTorch except 0 (JAX). Bold and underlined values show best and
second-best performance.
Throughput (Hz)
Latency (Sec)
OpenVLA-OFT
Diffusion Policy
and 0 (3.3B)while OpenVLA has 7.5B parameters. ACT
achieves the highest speed by combining L1 regression-based
single-pass action generation (like OpenVLA-OFT) with its
compact architecture. Also, despite its larger size, 0 outper-
forms both RDT-1B and Diffusion Policy in speed thanks
to its optimized JAX implementation (all other methods are
implemented in PyTorch).
proaches RDT-1Bs (84.1 Hz) despite being 7 larger, as it
generates actions in a single forward pass rather than requiring
multiple denoising steps as in RDT-1B.
VII. DISCUSSION
Our study on VLA fine-tuning design decisions reveals
how different components impact inference efficiency, task
lowing ability. These insights lead to our Optimized Fine-
Tuning (OFT) recipe, which enables effective VLA adaptation
to novel robots and tasks through parallel decoding, action
ally) FiLM language conditioning. The success of OFT is
particularly noteworthy with OpenVLA: despite having no
exposure to bimanual robots or multi-view image inputs during
configurations and match or even outperform more recent
diffusion-based VLAs (0 and RDT-1B) which have encoun-
tered bimanual manipulators and multiple input images during
pretraining. This demonstrates that a well-designed fine-tuning
recipe can have a significant impact on final performance, and
existing VLAs can be successfully adapted to new robotic
systems without extensive retraining from scratch. Moreover,
our results show that a simple L1 regression-based approach
with a high-capacity model such as OpenVLA is quite ef-
fective for adapting to novel robots and tasks. This approach
offers practical advantages over diffusion-based methods: the
simpler algorithm leads to faster training convergence and
inference speed while maintaining strong performance, making
it particularly suitable for real-world robotics applications.
VIII. LIMITATIONS
While our Optimized Fine-Tuning (OFT) recipe shows
promise for adapting VLAs to novel robots and tasks, several
important questions remain.
Handling multimodal demonstrations. Our experiments
use focused demonstration datasets with a consistent strategy
per task. While L1 regression may help smoothen out noise
in training demonstrations by encouraging the policy to learn
the median mode in demonstrated actions, it may struggle to
accurately model truly multimodal action distributions where
multiple valid actions exist for the same input, which may not
be ideal in cases where the ability to generate alternative action
sequences would be beneficial for task completion. Conversely,
diffusion-based approaches may better capture such multi-
modality but risk overfitting to suboptimal modes in training
data (see our website for discussions and video illustrations
of these nuances). Understanding OFTs effectiveness with
multimodal demonstrations remains an important direction for
future work.
Pretraining versus fine-tuning. Our study focuses specif-
ically on fine-tuning VLAs for downstream tasks. Whether
OFTs benefits extend effectively to pretraining, or whether
more expressive algorithms like diffusion are necessary for
large-scale training, requires further investigation.
Inconsistent language grounding. Our ALOHA exper-
iments reveal that OpenVLA without FiLM exhibits poor
language grounding, despite showing no such issues in
LIBERO simulation benchmark experiments. The source of
this discrepancywhether from the lack of bimanual data
in pretraining or other factorsremains unclear and warrants
further study.
ACKNOWLEDGMENTS
We thank the Stanford Center for Research on Foundation
Models (CRFM) and Stanford Institute for Human-Centered
AI (HAI) for providing computational resources that supported
this research. Toyota Research Institute (TRI) provided funds
to assist the authors with their research, but this article solely
reflects the opinions and conclusions of its authors and not
TRI or any other Toyota entity. This work was also in
part supported by the Robotics and AI Institute and ONR
grant N00014-22-1-2621. We thank Physical Intelligence for
providing beta access to the 0 model used in our ALOHA
robot evaluations. We also thank Dan Fu, Karl Pertsch, and
Alec Lessing for insightful discussions which contributed to
the development of this work. Lastly, we are grateful to Moritz
Reuss for providing additional experimental results for MDT
and engaging in helpful discussions related to this work.
REFERENCES
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen
Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-
Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui
Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Ret-
Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan
Do as i can, not as i say:
Grounding language in robotic affordances, 2022. URL
Suneel Belkhale and Dorsa Sadigh.
better vla with a smaller footprint, 2024.
openvla-mini.
Kevin Black, Noah Brown, Danny Driess, Adnan Es-
language-action flow model for general robot control.
arXiv preprint arXiv:2410.24164, 2024.
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:
Vision-language-action models transfer web knowledge
to robotic control.
arXiv preprint arXiv:2307.15818,
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran
Song. Diffusion policy: Visuomotor policy learning via
action diffusion. The International Journal of Robotics
Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru
robots using vision-language models.
arXiv preprint
Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori,
Yusuke Noda, Paul Tang, Ehsan Adeli, Shrinidhi Kow-
shika Lakshmikanth, Kevin Schulman, Arnold Milstein,
An interactive agent foundation model.
preprint arXiv:2402.05929, 2024.
Introducing
human-like
reasoning
introducing-rfm-1-giving-robots-human-like-reasoning-capabilities.
Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Il-
shot object navigation. In Proceedings of the IEEECVF
Conference on Computer Vision and Pattern Recognition,
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770778, 2016.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415, 2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models. Advances in neural infor-
mation processing systems, 33:68406851, 2020.
Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu,
Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai,
and Yuntao Chen.
Diffusion transformer policy: Scal-
ing diffusion transformer for generalist vision-language-
action learning, 2025. URL
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
models. arXiv preprint arXiv:2106.09685, 2021.
Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun
An embodied
generalist agent in 3d world.
In Proceedings of the
International Conference on Machine Learning (ICML),
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch.
Language models as zero-shot plan-
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah
reasoning through planning with language models, 2022.
Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu
3d value maps for robotic manipulation with language
models. arXiv preprint arXiv:2307.05973, 2023.
Tobias Julg, Wolfram Burgard, and Florian Walter. Re-
fined policy distillation: From vla generalists to rl ex-
perts. arXiv preprint arXiv:2503.05833, 2025.
Siddharth Karamcheti, Suraj Nair, Annie S. Chen,
and Percy Liang.
Language-driven representation
learning for robotics.
Siddharth
Prismatic
Investigating
visually-conditioned language models.
arXiv preprint
Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ash-
win Balakrishna, Sudeep Dasari, Siddharth Karam-
Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: A
large-scale in-the-wild robot manipulation dataset. arXiv
preprint arXiv:2403.12945, 2024.
Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted
Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla:
An open-source vision-language-action model.
preprint arXiv:2406.09246, 2024.
Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu,
Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan
models as effective robot imitators.
arXiv preprint
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maxim-
ilian Nickel, and Matt Le. Flow matching for generative
modeling. arXiv preprint arXiv:2210.02747, 2022.
Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang
knowledge transfer for lifelong robot learning. Advances
in Neural Information Processing Systems, 36, 2024.
Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan,
Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun
Zhu. Rdt-1b: a diffusion foundation model for bimanual
manipulation. arXiv preprint arXiv:2410.07864, 2024.
Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, Yoonho
Bidirectional
resampling. arXiv preprint arXiv:2408.17355, 2024.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-
uan Li, and Jun Zhu.
guided sampling of diffusion probabilistic models. arXiv
preprint arXiv:2211.01095, 2022.
Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayara-
tation via value-implicit pre-training.
arXiv preprint
Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert
representations and rewards for robotic control.
International Conference on Machine Learning, pages
Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason
Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we
in the search for an artificial visual cortex for embodied
intelligence? Advances in Neural Information Processing
Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea
representation for robot manipulation. In CoRL, 2022.
Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram
ham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar,
et al. Open x-embodiment: Robotic learning datasets and
rt-x models. arXiv preprint arXiv:2310.08864, 2023.
Maxime Oquab, Timothee Darcet, Theo Moutakanni,
Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fer-
without supervision. arXiv preprint arXiv:2304.07193,
Cheng Pan, Kai Junge, and Josie Hughes.
language-action model and diffusion policy switching
enables dexterous control of an anthropomorphic hand.
arXiv preprint arXiv:2410.14022, 2024.
Ethan Perez, Florian Strub, Harm De Vries, Vincent
with a general conditioning layer. In Proceedings of the
AAAI conference on artificial intelligence, volume 32,
Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny
enization for vision-language-action models, 2025. URL
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Amanda Askell, Pamela Mishkin, Jack Clark, et al.
Learning transferable visual models from natural lan-
guage supervision. In International conference on ma-
chine learning, pages 87488763. PMLR, 2021.
Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel,
and Rudolf Lioutikov. Multimodal diffusion transformer:
Learning versatile behavior from multimodal goals, 2024.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A
reduction of imitation learning and structured prediction
to no-regret online learning.
In Proceedings of the
fourteenth international conference on artificial intelli-
gence and statistics, pages 627635. JMLR Workshop
and Conference Proceedings, 2011.
distilled
arXiv preprint
Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z Zhao, Ar-
chit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine,
and Chelsea Finn.
Yell at your robot: Improving
on-the-fly from language corrections.
arXiv preprint
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit
situated robot task plans using large language mod-
Chan Hee Song, Jiaman Wu, Clayton Washington,
Brian M. Sadler, Wei-Lun Chao, and Yu Su.
agents with large language models, 2023. URL https:
arxiv.orgabs2212.04088.
Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models.
arXiv preprint
Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrish-
object manipulation using pre-trained vision-language
models. arXiv preprint arXiv:2303.00905, 2023.
Mingxing Tan and Quoc Le.
model scaling for convolutional neural networks.
International conference on machine learning, pages
Octo Model Team, Dibya Ghosh, Homer Walke, Karl
open-source generalist robot policy.
arXiv preprint
Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua
dynamics models are scalable learners for robotic ma-
nipulation. arXiv preprint arXiv:2412.15109, 2024.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.
Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan
dre Wang He, Vivek Myers, Moo Jin Kim, Max Du,
Bridgedata v2: A dataset for robot learning at
scale. In Conference on Robot Learning, pages 1723
lingo-2-driving-with-language.
Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun
efficient vision-language-action models for robotic ma-
nipulation. arXiv preprint arXiv:2409.12514, 2024.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer.
Sigmoid loss for language image pre-
training. In Proceedings of the IEEECVF International
Conference on Computer Vision, pages 1197511986,
Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea
Finn. Learning fine-grained bimanual manipulation with
low-cost hardware.
arXiv preprint arXiv:2304.13705,
Tony Z Zhao, Jonathan Tompson, Danny Driess, Pete
Ayzaan Wahid. Aloha unleashed: A simple recipe for
robot dexterity. arXiv preprint arXiv:2410.13126, 2024.
Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang,
Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-
arXiv preprint arXiv:2403.09631, 2024.
Hongkuan Zhou, Xiangtong Yao, Yuan Meng, Siming
Language-conditioned learning for robotic manipulation:
A survey. arXiv preprint arXiv:2312.10807, 2023.
APPENDIX
A. Model Architecture Details
Base OpenVLA Architecture. OpenVLA combines a fused
vision backbone (with both SigLIP  and DINOv2
vision transformers), a Llama-2 7B language model ,
and a 3-layer MLP projector with GELU activation  for
projecting visual features into the language embedding space.
The original model processes a single third-person image
and a language instruction (e.g., put eggplant into pot).
The fused vision encoder extracts 256 patch embeddings from
each vision transformer, concatenates them along the hidden
space. These projected features are concatenated with language
embeddings along the sequence dimension before being pro-
cessed by the Llama-2 decoder to output a 7-dimensional robot
action representing delta end-effector pose, represented by a
string of discrete action tokens.
OpenVLA-OFT architecture modifications. OpenVLA-
OFT introduces six key changes:
1) processes multiple input images (e.g., third-person image
plus wrist camera images) through the shared SigLIP-
DINOv2 backbone
2) projects robot proprioceptive state to language embedding
space via a 2-layer MLP with GELU activation
3) replaces causal attention with bidirectional attention for
parallel decoding
4) substitutes the language model decoder output layer with
a 4-layer MLP (ReLU activation) for generation of con-
tinuous actions (instead of discrete actions)
5) outputs chunks of K actions instead of single-timestep
6) (for OpenVLA-OFT) adds FiLM  modules that use
the average task language embedding to modulate visual
features in both SigLIP and DINOv2 vision transformers
(see Appendix C for details)
The complete OpenVLA-OFT architecture is illustrated in
Figure 1.
B. Implementation Details
1) Parallel Decoding Implementation:
In the original
OpenVLA autoregressive training scheme, the model receives
ground-truth action tokens shifted right by one position as
input (a setup known as teacher forcing). A causal attention
mask ensures the model only attends to current and previous
tokens. At test time, each predicted token is fed back as input
for the next prediction.
For parallel decoding, we replace this input with empty
action embeddings that differ only in their positional encoding
values (similar to ). We also use a bidirectional attention
mask (instead of causal), enabling the model to leverage
all intermediate features non-causally when predicting each
element in the action chunk.
2) Continuous Action Representations: For discrete actions,
increasing the number of bins used for discretization improves
precision but reduces the frequency of individual tokens in the
training data, potentially hurting generalization. On the other
directly model the action distribution without lossy discretiza-
Our continuous representation implementations use the fol-
lowing specifications:
L1 regression: The MLP action head consists of 4 layers
with ReLU activation, mapping final Llama-2 decoder layer
hidden states directly to continuous actions.
DDIM  sampler with 50 diffusion timesteps
Squared cosine beta schedule following [5, 57]
4-layer noise predictor with same MLP architecture as
the L1 regression head
3) Input Processing Details: Passing each input image
through the OpenVLA fused vision encoder produces 256
patch embeddings, which are projected to the langauge model
embedding space via a 3-layer MLP with GELU activation
. Low-dimensional robot states are also projected to the
language embedding space through a 2-layer MLP with GELU
activation.
C. Feature-wise Linear Modulation (FiLM) Implementation
FiLM schematic. Section IV-C describes how we imple-
ment feature-wise linear modulation (FiLM)  for Open-
VLA. Figure 8 illustrates our implementation. OpenVLA has
a fused vision encoder with both SigLIP  and DINOv2
vision transformers, and we apply FiLM to both transformers.
Design considerations. In our implementation, following
Perez et al. , we multiply F by (1  ) instead of  since
and  are near zero at initialization. This helps preserve the
visual encoders original activations at the start of fine-tuning,
minimizing perturbation in the pretrained representation.
Implementation specifics. The functions f(x) and h(x)
that project language embeddings to obtain  and  are imple-
mented as simple affine transformations. Separate projectors
are learned for each transformer block to allow for block-
specific modulation patterns. This design enables the model to
learn different modulation patterns at different levels of visual
feature processing.
One might initially consider modulating each patch embed-
ding independently, as opposed to each hidden dimension of
each embedding as discussed in Section IV-C. However, our
spatially-agnostic modulation approach more closely mirrors
FiLMs operation in convolutional networks, where modula-
tion applies globally across spatial dimensions since entire
feature maps are scaled and shifted by individual elements of
and . This design choice better maintains the benefits of
FiLM and improves the policys language grounding substan-
tially. We find that an alternative formulation that modulates
each patch embedding independently leads to weaker language
grounding.
D. OpenVLA-OFT Hyperparameters and Training Details
OpenVLA-OFT training details for LIBERO. Hyper-
parameters for OpenVLA-OFT fine-tuning on LIBERO are
Fig. 8: Feature-wise linear modulation (FiLM) implementation in OpenVLAs vision backbone. We integrate FiLM  into both SigLIP  and
DINOv2  vision transformers in OpenVLAs fused vision backbone. The average task description embedding modulates visual features through scale and
shift operations at each transformer block, enhancing language-vision integration. This modification significantly improves language following in ALOHA
listed in Table IV. We train until the mean L1 loss between
predicted and ground-truth normalized actions (scaled between
[1, 1]) falls below 0.01. For faster convergence, we decay
the learning rate from 5e-4 to 5e-5 after 100K gradient steps.
We evaluate checkpoints every 50K steps, with the 150K
checkpoint achieving best performance in all task suites except
for LIBERO-Goal. Note that we do not use FiLM for LIBERO
experiments since the fine-tuned policies without it already
demonstrate good language grounding.
OpenVLA-OFT training details for ALOHA. Hyper-
parameters for OpenVLA-OFT training on ALOHA tasks
(with FiLM in the augmented OFT recipe) are shown in
Table V. We maintain the same convergence criterion as in
the LIBERO experiments (training until mean normalized L1
loss falls below 0.01) and similar learning rate decay strategy
(again 10 reduction, but after 50K gradient steps instead
of 100K, since the ALOHA datasets are smaller). For the two
clothes folding tasks which do not require language grounding,
we still include FiLM to verify that these additional parameters
do not impair task execution.
E. Baseline Methods Hyperparameters and Training Details
ACT training details for ALOHA. Table VI lists hyper-
parameters for ACT  trained from scratch on each task.
For non-language-dependent tasks (fold shorts, fold shirt),
we use the default ResNet-18  backbone, which does
not include language conditioning. For language-dependent
tasks (scoop X into bowl, put X into pot), we implement
EfficientNet-B0  with FiLM , similar to . While
the authors of ACT recommend training for at least 5K epochs,
we extend training to 10K-70K epochs per task to improve
performance.
Diffusion Policy training details for ALOHA. For Dif-
fusion Policy training, we use the DROID implementation
, which conditions action predictions on DistilBERT
language embeddings of the task description. We list hyper-
parameters in Table VII.
RDT-1B training details for ALOHA. Hyperparameters
for RDT-1B fine-tuning are shown in Table VIII. The authors
of RDT-1B recommend training for 150K gradient steps,
but we observe that training converges in significantly fewer
steps since our fine-tuning datasets are much smaller than the
RDT-1B fine-tuning dataset. Therefore, we observe that it is
unnecessary to train for such a large amount of time. In fact, on
scoop X into bowl, the earlier 18K step checkpoint (73.3
success) outperforms the later 40K step checkpoint (70.0)
(we report the former in our ALOHA experiments).
0 training details for ALOHA. Table IX lists hyperpa-
rameters for 0 fine-tuning. We use full fine-tuning (the default
option in their codebase) and train until convergence.
F. ALOHA Evaluation Details
1) ALOHA Task Suite Details: Below are detailed specifi-
cations for each task in our ALOHA experiments:
1. fold shorts
nized folds
Episode length: 1000 timesteps (40 seconds)
Initial states: See Figure 9
2. fold shirt
nized bimanual folds
Episode length: 1250 timesteps (50 seconds)
Initial states: See Figure 10
3. scoop X into bowl
(raisins, almonds and green MMs, or pretzels) into bowl
validation)
Episode length: 900 timesteps (36 seconds)
Initial states: See Figure 11
4. put X into pot
15 validation)
Initial variation: 45 cm horizontal, 20 cm vertical for food
items; fixed pot pose
Episode length: 400 timesteps (16 seconds)
out-of-distribution evaluations)
Initial states: See Figures 12 (in-distribution) and 13 (out-
of-distribution)
2) ALOHA Task Scoring Rubric: The scoring rubrics and
detailed results for the four ALOHA tasks are shown in Tables
G. Additional Experiments
1) Single OpenVLA-OFT Policy for All LIBERO Task Suites
with OpenVLA-OFT policies trained on each task suite in-
dependently. In this section, we assess whether our method
scales to larger fine-tuning datasets by training one OpenVLA-
OFT policy on all four task suites combined. As shown in
Table XIV, this new policy achieves comparable average task
performance as the task suite-specific policiesconfirming
that our method scales to larger fine-tuning datasets.
2) Ablating FiLM in LIBERO: The FiLM ablation study in
Section VI suggests that FiLM is crucial for enabling strong
language following in real-world ALOHA robot tasks. In this
This relatively large number of demonstrations for the put X into pot
task is not necessary for satisfactory performance. It simply reflects an earlier
investigative phase of this work during which we encountered difficulties with
language grounding in learned policies and initially hypothesized that increas-
ing demonstration quantity might improve language following capabilities.
satisfactory language grounding, and we discovered that additional techniques
were needed for more reliable language grounding. We still decided to fine-
tune on the full dataset with 300 demonstrations nonetheless.
the LIBERO simulation tasks. We train a single OpenVLA-
OFT policy (with FiLM) on all LIBERO task suites combined
(similar to the previous section) and report task performance
results in Table XIV, comparing performance against the
OpenVLA-OFT policy trained without FiLM. In LIBERO,
FiLM leads to slightly higher average success rate, though
the difference here is minor compared to the findings in the
real-world ALOHA experiments, where language following is
more challenging due to the reasons discussed in Section IV-C.
3) Ablating the OpenVLA Pretrained Representation: We
evaluate the performance of OpenVLA-OFT policies produced
by fine-tuning the underlying Prismatic VLM  directly on
the LIBERO downstream datasets without OpenVLAs Open
X-Embodiment  robot pretraining. This ablation study in-
vestigates whether OpenVLAs robot-pretrained representation
remains valuable when subjected to a substantially different
fine-tuning approach such as OFT. The results in Table XV
demonstrate that the variant without the pretrained OpenVLA
representation consistently underperforms compared to the full
OpenVLA-OFT model, confirming the benefits of using the
pretrained representation for downstream policy learning.
4) Scaling Up OpenVLA-OFT to a Larger Real-World
Dataset (BridgeData V2): In Appendix G1, we observe that
a single OpenVLA-OFT policy can effectively fit all four
LIBERO task suite datasets combined, confirming that the
proposed method scales to larger fine-tuning datasets. In this
whether OpenVLA-OFT can also fit a real-world robotic ma-
nipulation dataset that is significantly larger and more diverse
than both the LIBERO datasets and the ALOHA robot datasets
discussed in Section VI. Specifically, we train OpenVLA-OFT
(without FiLM) on the BridgeData V2 dataset , which
contains 50,365 real WidowX robot demonstrations (25 more
than the four LIBERO task suites combined).
We evaluate OpenVLA-OFT on a subset of BridgeData V2
WidowX robot tasks from the evaluation suite used in the
original OpenVLA work . This representative subset cov-
ers the four types of generalization (visual, motion, physical,
semantic) as well as language grounding tasks. We compare
the task performance of OpenVLA-OFT with that of the
public OpenVLA checkpoint, scoring both methods using the
same criteria used in the OpenVLA work. As shown in Table
these tasks (69.2 versus 65.8, respectively)confirming
scalability of the proposed OFT approach to much larger and
more diverse real robot datasets. Further, we observe that
FiLM is not necessary for satisfactory language following
in Bridge tasks, likely because the base model has shown
effective language following in Bridge tasks.
Although the base OpenVLA model was already pretrained on Bridge
recipe since the architecture, learning algorithm, and action representation and
decoding scheme have been modified significantly from the pretraining setup.
In fact, the initial action regression L1 loss in the beginning of OpenVLA-
OFT training on Bridge is large (roughly 0.5 when actions are normalized to
the scale [1, 1]).
TABLE IV: OpenVLA-OFT hyperparameters for LIBERO. We specify the hyperparameters for the full OpenVLA-OFT approach which includes parallel
hyperparameter
8 x NVIDIA A100 or H100 (80GB VRAM)
learning rate (LR)
total batch size
64 (8 per GPU)
train steps
150K for LIBERO-Spatial (with LR5e-5 after 100K steps);
150K for LIBERO-Object (with LR5e-5 after 100K steps);
50K for LIBERO-Goal;
150K for LIBERO-Long (with LR5e-5 after 100K steps)
input images
1 third-person camera image, 1 wrist-mounted camera image
input image size
use observation history
no (use single-step inputs)
LoRA rank
action chunk size
8 steps (predict 8, execute all 8 open-loop at test time)
use proprio (robot state)
use FiLM
trainable parameters
279M total (111M LoRA adapter  151M action head  17M proprio projector)
image augmentations
90 random crops, color jitter:
randomresizedcropdict(scale[0.9, 0.9], ratio[1.0, 1.0])
randombrightness[0.2]
randomcontrast[0.8, 1.2]
randomsaturation[0.8, 1.2]
randomhue[0.05]
TABLE V: OpenVLA-OFT hyperparameters for ALOHA experiments. We specify the hyperparameters for the full OpenVLA-OFT approach which
includes parallel decoding, action chunking, continuous actions with L1 regression, additional inputs (two wrist camera images and robot state), and FiLM.
hyperparameter
8 x NVIDIA A100 or H100 (80GB VRAM)
learning rate (LR)
total batch size
32 (4 per GPU)
train steps
100K for fold shorts (with LR5e-5 after 50K steps);
70K for fold shirt (with LR5e-5 after 50K steps);
50K for scoop X into bowl;
100K for put X into pot (with LR5e-5 after 50K steps);
input images
1 third-person camera image, 2 wrist-mounted camera images (left wrist  right wrist)
input image size
use observation history
no (use single-step inputs)
LoRA rank
action chunk size
25 steps (predict 25, execute all 25 open-loop at test time)
use proprio (robot state)
use FiLM
trainable parameters
853M total (111M LoRA adapter  269M action head  17M proprio projector  456M FiLM projectors)
image augmentations
90 random crops, color jitter:
randomresizedcropdict(scale[0.9, 0.9], ratio[1.0, 1.0])
randombrightness[0.2]
randomcontrast[0.8, 1.2]
randomsaturation[0.8, 1.2]
randomhue[0.05]
TABLE VI: ACT hyperparameters for ALOHA experiments. We follow default parameters from Zhao et al.  with three modifications: increased batch
size (8 to 64), reduced chunk size (100 to 25 to match other methods in our experiments), and EfficientNet-B0 with FiLM  for language-dependent tasks
instead of ResNet-18. All models are trained from scratch for at least 5000 epochs, as recommended by the authors of ACT.
hyperparameter
1 x NVIDIA Titan RTX (24GB VRAM)
learning rate (LR)
total batch size
train epochs
70K for fold shorts
30K for fold shirt
20K for scoop X into bowl
10K for put X into pot
input images
1 third-person camera image, 2 wrist-mounted camera images (left wrist  right wrist)
input image size
use observation history
no (use single-step inputs)
action chunk size
25 steps (predict 25, execute all 25 open-loop at test time)
use proprio (robot state)
image backbone
ResNet-18 for fold shorts and fold shirt tasks;
EfficientNet-B0 (w FiLM) for scoop X into bowl and put X into pot tasks
trainable parameters
84M for ResNet-18 variant; 80M for EfficientNet-B0 w FiLM variant
TABLE VII: Diffusion Policy hyperparameters for ALOHA experiments. This configuration follows the DROID implementation  with two
hyperparameter
1 x NVIDIA L40S (48GB VRAM)
learning rate (LR)
total batch size
train steps
30K for fold shorts
120K for fold shirt
80K for scoop X into bowl
40K for put X into pot
input images
1 third-person camera image, 2 wrist-mounted camera images (left wrist  right wrist)
input image size
use observation history
yes (2-step history)
action chunk size
24 steps (predict 24, execute all 24 open-loop at test time)
use proprio (robot state)
image backbone
ResNet-50
trainable parameters
diffusion sampling algorithm
number of diffusion steps
100 at train time; 10 at test time (similar to [5, 22])
image augmentations
color jitter, 80 random crops (as specified by default by )
TABLE VIII: RDT-1B hyperparameters for ALOHA experiments. This configuration follows default parameters from the RDT-1B codebase .
hyperparameter
1 x NVIDIA H100 (80GB VRAM)
learning rate (LR)
total batch size
train steps
86K for fold shorts
30K for fold shirt
18K for scoop X into bowl (performs better than 40K checkpoint)
150K for put X into pot
input images
1 third-person camera image, 2 wrist-mounted camera images (left wrist  right wrist)
input image size
use observation history
yes (2-step history)
action chunk size
25 steps (predict 64 as in , execute 25 open-loop at test time)
use proprio (robot state)
trainable parameters
diffusion sampling algorithm
DDPM  at train time; DPM-Solver  at test time
number of diffusion steps
1000 at train time; 5 at test time (same as in )
image augmentations
color jitter, image corruption (see  for details)
TABLE IX: 0 hyperparameters for ALOHA experiments. This configuration follows the default settings specified in the original 0 project codebase .
hyperparameter
1 x NVIDIA A100 or H100 (80GB VRAM)
learning rate (LR)
2.5e-5 peak LR (1K steps linear warmup, 29K steps cosine decay to 2.5e-6)
total batch size
train steps
115K for fold shorts
75K for fold shirt
80K for scoop X into bowl
80K for put X into pot
input images
1 third-person camera image, 2 wrist-mounted camera images (left wrist  right wrist)
input image size
use observation history
no (use single-step inputs)
action chunk size
25 steps (predict 25, execute all 25 open-loop at test time)
use proprio (robot state)
trainable parameters
diffusion sampling algorithm
flow matching
number of integration steps
image augmentations
random crop (for non-wrist images), random rotation (for non-wrist images), color jitter:
augmax.RandomCrop(int(width  0.95), int(height  0.95))
augmax.Rotate((-5, 5))
augmax.ColorJitter(brightness0.3, contrast0.4, saturation0.5)
(same as )
Fig. 9: Initial states for fold shorts task evaluations. We use the smallest variance in initial positions for this task due to the small number of training
demonstrations (20) compared to other tasks. During the first eight rollouts, the shorts position is incrementally adjusted by up to 3 cm along the vertical
axis of the tabletop surface. In the final two rollouts, the shorts are rotated 10 degrees clockwise to align the bottom edge to be parallel to the tabletops
horizontal axis.
Fig. 10: Initial states for fold shirt task evaluations. The bottom edge of the shirt varies by 15 cm along the tabletops vertical axis across different trials.
Fig. 11: Initial states for scoop X into bowl task evaluations. The green bowls position varies by up to 10 cm across both vertical and horizontal table
axes. Simultaneously, the metal spoons position varies by 3 cm along the horizontal axis. Each row in this grid displays a set of three identical initial states
since we rotate among three target food ingredients (raisins, almonds and green MMs, and pretzels) with the same initial states, before moving on to
the next set of initial states.
Fig. 12: Initial states for put X into pot task evaluations (in-distribution version). Food object positions vary by 15 cm vertically and 40 cm horizontally
across trials, while the pot remains stationary. Each grid row presents three identical initial states, as we cycle through the three target objects (green pepper,
red pepper, and yellow corn) using the same initial configurations before we move on to the next set of initial configurations. (Refer to Figure 13 for the
out-of-distribution version of this task.)
Fig. 13: Initial states for put X into pot task evaluations (out-of-distribution version). Unlike the in-distribution version of this task (Figure 12), this
version introduces unseen green and orange bowls as distractor objects to assess robustness to novel visual inputs. Food object positions vary by 15 cm
vertically and 30 cm horizontally across trials, with the pot remaining stationary. Each grid row presents three identical initial states, as we cycle through
the three target objects (green pepper, red pepper, and yellow corn) using the same initial configurations before we move on to the next set of initial
configurations.
TABLE X: Detailed results for the fold shorts ALOHA task. Success is assessed using a staged scoring system, with 20 points allocated to each of five
task stages. Points are awarded cumulatively, requiring successful completion of prior stages to receive a nonzero score in subsequent stages. The reported
values represent the average performance across all experimental trials.
Grasped bottom edge
Folded in half
Grasped waistband
Folded in half again
Moved shorts forward
(20 pts)
(20 pts)
(20 pts)
(20 pts)
(20 pts)
(100 pts)
Diffusion Policy
OpenVLA-OFT
TABLE XI: Detailed results for the fold shirt ALOHA task. Success is assessed using a staged scoring system, with 10 points allocated to each of
ten task stages. Points are awarded cumulatively, requiring successful completion of prior stages to receive a nonzero score in subsequent stages. A penalty
of -10 points is applied if a large portion of the shirt sticks out after the final fold, as this is a common error for one method and is visually apparent. The
reported values represent the average performance across all experimental trials.
Grasped bottom edge
Folded in half
Grasped sleeves
Folded sleeves over
Grasped bottom edge
Folded in half
Grasped right edge
Folded in half
Released shirt
Moved shirt forward
Large part sticks out
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(10 pts)
(-10 pts)
(100 pts)
Diffusion Policy
OpenVLA-OFT
TABLE XII: Detailed results for the scoop X into bowl ALOHA task. Success is assessed using a staged scoring system. Points are awarded cumulatively,
requiring successful completion of prior stages to receive a nonzero score in subsequent stages. Penalties of -5 points are applied if any food is spilled onto
the table or if the spoon is not fully emptied during pouring. The reported values represent the average performance across all experimental trials.
Moved bowl to center
Grasped spoon
Approached correct container
Scooped correct item
Poured into bowl
Placed spoon next to bowl
Spilled food on table
Did not empty spoon
Diffusion Policy
OpenVLA-OFT (no FiLM)
OpenVLA-OFT
TABLE XIII: Detailed results for the put X into pot ALOHA task (combined results for in-distribution and out-of-distribution evaluations).
Success is assessed using a staged scoring system. Points are awarded cumulatively, requiring successful completion of prior stages to receive a nonzero score
in subsequent stages. The reported values represent the average performance across all experimental trials.
Opened pot
Approached correct object
Touched correct object
Grasped correct object
Put correct object into pot
Placed lit on pot
Diffusion Policy
OpenVLA-OFT (no FiLM)
OpenVLA-OFT
TABLE XIV: Additional LIBERO experiments: Single policy for all LIBERO task suites and FiLM ablation study. Here we train a single policy on
all four LIBERO task suites combined, without FiLM (OpenVLA-OFT) and with FiLM (OpenVLA-OFT). Policy inputs here include a third-person
camera image, wrist camera image, robot proprioceptive state, and a language instruction. We observe comparable task performance when training on all task
suites together versus training on each of them independently. In addition, we find that FiLM has little effect on task performance in LIBERO, given that the
variant without FiLM already exhibits good language following.
OpenVLA-OFT (1 policy per suite) (original, from Table I)
OpenVLA-OFT (1 policy for all 4 suites)
OpenVLA-OFT (1 policy for all 4 suites,  FiLM)
TABLE XV: Ablation experiment: Fine-tuning OpenVLA from scratch with the OFT recipe. Policy inputs here include a third-person camera image,
a wrist camera image, robot proprioceptive state, and a language instruction. The from-scratch policies generally perform worse than the full OpenVLA-
OFT policies, confirming that OpenVLAs pretrained representation is beneficial for downstream policy performance even when the fine-tuning recipe differs
substantially from the pretraining recipe.
OpenVLA-OFT
OpenVLA-OFT (scratch)
TABLE XVI: OpenVLA-OFT in BridgeData V2 WidowX robot evaluations. We use a subset of the tasks from the original OpenVLA BridgeData V2
evaluation suite, along with the corresponding scoring criteria. Each score is the average over 10 trials per task. OpenVLA-OFT surpasses OpenVLA in
average task performance and even demonstrates effective language following without FiLM.
Category
OpenVLA Score
OpenVLA-OFT Score
Visual gen
Put Eggplant into Pot
Motion gen
Lift Eggplant
Physical gen
Flip Pot Upright
Semantic gen
Lift White Tape
Language grounding
Put {Blue Cup, Pink Cup} on Plate
Language grounding
Lift {Cheese, Red Chili Pepper}
