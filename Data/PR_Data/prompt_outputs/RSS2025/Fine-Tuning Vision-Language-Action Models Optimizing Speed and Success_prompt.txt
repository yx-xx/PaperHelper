=== PDF文件: Fine-Tuning Vision-Language-Action Models Optimizing Speed and Success.pdf ===
=== 时间: 2025-07-22 10:00:36.694295 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Fine-Tuning Vision-Language-Action Models:
Optimizing Speed and Success
Moo Jin Kim1
Chelsea Finn1
Percy Liang1
AbstractRecent vision-language-action models (VLAs) build
upon pretrained vision-language models and leverage diverse
robot datasets to demonstrate strong task execution, language
following ability, and semantic generalization. Despite these
tuning to achieve good performance, yet how to most effectively
fine-tune them is unclear given many possible strategies. In
this work, we study key VLA adaptation design choices such
as different action decoding schemes, action representations,
and learning objectives for fine-tuning, using OpenVLA as our
representative base model. Our empirical analysis informs an
Optimized Fine-Tuning (OFT) recipe that integrates parallel
and a simple L1 regression-based learning objective to altogether
improve inference efficiency, policy performance, and flexibility
in the models input-output specifications. We propose OpenVLA-
art on the LIBERO simulation benchmark, significantly boosting
OpenVLAs average success rate across four task suites from
76.5 to 97.1 while increasing action generation throughput
by 26. In real-world evaluations, our fine-tuning recipe enables
OpenVLA to successfully execute dexterous, high-frequency con-
trol tasks on a bimanual ALOHA robot and outperform other
VLAs (0 and RDT-1B) fine-tuned using their default recipes,
as well as strong imitation learning policies trained from scratch
(Diffusion Policy and ACT) by up to 15 (absolute) in average
success rate. We release code for OFT and pretrained model
checkpoints at
I. INTRODUCTION
Recent vision-language-action models (VLAs)robot poli-
cies built by fine-tuning pretrained vision-language models on
large-scale robot datasets for low-level robotic controlhave
demonstrated strong task performance, semantic generaliza-
tasks [4, 34, 24, 23, 7, 15, 8, 53, 58, 3]. Despite their strengths,
fine-tuning is crucial for satisfactory deployment of VLAs on
novel robots and tasks, yet it is unclear what the most effective
approach for adaptation is given the large design space. A
robotics practitioner who wishes to fine-tune a VLA to a new
robot setup and task may default to using the same training
recipe used for pretraining (or a parameter-efficient variant),
but it is not obvious whether this would yield the best policy,
and there is limited empirical analysis of alternative fine-tuning
approaches in the literature.
1Stanford University. Correspondence to: Moo Jin Kim <moojinkcs.
stanford.edu>, Chelsea Finn <cbfinncs.stanford.edu>, Percy
Liang <pliangcs.stanford.edu>.
Prior work has begun exploring VLA adaptation strategies,
with Kim et al.  proposing parameter-efficient fine-tuning
via LoRA. However, their autoregressive action generation
remains too slow (3-5 Hz) for high-frequency control (25-
50 Hz), and both LoRA and full fine-tuning of autoregressive
VLAs often yield unsatisfactory performance in bimanual ma-
nipulation tasks [54, 27, 3]. While recent approaches improve
efficiency through better action tokenization schemes [2, 38],
achieving 2 to 13 speedups, significant latency between
action chunks (e.g., 750 ms for the recent FAST approach )
still limits real-time deployment on high-frequency bimanual
robots. Exploring alternative VLA adaptation approaches that
achieve both satisfactory speed and quality remains an under-
explored area of research.
In this work, we study key design decisions for adapt-
ing VLAs to novel robots and tasks using OpenVLA, a
representative autoregressive VLA, as our base model. We
examine three key design choices: action decoding scheme
(autoregressive vs. parallel generation), action representation
(discrete vs. continuous), and learning objective (next-token
prediction vs. L1 regression vs. diffusion). Our study reveals
several key insights that build on each other: (1) parallel
decoding with action chunking not only boosts inference
efficiency but also improves success rates on downstream tasks
while enabling greater flexibility in the models input-output
specifications; (2) continuous action representations further
improve model quality compared to discrete representations;
and (3) fine-tuning the VLA with an L1 regression objective
yields comparable performance to diffusion-based fine-tuning
while offering faster training convergence and inference speed.
Building on these insights, we introduce OpenVLA-OFT:
an instantiation of an Optimized Fine-Tuning (OFT) recipe that
integrates parallel decoding and action chunking, continuous
action representations, and an L1 regression objective to en-
hance inference efficiency, task performance, and model input-
output flexibility while maintaining algorithmic simplicity.
We conduct experiments on both the standardized LIBERO
simulation benchmark and dexterous tasks on a real bimanual
ALOHA robot. In LIBERO, OpenVLA-OFT establishes a new
state of the art by achieving 97.1 average success rate across
four task suites, outperforming both fine-tuned OpenVLA
policies  (76.5) and 0 policies  (94.2) while
achieving a 26 speedup in action generation with 8-step
action chunks. For real-world ALOHA tasks , we augment
our recipe with FiLM  for enhanced language grounding,
Fig. 1: OpenVLA-OFT on the bimanual ALOHA robot. Our Optimized Fine-Tuning (OFT) recipe enhances fine-tuned OpenVLA policies through
improved inference efficiency, model quality, and input-output flexibility. The resulting OpenVLA-OFT policies execute diverse dexterous manipulation tasks
on a real-world bimanual robot at high control frequencies (25 Hz). The  suffix indicates the integration of feature-wise linear modulation (FiLM) ,
which strengthens language grounding in tasks where accurate language understanding is critical for success.
denoting the augmented recipe as OFT. OpenVLA-OFT
successfully executes dexterous bimanual tasks like folding
clothes and manipulating target food items based on the users
prompt (see Figure 1), outperforming both fine-tuned VLAs
(0 and RDT-1B) and prominent imitation learning policies
trained from scratch (Diffusion Policy and ACT) by up to 15
(absolute) in average success rate. With 25-timestep action
base OpenVLA, demonstrating that our new fine-tuning recipe
enables real-time robot control with strong task performance
and language following ability.
II. RELATED WORK
Prior works have leveraged language and vision founda-
tion models to enhance robotic capabilities, using them as
pretrained visual representations that accelerate robotic policy
learning [30, 33, 31, 20, 32], for object localization in robotics
tasks [9, 47], and for high-level planning and reasoning [1, 17,
fine-tuning vision-language models (VLMs) to directly predict
low-level robotic control actions, producing vision-language-
and generalization to out-of-distribution test conditions
and unseen semantic concepts. These works focus primarily on
model development, while we focus on developing a recipe for
fine-tuning such models, justifying individual design decisions
with insights that we gain from our empirical analysis.
Despite the importance of fine-tuning for real-world VLA
remains limited. While Kim et al.  study various parameter
update strategies and from their findings show that LoRA fine-
tuning enables effective adaptation to single-arm robots oper-
ating at low control frequencies (< 10 Hz), their analysis does
not extend to bimanual robots with high control frequencies
(25-50 Hz), a more complex control scenario. We address
this gap by exploring VLA adaptation design decisions for fast
inference and reliable task execution on a real-world bimanual
manipulator with a 25 Hz controller.
Recent works by Belkhale and Sadigh  and Pertsch
et al.  improve VLA efficiency through new action to-
kenization schemes, using vector quantization or discrete co-
sine transform-based compression to represent action chunks
(sequences of actions) with fewer tokens than simple per-
dimension binning (as used in RT-2  and OpenVLA ).
While these approaches achieve 2 to 13 speedups for
autoregressive VLAs, we explore design decisions beyond
autoregressive modeling, which remains inherently limited by
iterative generation. Our parallel decoding approach, when
paired with action chunking, achieves significantly greater
(0.07 ms for single-arm tasks with one input image and 0.321
ms for bimanual tasks with three input images).
Another line of research [54, 27, 3] demonstrates effective
VLA fine-tuning for high-frequency, bimanual manipulation
using generative approaches like diffusion or flow match-
Fig. 2: Key design decisions for VLA fine-tuning. Left: Comparison between autoregressive decoding, which generates actions sequentially, and parallel
next-token prediction and continuous action values with L1 regression or diffusion modeling objectives. The original OpenVLA training scheme includes
autoregressive decoding, discrete actions, and next-token prediction.
ing. While these diffusion-based VLAs achieve higher action
throughput than autoregressive VLAs by generating multi-
timestep action chunks simultaneously, they introduce compu-
tational trade-offs through slower training and multiple denois-
ing or integration steps at inference time. Furthermore, these
diffusion VLAs vary considerably in architecture, learning
specificationsand which design elements most significantly
impact performance remains unclear. Through controlled ex-
regression objective can match more complex approaches in
task performance while achieving significantly greater infer-
ence efficiency.
low-level control policy in addition to a slower VLA ,
in this work we fine-tune the base VLA end-to-end with
techniques that enable high efficiency without needing a
separate controller. Further, unlike other works that collect
online interaction data to continually update specialist policies
via reinforcement learning , here we focus on the sim-
pler imitation learning paradigm in which we develop high-
performing policies just by training once offline on a fixed
dataset of expert task demonstrations.
III. PRELIMINARIES
Original OpenVLA formulation. We use OpenVLA
as our representative base VLA, a 7B-parameter manipulation
policy created by fine-tuning the Prismatic VLM  on
1M episodes from the Open X-Embodiment dataset . See
Appendix A for architecture details. OpenVLAs original train-
ing formulation uses autoregressive prediction of 7 discrete
robot action tokens per timestep: 3 for position control, 3
for orientation control, and 1 for gripper control. It employs
next-token prediction with cross-entropy loss as its learning
formulations including parallel decoding, continuous action
diffusion modeling in the next few sections.
Action chunking. Prior works have shown that action
chunkingi.e., predicting and executing a sequence of future
actions without intermediate replanningimproves policy suc-
cess rates across many manipulation tasks [56, 5, 28]. How-
action chunking impractical, as generating even a single-
timestep action takes 0.33 seconds on an NVIDIA A100 GPU.
For a chunk size of K timesteps and action dimensionality
versus just D passes without chunking. This K-fold increase in
latency makes action chunking impractical for high-frequency
robots under the original formulation. In the next section,
we present a parallel generation scheme that enables efficient
action chunking.
IV. STUDYING KEY VLA FINE-TUNING DESIGN
DECISIONS
In this section, we first outline key design decisions for
adapting VLAs to novel robot setups and tasks and provide
details on their implementation.
A. VLA Fine-Tuning Design Decisions
Existing approaches that fine-tune VLAs using the base
models autoregressive training recipe face two key limitations:
slow inference speed (3-5 Hz) unsuitable for high-frequency
To address these challenges, we investigate three key design
components for VLA fine-tuning:
(a) Action generation strategy (Figure 2, left): We com-
pare autoregressive generation, which requires sequential
token-by-token processing, with parallel decoding, which
generates all actions simultaneously and enables efficient
action chunking.
(b) Action representation (Figure 2, right): We examine
discrete actions (256-bin discretization of normalized ac-
tions) processed through softmax-based token prediction,
versus continuous actions directly generated by an MLP
action head. For discrete actions, the final hidden states
of the language model decoder are linearly projected into
form the probability distribution over action tokens. For
continuous actions, the final hidden states are instead
mapped directly to normalized continuous actions by a
separate action head MLP.
(c) Learning objective (Figure 2, right): We compare
policies fine-tuned with next-token prediction for discrete
conditional denoising diffusion  for continous actions
(similar to Chi et al. ).
We conduct our study using OpenVLA  as the base
relatively small training datasets (500 demonstrations versus
1M demonstrations for pretraining).
B. Implementing Alternative Design Components
The base OpenVLA model originally employs autoregres-
sive generation of discrete action tokens optimized via next-
token prediction. We implement alternative design decisions
for fine-tuning while keeping the original pretraining un-
changed. We describe the key implementation aspects below,
with further details explained in Appendix B.
Parallel decoding and action chunking. Unlike autore-
gressive generation which requires sequential token prediction,
parallel decoding enables the model to map input embeddings
to the predicted output sequence in a single forward pass. We
modify the model to receive empty action embeddings as input
and replace the causal attention mask with bidirectional atten-
This reduces action generation from D sequential passes to a
single pass, where D is the action dimensionality.
Parallel decoding naturally extends to action chunking: to
predict actions for multiple future timesteps, we simply insert
additional empty action embeddings in the decoders inputs,
which are then mapped to a chunk of future actions. For chunk
size K, the model predicts KD actions in one forward pass,
increasing throughput K-fold with minimal latency impact.
While parallel decoding may theoretically be less expressive
than autoregressive approaches, our experiments show no
performance degradation across diverse tasks.
Continuous action representations. OpenVLA originally
uses discrete action tokens where each action dimension is
normalized to [1, 1] and uniformly discretized into 256
bins. While this approach is convenient since it requires
no architectural modifications to the underlying VLM, the
discretization process can sacrifice fine-grained action de-
tails. We study continuous action representations with two
learning objectives drawn from prominent imitation learning
gression by replacing the decoders output embedding layer
with an MLP action head that directly maps final decoder
layer hidden states to continuous action values. The model is
trained to minimize the mean L1 difference between predicted
and ground-truth actions, maintaining the efficiency benefits of
parallel decoding while potentially improving action precision.
denoising diffusion modeling where the model learns to pre-
dict noise added to action samples during forward diffusion.
During inference, the policy gradually denoises noisy action
samples via reverse diffusion to produce real actions. While
this approach offers potentially more expressive action mod-
diffusion steps in our implementation), impacting deployment
latency even with parallel decoding.
Additional model inputs and outputs. While the original
OpenVLA processes a single camera view, some robot setups
include multiple viewpoints and additional robot state informa-
tion. We implement a flexible input processing pipeline: For
camera images, we use OpenVLAs dual vision encoder to
extract 256 patch embeddings per view, which are projected
into the language embedding space with a shared projector
network. For low-dimensional robot state inputs (e.g., joint
angles and gripper state), we employ a separate projection
network to map these into the same embedding space as one
additional input embedding.
All input embeddingsvisual features, robot state, and
language tokensare concatenated along the sequence dimen-
sion before being passed to the decoder. This unified latent
representation enables the model to attend to all available
information when generating actions. Combined with parallel
decoding and action chunking, this architecture can efficiently
process rich multimodal inputs while generating multiple
timesteps of actions, as illustrated in Figure 1.
C. Augmenting OpenVLA-OFT with FiLM for Enhanced Lan-
guage Grounding.
Challenges with language following. When deploying on
the ALOHA robot setup with multiple viewpoints including
from wrist-mounted cameras, we observe that policies can
struggle with language following due to spurious correlations
in visual inputs. During training, policies may learn to latch
onto such spurious correlations when predicting actions, rather
than properly attending to the language instructions, resulting
in poor adherence to the users commands at test time. Further-
in a taskfor example, after grasping the spoon and deciding
which ingredient to scoop in the scoop X into bowl task
discussed in Section VI. Therefore, without special techniques,
training the model to appropriately focus on language inputs
can be particularly challenging.
FiLM. To enhance language following, we employ feature-
wise linear modulation (FiLM) , which infuses language
embeddings into the visual representations so that the model
pays more attention to the language inputs. We compute
the average of the language embeddings x from the task
description and project it to obtain scaling and shifting vectors
and . These vectors modulate the visual features F through
an affine transformation:
FiLM(F, )  F  (1  ) F
A crucial implementation detail is the choice of what
represents a feature for modulation in vision transformers.
While one might naturally consider treating individual patch
embeddings as features to be modulated, we find that this
approach results in poor language following. Instead, drawing
from how FiLM operates in convolutional networks, where
modulation applies spatially-agnostically by scaling and shift-
ing entire feature maps, we apply each element of  and
to the corresponding hidden unit across all visual patch
Fig. 3: LIBERO simulation benchmark  task suites. We study VLA
fine-tuning design decisions using four representative task suites. Here we
depict two of ten tasks per suite.
embeddings so that  and  influence all patch embeddings.
where DV iT is the number of hidden dimensions (i.e., the
number of elements in each of the patch embeddings in the
vision transformers latent representations).
We apply FiLM after the self-attention layer and before
the feedforward layer in each vision transformer block, with
separate projectors for each block (see Figure 8). Additional
implementation details are provided in Appendix C. We only
use FiLM for the ALOHA experiments discussed in Section
of spurious correlations in visual inputs.
V. EXPERIMENTS: EVALUATING VLA FINE-TUNING
DESIGN DECISIONS
In this section, we evaluate the effects of our proposed VLA
adaptation design decisions through controlled experiments
aimed at answering three key questions:
1) How does each design decision affect the fine-tuned
policys success rate on downstream tasks?
2) How does each design decision affect model inference
efficiency (action generation throughput and latency)?
3) How do the alternative fine-tuning formulations affect
flexibility in model input-output specifications?
A. LIBERO Experimental Setup
We evaluate on the LIBERO simulation benchmark ,
which features a Franka Emika Panda arm in simulation
with demonstrations containing camera images, robot state,
task annotations, and delta end-effector pose actions. We use
four task suitesLIBERO-Spatial, LIBERO-Object, LIBERO-
strations across 10 tasks to assess policy generalization to
different spatial layouts, objects, goals, and long-horizon tasks.
Following Kim et al. , we filter unsuccessful demon-
strations and fine-tune OpenVLA via LoRA  on each
task suite independently. We train for 50-150K gradient steps
for non-diffusion methods and 100-250K steps for diffusion
methods (which converge slower), using a batch size of 64-128
across 8 A100H100 GPUs. We test checkpoints every 50K
steps and report the best performance for each run. Unless
specified otherwise, policies receive one third-person image
and language instruction as input. For methods using action
Policy baseline , and execute full chunks before replanning,
which we find improves both speed and performance. See
Appendix D for hyperparameter details.
Our primary baseline in this study is the base OpenVLA
model fine-tuned using the original fine-tuning recipe. How-
from prior state-of-the-art imitation learning methods, such as
Diffusion Policy , Octo , DiT Policy , Seer ,
90 pretraining data.
B. LIBERO Task Performance Comparisons
For satisfactory deployment, robot policies must demon-
strate reliable task execution. We first assess how different
VLA fine-tuning design decisions affect success rates on the
LIBERO benchmark.
Our efficiency analysis (which we discuss later) reveals that
parallel decoding (PD) and action chunking (AC) together
are necessary for high-frequency control (25-50 Hz), espe-
cially for bimanual robots with double the amount of action
dimensions. We therefore evaluate OpenVLA policies with
both techniques used jointly, comparing variants using discrete
actions with diffusion.
Results in Table I show that parallel decoding and ac-
tion chunking not only increase throughput but also improve
performance significantly, raising average success rates by
14 (absolute) over autoregressive OpenVLA policies. This
improvement is particularly pronounced in LIBERO-Long,
suggesting that action chunking helps capture temporal de-
pendencies  and reduce compounding errors , which
ultimately leads to smoother and more reliable task execution.
In addition, we find that using continuous action variants
further improves success rates by 5 (absolute) over the
discrete action variant, likely due to higher precision in the
action predictions. L1 regression and diffusion variants achieve
comparable performance, indicating that the high-capacity
OpenVLA model can effectively model the multi-task action
distribution even with simple L1 regression.
C. LIBERO Inference Efficiency Comparisons
Efficient inference is crucial for deploying VLAs on high-
frequency control robots. We now evaluate how parallel de-
coding (PD), action chunking (AC), and continuous action
representations affect model inference speed. We measure
average latency (time to generate one robot action or action
chunk) and throughput (total actions generated per second)
by querying each model variant 100 times on an NVIDIA
A100 GPU. Each query processes a 224 x 224 px image and
TABLE I: LIBERO task performance results. Success rates (SR) across LIBERO benchmark task suites . Results include policies fine-tuned from
pretrained base models (Octo, DiT Policy, Seer, 0), models trained from scratch (Diffusion Policy, Seer (scratch), MDT), and OpenVLA variants using
different fine-tuning design decisions: parallel decoding (PD), action chunking (AC, chunk size K  8 timesteps), and continuous actions with L1 regression
(Cont-L1) or with diffusion (Cont-Diffusion). Unless otherwise specified, OpenVLA diffusion policies use 50 diffusion steps at both train and test time.
OpenVLA results from this work are averaged over 500 trials for each task suite (10 tasks  50 episodes). Our full OpenVLA-OFT method with additional
inputs achieves state-of-the-art 97.1 average success rate. Baseline results are from the original papers, except for Diffusion Policy, Octo, and the original
OpenVLA policies, whose results are reported by Kim et al. . Bold and underlined values indicate best and second-best performance. We separate the
comparisons based on which inputs the policies receive as well as whether or not they were trained using modified datasets following Kim et al.  (in
the modified versions, actions with near-zero magnitude and unsuccessful demonstrations are filtered out).
: MDT results with 100 language annotations
were obtained through direct correspondence with the authors.
Policy inputs: third-person image, language instruction
(Modified training dataset; unsuccessful demonstrations filtered out)
Diffusion Policy (scratch)
Octo (fine-tuned)
DiT Policy (fine-tuned)
OpenVLA (fine-tuned)
OpenVLA (fine-tuned)  PDAC
OpenVLA (fine-tuned)  PDAC, Cont-Diffusion
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
Policy inputs: third-person image, wrist camera image, robot proprioceptive state (optional), language instruction
(Original unfiltered training dataset)
MDT (scratch; with 2 language annotations)
MDT (scratch; with 100 language annotations)
Seer (scratch)
Seer (pretrained on LIBERO-90, then fine-tuned)
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
Policy inputs: third-person image, wrist camera image, robot proprioceptive state (optional), language instruction
(Modified training dataset; unsuccessful demonstrations filtered out)
0  FAST (fine-tuned)
0 (fine-tuned)
OpenVLA-OFT (OpenVLA (fine-tuned)  PDAC, Cont-L1) (ours)
TABLE II: LIBERO inference efficiency results. Action generation through-
put and latency for 7-dimensional actions, averaged over 100 queries on an
NVIDIA A100 GPU. Each query processes a 224  224 px image and a
LIBERO task command (pick up the alphabet soup and place it in the
basket). We compare OpenVLA variants with parallel decoding (PD), action
chunking (K  8 timesteps), and continuous actions using L1 regression or
diffusion objectives. For the diffusion variants, we report inference efficiency
measurements with varying numbers of denoising steps at test time using
the DDIM sampler , along with the corresponding success rates in the
LIBERO-Long task suite. In the final row, we report the inference efficiency
of our proposed OpenVLA-OFT formulation with additional inputs, such as
wrist camera image and robot state.
Throughput (Hz)
Latency (Sec)
LIBERO-Long SR ()
Ttrain  50, Ttest  50
Ttrain  50, Ttest  10
Ttrain  50, Ttest  5
Ttrain  50, Ttest  2
Ttrain  50, Ttest  1
Additional Inputs (wrist img, proprio)
a sample LIBERO language instruction (pick up the alphabet
soup and place it in the basket).
Results in Table II show that parallel decoding reduces
latency and increases throughput by 4 by replacing 7 se-
quential forward passes through the decoder portion of the
policy with a single pass. Adding action chunking (K  8)
increases latency by 17 due to longer attention sequences
in the decoder, but when combined with parallel decoding, it
dramatically improves throughput, achieving a 26 speedup
over baseline OpenVLA. The continuous actions variant with
L1 regression shows negligible difference in efficiency since
the additional MLP action head adds minimal computational
cost compared to the base model. The primary diffusion
variant requires 50 denoising steps (since this is the num-
ber of steps specified at train time) and thus suffers from
high latency. However, it still achieves the same effective
throughput as baseline OpenVLA due to parallel decoding
and action chunking. This means that despite longer pauses
between action chunks, the 50-step diffusion variant still
completes robot episodes at the same speed as the original
autoregressive variant. We also test the OpenVLA diffusion
variant using less than 50 denoising steps at inference time
(enabled by the DDIM sampler ), matching state-of-the-
art diffusion and flow matching methods that use 5-10 steps at
inference time [40, 27, 3]. These configurations result in much
greater inference efficiency, as shown in Table II. However,
the success rates decrease with fewer denoising steps due to
reduced model quality.
D. Model Input-Output Flexibility
As explained in Section IV-B and validated by our efficiency
evaluations in the prior section, parallel decoding enables
OpenVLA to generate action chunks with minimal latency
significant speedup realized by parallel decoding and action
chunking creates headroom for processing additional model
inputs as well. We demonstrate this by fine-tuning OpenVLA
with additional inputs such as robot proprioceptive state and a
robot wrist-mounted camera image, which doubles the number
of visual patch embeddings being passed into the language
model decoder, from 256 to 512. Despite this substantial
increase in input sequence length, the fine-tuned OpenVLA
policy maintains high throughput (71.4 Hz) and low latency
(0.112 sec), as shown in Table II.
Evaluating these policies with additional inputs on the
LIBERO benchmark reveals further improvements in average
success rate across all task suites (Table I). Notably, our
enhanced fine-tuned OpenVLA policies outperform even the
best fine-tuned 0 policies [3, 38]which benefit from a
base model with larger-scale pretraining and a more sophis-
ticated learning objective (flow matching )as well as
Multimodal Diffusion Transformer (MDT)  and Seer
policies. Even with a simpler base model pretrained on less
data than more recent VLAs, we find that our alternative VLA
adaptation design decisions empower fine-tuned OpenVLA
policies to establish a new state of the art on the LIBERO
benchmark.
E. Optimized Fine-Tuning Recipe
Based on the demonstrated improvements in task perfor-
we propose an Optimized Fine-Tuning (OFT) recipe for
VLA adaptation that combines three key components:
1) parallel decoding with action chunking
2) continuous action representation
3) L1 regression objective
These design choices work together to produce strong
policies that can be deployed at high frequencies while main-
taining algorithmic simplicity. We denote policies fine-tuned
from the OpenVLA base model using our OFT recipe as
OpenVLA-OFT. In Section VI, we evaluate OpenVLA-OFTs
capabilities on dexterous, bimanual manipulation tasks in the
real world using a high-frequency control robot.
F. Additional Experiments
Given that the alternative fine-tuning formulation, along
with additional model inputs and outputs, induces a distri-
bution shift between the base VLAs pretraining and fine-
pretrained representations are helpful and have any influence
on the results we have reported. We conduct an ablation
study in Appendix G2 to address this question, ablating
the OpenVLA pretraining phase and directly fine-tuning the
underlying pretrained VLM with the OFT recipe. As shown
in Table XV, the base OpenVLA pretrained representations are
indeed still beneficial for robotic policy learning, as removing
them leads to a 5.2 drop in average success rate (absolute)
in our LIBERO evaluation suite.
(See Appendix G for other additional experiments, including
scaling OpenVLA-OFT up to larger datasetsin both the
LIBERO simulation setting as well as a real-world single-arm
robot manipulation setting with over 50K demonstrations from
the BridgeData V2 dataset .)
VI. EXPERIMENTS: ADAPTING OPENVLA TO A
REAL-WORLD ALOHA ROBOT
While our experimental results in the prior section demon-
strate OpenVLA-OFTs effectiveness in simulation, successful
deployment in the real world, on robot platforms that differ
substantially from those seen during pretraining, is crucial
for showing broad applicability. We thus assess the efficacy
of our optimized fine-tuning recipe on the ALOHA robot
at a high control frequency. We evaluate on novel dexterous
manipulation tasks that have never been encountered before
during OpenVLAs pretraining (which only involves single-
arm robot data).
Prior works [54, 27, 3] have shown that vanilla LoRA fine-
tuning with autoregressive VLAs  is impractical for such
even lower for bimanual tasks) falls well below the 25-50 Hz
required for real-time deployment. We therefore exclude this
baseline from our experiments and compare more effective
methods that we discuss shortly.
In this section, we use an augmented version of our VLA
fine-tuning recipe (OFT) that additionally includes feature-
wise linear modulation (FiLM) for enhanced language ground-
policy instantiated through this augmented fine-tuning recipe
as OpenVLA-OFT.
A. ALOHA Experimental Setup
The ALOHA platform comprises two ViperX 300 S arms,
three camera viewpoints (one top-down, two wrist-mounted),
and robot state inputs (14-dimensional joint angles). It operates
at 25 Hz (reduced from the original 50 Hz to enable faster
training while still maintaining smooth robotic control), with
actions representing target absolute joint angles. This setup dif-
fers significantly from OpenVLAs pretraining, which includes
single-arm robot data only, a single camera viewpoint from
a third-person camera, no robot state inputs, low-frequency
control (3-10 Hz), and relative end-effector pose actions. The
distribution shift poses a challenge to the adaptation of this
We design four representative tasks testing deformable
object manipulation, long-horizon skills, tool usage, and
language-driven control:
1) fold shorts: Fold white shorts on a table with two
consecutive bimanual folds. Training: 20 demonstrations.
Fig. 4: ALOHA task performance results. Comparison between policies trained from scratch (ACT, Diffusion Policy) and fine-tuned VLAs (RDT-1B, 0,
OpenVLA-OFT) across four ALOHA manipulation tasks. OpenVLA-OFT enhances the base model with parallel decoding, action chunking, continuous
achieving the highest average performance. Scores represent average percent completion of each task (see rubrics and detailed results in Appendix F2).
2) fold shirt: Fold white T-shirt through multiple synchro-
nized bimanual folds, testing contact-rich, long-horizon
manipulation. Training: 30 demonstrations. Evaluation:
10 trials.
3) scoop X into bowl: Move bowl to center of table with
left arm, scoop specified ingredient (raisins, almonds
and green MMs, or pretzels) with right arm using
metal spoon. Training: 45 demonstrations (15 per ingre-
dient). Evaluation: 12 trials (4 per ingredient).
4) put X into pot: Open pot with left arm, place specified
item (green pepper, red pepper, or yellow corn)
with right arm, close pot. Training: 300 demonstrations
(100 per object). Evaluation: 24 trials (12 in-distribution,
12 out-of-distribution).
We fine-tune OpenVLA using OFT on each task indepen-
dently for 50-150K gradient steps (total batch size 32 with
8 A100H100-80GB GPUs) with action chunk size K  25.
At inference time, we execute the full action chunk before
requerying the model for the next chunk.
B. Methods in Comparison
The ALOHA tasks present a significant adaptation chal-
lenge for OpenVLA as the base model, given the substantial
differences from its pretraining platforms in terms of con-
trol frequency, action space, and input modalities. For this
VLAsRDT-1B  and 0 that were pretrained on
bimanual manipulation data and might reasonably be expected
to perform better on these downstream tasks. We evaluate
these models after fine-tuning them using their authors rec-
ommended recipes, and these methods serve as important
points of comparison. Additionally, to provide comparisons
with computationally efficient alternatives, we evaluate two
popular imitation learning baselines: ACT  and Diffusion
To enable language following in these baseline methods,
we use language-conditioned implementations. For ACT, we
modify EfficientNet-B0  to process CLIP  language
embeddings via FiLM [37, 43]. For Diffusion Policy, we use
the DROID dataset  implementation that conditions action
denoising on DistilBERT  language embeddings, modified
to support bimanual control and multiple image inputs.
C. ALOHA Task Performance Results
We evaluate all methodsACT, Diffusion Policy, RDT-1B,
vide fine-grained assessment, we use a predetermined rubric
that assigns scores for partial task completion (see Appendix
F for details). Figure 4 shows aggregate performance scores,
while Figure 5 specifically tracks language following ability
for the language-dependent tasks.
Performance of non-VLA baselines. The baseline methods
trained from scratch show varying levels of success. ACT,
while able to complete basic tasks, produces less precise
actions and achieves the lowest overall performance. Diffu-
sion Policy demonstrates stronger capabilities, matching or
exceeding RDT-1Bs reliability on the clothes folding and
scooping tasks. However, it struggles with the put X into pot
task which has a larger training dataset, suggesting limited
scalability compared to VLA-based approaches.
Performance of fine-tuned VLAs. Fine-tuned VLA poli-
cies generally outperform the from-scratch baselines in both
task execution and language following, consistent with prior
findings [27, 3]. Among VLAs, we observe distinct charac-
its Alternating Condition Injection scheme , but shows a
limitation in handling closed-loop feedback. As visualized in
Figure 6, it often fails to correct mistakes in the scoop X into
We use this FiLM-EfficientNet implementation only for language-
dependent tasks (scoop X into bowl and put X into pot). For clothes
folding tasks, we use the original ResNet-18  backbone as in .
Fig. 5: ALOHA language following results. Success rates in approaching
language-specified target objects for language-dependent tasks. Fine-tuned
VLAs follow the users command more frequently than policies trained from
scratch. OpenVLA-OFT shows the strongest language grounding, though
removing FiLM  reduces success to chance level.
Fig. 6: Sample rollouts contrasting RDT-1B and 0 error handling in
ALOHA tasks. Top: In some cases, RDT-1B fails to respond to missed
bowl placement, continuing to pour ingredients into empty space. Bottom:
0 demonstrates adaptive behavior by reattempting green pepper grasp after
initial failure, highlighting better visual feedback integration. See our website
for videos.
bowl taskfor instance, continuing to pour ingredients into
an imaginary bowl after missing the actual bowl, suggesting
over-reliance on proprioceptive state over visual feedback.
On the other hand, 0 demonstrates more robust execution
with smoother motions and better reactivity to feedback, often
successfully recovering from initial failures (as shown in
Figure 6). While its language following slightly trails RDT-
the strongest baseline. Finally, OpenVLA-OFT achieves the
highest performance across both task execution and language
following (see Figure 7 for examples of successful task
rollouts). This is particularly noteworthy given that the base
OpenVLA model was pretrained only on single-arm data,
while RDT-1B and 0 were pretrained on substantial bimanual
Fig. 7: Sample successful OpenVLA-OFT rollouts on the ALOHA robot.
OpenVLA-OFT can fold clothes, use a metal spoon to scoop and pour
targeted trail mix ingredients into a bowl, and place targeted objects into
a pot. See our website for videos.
datasets (6K episodes and 8K hours of bimanual data, respec-
tively). This suggests that the fine-tuning technique can be
more crucial than pretraining data coverage for downstream
performance.
Ablation study of FiLM. We evaluate the importance of
FiLM in our OpenVLA-OFT approach by ablating it and
assessing the policies language following ability on the last
two tasks, which require good language grounding for suc-
cessful execution. As shown in Figure 5, language following
drops to 33 in both tasksequal to randomly choosing the
correct instruction. This demonstrates that FiLM is essential
for preventing the model from overfitting to spurious visual
features and ensuring proper attention to language inputs.
Please see the project website for ALOHA robot rollout
videos and an in-depth qualitative analysis of all methods:
D. ALOHA Inference Efficiency Comparisons
We evaluate inference efficiency by measuring action gen-
eration throughput and latency across 100 queries for each
method. We report the results in Table III. The original
OpenVLA formulation, even with just the additional wrist
camera inputs, shows poor efficiency with 1.8 Hz throughput
and 0.543 sec latency. In contrast, OpenVLA-OFT achieves
77.9 Hz throughput, though its latency is higher compared to
the policies in the prior LIBERO experiments since it must
process two additional input images.
demonstrate
throughput
OpenVLA-OFT due to their smaller architectures: ACT
(84M parameters), Diffusion Policy (157M), RDT-1B (1.2B),
TABLE III: ALOHA robot inference efficiency results. Throughput and
latency measurements averaged over 100 queries on an NVIDIA A100 GPU.
Each query processes three 224  224 px images, 14-D robot state, and a
task command (scoop raisins into 
