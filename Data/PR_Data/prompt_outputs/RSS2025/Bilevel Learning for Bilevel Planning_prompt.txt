=== PDF文件: Bilevel Learning for Bilevel Planning.pdf ===
=== 时间: 2025-07-22 09:42:32.686206 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Bilevel Learning for Bilevel Planning
Bowen Li, Tom Silver, Sebastian Scherer, and Alexander Gray
Carnegie Mellon University, Centaur AI Institute, Princeton University
P1():True
Generalized Solution
P2(,):False
P2(,):False
Test Time: Unseen State
1.MoveTo
4.WalkOn
7.MoveTo
Bilevel Learning
Training Time: Drop Target (high) into Container (low)
Invented Neural Predicates
P2(?r,?p)
1.MoveTo
Ground Neural Predicates
Bilevel Planning
4.MoveTo
Container
One Platform
Two Platforms
Container
7.WalkOn
P3(?r,?t)
Bilevel Planner
Drop Target (high) into Container (high)
Goal Achieved!
Fig. 1: (Top) Our bilevel learning framework invents neural predicates from training demonstrations (with one platform), which enable the
learning of a hybrid bilevel planner. (Bottom) The invented predicates realize zero-shot compositional generalization over objects (with two
platforms), where a longer solution with different action compositions is generated. Continuous action parameters are omitted for simplicity.
AbstractA robot that learns from demonstrations should
not just imitate what it seesit should understand the high-
level concepts that are being demonstrated and generalize them
to new tasks. Bilevel planning is a hierarchical model-based
approach where predicates (relational state abstractions) can
be leveraged to achieve compositional generalization. However,
previous bilevel planning approaches depend on predicates that are
either hand-engineered or restricted to very simple forms, limiting
their scalability to sophisticated, high-dimensional state spaces.
To address this limitation, we present IVNTR, the first bilevel
planning approach capable of learning neural predicates directly
from demonstrations. Our key innovation is a neuro-symbolic
bilevel learning framework that mirrors the structure of bilevel
planning. In IVNTR, symbolic learning of the predicate effects
and neural learning of the predicate classifiers alternate, with
each providing guidance for the other. We evaluate IVNTR in six
diverse robot planning domains, demonstrating its effectiveness
in abstracting various continuous and high-dimensional states.
While most existing approaches struggle to generalize (with < 35
success rate), our IVNTR achieves an average success rate of
77 on unseen tasks. Additionally, we showcase IVNTR on a
mobile manipulator, where it learns to perform real-world mobile
manipulation tasks and generalizes to unseen test scenarios that
feature new objects, new states, and longer task horizons. Our
findings underscore the promise of learning and planning with
abstractions as a path towards high-level generalization. Project
Work was partly done during internship at Centaur AI Institute. Corre-
spondence to {bowenli2,basti}andrew.cmu.edu.
I. INTRODUCTION
Imitation learning has made significant recent strides [15],
but generalization remains an open challenge, especially when
new tasks require recomposing high-level concepts that are
only implicit in the training data [6, 7]. In Figure 1, a robot
has seen demonstrations of stepping onto a platform to grasp
an object, and other demonstrations of dropping the object into
a container. Now, faced with a new task where the container
is also elevated, the robot should first move the two platforms
finally step onto the other to drop the object. Note that the
platform arrangements must be completed before grasping the
This kind of learning and reasoning requires compositional
generalization (new objects); sequential generalization (new
and longer action sequences); and long-horizon planning with
continuous state and action spaces with sparse feedback (goals).
In sum, the robot should not just imitate demonstrations, but
also understand and leverage the high-level concepts within
the low-level states that are being demonstrated.
One promising direction to address these challenges is to
learn and plan with abstractions [814]. In this work, we con-
tinue a line of recent inquiry on learning abstractions for bilevel
planning [1521]. In bilevel planning, continuous low-level
states are mapped into a symbolic relational state space defined
by predicates such as Viewable(?robot,?target) or
On(?robot,?platform). Planning proceeds jointly in the
symbolic high-level space and the continuous low-level space.
The key idea is that this hybrid planning can be more efficient
and effective than reasoning solely in the low-level space.
The performance of bilevel planning depends substantially
on the predicates used to define the abstract state space .
To avoid the need for a human engineer to manually define
predicates for every new domain, recent work has considered
learning predicates from data [17, 2125]. Broadly, three
approaches have emerged. The most direct one relies on
human feedback (labels or guidance) during the predicate
learning process [2426], which is labor-intensive and does
not guarantee useful abstractions for planning . The second
approach invents predicates with surrogate objectives that
are easy to optimize, e.g., reconstruction loss [2729] or
bisimulation [12, 30]. While these methods simplify learning,
they complicate planning due to the mismatch between the
surrogate objectives and the actual planning goals . The
third approach directly invents predicates for efficient planning,
making planning easy but learning hard, as objectives like
total-planning-time and expected-planning-success are difficult
to optimize . To address this, previous works have used
program synthesis with classical grammars  and foundation
model-based techniques [21, 31]. However, in both cases, the
predicates are invented from programmatic and pre-defined
Our main contribution is bIleVel learNing from TRansitions
(IVNTR), the first approach capable of learning neural pred-
icates that are optimized for efficient and effective bilevel
planning. Since directly incorporating the planning objective
into network training is challenging, our IVNTR instead
constructs a candidate neural predicate pool, which is later
subselected . The key insight behind our approach is to
center learning around the effects of predicates, which provide
two major benefits: (1) they enable the derivation of supervision
labels for transition pairs, yielding a well-structured learning
objective for training the neural network; and (2) the inherent
sparsity of predicate effects, combined with neural learning
To this end, IVNTR presents a novel bilevel learning framework,
inspired by the structure of bilevel planning itself. Similar to
the alternation between high-level symbolic search and low-
level neural sampling in bilevel planning, IVNTR interleaves
symbolic effect learning and neural classifier learning in an
iterative process. In each iteration, the symbolic learning
proposes a candidate predicate effect across different actions,
which provides labels for neural learning on transition pairs.
Once the neural classifier converges, its validation loss guides
the symbolic learning to propose the next candidate that could
minimize the loss in the new iteration. This iterative bilevel
learning ultimately yields a compact set of neural predicates,
which are then selected to optimize the planning objective .
The final set of invented predicates seamlessly integrates into
operator and sampler learning frameworks [17, 32], ultimately
forming a fully functional bilevel planner.
To evaluate the effectiveness of IVNTR, we conduct exten-
sive experiments across six diverse robot planning domains.
These domains feature a wide range of low-level state represen-
clouds. Furthermore, as shown in Figure 1, by leveraging rela-
tional predicates and AI planning, IVNTR zero-shot generalizes
to tasks with unseen entity compositions. Finally, we deploy
IVNTR on a quadruped mobile manipulator (Boston Dynamics
Spot) for two long-horizon mobile manipulation tasks. The
learned predicates successfully abstract complex continuous
states into representations compatible with the AI planner, while
also providing actionable guidance for the samplers. We believe
IVNTR represents a pivotal step towards learning high-level
abstractions from sophisticated low-level states.
II. PROBLEM FORMULATION
We propose a method that uses an offline demonstration
dataset to learn planning abstractions that generalize to test
tasks with unseen objects and action compositions. In this
the notation system introduced in previous work ; see
Appendix A for a complete notation glossary.
Planning problems are defined within a certain planning
we can sample a planning task T T  O, x0, g.
is a finite set of object types  . For example, the
Climb-Transport domain depicted in Figure 2 has three object
associated with a set of features that characterize the state
of an object of that type.1 For example, robot has features
others. A specific task T is characterized by objects O
{o1, o2,    , oN}, each associated with one type in . Objects
are fixed within tasks but vary between tasks. The state of a
task x X is defined by an assignment of feature values to
all objects in the task. For simplicity of exposition, we assume
that a state with N objects can be represented as a matrix
x RNK for some domain-specific constant K; however,
we show in experiments that our approach can be applied to
more sophisticated object-centric state representations as well.
The action space for a domain is characterized by a set of M
parametrized controllers C  {C1, C2,    , CM}, each of which
has an object type signature (1, 2,    , v) and a continuous
parameter space . For example, in Figure 2, MoveToReach
has type signature (robot, platform), and continuous param-
eters  SE(2) defining an offset 2D pose for the robot
relative to the platform. A ground action is a controller with
fully specified parameters, e.g., MoveToReach(r1, p1, ) for
a certain  . We use underline notation to represent
is controller with object parameter placeholders, which are
typically prefixed with ?, e.g., MoveToReach(?r, ?p, ). States
and actions ar
