=== PDF文件: DDAT Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories.pdf ===
=== 时间: 2025-07-22 16:13:11.350404 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Admissible Robot Trajectories
Jean-Baptiste Bouvier, Kanghyun Ryu, Kartik Nagpal, Qiayuan Liao, Koushil Sreenath, Negar Mehr
ICON Lab and Hybrid Robotics Lab, University of California Berkeley
Diffusion model D
Projection P
Complete projection:
Partial projection:
Fig. 1: (top left) Overview of Diffusion policies for Dynamically Admissible Trajectories (DDAT). Diffusion model D is trained
to predict a trajectory  given a trajectory  from the training dataset corrupted by noise . If the noise level  of signal  is
sufficiently small, P projects  to the dynamically admissible trajectory p. The loss p is used to update D. (bottom
left) To smoothly incorporate projections into the training and inference of our diffusion models we use partial projections
of infeasible trajectories. The total projection error P et is significantly reduced by not projecting s4 on the reachable set
of s3. (right) Demonstration of DDAT on the Unitree GO2. A vanilla diffusion policy fails at walking through the cones in
open-loop. By accounting for the quadrupeds dynamics our open-loop diffusion policy succeeds in following the corridor.
AbstractDiffusion models excel at creating images and videos
thanks to their multimodal generative capabilities. These same
capabilities have made diffusion models increasingly popular in
robotics research, where they are extensively used for generating
robot motion. However, the stochastic nature of diffusion models
is fundamentally at odds with the precise dynamical equations
describing the feasible motion of robots. Hence, generating
dynamically admissible robot trajectories is a challenge for
diffusion models. To alleviate this issue, we introduce DDAT:
Diffusion policies for Dynamically Admissible Trajectories to gen-
erate admissible trajectories of black-box robotic systems using
diffusion models. To generate such trajectories, our diffusion
policies project their predictions onto a dynamically admissible
manifold during both training and inference to align the objective
of the denoiser neural network with the dynamical admissibility
constraint. Due to the auto-regressive nature of such projections
as well as the black-box nature of robot dynamics, trajectory
projections are challenging. We thus enforce admissibility by
iteratively sampling a polytopic under-approximation of the
reachable set of a state onto which we project its predicted suc-
By producing accurate trajectories, this projection eliminates
the need for diffusion models to continually replan, enabling
one-shot long-horizon trajectory planning. We demonstrate that
our proposed framework generates higher quality dynamically
admissible robot trajectories through extensive simulations on a
quadcopter and various MuJoCo environments, along with real-
world experiments on a Unitree GO1 and GO2. Code available
on our website:
I. INTRODUCTION
Diffusion models have achieved state-of-the-art perfor-
mance in image and video generation thanks to their multi-
modal generative capabilities [18, 39, 40]. In robotics planning
and control tasks these diffusion models have shown promise
thanks to their ability to generate entire trajectories at once,
avoiding compounding errors and improving long-term perfor-
mance [9, 20]. These diffusion planners generate trajectories
by iteratively reducing the noise of an initial random sequence
until obtaining a trajectory without noise . However, this
stochastic denoising process is fundamentally at odds with
the precise equations describing the dynamically admissible
motion of robots. Indeed, the deterministic equations of motion
of a robot only consider a transition from state st to st1 to be
admissible if st1 is in the reachable set of st. This reachable
set is typically a bounded manifold of dimension much lower
than the state space due to the robots limited actuation .
Learning to sample exclusively from such reachable sets
is extremely challenging due to their small size and their
dependence on the state st. Hence, diffusion models typically
generate inadmissible sequences of states for underactuated
planners to fully-actuated systems like Maze2D  and
The typical approach to deploy diffusion planning on under-
actuated robots is to regenerate an entire trajectory every few
timesteps as the state actually reached by the robot differs from
the one predicted with the inadmissible trajectory generated by
the diffusion model [6, 11, 19, 22, 55]. However, this approach
is computationally demanding and wasteful. More promising
is to project the predicted trajectory onto the dynamically
admissible manifold after inference [16, 22, 30, 33, 36, 54].
when the trajectory is too far from being admissible .
For the diffusion policy to account for this projection and
correct its prediction, works [10, 16, 35, 50] have instead used
projections throughout inference.
To address the dynamic feasibility issue in diffusion plan-
ning from offline data, we introduce Diffusion policies for Dy-
namically Admissible Trajectories (DDAT). These models
use a chronological trajectory projector capable of transform-
ing a generated sequence of states into an admissible trajectory.
On the contrary to work , and inspired by [10, 16, 50],
the projections in DDAT occurs throughout and after inference
instead of only after inference. Differing from works [10, 16,
50], these projections are also implemented during training
of the diffusion models to align their training objectives with
their inference phase, as suggested in work . Furthermore,
works [5, 10, 16, 50] only concern themselves with a single
than dynamical admissibility due to its autoregressive nature.
Another advantage of generating feasible trajectories is that
we do not need to sample and denoise a large batch of
trajectories hoping to find an admissible one, which provides
significant computational gain compared to methods like .
Only works [31, 36] studied dynamical feasibility in diffusion
Our main contributions in this work are as follows.
1) We generate dynamically admissible trajectories with
diffusion models for black-box systems.
2) We design an algorithm to train diffusion models to
respect dynamical admissibility constraints.
3) We compare various projections and diffusion model
architectures to enforce dynamic admissibility.
The remainder of this work is organized as follows. In
Section II, we provide a survey of related works. In Sec-
tion III, we introduce our problem formulation along with
our framework. In Section IV, we define several projectors
to enforce the dynamical admissibility of trajectories. In Sec-
tion V, we incorporate these projectors into diffusion models.
In Section VI, we implement our proposed Diffusion policies
for Dynamically Admissible Trajectories. We present our
simulation results on a quadcopter, the Gym Hopper, Walker
and HalfCheetah, and the Unitree GO1 and GO2 quadrupeds
in Section VII, while Section VIII demonstrates the hardware
implementation of DDAT on a Unitree GO1 and a GO2.
inclusive is denoted by [[a, b]]. The convex hull of a set of
points p1, ..., pm Rn is denoted as conv
i1 ipi : i [0, 1], Pm
. A value a sampled
uniformly from a set A is denoted as a U
. A Bernoulli
variable b taking value 1 with probability p and 0 otherwise
is denoted by b B(p).
II. RELATED WORKS
In this section, we review the literature relevant to our area
of interest, namely constraint enforcement on diffusion models
and diffusion planning.
A. Enforcing constraints on diffusion models
1) Soft constraints:
Most diffusion works employ soft
constraints to encourage satisfaction of a given constraint,
either with classifier guidance [6, 25, 30], classifier-free guid-
47], either only during inference [8, 16] or during both training
and inference [5, 6, 25, 42, 47]. However, these soft constraints
cannot guarantee satisfaction for all the samples generated
by the diffusion process. Thus, most of these works need to
sample large batches of trajectories among which they try to
select the prediction closest to satisfying the constraint [6, 8].
2) Hard constraints:
Contrasting the works presented
satisfaction. The first hard constraints imposed on diffusion
models were [0, 255] pixel cutoffs, which led to saturated
images . To mitigate this quality loss,  proposed to
replace cutoffs with reflection on the constraints boundaries. In
duced log-barrier as a constraint enforcer on manifolds. When
saturation is not an issue, projection remains the most widely
adopted approach to enforce constraints [10, 22, 35, 36, 50].
While a single projection at the end of inference is sufficient to
enforce a constraint , works [10, 50] realized that several
projection steps interleaved with the denoising process yield
higher quality constraint-satisfying samples. Bringing these
advances to diffusion planning,  enforces state and action
bounds by projecting partially denoised trajectories during
inference. Most of these works also agree that constraints
should mostly be enforced at low noise levels where the
sampled predictions are not just random sequences [5, 25, 50].
diffusion planning significantly more challenging.
B. Diffusion planning
We will now review the literature on diffusion planning and
discuss how they handle dynamical feasibility.
1) Dynamic feasibility enforced by planning on actions:
The easiest solution to generate dynamically admissible trajec-
tories is to let the diffusion model directly generate sequences
of actions since the resulting sequence of states will be
admissible by definition [9, 25, 46, 53]. The problem with
this approach is the high variability and lack of smoothness of
sequences of actions, rendering them much more challenging
to predict than sequences of states and hence leading to lower
quality plans . This observation led works [34, 41] to
use transformers and work  to use diffusion models to
learn the temporal relations between states and actions, and a
diffusion model to predict actions. These added models create
an unnecessary computational overhead only to mitigate the
difficulty of planning over actions.
2) Diffusion planning on states: The most straightforward
approach to diffusion planning is to let the diffusion model
predict the sequence of states directly. This sequence of
states can be a reward maximizing trajectory  or predicted
states of other agents . However, these state trajectories
are generated through a stochastic process and thus have
no guarantees of admissibility. This observation prompted
replanning approaches, which generate an entire new trajectory
every few timesteps as the state actually reached by the robot
differs from the one predicted with the potentially inadmissible
trajectory generated by the diffusion model . Due to the
slow nature of replanning with diffusion models, [11, 19] focus
on accelerating diffusion inference for real-time planning.
Other works are using restoration gaps  and constraint
gradients  to guide their diffusion models, but none of
these works guarantee satisfaction of the robots dynamics.
Most closely related to this work is the emerging literature
enforcing constraints in diffusion planning with quadratic
tions . Only works [31, 36] consider dynamic constraints,
but only test their approaches on linear fully-actuated systems.
3) Diffusion planning on states and actions: Joint predic-
tion of state and actions can enhance planning quality by
improving temporal coherence of planned trajectories .
training data and does not enforce it with a dynamics model,
leading to infeasible trajectories. AdaptDiffuser  uses
neural network-based inverse dynamics to iteratively revise
the state sequence and predicted reward. However, this learned
inverse model may differ from the true inverse dynamics and
allow infeasible trajectories. As a result, [25, 33] use diffusion
models only to generate initial guess for trajectory optimiza-
tion solvers capable of enforcing dynamic feasibility. Thus,
none of the diffusion planning literature formally addresses
the dynamic admissibility problem.
III. FRAMEWORK
Let us now introduce our framework. We consider a robot
of state st at time t with black-box deterministic discrete-time
dynamics of form
s0 U(S0),
where action at belongs to the admissible action set A Rm
and S0 is the set of initial states in the state space S Rn.
We emphasize that dynamics (1) are a black-box, i.e. the
equations describing f are not available, but the user can
obtain st1 given st and at. This setting corresponds to a
numerical simulator which is typically available for any robot.
We now want to use diffusion policies to generate trajectories
of robot (1).
A. Diffusion planning
Diffusion planning refers to using diffusion models [18, 39,
40] to generate trajectories of a given system. Due to the black-
box nature of dynamics (1), this imitation learning approach
requires a training dataset D of example trajectories represen-
tative of a desired data distribution pdata. A trajectory of length
H  1 is a sequence of states  :
The objective is to learn how to sample trajectories from pdata.
To do so, a forward diffusion process starts by corrupting
dataset D with Gaussian noise  N(0, 2I) parametrized
by  sampled as ln() N(pm, p2
s) with parameters from
defined in Appendix C. Diffusion planning can be viewed
as learning to reverse this process. More specifically, we
iteratively reduce the noise level of a randomly sampled
sequence until obtaining a trajectory belonging to pdata, this
process is called denoising. We train a neural network D to
generate denoised trajectories from corrupted ones    by
minimizing
ln()N(pm,p2s)
To generate trajectories from the desired distribution pdata
we implement a first-order deterministic sampler relying on
D and detailed in Appendix C simplified from . This
sampler starts from a random sequence 0 N(0, 2
and iteratively reduces its noise level from i to i1 with
i1  S(i; i, i1) where 0 >> 1 and N  0. The
derivation of denoising operator
S(i; i, i1) : i1
is detailed in Appendix C. After N iterations we eventually
obtain a trajectory N pdata.
B. Problem formulation
Definition 1. A trajectory {s0, ..., sH} is admissible by dy-
namics (1) if and only if there exist a sequence of actions
AH such that st1  f(st, at) for all
long to the reachable set of its predecessor st.
Definition 2. The reachable set R(st) of a state st is the set
of all feasible next states:
f(st, a) : for all a A
Throughout this work, we will emphasize the difference
between sequences of states, where iterated states are not
necessarily reachable from their predecessor, and admissible
trajectories satisfying exactly and recursively dynamics (1).
This distinction leads us to our problem of interest.
Problem 1. How to generate dynamically admissible trajec-
tories for a black-box model using diffusion policies?
While diffusion policies excel at generating sequences of
states [11, 20, 26], the difficulty of Problem 1 resides in guar-
anteeing their dynamical admissibility, which is challenging
for two reasons. The first difficulty comes from the chrono-
logical nature of the required projections of each st1 onto
the reachable set R(st), which are computationally expensive
and cannot be parallelized. The second challenge resides in the
black-box nature of dynamics (1), hence preventing a closed-
form expression of the reachable sets R(st) of (3). Even with
known nonlinear dynamics, approximating reachable sets is
still a challenging and active research topic . We will
discuss how to address these challenges in the next section.
IV. MAKING TRAJECTORIES ADMISSIBLE
In this section we propose several projections schemes
to make the trajectories generated by our diffusion models
become dynamically admissible. Each one of these approaches
have their own pros and cons in terms of computation, admis-
sibility guarantees, and whether they require action predictions
along with the state predictions.
A. Reachable sets underapproximation
To ensure dynamic feasibility of a sequence of states
{s0, s1, ..., sH}, we need to chronologically project each st1
onto the reachable set of its predecessor R(st). However, the
black-box nature of dynamics (1) prevents any closed-form
expression of the reachable set (3). To render this problem
and underapproximate it with a polytope. This assumption
holds for sufficiently smooth nonlinear systems and small
timesteps  but fails for robots with instantaneous contact
forces. Fortunately, we will see in our experiments of Sec-
tion VII that the nonconvexity of the reachable set has but a
small impact on our approach.
We assume to know a polytopic underapproximation of
the admissible action set A with vertices v1, ..., vq A
such that conv
A. This assumption is typically
verified as A is often a known hyperrectangle of dimension
m delimited by q  2m vertices . Then, given a state st,
we underapproximate its reachable set with
C(st) : conv
f(st, v1), ..., f(st, vq)
which is the convex hull of the successors of st by black-
box dynamics f of (1) and actions vi. We now need to
iteratively project each predicted next state st1 onto this
tractable approximation of the reachable set of st starting from
s0 with projector
: arg min
We illustrate a projection step in Fig. 3 and summarize the
whole trajectory projection in Algorithm 1.
Remark 1. The projected trajectory  computed by Algo-
rithm 1 is not necessarily admissible as discontinuities of
dynamics (1) can make the actual reachable set R(st) noncon-
vex. Then, C(st) might not be a subset of R(st) and the pro-
jected next state can be infeasible if P
Fig. 3: Illustration of trajectory projection (5). The ex-
tremal actions v1, v2, v3, v4 of the admissible action set A
are applied to st with dynamics f to get extremal next
states f
. Their convex
hull generates a polytope C(st) underapproximating the actual
reachable set R(st). The predicted next state st1 is then
projected by P onto C(st) to obtain admissible next state st1.
Algorithm 1 State Trajectory Projection
initialize the projected trajectory
C(st)  conv
f(st, v1), ..., f(st, vm)
next state projection
append to
B. Reference projection of state trajectories
While Algorithm 1 makes a trajectory admissible, its it-
erative nature leads to compounding errors and diverging
trajectories. For instance, Fig. 4 shows that Algorithm 1 cannot
project a Hopper trajectory  onto the closest admissible
trajectory and instead leads to divergence. Indeed, the greedy
nature of Algorithm 1 minimizes the distance at the current
step without accounting for the divergence it may cause later
on. A dynamic programming approach could anticipate this
issue but at a prohibitive computational cost.
timesteps
Hopper height (m)
projected
Fig. 4: Diverging projection of the Hopper height. Our diffu-
sion model samples the orange trajectory given the initial state
of the blue trajectory, which is loaded from our dataset. The
projection of orange with Algorithm 1 yields the diverging
but admissible green trajectory. However, green is not the ad-
missible trajectory closest to orange since blue is admissible
and much closer to orange than green.
To compensate for the compounding errors of Algorithm 1,
we instead propose to guide projection P of (5) towards an
admissible next state better aligned with the desired trajectory
using a reference trajectory. More specifically, we argue that
the projection of st1 should also minimize the distance to a
reference next state sref
t1 as follows:
: arg min
st1csref
where  > 0 is a trade-off coefficient between the projection
and the proximity to the reference. with Pref  P when   0.
Replacing P with Pref in line 3 of Algorithm 1 yields the
reference trajectory projection Algorithm 2.
Algorithm 2 Reference Trajectory Projection
reference trajectory  ref
initialize the projected trajectory
C(st)  conv
f(st, v1), ..., f(st, vm)
st1  Prefst1, C(st), sref
next state projection
append to
C. Using action predictions for better state projections
While Algorithms 1 and 2 perform relatively well in low-
dimensional state spaces such as the Hopper of Gymna-
ate admissible trajectories  of the Walker2D and HalfCheetah
>> 0. This accuracy degradation is most likely
caused by the non-convexity of the reachable sets following
Remark 1 and the increased action space dimension which
enlarges set (4) and keeps the projection far from the reference.
To increase the precision of Algorithms 1 and 2, we propose
to use a prediction of the best action to restrict the search
space (4) of projections (5) and (6) to a small region centered
on this prediction instead of naively testing the entire action
space. To predict the action corresponding to each state
to predict both state and action sequences simultaneously. We
use the predicted action at as the center of an action search
space shrunk by a factor  (0, 1) and delimited by vertices
for all i [[1, m]]. These vertices lead to a correspondingly
reduced reachable set
C(st)  conv
f(st, v1), ..., f(st, vm)
illustrated in Fig. 5.
Projection P over polytope C(st) of (8) is a convex opti-
f(st,v1)
f(st,v2)
f(st,v3)
f(st,v4)
f(st,v1)
f(st,v2)
f(st,v3)
f(st,v4)
Fig. 5: Illustration of the reduced search space C(st) of (8)
resulting from the reduced action space conv(v1, v2, v3, v4)
of (7) by a factor  surrounding action prediction at. Finding
the best next state st1 is much easier in the smaller set C(st)
than in the larger C(st).
mization
: arg min
i f(st, vi) (9)
if(st, vi)
convex combination of its vertices, i.e., c  P if(st, vi) with
P i  1. The optimal coefficients of (10) can then be used
to approximate the corresponding action at  P
i vi. This
approximation is exact for control affine dynamics as detailed
in Remark 2 of Appendix B. We summarize this state-action
projection in Algorithm 3.
Algorithm 3 State Action Trajectory Projection
predicted actions
initialize the projections
for i  1 to m do
extremal actions
solve (10)
st1  P f(st, v), at  P v
predictions
{st1},   {at}
append to ,
D. Using action predictions for admissible state prediction
While Algorithm 3 uses the action predictions to guide
the next state projection, we can use this action prediction
to directly obtain an admissible next state. If we replace the
predicted next state st1 by the state actually obtained when
applying the predicted action at on the current state st we
obtain the following projector:
PA(st, at) : f(st, at).
PA(st, at) and at  at guarantees the admissibility of the
Fig. 6: Illustration of projectors PA  f(st, at) and PSA
f(st, at  at) with its feedback correction policy  lever-
aging the open-loop error to generate a corrective action at
instead of discarding prediction st1 like PA.
projected trajectory  while also removing convex optimiza-
tion (10).
While projector (11) entirely discards the next state pre-
diction st1, we can leverage its information to design a
smarter projector. We propose to use a feedback correction
term at on the action at based on the distance between its
corresponding next state PA and the desired next state st1,
as illustrated in Fig. 6. Such a projector PSA leverages the
combined information of state and action predictions as
PSA(st, at, st1) : f(st, at  at).
To calculate the feedback correction at we prefer a small
neural network  to an optimization algorithm because we
need PSA to be differentiable and sufficiently fast to be
incorporated into the training of our diffusion models. A
model-based approach would not work either due to the black-
box nature of f. Then, at :
st1 PA(st, at)
the dataset of trajectories D, we can extract admissible triplets
(st, at, st1), sample a correction term at N(0, 2I),
correct next state s
atN(0,2I)
(st,at,st1)D
V. INCORPORATING PROJECTIONS IN DIFFUSION
In this section, we discuss how we can incorporate the
different projectors of Section IV into our diffusion model
to learn diffusion policies generating dynamically admissible
trajectories.
A. Training with projections
We introduce Algorithm 4 to train and sample admissible
trajectories from our diffusion models. Each training iteration
begins with the sampling of a noise level  describing the
magnitude of noising sequence . This noise sequence  is
added to an admissible trajectory  from dataset D. The
diffusion neural network predicts a denoised trajectory  from
the corrupted signal . If the noise level  is sufficiently low
we project  with one of the projectors P of Section IV. The
difference between this projection and the original noise-free
trajectory  is then used as a loss to update neural network D.
The curriculum deciding at which noise levels  projections
can occur is discussed in Section V-B.
After training D, we can use our model to plan a trajectory
starting from a given initial state s0. This inference phase
starts with sampling a random sequence 0 and imposing its
first term to be s0. Then, we iteratively process the trajectory
through neural network D and the projector P with which it
was trained. After N iterations of progressively reducing the
noise level of our sample we obtain a noise-free admissible
trajectory prediction starting from s0. Since the projections are
necessary during inference to generate admissible trajectories,
we included the same projections into the training to match
the inference.
Algorithm 4 DDAT
Training
N(pm, p2
noise level, Appendix C.
noise sequence
training trajectory
denoised prediction
projection
gradient step
Inference
initialize trajectory
known initial state
i1 S(i; i, i1)
denoised prediction (2)
known initial state
projection
noise-free sample
While Algorithm 4 is written to predict trajectories of states,
it can similarly generate states and actions in parallel and use
projectors PA
by substituting  for (, ), where  is
the action sequence corresponding to .
To use the reference projector Pref of (6) during training,
we can obviously use the trajectories from the dataset as
references to guide the projection. However, during inference,
this dataset might be unavailable, or there might not be a
trajectory matching closely the desired one. To solve this issue
we leverage the imitation capabilities of diffusion models as
illustrated in Fig. 4 where the sampled trajectory matches
exactly the desired trajectory. Hence, we use as reference the
sampled trajectory before projection, i.e., line 6 of the Infer-
ence Algorithm 4 becomes i1 Pref
B. Projection curriculum
Motivated by the different approaches employed in the lit-
in our diffusion models. Indeed, work  uses projections
after each iteration of inference, while work  only projects
trajectories at the end of inference. In between these two
only meaningful when the level of noise on the trajectory
is sufficiently low. We arrived to the same conclusion for
scheduling projections during training where a meaningless
projection of random states would create a large loss destabi-
lizing the training of the diffusion neural network.
We thus created a projection curriculum to ensure projec-
tions only occur at sufficiently low noise levels  < min. We
realized empirically that the sudden contribution of projections
to the training loss as  passes below min can still destabi-
lize training as trajectory projections can create a significant
additional loss. We thus decided to create an intermediary
transition region between the projection regime of  < min
and the noisy regime without projections of  > max. In
this transition region the probability p of projecting each state
transition (st, st1) grows linearly as  approaches min. More
the noise level  as follows:
P(st1, ) : (1 b)P(st1, )  bst1 with b B
where p() :
if  > max,
if  [min, max],
if  < min,
where b is a Bernoulli variable taking value 1 with probability
p() and 0 otherwise. Thus, projection P occurs with prob-
ability 1 p(). In this transition regime, not projecting all
the state transitions of a trajectory breaks the compounding
effect and significantly reduces the total projection error as
illustrated in Fig. 1.
VI. IMPLEMENTATION
In this section, we provide the implementation details for
training and evaluating our proposed diffusion models.
A. Test environments
We deploy our approach on a set of different robotics
Hopper (12 states, 3 actions),
Walker (18 states, 6 actions),
HalfCheetah (18 states, 6 actions),
Quadcopter (17 states, 4 actions),
Unitree GO1 and GO2 (37 states, 12 actions).
The Hopper, Walker, and HalfCheetah environments are from
OpenAI Gym environments  with MuJoCo physics en-
gine . Unitree GO1 and GO2 are quadruped robots also
simulated with MuJoCo [51, 52]. Finally, we also test DDAT
in a quadcopter simulation . All these environments are
underactuated as a planned next state can be infeasible from
the current state due to the robots limited actuation. We collect
hundreds of admissible trajectories from these simulation envi-
ronments to create datasets D as discussed in Appendix D. For
each of these environments we trained a variety of diffusion
models predicting either only state trajectories, or only action
sequences (conditioned on the given initial state), or predicting
both states and actions. For each of these modalities we used
Algorithm 4 to train models with various projection schemes
best adapted to the specificities of each environment.
B. Diffusion architecture
The objective of our diffusion models is to predict se-
widely used U-Net [20, 33]. We implemented diffusion trans-
formers (DiT)  adapted to trajectory predictions by
and relying on the notations and best practices of . More
details on the diffusion process and our DiT architecture can
be found in Appendix C.
To incorporate our trajectory projections into the training
of our diffusion policy D as illustrated in Fig. 1, we need
our projectors to be differentiable, which is accomplished by
using cvxpylayers  to solve the convex optimization (10).
of the loss L  P(D(  )) to D and thus have
to go through projector P. Since the black-box simulator f
is typically not differentiable and is a part of the projections,
the gradients are not propagated perfectly through P.
The inference of Algorithm 4 is performed on a batch
of trajectories for each initial state s0 and is followed by
a selection phase to pick the best trajectory according to a
desired metric. For most of our robots we select the predicted
trajectory satisfying known state constraints for the longest
time interval. For instance, on the Hopper, Walker, and Unitree
this is the time before falling.
C. Evaluation metrics
We will evaluate our approach by answering the following
questions.
Q1 How close to dynamical admissibility are the different
DDAT models?
Q2 Can a diffusion model generate higher quality trajectories
with projections starting at the beginning of inference,
after inference, or progressively during inference?
Q3 Does training diffusion models with a projector help them
generate better trajectories than only using the projector
during inference?
Q4 Is it easier to generate high-quality admissible trajectories
when the diffusion model predicts the states, the actions,
or both?
To quantify the dynamical admissibility of trajectories we
need to measure the distance between each predicted next
state st1 and the closest admissible state belonging to the
reachable set of st. We thus need a ground-truth inverse
dynamics model, used only for evaluation and defined as
ID(st, st1) : arg min
st1 f(st, a)
where the minimum is well-defined if A is compact and f is
continuous. When st1 is reachable from st, the minimum
of the norm in (14) is 0 and f
Given a sequence of states
we autoregressively apply the inverse dynamics model to find
the closest admissible next state to the prediction as detailed
in Algorithm 5 located in Appendix B. Solving optimization
problem (14) with sufficient precision to make ID a ground-
truth model is challenging due to the black-box nature of f.
We discuss different approaches to solve (14) in Appendix B.
Equipped with this inverse dynamics model, we use the fol-
lowing two criteria to quantify the admissibility of trajectories.
Definition 3. The statewise admissibility error (SAE) is the
distance between next state prediction and closest admissible
next state obtained from the inverse dynamics:
SAE(st, st1) :
Definition 4. The cumulative admissibility error (CAE) is the
distance between a predicted trajectory
and its closest admissible projection
where s0 : s0 and st1  f
The CAE of a trajectory will be larger than its average SAE
due to compounding errors in the CAE.
Model nomenclature: To distinguish the models evaluated,
their name starts with a letter S, A, or SA depending on
whether they predict states, actions or both. The second part
denotes the projector used, i.e., P (5), Pref (6), PA (11),
or PSA (12). The projector can take two subscripts:  if a
projection curriculum is used and I if the projector is only
used at inference, it is otherwise assumed to be also used
during training. Finally, conditioning of models on a variable
v is denoted by (v).
VII. EXPERIMENTS
We will now answer Q1 in Section VII-A, Q2 in Sec-
tion VII-B, Q3 in Section VII-C and Q4 in Section VII-D.
A. Generating admissible trajectories
To answer Q1 we generate state trajectories with various
models and evaluate the distance to admissibility of these
trajectories by comparing their SAE and CAE. Since the
inverse dynamics (14) are used as verification tool, none of the
state models can achieve a smaller error than the precision of
the inverse dynamics. This precision is measured by evaluating
ID on a dataset of admissible trajectories. On the other hand,
trajectories generated with projectors PA of (11) and PSA
of (12) are automatically admissible since generated by their
corresponding action sequences.
Table I shows that applying any of our projection schemes
to the diffusion models reduces both their statewise and
cumulative admissibility error, even when the projection is
only applied at inference. Models predicting both states and
actions tend to have smaller admissibility errors than mod-
els predicting only states. On the Hopper and Walker our
state-action models are only one order of magnitude away
from the inverse dynamics precision whereas the HalfCheetah
and Quadcopter required extremely precise inverse dynamics
model. Thus Table I allows to answer Q1.
B. Projection curriculum during inference
We will now address Q2 motivated by the discussion
of Section V-B on the best projection curriculum to adopt.
HalfCheetah
Quadcopter
TABLE I: Distance to admissibility of 100 trajectories gen-
erated by diffusion models. The errors are calculated with
the inverse dynamics models (ID) whose precision limit that
of the models. Applying any projection scheme decreases the
admissibility errors by orders of magnitude on all robots.
Work  uses projections at each step of the inference pro-
by min  max  80, highest noise level of inference to
ensure a projection takes place at each iteration. To reproduce
the setting of  with post-inference projection we set
min  max  0.0021. To emulate the approaches of
[5, 25, 50] we select min  0.0021 and max  0.2.
Table II and Figures 9a and 9d show no significant differ-
ence in terms of admissibility based on the projection curricu-
lum. However, the quality of the trajectories is significantly
reduced by projections at high noise level as shown in Table II
and Fig. 7a and 8. This answers Q2.
C. Training with projections
Let us now answer Q3 on whether incorporating projections
during training helps diffusion models generate better samples.
Our intuition was that projections during training would better
prepare the diffusion models for the projections at inference.
trajectory quality between the models trained with projections
and those using projections only at inference. The only sig-
nificant quality difference is caused by employing different
projectors. In terms of admissibility, Figures 9b, 9c, 9e, and
timesteps
Ratio of falls
Hopper falling ratio ( )
Projections
pre-inference
mid-inference
post-inference
(a) Hopper SAP with three projection cur-
ricula. Starting projections at the beginning
of inference (blue) makes the Hopper fall
much earlier, nearly 90 at 150 timesteps
compared to 20 for the other curricula,
which perform equivalently.
timesteps
Ratio of falls
Hopper falling ratio ( )
No projections S
Inference with S
Trained with S
Inference with S
Trained with S
(b) Fall ratios for the Hopper state models S,
I and SPref
. The falling ratio
is consistently higher without projections and
with the naive projector P of Algorithm 1
compared to the reference projector Pref of
Algorithm 2. Training with projections or
only using projections at inference has no
significant impact.
timesteps
Ratio of falls
Walker falling ratio ( )
No projections SA
Inference with SA
Trained with SA
Inference with SA
Trained with SA
(c) Fall ratios for the Walker state-action
models SA, SAPI, SAP, SAPSA
. Without projections, the Walker falls
much earlier than using any projections. The
state-action projector PSA of (12) prevents
the Walker from falling for longer than pro-
jection P of (9). Training with projections
has no significant impact.
Fig. 7: Ratios of trajectories deployed open-loop having fallen at a given timestep for the Hopper and Walker. The shade is
the maximum and minimum number of trajectories having fallen at each timestep over 5 runs of 100 trajectories each.
parameters
Hopper SAP
Quadcopter SAPSA
survival ()
reward ()
survival ()
reward ()
TABLE II: Distance to admissibility of generated trajectories projected either at each step of inference (pre), gradually during
inference (mid), or only after inference (post). The results are averaged over 500 trajectories and clearly demonstrate that
projections at high noise level (pre) hurt significantly the quality of the trajectories in terms of survival and reward. The
admissibility is only marginally affected by the projection curriculum. The difference between post-inference projections and
mid-inference projections is not significant. The Quadcopter trajectories are projected with SAPSA
and hence are all admissible.
Quadcopter trajectories
Projections
pre-inference
mid-inference
post-inference
Fig. 8: Quadcopter trajectories generated by the SAPSA
model with projections starting either pre-inference, or mid-
inference or post-inference. The desired behavior exhibited
in Fig. 10c slaloms between the obstacles. None of the
trajectories generated by the pre-inference model pass the
by the other two projection curricula pass the slalom. Hence,
projections at the beginning of inference strongly reduce the
quality of the samples.
9f show no significant difference between models trained
with projections and those only using them at inference.
Despite our best effort to incorporate projections into the
training of diffusion models, projections still disrupt training
by significantly increasing the training loss. Thus our answer
to Q3 is negative, training with projections is not helpful.
D. Diffusion modality
Question Q4 on choosing the best prediction modality is
motivated by the large literature embracing each one of these
approaches as discussed in Section II-B. We have already
introduced models predicting states only and others generating
both states and actions. The last category generates action
sequences conditioned on the current state. By generating only
in  actions are more volatile than states rendering action
sequences harder to predict. However, this claim was refuted
in action planning models such as . The action space is
typically smaller than the state space, which also plays in favor
of action prediction.
Table I shows that the models predicting both states and
actions generate trajectories closer to admissible than the ones
predicting only states. However, in term of admissibility all
timesteps
Hopper Statewise Admissibility Error ( )
Projections
pre-inference
mid-inference
post-inference
(a) Hopper SAE for state-action models
with different projections curricula.
timesteps
Hopper Statewise Admissibility Error ( )
No projections S
Inference with S
Trained with S
Inference with S
Trained with S
(b) Hopper SAE for state models.
timesteps
Walker Statewise Admissibility Error ( )
No projections SA
Inference with SA
Trained with SA
Inference with SA
Trained with SA
(c) Walker SAE for state-action models.
timesteps
Hopper Cumulative Admissibility Error ( )
Projections
pre-inference
mid-inference
post-inference
(d) Hopper CAE for state-action models
with different projections curricula.
timesteps
Hopper Cumulative Admissibility Error ( )
No projections S
Inference with S
Trained with S
Inference with S
Trained with S
(e) Hopper CAE for state models.
timesteps
Walker Cumulative Admissibility Error ( )
No projections SA
Inference with SA
Trained with SA
Inference with SA
Trained with SA
(f) Walker CAE for state-action models.
Fig. 9: Statewise (SAE) and Cumulative (CAE) Admissibility Errors for different Hopper and Walker models collected over 5
runs of 100 initial conditions with 8 samples each. The shade shows the standard deviation. Figures (b) (c), (e) and (f) show
that projections drastically improve the admissibility of models. Figures (a) and (d) show small difference in admissibility
when projecting during inference
HalfCheetah
Quadcopter
survival ()
reward ()
survival ()
reward ()
survival ()
reward ()
survival ()
reward ()
TABLE III: Mean and standard deviation of reward and survival rates () over 100 initial states with 8 trajectories sampled
for each. As in  we used Model Predictive Path Integral (MPPI)  as a baseline, but it struggled due to the long-horizon
nature of our tasks. The data row refers to our training data generated by closed-loop RL or MPC policies, which is why their
performance exceeds all the open-loop models. Among all the models tested SAPSA
consistently perform among the best.
models predicting actions are superior to the states-only ones
since their predicted sequence of actions always yield an
admissible trajectory.
D-MPC  is an offline model-based approach learning a
dynamics model and a policy using two different diffusion
models. Because our work assumes access to a dynamics
its diffusion policy. We trained such a model generating
sequences of actions conditioned on the initial state s0 on our
datasets using our DiT architecture under the acronym A(s0).
As shown in Table V the Action model performance can
vary from worse to best on extremely similar environments.
Except for the Hopper where A(s0) performs poorly, it shows
decent results on the other environments of Table IV. Due to
its inconsistency predicting only action sequences does not
perform as well as state-action models like SAPSA
survival  ()
reward ()
HalfCheetah
Quadcopter
TABLE IV: Reward and survival comparison between S, SA,
and A models averaged over 100 initial states and 8 trajectories
generated for each. We could not evaluate state models for the
HalfCheetah due to the lack of performant inverse dynamics
model. Despite their lack of admissibility guarantees state
models perform well on the Hopper and Quadcopter. State-
action model SAPSA
is consistently near the best while action
models A(s0) achieve inconsistent results.
answers Q4.
survival ()
reward ()
Unitree GO1
A(c, s0)
Unitree GO2
A(c, s0)
TABLE V: Reward and survival rates () std over 100
initial states s0 with 64 samples generated per s0. All these
models predict actions implemented open-loop on the robots
and conditioned on the velocity command c. We could not
evaluate state models for the Unitree due to the lack of
performant inverse dynamics model. The action only model
A(c, s0) ranks respectively best and worse on the GO1
and GO2, whereas state-action model SAPA
(c) performs
consistently well on both robots.
VIII. HARDWARE EXPERIMENTS
In this section, we evaluate DDAT on real-world hardware.
dimensional trajectories deployable on hardware in open-
loop. Moreover, DDAT causes fewer failures than a baseline
diffusion model and outperforms it in task completion. We
train both diffusion policies in the Unitree GO2 environment
and generate trajectories walking forward for 10 seconds. To
evaluate walking behavior, we aim for the quadruped to pass
a goal line 3 meters in front of the start position while staying
within a 1 meter corridor. If the quadruped leaves the sidelines
or begins to fall, we consider it a failure. For this evaluation,
we sampled 3 trajectories from each model and performed
10 trials. As such, we performed a total of 30 trials for each
Success ()
Exits ()
Falls ()
TABLE VI: Evaluation of 60 walking hardware trials of
trajectories generated by diffusion models on the Unitree GO2.
If the quadruped ever left the prescribed region (Exits) or
lost balance to the point of falling, we considered the runs
as failures.
cess rates compared to the vanilla diffusion policy. Specifically,
DDAT was successful in more than twice as many trials as the
diffusion policy baseline. The basel
