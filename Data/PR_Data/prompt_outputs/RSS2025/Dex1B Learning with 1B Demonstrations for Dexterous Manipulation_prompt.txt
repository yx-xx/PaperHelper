=== PDF文件: Dex1B Learning with 1B Demonstrations for Dexterous Manipulation.pdf ===
=== 时间: 2025-07-22 15:48:03.220680 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Dexterous Manipulation
Jianglong Ye, Keyi Wang, Chengjing Yuan, Ruihan Yang, Yiquan Li, Jiyue Zhu
Yuzhe Qin, Xueyan Zou, Xiaolong Wang
UC San Diego
Fig. 1: The Dex1B benchmark consists of 1B generated high-quality demonstrations for grasping and articulation tasks. In the
bottom row, we show the deployment results of the simulation (small image) and real world (big image) using DexSimple trained
with Dex1B. This demonstrates that Dex1B is scalable and generalizable to real environments.
AbstractGenerating large-scale demonstrations for dexterous
hand manipulation remains challenging, and several approaches
have been proposed in recent years to address this. Among
enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-
with generative models. The dataset contains one billion demon-
strations for two fundamental tasks: grasping and articulation.
To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies addi-
tional conditions to enhance diversity. We validate the model on
both established and newly introduced simulation benchmarks,
Equal contribution.
where it significantly outperforms prior state-of-the-art methods.
through real-world robot experiments.
I. INTRODUCTION
Dexterous manipulation with hand has been a long-standing
topic in robotics. While its highly flexible and dynamic
nature allows for more complex and robust manipulation
it very challenging to achieve its ideal function. In fact,
with recent advancements in applications using parallel-jaw
grippers [53, 17, 11, 54, 3], researchers in the community
have started questioning the necessity of dexterous hands and
having doubts about whether hands are only making problems
We argue that dexterous hand is indeed valuable, but we
just did not have enough data to capture the diverse and
complex distributions required for effective dexterous manip-
ulation. To address this data scarcity, previous works have
explored various approaches, including human demonstra-
tions [25, 37, 10, 9], optimization-based methods [45, 8, 44],
reinforcement learning (RL)-based techniques [12, 55]. While
these methods help generate demonstrations at a certain scale,
they each have limitations: human annotation is costly and
to initialization, and RL-based techniques lack data diversity.
distribution of demonstration datasets, motivating us to explore
how generative models can be leveraged for data generation.
And we identify two key issues when applying generative
models on data generation: i). Feasibility: The success rate
of generative models is often lower than that of discriminative
models. ii). Diversity: While generative models can produce
more diverse actions than discriminative models, they still tend
to interpolate between the seen demonstrations, which may
maintain or even reduce the original level of diversity of whole
dataset rather than expanding it.
To address the feasibility issue, we propose incorporating
geometric constraints into the generative model, which sig-
nificantly improves its performance. We also integrate opti-
mization techniques with generative models, leveraging the
strengths of both approaches: optimization ensures physically
plausible results, while generative models enable efficient
large-scale generation. To improve diversity, we introduce
additional conditions to the generative model and prioritize
sampling actions for less frequent condition values, encourag-
ing the model to generate actions that differ more from existing
ones in the dataset.
Our approach begins with an optimization-based method
to construct a small yet high-quality seed dataset of dexter-
ous manipulation demonstrations. We then train a generative
model on this dataset and use it to scale up data generation
efficiently. To mitigate biases introduced by optimization, we
propose an debias mechanism, which systematically improves
the diversity of generated data. This framework results in
to DexGraspNet , which operates on the object set of
similar scale, our dataset offers 700 more demonstrations,
significantly enriching the available training data for learning-
based models. Unlike previous approaches that rely solely
on human annotation or optimization, our method combines
optimization and neural networks, achieving a superior balance
between cost, efficiency, and data quality.
To effectively leverage the scale and diversity of Dex1B,
we introduce DexSimple, a new baseline that extends prior
work  by incorporating conditional generation and en-
Num Demo-
nstrations
Grasping
Grasping
ContactPose
Grasping
Grasping
DexGraspNet
Grasping
Handover
Optim.  RL
Dex1B (Ours)
Grasping  Arti.
Optim.  Gen.
TABLE I: Dataset comparison on dexterous robotic manip-
ulation. Our proposed Dex1B is a multi-task trajectory-level
dataset with 1B demonstrations. Optim., Arti., Gen. are short
for optimization, articulation and generative models.
hanced loss functions. Despite its simplicity, DexSimple ben-
efits from the scale and diversity of Dex1B, achieving state-
of-the-art (SoTA) performance across dexterous manipulation
Our key contributions are as follows:
We introduce a novel iterative data generation pipeline
that combines optimization and generative models to gen-
erate large-scale dexterous demonstrations for grasping
and articulation tasks. Using this approach, we construct
stration dataset to date.
We propose a simple yet effective baseline method that
incorporates enhanced loss functions while supporting
conditional generation, making it particularly well-suited
for our iterative pipeline and policy deployment.
Leveraging Dex1B and DexSimple, we achieve a 22
performance improvement over previous best methods
on grasp synthesis tasks, setting a new benchmark in
dexterous manipulation.
II. RELATED WORK
Dexterous Hand Manipulation. Dexterous hand manipula-
tion is a pivotal area in robotics, concentrating on precise,
multi-fingered grasping and manipulation. Early research in
this field primarily addressed control-based methods, laying
the foundation for dexterous manipulation through studies in
caging to grasping techniques , grasp synthesis , and
manipulability in underactuated hands . Further develop-
ments in control-based approaches simplified the search space
for multi-fingered grasps [33, 34], as well as optimization
processes [56, 15], leading to highly precise, agile, and safe
manipulation. However, these methods generally lack gener-
alization across diverse environments and use cases.
Subsequent research shifted towards learning-based ap-
proaches to enhance flexibility and scalability [1, 32]. This
includes generating pose vectors directly [21, 14, 50], utilizing
intermediate representations [40, 47], and leveraging contact
maps [5, 43]. While learning-based methods offer increased
limitation addressed in this work. Recent works [28, 41] em-
ploy RL-based methods to transfer robust grasping capabilities
of dexterous robotic hands to the real world, using point cloud
and RGB inputs, respectively.
Manipulation Dataset. Existing manipulation datasets can be
broadly categorized into synthetic and real-world collections.
Synthetic datasets like ObMan  and DDGdata  utilize
the GraspIt planner to generate grasping poses but suffer
from limited diversity due to naive search strategies. Real-
world datasets with human hand poses offer more natural
annotations and physics constraints, and DexYCB  which
captures multi-view RGBD recordings. ContactDB  and
ContactPose  further enhance grasp understanding by in-
corporating thermal imaging and detailed contact information,
respectively. However, these real-world datasets are inherently
restricted to human hand structures and common daily hand
poses. In contrast, our approach leverages optimization and
neural networks to generate diverse manipulation trajectories
that transcend these limitations. Our work is related to prior
tion of generative models. Our approach differs in two ways:
(1) Scale: our pipeline aims to generate 1B demonstrations
rather than thousands. (2) Diversity: we increase diversity by
conditioning on object geometry instead of using a classi-
fier. The recent work DexGraspNet2.0  proposes learning
a diffusion model over large-scale optimized demonstration
datasets. In contrast, our work integrates the generative model
directly into the data generation pipeline instead of relying
solely on optimization. Additionally, we unify grasping and
articulation tasks in both our model design and benchmark,
while they focus on grasping only. We presents the differences
of several representative manipulation datasets in Tab. I.
III. DEX1B BENCHMARK
We introduce a comprehensive benchmark for two fundamen-
tal dexterous manipulation tasks: grasping and articulation.
In the grasping task, the robot hand must reach for and lift
an object, whereas the articulation task requires the hand to
manipulate an articulated object to achieve a specific degree
of opening. Our benchmark consists of over 6,000 diverse
objects and provides one billion demonstrations across three
dexterous hands: the Shadow Hand, the Inspire Hand, and
the Ability Hand. Each demonstration consists of a complete
action sequence, from initial reaching to object manipulation.
To generate these demonstrations, we synthesize key hand
poses at critical interaction points with the object, while the
remaining action sequencessuch as reaching, lifting, and
openingare generated using motion planning. The evaluation
of our benchmark is conducted with ManiSkill [42, 48].
Overview of Data Generation. Broadly, hand pose gener-
ation for dexterous manipulation can be approached through
optimization-based methods or generative models. While opti-
mization methods can be effective, they are often computation-
ally expensive, especially for large-scale generation, and tend
to bias the dataset toward simpler cases. On the other hand,
generative models rely on an initial dataset to learn meaningful
data distributions. In this work, we integrate both approaches
to leverage their strengths.
As illustrated in Figure 2, we begin by constructing a small-
scale seed dataset using optimization. This seed dataset serves
as the foundation for training a generative model to learn
its underlying data distribution. The trained generative model
is then used to produce additional demonstrations. However,
since the generative model inherently inherits the biases of
the seed dataset, we introduce a debiasing strategy to enhance
diversity. Specifically, we condition the generative model on
targeted factors to generate hand poses under less frequently
observed conditions, thereby expanding the dataset beyond the
initial distribution. By iteratively refining the generative model
through repeated training and debiasing operations, we con-
struct our final dataset, Dex1B, which achieves both diversity
and robustness in dexterous manipulation demonstrations.
Optimization for Seed Dataset. To generate the seed dataset,
we implement an efficient optimization method for hand pose
synthesis based on previous work [45, 8], while including
new features like scene-level collision avoidance and support
for various hands. Although the optimization process is well-
engineered (1,000 grasps per minutes on a single GPU), gen-
erating one billion demonstrations remains computationally
expensive. Therefore, we only use optimization to create a
small-scale seed dataset (around 5 million poses). A dexterous
hand pose is parameterized by a tuple g  (T, R, ), where
R3 for global translation, R SO(3) for global
for Shadow hand, d  6 for Inspire and Ability hand). The
object geometry is represented by mesh O. Unlike previous
methods [45, 8], which use link meshes to model hand
spheres (around 10 spheres for each link). This approximation
significantly accelerates the optimization process.
Our optimization energy function for the grasping task
Egrasp is given by:
Egrasp  Efc  wdisEdis  wsdfEsdf  wjEj  wsEs,
where Efc is the force closure energy term, Edis minimizes
contact distance, Esdf prevents penetration, Ej enforces joint
for the corresponding terms. The penetration term Esdf is
formulated based on the sphere-based hand representation. A
sphere is defined by (c, r), where the center c R3 and the
radius r R. The center c is transformed with the link pose
using forward kinematics. The SDF penetration term is given
ri SDF(ci, O),
where SDF() is the signed distance function (SDF) query
between a 3D point and a mesh. To prevent scene collisions,
we incorporate SDF queries for other meshes (e.g., the table),
and the final SDF for a sphere is computed as the maximum
value across different meshes. Detailed formulation of other
energy terms can be found in previous works [45, 24].
Fig. 2: Dex1B demonstration collection. The engine takes object assets and hand pose initialization as input, using a control-
based optimization algorithm to generate the Seed dataset. Then the Seed dataset is used as the training data for DexSimple,
else for Dex1Bfor the last iteration. Then DexSimple will generate a scaled proposal dataset with  as the scaling ratio. For
the proposal dataset, we then use the simulation critic and debiased algorithm to create the debiased dataset for optimization
refinement.
While the force closure energy term Efc is suitable for the
grasping task, achieving force closure in the articulation task
is usually difficult and unnecessary. Instead, the articulation
task requires the hand pose to generate specific forces and
drawer. Therefore, we replace the force closure energy term
Efc with the task wrench space term Etws introduced in ,
which approximates the difference between a target wrench
space and the current wrench space of a given hand pose. The
optimization energy function for the articulation task Earti is
defined as:
Earti  Etws  wdisEdis  wsdfEsdf  wjEj  wsEs.
The target wrench space for articulated objects with revolute
joints consists of a torque aligned with the joint axis and
arbitrary forces. The wrench space for objects with prismatic
joints consists of forces sampled from a 30cone aligned with
the joint axis and zero torque.
The optimization process is implemented using Warp-
Lang  and accelerated with its BVH mesh structure. The
optimized hand poses are evaluated using the simulator, and
the successful ones are retained as a seed dataset.
Generative Models for Scaling-up Demonstrations. Gener-
ative models are widely adopted for capturing the distribution
of action demonstrations. However, applying these models for
data generation still presents several challenges: i). Feasibility:
The success rate of generative models is often lower than that
of discriminative models, leading to a higher proportion of
infeasible samples. ii). Limited Diversity: While generative
models can produce more diverse actions than discriminative
of diversity of whole dataset rather than expanding it.
To address the feasibility issue, we first incorporate geo-
metric constraints during the generation process, enabling our
model to outperform state-of-the-art generative models (see
Sec. IV for model details and Sec. V for experiments). In
hand poses to prevent penetration and ensure that the fingers
closely cover the object. The energy function for the post-
optimization stage of both tasks, Epost, is defined as:
Epost  wdisEdis  wsdfEsdf  wjEj  wsEs.
We exclude the task-specific terms Efc and Etws here since
we only aim to make slight adjustments to the finger positions.
Although we use optimization in this stage, the overall data
icantly more efficient than pure optimization. The sampling
process is approximately 100 times faster, and the number of
iterations in post-optimization is substantially lower than in
the pure optimization (100 vs. 6000). The refined hand poses
are then evaluated using the simulator.
To improve diversity, we encourage the generative model
to sample actions that differ more from existing actions in
the dataset while maintaining success rate. To achieve this,
we introduce an additional condition to the generative model
and prioritize sampling actions for less frequent conditions.
point on the object. We first define the heading direction
v R3 of a hand pose as the vector from the palm center
to the midpoint between the thumb tip and the middle finger
tip. The closest point along this direction is then assigned
as the associated point of the hand pose. We adapt our
generative model to take the feature vector of a 3D point
as a condition for generating corresponding actions. During
data generation, we first statistically compute the probability
of each point associated with existing actions on the object
and then sample new actions inversely proportional to this
probability. Additionally, we statistically count the number of
existing actions for each object and sample more actions for
the more challenging ones.
After increasing the dataset size and diversity, retraining
the model on the expanded dataset can further improve its
performance. This iterative data generation process can be
repeated multiple times to progressively refine both the model
and the dataset.
Motion Planning. With the key-frame action, motion planning
is still required to complete both tasks. For the reaching
Fig. 3: DexSimple Pipeline. Our model takes in hand parameters and object point clouds as fixed input for CVAE, while root
point cloud embeddings are re-emphasized at the latent space. The output of CVAE is the forward kinematics define the hand
pose trajectory optimized by the effective loss function.
stage in both tasks, motion planning can be formulated as
an optimization problem that maximizes smoothness while
avoiding collisions. Given a manually defined starting action
and a goal action, we first linearly interpolate between them,
and then optimize the intermediate actions using the following
energy function:
Ereach  wsmooth
gi gi12  wsdf
Esdf(gi),
where {g0, g1, . . . , gN} denotes the sequence of hand poses
along the trajectory, wsmooth and wsdf are weights for smooth-
ness and collision avoidance respectively, and Esdf(gi) is
computed based on the sphere-based SDF errors with respect
to nearby scene meshes. Minimizing Ereach produces a trajec-
tory that is both smooth and collision-free. Compared to other
motion planning libraries, this simple optimization is naturally
suitable for large-scale parallel data generation.
After reaching, for the grasping task, we will execute an
over-shoot action to grasp the object and increase the height
of the target action to lift it. For the articulation task, we
will follow the trajectory based on the given joint axis (rotate
along the revolute joint axis or translate along the prismatic
joint axis). The complete planned trajectory is executed in the
simulator for evaluation.
IV. DEXSIMPLE MODEL
While a large body of generative models [20, 26, 51, 23, 46]
have been proposed for dexterous hand manipulation in recent
mains limited. In this work, we revisit the simple CVAE model
and demonstrate that incorporating an SDF-based geometric
constraint during training enables it to outperform state-of-
the-art methods by a large margin. Furthermore, we integrate
additional condition over the base model to support diverse
data generation.
Vision Encoder and CVAE. We employ a point cloud P
RN3 as the visual input, using a full point cloud sampled
from the object mesh for data generation and a single-view
depth map for policy deployment. We utilize PointNet  to
encode the point cloud into a global feature vector fobj Rd
and local feature vectors fp Rd for each point p R3:
The VAE model uses a multi-layer perceptron (MLP) to
encode the hand pose g into the mean and standard deviation
vectors of a latent distribution. A sample is drawn from this
distribution and passed to the MLP decoder to reconstruct the
original hand pose. After concatenating conditional vectors
(e.g., the global point cloud feature vector fobj) to both the
inputs of the VAE encoder and decoder, the CVAE model can
generate samples under a given condition:
,   Enc(g, fobj),
g  Dec(z, fobj).
In our work, we simply concatenate additional vectors to
incorporate more conditions.
Objectives. The CVAE training is supervised by the standard
reconstruction loss LR  g g2
2 and the KL divergence
loss LKL  DKL
. To enforce geometric
the optimization stage, building a BVH structure for each
object at each iteration during training is time-consuming.
We therefore use a sampled point cloud to represent the
object geometry while still using spheres to represent the hand
geometry. The SDF loss is then given by:
where C is the set of spheres representing the hand geometry
and rc is the radius of sphere c. This loss is not only straight-
forward to implement in PyTorch, but we also empirically
find that training with the point-sphere SDF loss is more
stable compared to the mesh-sphere SDF loss used in the
optimization stage. With this SDF loss term, our proposed
DexSimple outperforms SOTA methods by a large margin (see
The overall loss for the base model is defined as:
L  RLR  KLLKL  SDFLSDF,
where R, KL, and SDF are weights that balance different
loss terms.
Conditions for Data Generation. As mentioned in Sec. III,
each hand pose is associated with a single object 3D point
p by finding the closest point along its heading direction v.
To achieve this, we concatenate the corresponding local object
feature vector fp with the CVAE conditional vector. We further
enforce this condition by introducing an additional loss term
Lpoint that penalizes the distance between the point p and the
generated hand pose heading direction.
V. EXPERIMENTS
We firstly evaluate the effectiveness of the proposed gen-
erative model, DexSimple, for grasp synthesis on the Dex-
GraspNet  benchmark. Then, we provide details on the
synthesized Dex1B demonstration dataset and compare it with
human-annotated demonstration datasets on both lifting and
articulation tasks. Additionally, we downscale Dex1B and
evalute the performance of methods trained on it. Finally,
ablation studies are conducted to validate our design choices.
A. Grasping Synthesis Evaluation
Grasping is essential in most manipulation tasks, we firstly
evalute the proposed methods effectiveness in grasp synthesis
using the DexGraspNet  benchmark. We train DexSimple
solely with the benchmarks provided training data, reducing
the output to a single frame and omitting conditioning during
training. The validation is conducted in the Isaac Gym simu-
lator  using ShadowHand . We adhere to the metrics
established in the benchmark to ensure fair comparisons with
baseline methods, which are divided into two categories:
quality (Success Rate, Q1-score, Penetration) and diversity (H
mean and H std). We follow the implementations 1 in [45, 26].
We compare with DDG , GraspTTA , the generation
module in UniDexGrasp  (abbreviated as UDG), and
We present quantitative results in Table II. Many grasp
generation methods, such as UGG , commonly employ
post-optimization to enhance performance. To ensure a fair
ated as Opt) in the table. The results show that the proposed
generative model, DexSimple, outperforms all baseline meth-
ods by a large margin. In terms of quality, DexSimple (with
post-optimization) achieves the highest success rate (86.0),
the highest Q1 score (0.125), and the lowest penetration (0.13).
For diversity, DexSimple outperforms baseline with a higher
entropy mean of 8.56.
UGG  proposes a learning-based discriminator to filter
1Please refer to supp. material for details.
Diversity
Opt Filter
SR Q1 Pen
H mean H std
DexSimple
GraspTTA
DexSimple
DexSimple
TABLE II: Grasping synthesis results on the DexGrasp-
Net  benchmark. The proposed generative model, DexSim-
evaluation results are taken from UGG . Opt, SR, and
Pen are short for Optimization, Success Rate, and Penetration,
respectively.
the success rate of DexSimple without post-optimization and
filtering is slightly lower than that of DDG ; this is
expected as our method is a generative model while DDG
is a regression model, and our method achieves much higher
diversity (8.53 vs. 5.68).
B. Dataset Analysis
Tasks Definition. While the proposed iterative data generation
pipeline can be applied to multiple hands, we take Shad-
owHand as an example to detail our data curation process.
Beyond the grasping synthesis task in Sec. V-A, we focus
on two tasks: Grasping and Articulation. The goal of the
grasping task is to reach an object placed on a table, grasp
contact between the hand and the object. We show example
trajectories from Dex1B for the grasping task in Fig. 5. The
articulation task requires reaching an articulated structure (e.g.,
a laptop, box, or faucet) and opening it to increase its joint
angle by 0.5 while maintaining contact between the hand and
the object.
For the grasping task, we utilize all 5751 object assets
collected by DexGraspNet  and exclude all objects that
cannot stand stably on the table. Note that the settings between
DexGraspNet and our dataset are different. Our focus is
on table-top tasks, while DexGraspNet focuses on grasping
objects in free space. Approximately 90 of grasping demon-
stration in DexGraspNet would collide with the table. For the
articulation task, we utilize 650 articulation assets collected
by DexArt  and ManiSkill . We adopt the official
trainingtesting splits provided by previous works [45, 16, 2].
Dataset Curation. Our dataset curation starts by optimiz-
ing a seed dataset with 5 million demonstrations. We first
train DexSimple on this seed dataset to create a 50 million
proposal dataset. After optimization refinement, we use the
ManiSkill SAPIEN  simulation to filter out unsuc-
cessful trajectories and rebalance the data as described in
Sec. III. We then retrain DexSimple on the 50 million dataset
to produce a 500 million proposal dataset, and repeat this
Fig. 4: Diverse demonstrations for objects from traintest splits. We show only the contact frame for clarity.
Fig. 5: Lifting trajectory from Dex1B dataset.
Fig. 6: Probability distribution of joint values from Dex1B
and DexYCBARCTIC. The distribution of Dex1B is more
evenly spread, centering around the mean joint values.
process. Finally, we collect a dataset with 950 million (around
1 billion) successful trajectories.
Implementation Details. Compared to DexGraspNet , our
implementation of pure optimization-based grasp generation is
30 times faster, requiring only 2 minutes to generate 2000
grasps for 6000 steps on a single RTX-3090, while also
achieving a higher success rate (27 vs. 20 for Shadow-
Hand). When initialized with the network, our method is even
700 times faster, including CVAE sampling and 100 post-
optimization steps. All CUDA and geometry-related operations
are implemented using warp-lang .
In the proposed DexSimple, we adopt PointNet  as the
visual encoder, extracting object features in a dimension of
256. For the CVAE, both the input and output dimensions are
Nframe  NDOF, and the latent vector dimension is set to 256.
Comparison.
demonstrate
Dex1B by comparing it to two large-scale, human-annotated,
trajectory-level datasets: DexYCB  and ARCTIC .
DexYCB includes 20 objects, with approximately 500 trajec-
tories for each hand. ARCTIC includes 10 articualtion objects
with a total of 301 trajectories. We follow
[51, 10] to gen-
erate robot demonstrations from the DexYCB and ARCTIC
Eval on DexYCB
Eval on Dex1B
Training Data Train set Test set Train set Test set
BC w. PointNet DexYCB
DexSimple
BC w. PointNet Dex1B (ours)
DexSimple
Dex1B (ours)
(a) Lifting task comparison on DexYCB  and Dex1B.
Eval on ARCTIC
Eval on Dex1B
Training Data Train set Test set Train set Test set
BC w. PointNet ARCTIC
DexSimple
BC w. PointNet Dex1B (ours)
DexSimple
Dex1B (ours)
(b) Articulation task comparison on ARCTIC  and Dex1B.
TABLE III: Benchmarks on (a) lifting tasks with DexYCB
and our datasets, and (b) articulation tasks with ARC-
TIC  and our datasets. Models trained on Dex1B consis-
tently outperform those trained on DexYCBARCTIC across
various tasks, baselines, and splits.
trajectories and adding noise to generate a larger number
of physically plausible demonstrations. We collected 62
and 64 of all trajectories from the DexYCB and ARCTIC
We highlight the diversity of Dex1B. Using the proposed
debiasing approach in Sec.III, the diversity across object cate-
enhanced by generating additional samples for underrepre-
sented data. Besides, we discretize the range of joint angles
into bins and estimate a probability distribution over them.
Fig. 6 shows the distribution of two joints. Unlike DexYCB
and ARCTIC, which often have joint values concetrate at
the limits, the distribution of Dex1B is more evenly spread,
centering around the mean joint values. This is achieved by
debiasing approach and including a regularization term that
discourages the hand from getting too close to the joint limits
during optimization. Qualitative results are presented in Fig. 4.
Benchmarks. We benchmark two methods for grasping and
articuation tasks on our datasets, and compare them with the
Fig. 7: Qualitative results for both grasping and articulation tasks. We show only the contact frame for clarity.
Fig. 8: Scaling the number of demonstrations used for
training. For both tasks, our model consistently improves with
more training data.
same methods trained on DexYCB  and ARCTIC . In
addition to the proposed DexSimple, we implement a vanilla
behavioral cloning with PointNet  (referred to as BC w.
PointNet). This model takes the object point cloud, current
hand joint values, and poses as input to predict chunked actions
for the next n  50 frames. The predicted actions are then
merged using a temporal weighting technique form ACT .
The results are reported in Tab. III. When comparing models
trained on Dex1B to those trained on DexYCBARCTIC, we
consistently find that the former outperforms the latter across
ing methods perform better when trained on our larger and
more diverse Dex1B dataset. Tab. III also demonstrates that
the proposed generative method, DexSimple, achieves better
performance than the regression-based BC baselines on both
the relatively small DexYCBARCTIC dataset and the larger-
scale Dex1B. For lifting task, it also can be clearly observed
that models trained on DexYCB struggle to generalize to
unseen objects. Qualitative results are shown in Fig. 7.
C. Scaling the Dataset
To investigate the effect of training data size on perfor-
impact on the success rates of both the lifting and articulation
tasks. As shown in Fig. 8, the performance degradation ratio
increases as data is reduced, illustrating that the success
rates of the proposed DexSimple consistently improve with
more training data. Notably, we observe that perfo
