=== PDF文件: Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI with Man.pdf ===
=== 时间: 2025-07-22 15:44:22.409715 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating GPU Parallelized Robot Simulation
and Rendering for Generalizable Embodied AI with
ManiSkill3
Stone Tao1,3, Fanbo Xiang1,3, Arth Shukla1,3, Yuzhe Qin1, Xander Hinrichsen1, Xiaodi Yuan1,3,
Chen Bao2, Xinsong Lin1, Yulin Liu 1,3, Tse-kai Chan1, Yuan Gao1, Xuanlin Li1, Tongzhou Mu1,
Nan Xiao1, Arnav Gurha1, Viswesh Nagaswamy Rajesh1, Yong Woo Choi1, Yen-Ru Chen1,
Zhiao Huang1,3, Roberto Calandra4, Rui Chen5, Shan Luo6, Hao Su1,3
1University of California San Diego, 2Carnegie Mellon University, 3Hillbot, 4TU Dresden
5Tsinghua University, 6Kings College London
AbstractSimulation has enabled unprecedented compute-
scalable approaches to robot learning. However, many existing
simulation frameworks typically support a narrow range of
scenestasks and lack features critical for scaling generaliz-
able robotics and sim2real. We introduce and open source
simulator with contact-rich physics targeting generalizable ma-
nipulation. ManiSkill3 supports GPU parallelization of many
aspects including simulationrendering, heterogeneous simula-
with rendering on ManiSkill3 uses 2-3x less GPU memory
usage than other platforms and achieves up to 30,000 FPS
in benchmarked environments due to minimal pythonpytorch
overhead in the system, simulation on the GPU, and the use of
the SAPIEN parallel rendering system. Tasks that used to take
hours to train can now take minutes. We further provide the most
comprehensive range of GPU parallelized environmentstasks
spanning 12 distinct domains including but not limited to mobile
in realistic scenes designed by artists or real-world digital twins.
In addition, millions of demonstration frames are provided from
motion planning, RL, and teleoperation. ManiSkill3 also provides
a comprehensive set of baselines that span popular RL and
learning-from-demonstrations algorithms.
I. INTRODUCTION
One of the grand challenges of robotics is robust and
generalized manipulation. However, unlike vision and lan-
guage research, there are still no good datasets for robotic
manipulation that can be trained on. One approach has been to
create human-scalable real-world teleoperation tools [18, 12]
to then perform imitation learning. Another is to set up real-
world reinforcement learning to fine-tune offline trained poli-
cies . However, real-world imitation learning approaches
require enormous amounts of data that are infeasible to collect
efficiently at low costs only to achieve relatively low success
rates that are otherwise impractical for real-world deploy-
ment . Real-world reinforcement learning approaches are
generate real-world rewardssuccess and environment resets.
GPU parallelized simulations such as Isaac  and
Mujocos MJX  have made massive advancements in
solving some robotics problems such as robot locomotion
by training in large-scale GPU parallelized simulations
with reinforcement learning (RL) . GPU parallelized
simulation makes data incredibly cheap to generate. However,
when it comes to manipulation, success is often limited to
narrower ranges of manipulation tasks and typically requires
strong state estimation  to replace visual inputs like RGB
or pointcloud. Existing GPU simulators have limitations that
hinder the generalization and scalability of previous work.
These simulators lack support for heterogeneous simulation,
where each parallel environment contains different scenes.
capabilities.
algorithms
reinforcement
learning (RL) that operate on visual input train too slowly to
be practical. We propose ManiSkill3 to address past limitations
and open source the framework under the Apache-2.0 license,
building upon past work in ManiSkill 1 and 2 [38, 19]. The
majority of features presented in this paper are already open-
sourced.
maniskill.ai.
maniskill.readthedocs.io.
iskill.readthedocs.ioenlatestuser guidedemosgallery.html.
The core contributions of ManiSkill3 that set it apart from
existing simulators are as follows:
1) State-of-the-art GPU Parallelized Simulation and
tasks faster than it would have taken on other simulators due to
fast parallel rendering and low overhead in the system design
of ManiSkill3, leading to highly efficient use of the GPU.
Depending on task the simulation  rendering FPS can reach
up to 30,000, massively accelerating visual data collection
relative to most other simulators. Importantly ManiSkill3
maintains extremely low GPU memory usage, typically 2-3x
lower than that of other simulators which enables on device
visual RL and larger neural networks during training.
2) Most comprehensive range of environments with 12
different categories of environments and 20 different
robots provided out of the box: ManiSkill3 out of the
box provides a diverse set of different types of environments
including but not limited to mobile manipulation, room-scale
Fig. 1: Multiple distinct task categories are displayed, ranging from room-scale tasks to humanoid interactions and drawing
tasks. Majority of tasks shown are GPU-parallelized, simulating  rendering at state-of-the-art speeds and GPU memory
efficiency. Scenes are from ReplicaCAD and AI2-THOR.
further support 20 different robot embodiments out of the
box such as quadrupeds, floating grippers, humanoids, and
dextrous hands. Furthermore we support several sim2real
and real2sim setups for manipulation. Importantly, extensive
documentationtutorials are provided to teach users on how to
add new environmentsrobots, as well as how to make open-
source contributions to expand the repository of simulated
tasksrobots. A core focus of ManiSkill3 is not on building
a lot of environments in each category, but instead building
many templateexamples that users can then build on top of
themselves for their own use-cases.
3) Heterogeneous Simulation for Generalizable Learn-
completely different objects, articulations, even entire room-
scale scenes in each parallel environment. This is done thanks
to a data-oriented system design and easy-to-use API to
manage GPU memory of objectsarticulations even if they may
have different degrees of freedom.
4) Simple Unified API to Easily Manage and Build
GPU Simulated Tasks: ManiSkill3 distinguishes itself from
other GPU-parallelized robotics simulators and benchmarks
by offering a user-friendly API for creating diverse robotics
environments. Improvements include object-oriented APIs and
the elimination of complex tensor indexing. The platform
provides feature-rich tooling to streamline various operations,
such as domain randomization (e.g., camera poses, object
materials), trajectory replay, controller action conversion, and
5) Scalable Dataset Generation Pipeline from Few
is difficult, we provide a pipeline that leverages demonstration
to learn a generalized neural network policy from a few
teleoperatedhardcoded demonstrations. The generalized task-
specific neural network policy is then used to rollout many
more demonstrations to form larger datasets.
II. RELATED WORK
Robotics Simulation Frameworks: Isaac Lab (previously
called Isaac Orbit)  and Mujoco  are some open-source
general-purpose rigid-body GPU parallelized robotics simula-
tors. Isaac Lab and Brax  (which supports the Mujoco
MJX backend) are the most similar to ManiSkill3 in that they
provides out of the box environments for reinforcement learn-
ingimitation learning, as well as APIs to build environments.
There are robotics frameworks like Robocasa , Habitat,
, AI2THOR , OmniGibson , RLBench  that
only have CPU sim backends and thus run magnitudes slower
than Isaac LabManiSkill3, limiting researchers to often only
explore imitation learningmotion planning approaches instead
of reinforcement learningonline learning from demonstrations
methods. Isaac Lab relies on the closed source Isaac Sim
framework for GPU parallelized simulation and rendering
whereas ManiSkill3 relies on the open-source SAPIEN
for the same features. BraxMujoco uses the MJX backend
and currently does not have parallel rendering. Both Isaac
Lab and ManiSkill3 use PhysX for GPU simulation. Note
that the environments and baselines leveraging the fast parallel
rendering in ManiSkill3 is concurrent work to Isaac Lab.
Robotics Datasets: Amongst existing datasets there are
typically two kinds, real-world and simulated datasets. Open-
X  is one of the largest real-world robotics datasets but
ManiSkill3
Isaac Lab
RoboCasa
OmniGibson
Parallelized Simulation
Parallelized Rendering
Parallelized Heterogeneous Simulation
Large Scale Demonstrations
Realistic Object Physics
Photorealistic Rendering
Room-Scale Scenes
Visual RL Baselines
Vision-based sim2real setups
Trajectory replayconversion
Task Categories
Interactive GUI
TABLE I: Comparison of major features across different open-source robotics frameworkstools.
suffers from issues with inconsistent data labels and overall
poor data quality. DROID  addresses some of Open-
Xs problems by using a consistant data collection platform.
of human labor to collect data and are inherently difficult
to scale up to the sizes of typical visionlanguage datasets.
Among simulated datasets, frameworks like AI2-THOR ,
and OmniGibson  have complex room-scale scenes but
do not readily provide demonstrations or ways to generate
large-scale demonstrations for use in robot learning. Robocasa
has a myriad of tasks and realistic room-scale scenes, but
further leverages MimicGen  to scale human teleoperated
demonstrations by generating new demonstrations.
ManiSkill3 sources large-scale demonstrations through a
combination of different methods. For easier tasks, motion
planning and rewards for RL are used to generate demonstra-
tions. For more complex tasks without easily defined motion
planning scripts or reward functions, ManiSkill3 relies on
online learning from demonstrations algorithms like RLPD
and RFCL , which are more flexible compared to
MimicGen used in Robocasa as MimicGen makes a number of
assumptions about the task (end-effector action spaces, little
to no geometric variations, engineered task stage indicators).
III. CORE FEATURES OF MANISKILL3
ManiSkill3 is the most feature-rich GPU simulation frame-
work compared to popular alternatives as shown in Table I. For
the largest features, we detail them in the subsections below.
A. Diverse Tasks Supported Out Of The Box
The design of ManiSkill3 enables support for many different
kinds of task categories via a flexible task-building API. Of the
existing popular robotics simulators ManiSkill3 supports the
most categories of different tasks. Concretely we categorize
the 12 distinct categories as follows: Table top manipula-
ital twins, and soft body manipulation environments. The
majority of these tasks are GPU parallelized and can be
rendered fast in parallel as well, with examples of the tasks
shown in Fig. 1. Each of these task categories have various
optimizations done to run more accurately andor faster. Other
simulators typically support a smaller subset of the type of
tasks ManiSkill3 supports easily. Additional details on the
exact optimizationsimplementations and available robots are
detailed in Appendix VII.
B. GPU Parallelized Simulation and Rendering
ManiSkill3 distinguishes itself from its predecessors and
other robotics simulators by offering robust support for GPU-
parallelized simulation and rendering. ManiSkill3 is the first
general benchmark to enable fast RL from visual inputs on
complex robot manipulation tasks, with Isaac Lab recently
adding a similar feature. Tasks such as picking up a cube
or controlling a quadruped to reach a goal from pixel inputs
are now solved on the order of minutes instead of hours. RL
training resultsspeed are detailed in Section IV-C.
The performance results shown in Figure 4 are the results
after simulating  rendering RGB, depth, and segmentation
data simultaneously for various tasks. In terms of speed and
GPU memory use, Figure 2 shows ManiSkill3 outperforms
Isaac Lab, particularly when it comes to rendering common
real-world camera resolutions which can be important for
sim2real and real2sim transfer. In particular, with 128 parallel
environments for the benchmarked task, ManiSkill3 uses just
3.5GB of GPU memory whereas Isaac Lab uses 14.1GB.
The memory efficiency of the ManiSkill3 platform allows
for more room for e.g. RL replay buffers or larger neural
network models such as large vision language action models.
Training and inference can be kept extremely optimized on a
single GPU as a result without needing to store any data on
the CPU. From experimentation with visual RL, we find that
GPU memory efficiency becomes much more important as the
FPS gains from more parallel environments become marginal.
GPU memory efficiency is especially important for off-policy
algorithms like TD-MPC2  and SAC  that typically
maintain replay buffer sizes on the order of 105 106 frames.
For example storing RGB data from one 128x128 camera
would require at least 9GB of GPU memory for a replay
buffer of size 200, 000, which can easily lead to out of memory
Fig. 2: GPU SimulationRendering of RGB speeds of the Cartpole environment with different camera setups ManiSkill3 and
Isaac Lab. Annotated numbers indicate GPU memory usage, with no data points beyond 128 environments for Isaac Lab due
to running out of GPU memory. Note that this rendering setting mimics that of real world datasets collected in Open-X and
Droid. Speed is dependent on a few factors, primarily the number of objects, geometry complexity of each object, as well as
simulationrendering configurations which can be tuned for speed or accuracy. As a result, it is possible the numberstrends
here may not hold for every environment.
Fig. 3: Comparison of ManiSkill3 (Top row) and Isaac Lab
(Bottom row) parallel rendering 640x480 RGB and depth
image outputs of the Cartpole benchmark task.
issues. For more in-depth performance benchmarking results
and comparisons of rendered outputs, see Appendix XI.
We acknowledge that this comparison is not strictly apples-
to-apples due to differences in rendering techniques. Isaac
Lab employs ray-tracing for parallel rendering, while the
ManiSkill3 results are generated using SAPIENs rasterization
renderer (see Figure 3 for a visual comparison), although
ManiSkill3 also supports a ray-tracing mode without paral-
lelization. Ray-tracing generally offers greater flexibility in
balancing rendering speed and quality through the adjustment
of parameters such as samples per pixel. Its worth noting that
the Isaac Lab data presented here uses the fastest rendering
rendering quality that may be helpful for sim2real. Despite
the use of different rendering techniques, we believe this
experiment provides a meaningful basis for comparison.
GPU parallelized simulation and rendering enable an en-
tirely new regime of running efficient domain randomiza-
tions. For example you can now quickly render over a 1000
different cameras, each with different extrinsicsintrinsics,
Fig. 4: GPU SimulationRendering speeds of various tasks
with a single 128x128 resolution camera with a simulation
frequency of 120 and control frequency of 60, meaning the
camera renders ever 2 sim steps. RGB, depth, and segmenta-
tion data are all simultaneously being rendered. The only big
variations between environments of the three curves are the
objects and robots being simulated.
of the parallel environments. A subset of 4 out of 1024
environments renders are shown in Figure 5 with different
settings. This type of visual diversity in simulation enables
much faster training of more visually robust policies and
is critical for sim2real applications. Furthermore, ManiSkill3
supports parallelized rendering of voxel and pointcloud for-
mats necessary for 3D robot learning approaches [53, 24, 45].
twins. For example, ManiSkill3 implements 4 of the environ-
ments in SIMPLER  which are evaluation digital twins that
enable the evaluation of generalist robotic policies trained on
real-world data like Octo  and Robotics Transformers (RT)
. ManiSkill3 digital twins can evaluate models like Octo
at 60x to 100x the speed of the real world without human
twin implementations in SIMPLER. The speed increase is due
to fast and efficient parallel rendering of large camera resolu-
tions (640x480) and flexible GPU parallelized controllers to
Fig. 5: Parallel rendering outputs of 1024 parallel environ-
ments for the StackCube and PushT tasks with a subset of
4 them visualized here. Original renders are size 128x128,
images shown are up-scaled for clarity. Top-row shows camera
pose randomization and bottom row shows texture randomiza-
match most real-world robot armsmanipulators. More details
on sim2realreal2sim support are covered in Section III-E.
C. Heterogeneous GPU Simulation
Fig. 6: Example tasks in ManiSkill3 showing heterogeneous
GPU simulation with different DoF articulations andor dif-
ferent numbers of objects being simulated in each parallel
environment.
ManiSkill3 is so far the only simulation framework that
completely supports heterogeneous GPU simulation. This is
the feature of being able to simulate different object geome-
with different DOFs across parallel environments. For exam-
and sample a random drawer link that needs to be opened to
succeed. In the Pick Clutter YCB task, we sample a different
number of YCB objects in each parallel environment and
sample one random object out of the clutter as the goal object
to pick. ManiSkill3 easily supports this kind of simulation and
further supports rendering these different scenes in parallel
all at once with an example 3rd view rendering illustrated
in Figure 6. Heterogeneous GPU simulation enables more
generalizable learning for manipulation as algorithms like PPO
can simultaneously train on every single object from the YCB
dataset  or the PartNetMobility dataset of cabinets .
D. Teleoperation
We provide a Virtual Reality (VR) Teleoperation system that
is seamlessly integrated into ManiSkill3 with immersive visual
feedback and supports wired connections. ManiSkill3 receives
real-time hand pose data from the teleoperator that is translated
into corresponding robot actions. At the same time, the system
streams 4K stereo video via Air Light VR (ALVR) to the VR
device at 60 Hz, ensuring a smooth and immersive all-scene
view experience. This allows the user to explore the entire
environment freely, making it possible to teleoperate long-
horizon tasks as well as more precise tasks by simply looking
closer in the simulation. A crucial control feature is where
your hand is, where the end-effector is, which tightly aligns
the operators hand movement with the robots end-effector.
Prior work provide ways to stream video feedback over the
internet via a hardware agnostic web app [11, 15] but tradeoff
some ease-of-use for better accessibility (such as long-range
remote teleoperation). Our integrated system enables wider
viewports and reduce the need for teleoperators to compensate
for hand-robot misalignment and visual disparities, visualized
in 7. Our VR teleop system is also compatible with real-world
teleoperation. For real-world operations, we use one or more
depth cameras to generate a point cloud of the scene. The point
cloud is then rendered and streamed as stereo RGB video to
the operator in real time, allowing for effective and immersive
control. Example code for configuring this system is provided
for teleoperating robot arms or floating robots with multiple
grippersfingers. See Appendix XII for more details on this
teleoperation system.
Fig. 7: Visualization of VR teleoperation system. Left: A
teleoperator using hand poses captured by the Meta Quest 3
headset to control robot motion in real-time. Middle: A 360-
degree scene displayed in the VR device, providing immersive
sensory feedback. Right: Trajectory replay.
E. Sim2Real and Real2Sim for Robot Manipulation
Towards the goal of robust real-world robotics beyond
using ManiSkill3 via digital twins on some tasks. Figure 8
showcases several digital twins supported with real world
counterparts. For the cube picking task, we train with RL
on simulation images and directly deploy to the real world,
achieving a real world success rate of 91.6 averaged across
3 RL training runs. See Section IV-D for more details on
rigid body manipulation sim2real. For the vision-tactile peg
insertion task, we simulate the tactile sensor made of silicone
as a softbody and refer readers to the results showcased in the
original work by  for those environments, which achieved a
Fig. 8: Three different kinds of digital twins in ManiSkill3.
Top row shows the real-world setup and bottom row shows
the digital twin. Left: Domain randomized digital twin of a
cube picking task. Middle: Digital twin of the vision-tactile
simulation of a key insertion task. Right: Real2sim digital twin
of a spoon placing task.
95.08 success rate in the real world. Finally, for the real2sim
digital twins we evaluate Octo and RT-1X on the ManiSkill3
GPU parallelized version of 4 tasks in SIMPLER . We
achieve a correlation between real-world success rates and
simulation success rates of 0.9284 and a Mean Maximum
Rank Violation (MMRV) value 0.0147, which is close to the
original values reported in SIMPLER. See Appendix VII-K
for more details on real2sim evaluations.
F. Simple Unified API for Building GPU Parallelized Robotics
A core reason behind the flexibility of the ManiSkill3
system to support so many different distinct task categories
is the clean and simple API for task-building. The simple API
also enables users to easily build and customize their own
robotics tasks without having to worry about complex GPU
memory management details or desigining robot controllers. In
addition to the API design extensive tutorials are provided for
customizing tasks. We describe two general features provided
by ManiSkill3 below:
1) Object-Oriented API for Articulations, Links, Joints, and
object-oriented API around the high-level articulationsactors
down to individual linksjoints and meshes. In contrast, Isaac-
GymEnvs requires users to instantiates relevant GPU buffers
for holding articulation state such as root pose and joint angles.
Isaac Lab improves on this with a partially object-oriented
articulation API that allows one to create an articulation object
(e.g., for a cabinet). However, one still has to often play around
with index values to get the relevant articulation data they
need. We use a cabinet opening task as a case study. In a
cabinet drawer opening task, to write good reward functions
you need to access the drawer links handle meshs pose, as
well as the joint angle between the drawer and the cabinet. A
visual comparison of the 3 APIs (simplified from the actual
code) is shown in Figure 9.
oriented and stored as batched Pose objects, enabling an easy
to read, method chaining pattern of programming for working
with poses. For the sake of an example, suppose that we have 2
poses P1, P2 and want to compute (P1P2)1P 1
, ManiSkill3
provides a much simpler and method chainable API to do this
compared to Isaac Lab, as shown in Figure 10.
2) Robots and Controllers:
ManiSkill3 supports both
URDF and Mujoco MJCF definition formats natively and
builds articulated robots based on the URDFMJCF directly.
For each robot, ManiSkill3 further provides a number of pre-
built configurable controller options for both GPU parallelized
joint position control and inverse-kinematic (IK) control, mod-
ified from ManiSkill2 for GPU simulation. ManiSkill3 builds
upon the PyTorch Kinematics package  to support inverse-
kinematic based controllers parallelized on the GPU, typically
used to control robot arm end-effectors. These options are
easily configured at runtime with either preset configurations
or user-supplied controller configurations. Currently, there are
20 different robots supported out of the box in ManiSkill3, a
subset of which are visualized in Figure 14 in the Appendix.
ples of how to tune and optimize robots for fast simulation,
which has often proven a stumbling block for those new to
robot simulation importing complex robots for the first time.
G. Demonstration Datasets
We leverage a variety of approaches to collectgenerate our
demonstration datasets infinitely at scale. For the simplest
based solutions to generate demonstration data. Some tasks
with easy-to-define reward functions have dense reward func-
tions defined and converged RL policies are used to gener-
ate demonstration data. For more difficult tasks, we collect
demonstration data (typically about 10 demonstrations) via
teleoperation tools. Then, we use RFCL  or RLPD
to run fast online imitation learning and generate data from
converged policies.
We further adapt the trajectory replay tool from ManiSkill2
to work with both CPU and GPU simulated demonstration
data. The replay tool enables users to change the observa-
tions stored (e.g., state or rgbd, and allow modifying the
rendering shaders used) as well as modifying the rewards
stored (dense or sparse). In tasks involving the Franka robot
type to another (e.g. joint position control to delta end ef-
fector pose control). Importantly we support collecting data
in CPUGPU simulation and replaying them in CPUGPU
simulation with different numbers of parallel environments via
explicit control over randomizationRNG seeding at the per-
parallel-environment level. This enables flexibility in trajectory
replay as data collected on one machine with more GPU
memory can be replayed on other machines with less GPU
memory that cannot use as many parallel environments.
Fig. 9: Code comparison for computing a grasp position on a cabinet handle and the joint angle of the cabinet drawer in 3
different GPU simulation frameworks.
Fig. 10: Code comparison for manipulating batched poses
IV. BASELINES AND RESULTS
ManiSkill3 provides several popular robot learning base-
lines as well as simple reproducible setups for end-to-end
trainable vision-based sim2real policies.
A. Reinforcement Learning
We provide two categories of RL baselines as follows:
Wall-time Efficient Reinforcement Learning: We include
a torch based vectorized implementation of model-free RL
algorithms PPO and SAC , as well as the state-of-the-
art model-based RL algorithm TD-MPC2 . Configurations
for baselines are tuned to minimize training wall time with
no regard to sample efficiency. Code for PPO and SAC is
implemented based on CleanRL  and leverages the torch
compile and cudagraphs acceleration features introduced by
Sample Efficient Reinforcement Learning: All of the RL
baselines in the wall-time efficient setting besides PPO are
included here with configurations tuned towards more gradient
updates and fewer environment steps to maximize sample
efficiency.
We share results on training speed in Section IV-C. More
in-depth details on RL setups and more training results are
shared in Appendix IX.
B. Learning From Demonstrations (LfD)  Imitation Learning
We provide two categories of LfD baselines as follows:
Offline Imitation Learning: We currently provide Behavior
, and PerACT  as baselines. We also support evaluat-
ing (but not training) several vision-language action (VLA)
We leave to future work to support training VLA models
on simulation data as RDT-1B has done with the previous
ManiSkill2 datasets.
Online Imitation Learning: Online imitation learning gen-
erally refers to algorithms that learn from demonstrations
in addition to collecting online environment transitions. We
currently provide the two state-of-the-art baselines: Reinforce-
ment Learning from Prior Data (RLPD) , and Reverse
Forward Curriculum Learning (RFCL) . Note that RFCL
leverages simulation state resets.
Mandlekar et al.  show that imitation learning algorithm
performance heavily depends on how the demonstrations were
when trained on motion planning or human teleoperated data,
but can perform very well if trained on data generated by
a deterministic neural network. With this caveat in mind,
we explicitly track in all our LfD baselines the number of
demonstrations used (an integer), what type of demonstrations
are used (RL generated, motion planning, or human), and
where the demonstrations are sourced from exactly (a longer
description e.g. neural net trained via TD-MPC2, teleopera-
tion via Meta-Quest VR). Imitation learning results on some
environments are shared in Appendix X.
C. RL Training Speed
We run experiments using PPO  on the ManiSkill3 GPU
simulation and the ManiSkill2 CPU simulation. ManiSkill2
was previously the fastest robotics simulationrendering
framework until ManiSkill3. The experiments were run on an
RTX-4090 GPU on the PickCube task, where a Franka robot
arm must grasp a randomly initialized cube and hold it still at
a random goal location. For the vision-based task no ground
truth data like cube pose is provided and the RL policy must
solve from proprioceptive information and one 128x128 RGB
image rendered by the environments 3rd-person camera. RL
hyperparameters are tuned to achieve the fastest training time
in both settings. Results in Figure 11 show that state and vision
based training are massively accelerated with GPU simulation
and rendering. PickCube with delta joint position control
from state-based observations in GPU simulation reaches near
100 success rate after about 1 minute of training, a 15x
speed up relative to ManiSkill2. From RGB observations with
parallel rendering PickCube is solved after about 10 minutes
of training, a 8x speed up relative to ManiSkill2. For results
and details of RL on more environments see Appendix IX.
D. Vision-Based Sim2Real Manipulation
To showcase the use-case of heterogeneous simulation and
parallel rendering, we provide a simple reproducible setup for
end-to-end training a vision-based manipulation policy that
deploys successfully zero-shot. The setup uses the low-cost
300 Koch robot arm and a phone camera for third-person
RGB observations, leveraging the open-source Hugging Face
LeRobot library  for the hardware and setup to enhance
reproducibility and accessibility. This setup is not limited to
Training Time (s)
Success Rate
PickCube State-Based Performance
Training Time (s)
Success Rate
PickCube RGB-Based Performance
PPO (GPU Sim)
PPO (CPU Sim)
Fig. 11: Wall-clock training time of PPO on GPUCPU sim-
ulation showing the average success rate over time across 5
seeds. Shaded areas correspond to the 95 confidence interval.
Fig. 12: Green screening visualized with Koch-v1.1 robot.
overlaid on real background. Middle: Sim RGB observations
overlaid on real RGB observations for alignment visualization.
the Koch arm; other more expensive robotic arms can be used
and likely perform better due to more precise hardware.
During simulation training and real-world evaluation, obser-
vations are restricted to RGB inputs and robot joint positions;
no demonstrations or privileged state information such as cube
pose is used, and the robot is controlled via continuous joint
level input. We note however that future work combining
simulation training with teleoperated demonstrations could
improve performance and training time. To match the real
background within simulation and reduce the visual sim2real
gap we green screen the real-world background. We skip
green-screening over the dynamic objects like the cube and
robot arm via the environments segmentation map, see Figure
12 for an example. To enhance generalization and accommo-
date variations in real-world setups, we domain randomize a
number of elements of the simulation. Task agnostic domain
randomizations include camera pose, lighting direction, and
the robot pose. Specific to the cube picking task we further
domain randomize the cube size, color, and friction.
In this demonstration we picked a random table in a
house and set up the green-screening and roughly aligned the
simulation camera pose with the real-world camera pose. Then
we train on the cube picking environment with PPO using a
simple Nature CNN backbone for image feature processing for
15 million samples using 256 parallel environments, taking
approximately 1 hour on a RTX 4090 GPU. The trained
policy is then zero-shot deployed on the real robot, using
the same controller the policy was trained on in simulation,
namely a delta target joint position controller. Evaluating the
final checkpoint across 3 training runs in the real-world 8
times each yielded a 2224  91.6 success rate. Real
world evaluations test on cubes of varying sizes, colors, and
start poses, demonstrating our vision-based sim2real setup is
capable of learning robust manipulation policies. Evaluation
videos can be found on our demo gallery page.
We further evaluated each intermediate checkpoint from the
3 RL training runs and plot the simulation and real-world
success rates in Figure 13. The figure shows a good correlation
between real-world and simulated success rates, indicating that
some sim2real digital twins built with ManiSkill can be fairly
accurate and reflect the real-world. Interestingly, we observe
that despite not modelling shadows or the exposed wiring
of the robot arm accurately, a successful sim2real policy is
still capable of being trained. See Appendix VIII for more
details on the exact setup, domain randomizations, controller
Environment Steps (106)
Success Rate
Grasp Cube
Environment Steps (106)
Success Rate
Simulation
Fig. 13: Koch pick-cube sim and real success rates on the
grasp cube subtask as well as the full success consisting of
training checkpoints for each of the 3 seeds are evaluated on
both sim and real 8 times, using a variety of cube colors and
sizes. Success rates are averaged across trials and shaded areas
represent 95 confidence intervals.
V. LIMITATIONS
While parallel rendering can enable some new and more
accessible approaches to sim2real and visual RL, it has its
limits. For complex environments with many geometries (e.g.
room-scale scenes) we can GPU parallel render and simulate
them but at a lower number of environments on one GPU.
While it is now possible to do some more generalizable
zero-shot RGB based sim2real with a relatively simple setup,
some reward engineering was required to encourage safer and
more robust grasp behaviors of RL trained policy. Moreover,
this is far from solving sim2real completely, but more of open-
ing a new avenue of potential approaches based on fast visual
RL in simulation. The sim2real demo is also limited to using
just static cameras and it can be interesting future work to
explore how to use methods like Gaussian Splatting for novel-
view rendering to support mounted cameras. One could also
explore algorithnmic changes andor domain randomizations
that can help remove the need for green screening.
Not all types of environments in ManiSkill are readily GPU
parallelized in the sense that there are a batch of parallel
environments. Most GPU parallelized environments are rigid-
body based. The soft body environments are not batched as
they use a significant portion of the GPU to simulate just a
single environment fast. The vision-tactile simulator that has
been tuned heavily for sim2real transfer uses a different set of
algorithms for physics simulation compared to the majority of
rigid-body only environments.
VI. CONCLUSION
ManiSkill3
introduces
state-of-the-art
workbenchmark
generalizable
robotics
simulation
and rendering. ManiSkill3 uses less GPU memory and
depending on scenario can run faster while also supporting
heterogeneous GPU simulation. Additionally we support the
most diverse range of robotics tasks compared to alternative
simulators. Importantly, we reliably support both sim2real and
real2sim environments in manipulation tasks with real-world
reproducible results. Moreover, we provide a immersive
VR teleoperation system option for users to collect data
for their own research. Furthermore, ManiSkill3 provides
an easy-to-use object-oriented API for building all kinds
of GPU simulated tasks, democratizing access to scalable
robot learning. Finally, demonstrations and RLIL baselines
with clearly defined metrics are open sourced for users
to use. We believe that our comprehensive approach to
building the open-osurce ManiSkill3 will encourage the
research community to tackle manipulation challenges more
extensively through compute-scalable simulation.
ACKNOWLEDGMENTS
The authors sincerely thank members of the Hao Su lab and
Hillbot for great discussion and feedback, including but not
limited to Nicklas Hansen, Rokas Bendikas, Jiayuan Gu, James
Hou. The authors also thank members of the open source
community for bug finding, contributions, and feedback on
this framework, including but not limited to Alexandre Brown,
Gaotian Wang, Dwait Bhatt, Jeremy Morgan. We especially
acknowledge the generous support from Qualcomm Embodied
AI Fund and Hillbot Inc. Stone Tao is supported in part by
the NSF Graduate Research Fellowship Program grant under
grant No. DGE2038238.
REFERENCES
Kshiteej Kalambarkar, Laurent Kirsch, Michael La-
hai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian
Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gre-
gory Chanan, Peng Wu, and Soumith Chintala. PyTorch
Bytecode Transformation and Graph Compilation.
29th ACM International Conference on Architectural
Support for Programming Languages and Operating
assetspytorch2-2.pdf.
Philip J. Ball, Laura M. Smith, Ilya Kostrikov, and
Sergey Levine. Efficient online reinforcement learning
with offline data.
In Andreas Krause, Emma Brun-
and Jonathan Scarlett, editors, International Conference
on Machine Learning, ICML 2023, 23-29 July 2023,
Machine Learning Research, pages 15771594. PMLR,
Anthony Brohan, Noah Brown, Justice Carbajal, Yev-
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
mine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas
Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-
Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deek-
sha Manjunath, Igor Mordatch, Ofir Nachum, Carolina
nell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia
Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T.
Brianna Zitkovich. RT-1: robotics transformer for real-
world control at scale.
Remi Cadene, Simon Alibert, Alexander Soare, Quentin
State-of-the-art machine learning for real-world robotics
in pytorch.  2024.
Berk C alli, Aaron Walsman, Arjun Singh, Siddhartha S.
marking in manipulation research: The YCB object
and model set and benchmarking protocols.
Justin Carpentier, Florian Valenza, Nicolas Mansard,
namics for poly-articulated systems.
tasks.github.iopinocchio, 20152021.
Justin Carpentier, Guilhem Saurel, Gabriele Buondonno,
Joseph Mirabel, Florent Lamiraux, Olivier Stasse, and
Nicolas Mansard.
The pinocchio c library  a fast
and flexible implementation of rigid body dynamics
algorithms and their analytical derivatives.
International Symposium on System Integrations (SII),
Linghao Chen, Yuzhe Qin, Xiaowei Zhou, and Hao Su.
via differentiable rendering and space exploration. IEEE
Robotics and Automation Letters, 2023.
Weihang Chen, Jing Xu, Fanbo Xiang, Xiaodi Yuan, Hao
for learning contact-rich manipulation with marker-based
visuotactile sensors. IEEE Transactions on Robotics, 40:
Yuanpei Chen, Yaodong Yang, Tianhao Wu, Shengjie
Stephen Marcus McAleer, Hao Dong, and Song-Chun
Towards human-level bimanual dexterous ma-
nipulation with reinforcement learning. In Thirty-sixth
Conference on Neural Information Processing Systems
Datasets and Benchmarks Track, 2022.
URL https:
openreview.netforum?idD29JbExncTP.
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback.
arXiv preprint
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback.
arXiv preprint
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
fusion policy: Visuomotor policy learning via action
diffusion. In Kostas E. Bekris, Kris Hauser, Sylvia L.
Systems XIX, Daegu, Republic of Korea, July 10-14,
Open X-Embodiment Collaboration, Abby ONeill, Ab-
dul Rehman, Abhiram Maddukuri, Abhishek Gupta, Ab-
hishek Padalkar, Abraham Lee, Acorn Pooley, Agrim
chit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakr-
Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen
Christine Chan, Christopher Agia, Chuer Pan, Chuyuan
eter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov,
Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen
Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi,
Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi
Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan
Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu,
Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan
Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan
Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle
Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Li-
onel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion
Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mo-
hit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas
Pannag R Sanketi, Patrick Tree Miller, Patrick Yin,
Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mi-
Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria
Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey
Siddhant Haldar, Siddharth Karamcheti, Simeon Ade-
Stefan Welker, Stephen Tian, Subramanian Ramamoor-
Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas
Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan
aolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan
Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan
Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Young-
woon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin
shuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo,
Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang,
Zipeng Fu, and Zipeng Lin.
Open X-Embodiment:
Robotic learning datasets and RT-X models.
arxiv.orgabs2310.08864, 2023.
Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia,
Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong
Wang. Bunny-visionpro: Real-time bimanual dexterous
teleoperation for imitation learning.
arXiv preprint
Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chan-
dramouli Rajagopalan, and Xiaolong Wang. Finetuning
offline world models in the real world. In Proceedings
of the 7th Conference on Robot Learning (CoRL), 2023.
C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan
differentiable physics engine for large scale rigid body
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein,
and Chelsea Finn. Humanplus: Humanoid shadowing and
imitation from humans. CoRR, abs2406.10454, 2024.
Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling,
Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao,
Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie,
Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: A uni-
fied benchmark for generalizable manipulation skills. In
International Conference on Learning Representations,
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine.
Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic
In Jennifer G. Dy and Andreas Krause, edi-
on Machine Learning, ICML 2018, Stockholmsmassan,
Proceedings of Machine Learning Research, pages 1856
v80haarnoja18b.html.
Ankur Handa, Arthur Allshire, Viktor Makoviychuk,
Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys
Balakumar Sundaralingam, and Yashraj S. Narang. Dex-
ulation to reality.
In IEEE International Conference
on Robotics and Automation, ICRA 2023, London, UK,
May 29 - June 2, 2023, pages 59775984. IEEE, 2023.
URL https:
Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2:
In International Conference on Learning Representations
(ICLR), 2024.
Shengyi Huang, Rousslan Fernand Julien Dossa, Chang
Joao G.M. Araujo. Cleanrl: High-quality single-file im-
plementations of deep reinforcement learning algorithms.
Journal of Machine Learning Research, 23(274):118,
2022. URL
Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu
3d value maps for robotic manipulation with language
models. arXiv preprint arXiv:2307.05973, 2023.
Stephen James, Zicong Ma, David Rovick Arrojo, and
Andrew J. Davison. Rlbench: The robot learning bench-
mark  learning environment.
IEEE Robotics and
Automation Letters, 2020.
Yunfan Jiang, Chen Wang, Ruohan Zhang, Jiajun Wu,
and Li Fei-Fei. Transic: Sim-to-real policy transfer by
learning from online correction. In Conference on Robot
Mukul Khanna, Yongsen Mao, Hanxiao Jiang, San-
jay Haresh, Brennan Shacklett, Dhruv Batra, Alexander
Savva. Habitat Synthetic Scenes Dataset (HSSD-200):
An Analysis of 3D Scene Scale and Realism Tradeoffs
for ObjectGoal Navigation. arXiv preprint, 2023.
Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ash-
win Balakrishna, Sudeep Dasari, Siddharth Karam-
Lawrence Yunliang Chen, Kirsty Ellis, Peter David
Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu,
Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abra-
ham Lee, Youngwoon Lee, Marius Memmel, Sungjae
Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan
nag R Sanketi, Archit Sharma, Cody Simpson, Quan
Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao,
Christopher Agia, Rohan Baijal, Mateo Guaman Cas-
Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario
Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Ji-
tendra Malik, Roberto Martn-Martn, Subramanian Ra-
Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey
wild robot manipulation dataset. 2024.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vander-
Interactive 3D Environment for Visual AI. arXiv, 2017.
Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gok-
Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-
Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-
vio Savarese, Hyowon Gweon, Karen Liu, Jiajun Wu, and
Li Fei-Fei. BEHAVIOR-1k: A benchmark for embodied
AI with 1,000 everyday activities and realistic simulation.
In 6th Annual Conference on Robot Learning, 2022. URL
8DoIe8G3t.
Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier
Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu,
Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao.
Evaluating real-world robot manipulation policies in sim-
ulation. arXiv preprint arXiv:2405.05941, 2024.
Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan,
Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun
Zhu. Rdt-1b: a diffusion foundation model for bimanual
manipulation. arXiv preprint arXiv:2410.07864, 2024.
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong
and Gavriel State. Isaac gym: High performance GPU
based physics simulation for robot learning. In Joaquin
Vanschoren and Sai-Kit Yeung, editors, Proceedings
of the Neural Information Processing Systems Track
on Datasets and Benchmarks 1, NeurIPS Datasets
Benchmarks
December
neurips.ccpaper2021hash
28dd2c7955ce926456240b2ff0100bde-Abstract-round2.
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn.
What matters in learning from offline human demon-
strations for robot manipulation. In Aleksandra Faust,
David Hsu, and Gerhard Neumann, editors, Conference
on Robot Learning, 8-11 November 2021, London, UK,
volume 164 of Proceedings of Machine Learning Re-
URL https:
proceedings.mlr.pressv164mandlekar22a.html.
Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Ireti-
ayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and
Dieter Fox.
scalable robot learning using human demonstrations. In
7th Annual Conference on Robot Learning, 2023.
Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu,
Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh,
Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck
robot learning environments.
IEEE Robotics and Au-
tomation Letters, 8(6):37403747, 2023. doi: 10.1109
Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna
large-scale benchmark for fine-grained and hierarchical
part-level 3D object understanding. In The IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera
and Hao Su. Maniskill: Generalizable manipulation skill
benchmark with large-scale demonstrations. In Thirty-
fifth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 2), 2021.
Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang,
Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Man-
of everyday tasks for generalist robots.
In Robotics:
Science and Systems (RSS), 2024.
Octo Model Team, Dibya Ghosh, Homer Walke, Karl
Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi,
Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and
Sergey Levine. Octo: An open-source generalist robot
policy. In Proceedings of Robotics: Science and Systems,
Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk,
Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dieter
robot arm-hand teleoperation system.
arXiv preprint
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi
plementations. Journal of Machine Learning Research,
20-1364.html.
Nikita Rudin, David Hoeller, Philipp Reist, and Marco
Hutter. Learning to walk in minutes using massively par-
allel deep reinforcement learning. In Aleksandra Faust,
David Hsu, and Gerhard Neumann, editors, Confer-
ence on Robot Learning, 8-11 November 2021, London,
URL https:
proceedings.mlr.pressv164rudin22a.html.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
algorithms.
URL http:
arxiv.orgabs1707.06347.
nipulation.
In Proceedings of the 6th Conference on
Robot Learning (CoRL), 2022.
Andrew Szot, Alex Clegg, Eric Undersander, Erik Wij-
Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur,
Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt
Dhruv Batra. Habitat 2.0: Training home assistants to re-
arrange their habitat. In Advances in Neural Information
Processing Systems (NeurIPS), 2021.
Stone Tao, Arth Shukla, Tse-kai Chan, and Hao Su.
Reverse forward curriculum learning for extreme sample
and demo efficiency. In The Twelfth International Confer-
ence on Learning Representations, ICLR 2024, Vienna,
J Terry, Benjamin Black, Nathaniel Grammel, Mario
Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-
forcement learning.
Advances in Neural Information
Processing Systems, 34:1503215043, 2021.
Emanuel Todorov, Tom Erez, and Yuval Tassa.
2012 IEEERSJ International Conference on Intelligent
Robots and Systems, pages 50265033. IEEE, 2012. doi:
Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron,
Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Tim-
othy P. Lillicrap, Nicolas Heess, and Yuval Tassa.
dm control: Software and tasks for continuous control.
Softw. Impacts, 6:100022, 2020. doi: 10.1016J.SIMPA.
Xinyue Wei, Minghua Liu, Zhan Ling, and Hao Su.
Approximate convex decomposition for 3d meshes with
collision-aware concavity and tree search. ACM Trans-
actions on Graphics (TOG), 41(4):118, 2022.
Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao
interactive environment.
In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June
Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu,
Muhan Wang, and Huazhe Xu.
3d diffusion policy:
Generalizable visuomotor policy learning via simple 3d
representations. In Proceedings of Robotics: Science and
Systems (RSS), 2024.
Tony Z. Zhao, Jonathan Tompson, Danny Driess, Pete
recipe for robot dexterity.
In 8th Annual Conference
on Robot Learning, 2024. URL
forum?idgvdXE7ikHI.
Sheng Zhong, Thomas Power, Ashwin Gupta, and Peter
Mitrano. PyTorch Kinematics, February 2024.
Appendix Table of Contents
Environments and Robots VII
Reinforcement Learning Baselines IX
Imitation Learning Baselines X
Simulation and Rendering Benchmarking XI
VR Teleoperation XII
VII. ENVIRONMENTS AND ROBOTS LIST
Fig. 14: A sample of 16 of the robots supported in ManiSkill3
on both CPU and GPU simulation.
This section covers key implementation and optimization
details of the general supported task categories. For a complete
list with videos of almost every task, see ManiSkill3 Tasks
Documentation. For videos of soft body and vision-tactile
tasks we refer readers to the ManiSkill2 paper [] and A
sample of 16 of the supported robots, which include mobile
in Figure 14. For a complete list with detailspictures of nearly
every robot, see ManiSkill3 Robots Documentation.
We emphasize here that while ManiSkill3 may not have the
most distinct number of tasks compared to some benchmarks,
the core contribution is supporting a diverse array of possible
tasks with open-sourced code that users can reference and use
to easily build more tasks.
A. Table Top Manipulation
Table-top manipulation is primarily related to controlling
one or more robot arms to manipulate an object on a table.
Robots like Franka Emika Panda and Universal Robots 5
fall under this category. Typical tasks may include picking
up objects, inserting a peg, assembling a structure, pushing
Fig. 15: Example table-top manipulation task showing the start
state and the solved state. The robot arm must grasp the white-
orange peg and insert it into the box.
Implementation Details: All robot arms are modified to
have certain impossible self-collisions disabled and some have
their collision meshes modified for faster simulation.
B. Mobile Manipulation
Fig. 16: Example mobile manipulation task showing the start
state and the solved state. The mobile robot arm must grasp
the handle indicated by the green sphere and pull it open.
Mobile manipulation here refers to tasks in which a robot
arm has a mobile base. Robots like Fetch and Stretch fall into
this category. Typical tasks may include placing objects on
off the ground, etc. Example task is shown in Figure 16.
Implementation Details: The default robot supported is
Fetch. The mobile base in particular is not simulated by driving
the wheels, and is modeled similarly to AI2-THOR
and Habitat  with one joint controlling forwardbackward
movement and another controlling rotation of the base. The
Fetch robot definition and collision meshes have further been
tuned to be simpler for faster simulation. Several impossible
self-collisions between some links have been explicitly ignored
to speed up simulation.
C. Room Scale Environments
ManiSkill3 provides out-of-the-box code to build the Repli-
caCAD environment from Habitat , all AI2-THOR envi-
ronments  using assets compiled by the authors of the
Habitat Synthetic Scenes Dataset , and the RoboCasa
scenes dataset . Photo-realism is also possible by turning
on the ray-tracing shader options when creating the environ-
AI2-THOR  and Habitat  have long-horizon mobile
manipulation tasks but rely on slow CPU simulation  slow
rendering systems, and do not support contact-rich physics
for manipulation (only magical grasp) and do not look photo-
realistic. Robocasa  has contact-rich long-horizon mobile
manipulation tasks in photorealistic room-scale environments.
around 25FPS as it does not use GPU parallelized simulation
and rendering. In contrast ManiSkill3 can simulate the com-
plex ReplicaCAD environment up towards 2000 FPS with
rendering.
Implementation Details: We further make several modifi-
cations to ReplicaCAD to make it completely interactive as
some of the collision meshes for articulations were modelled
incorrectly and thus did not support low-level grasping. Via
CoACD  we run convex decomposition on objects in AI2-
THOR scenes to generate simulatable non-convex collision
meshes so those objects can e.g. be grasped and moved
around correctly. Via manual annotation by ManiSkill3 au-
to be kinematic so they cannot be moved around (e.g. tables,
be fully simulated (e.g. apples, baseball bat, cups) to optimize
simulation speed.
D. Locomotion
Fig. 17: Example locomotion task showing the start state and
the solved state. The quadruped must use its 4 legs to to move
to the green goal.
Locomotion here refers to controlling robot joints to move
a robot from one location to another. Quadrupeds such as
AnyMAL-C and humanoids such as Unitree-H1 fall into this
category. Example task is shown in Figure 17.
Implementation Details: Similar to Isaac Lab, quadruped
robots in locomotion tasks are modified such that the large
majority of collision meshes are removed, leaving behind
just the feet, ankles, and the body visualized in Figure 18.
Moreover following Isaac Lab, joint limits are significantly
constrained such that random actions do not easily cause the
robot to fall over.
Fig. 18: Comparison of the visual and collision mesh of one
of the robot quadruped models, AnyMAL-C.
E. Humanoid  Bi-manual Manipulation
Fig. 19: Example humanoid manipulation task showing the
start state and the solved state. The humanoid must use both
arms to grasp the box and transport it to the other table.
Tasks here refer to the use of a humanoid embodiment such
as the Unitree H1 robot or bi-manual robot embodiments for
manipulation tasks. Example task shown in 19.
This type of task can be found in benchmarks like RoboCasa
and BiGym, although we note that RoboCasa and BiGym
do not reliably support working RL setups and are not GPU
parallelized. Isaac Lab currently does not have these types of
tasks out of the box.
Implementation Details: For some tasks where the robot
has legs, to simplify the task the legs are fixed in place so
that robot learning methods can focus on manipulation only
and train faster. Tasks still have the option to swap a version
of the robot where all joints are controlled although they are
much harder.
F. Multi-Agent Robots
Multi-Agent robots refer to support for controlling multiple
different robots in the same simulation to perform a task.
Setups such as multiple quadrupeds or robot arms fall into this
category. A common task is the handover of objects. Example
task shown in Figure 20.
This type of task can be found in Robosuite. Isaac Lab
supports this type of task, but not out of the box. Past envi-
ronments based on older versions of Isaac have example tasks
with multiple robot armshands running on GPU simulation
Fig. 20: Example multi-agent robotics task showing the start
state and solved state. The left robot must push a cube over
to the other robot to pick up.
Implementation Details: By default the action space is a
dictionary action space in multi-agent environments with a
dictionary key for each controllable agent, which follows the
standard PettingZoo API . PettingZoo is currently the most
popular interface for multi-agent RL environments. For users
who do not wish to do multi-agent RL they can flatten the
action space into a single vector if necessary via a environment
wrapper provided by ManiSkill3.
G. DrawingCleaning
Fig. 21: Example drawingcleaning task showing the start state
and solved state. The robot must draw a given SVG file (which
describes a set of lines), in this case the letter A.
DrawingCleaning
dynamically
addingremoving objects to simulate the effect of drawing
or cleaning. A task could be to draw the outline of a shape
on a canvas or clean dirty spots on a table surface. Example
task shown in Figure 21.
ManiSkill3 is the only framework that supports this kind of
task out of hte box with GPU parallelization and rendering.
Implementation Details: The drawingcleaning effect is
achieved by building ahead of time 1000s of small thin cylin-
ders that represent ink or dirty spots. For a drawing task,
all of these cylinders are hidden away from the camera view.
When a robot moves a drawing tool close to a surfacecanvas,
the cylinders have their pose set to be on top of the surface
right under where the drawing tool is. For a cleaning task,
all the cylindersdirty spots are visible and removed once
the cleaning tool moves over the dirty spot. Currently the
drawingcleaning environments in ManiSkill3 do not require
intricate grasping (nor is it the focus) of the drawingcleaning
H. Dextrous Manipulation
Fig. 22: Example dextrous manipulation robotics task showing
the start state and solved state. The robot hand must rotate the
object in place to a desired orientation.
Dextrous Manipulation refers to tasks often involving multi-
fingered hands and denserich contacts occurring during ma-
nipulation. An example task is in-hand rotation. Example task
shown in Figure 22.
This type of task has been heavily explored and exists in
Isaac Lab and ManiSkill3 with GPU parallelization.
Implementation Details: Not many special optimizations
are made here. Tactile sensing is further provided in environ-
ments via touch sensors placed on the dextrous hand at various
I. Vision Tactile Manipulation
Fig. 23: Example task of a key insertion task in simulation
(top left), the real world equivalent (top right), and plots of
the tactile feedback data (bottom).
Vision Tactile Manipulation refers to tasks that rely on pro-
cessing tactile information like images to solve manipulation
tasks. Tasks could include key insertion which require tactile
inputs to solve due to visual occlusions. Example task shown
in Figure 23
Implementation Details: The vision-tactile, sim2real, ma-
nipulation environments are ported over from the 2024 Man-
iSkill Vision-based-Tactile Manipulation Skill Learning Chal-
J. Classic Control
Fig. 24: Example control environments: CartPoleBalance,
Classic control is quite broad but in this context refers
to fake robots tasked with achieving some stable pose or
moving in a direction at desired speeds. Tasks include cartpole
DM-Control  has the most implemented control tasks
and Isaac Lab has a few GPU parallelized variants. ManiSkill3
has GPU parallelized simulationrendering variants of the
most control tasks.
Implementation Details: ManiSkill3 uses its MJCF loader
to load the MJCF robot definitions from the original DM-
Control repository and tunes robot pd joint delta position
controllers to align as closely as possible to the behavior seen
in DM-ControlMujoco.
K. Digital Twins
Digital twins have two variants included, environments for
real2sim and environments for sim2real. The distinction here
is real2sim environments simply need to be designed so that
a model trained on a real world equivalent of the simulation
environment achieves similar success rates when evaluated in
simulation. Sim2real digital twins are environments designed
so that models trained on simulation data can be more easily
used for real world deployment.
For real2sim, ManiSkill3 ports over and GPU parallelizes
some environments from SIMPLER , which enables effi-
cient evaluation of policies trained on real world data like the
generalist RT-X and Octo models. The primary tricks include
green-screening a real world image and texture matching
which have been copied over and parallelized. We ensured
the GPU parallelized port of SIMPLER achieves similar
resultsbehaviors as the original CPU simulated environments
as shown in Figure 25.
For sim2real, we provide tools useful for training policies in
simulation and directly deploying into the real-world. Given
the number of details involved in sim2real, we explain the
implementation details of vision-based sim2real in detail in
Appendix VIII.
Fig. 25: Evaluated success rates of generalist robotics models
like Octo and RT-1X on 4 different tasks. The correlation and
MMRV metrics are close to that of the original paper. MMRV
is Mean Maximum Rank Violation (lower the better), which
assess the accuracy of the consistency of the rankings of real
and sim policy evaluations.
L. Soft Body Manipulation Environments
Soft body manipulation refers to the manipulation of soft
body objects that can deform and morph in shape. Tasks
include excavating sand particles, pouring water etc.
ManiSkill3 soft body manipulation environments are the
same as ManiSkill2 which uses 2-way coupled rigid-MPM
simulation that enables rigid body objects to interact with soft
body objects. We point readers to the ManiSkill2 paper for
more details on soft body simulation.
VIII. VISION-BASED SIM2REAL
We describe how one can use ManiSkill3 to perform end-to-
end vision-based sim2real by training with PPO a RGB-based
manipulation policy in simulation and zero-shot deploying it
in the real world. Past work such as TRANSIC-Envs
provide reproducible sim2real digital twin setups but rely on
state estimation via ArUco markers for sim2real transfer and
does not support visual feedback. Dextreme  provides a
realistic in hand cube rotation environment but also does not
support efficient visual feedback and relies on accurate state
estimation. ManiSkill3 on the other hand has fast visual data
generation that enable mimicing real-world cameras inputs for
training at scale and fast. We note however the example setup
here is a task less complex than that of e.g. Dextreme but
point out that our reproducible setup is easily extendable and
could be the start of opening a new avenue of sim2real research
with efficient and fast large-scale simulation training on image
A. Hardware
We use the low-cost 300 Koch v1.1. robot arm and the
LeRobot library for a simple accessible python interface to
control the robot arm. Due to the simple but accessible
as the motors are less precise compared to more expensive
robot arms. Thus the vision-based sim2real demonstrated here
can easily work on other robot arms. Apart from the robot
we also setup one third view iPhone camera (any LeRobot
compatible camera can work) to record RGB observation data.
B. Controller  Action Space
We use a target delta joint position controller in both
simulation and the real-world at a control frequency of 30Hz.
At timestep t  0 after an (real or simulation) environment
reset we save an initial target joint position value qt  qt
where qt is the joint position of the robot arm at timestep t.
Given an action at at timestep t, we set qt1  qt  at.
At each timestep t, we drive the robots joint positions to qt.
In simulation this is equivalent to setting target joint position
values in the PhysX physics engine and the behavior is defined
by a PD controller. In the real world this is equivalent to setting
the next joint position of the robot arm to be qt, and most
robot arms have APIs that enable this functionality and will
internally run their own control loop to reach the set joint
position.
The actual action space of the environment is normalized
to a range of -1 to 1 for each joint. Given a predicted action
from a policy at we clip it first to the range of [1, 1] and then
unnormalize it to obtain at  at(hl)
, which is the delta action
applied to the qt. For the Koch robot arm l  0.05, h  0.05,
which represent a maximum change of 0.05 radians in either
direction of each joint.
We choose the target delta joint position controller as the
sim2real dynamics gap for quasi-static tasks (tasks where
objects only move when the robot manipulate them) is min-
imized. In both simulation and real-world in quasi-static
settings with small enough action sizes as long as within
the timeframe 
