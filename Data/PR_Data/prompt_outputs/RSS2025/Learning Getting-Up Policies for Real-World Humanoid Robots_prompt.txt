=== PDF文件: Learning Getting-Up Policies for Real-World Humanoid Robots.pdf ===
=== 时间: 2025-07-21 14:46:39.656934 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Learning Getting-Up Policies for
Real-World Humanoid Robots
Xialin He1
Runpei Dong1
Zixuan Chen2
Saurabh Gupta1
1University of Illinois Urbana-Champaign
2Simon Fraser University
(a) Getting Up from Supine (i.e., Lying Face Up) Poses
(b) Rolling Over from Prone (i.e., Lying Face Down) Poses
(c) Getting up on Different Terrains
Fig. 1: HUMANUP provides a simple and general two-stage training method for humanoid getting-up tasks, which can be
directly deployed on Unitree G1 humanoid robots . Our policies showcase robust and smooth behavior that can get up from
diverse lying postures (both supine and prone) on varied terrains such as grass slopes and stone tiles.
AbstractAutomatic fall recovery is a crucial prerequisite
before humanoid robots can be reliably deployed. Hand-designing
controllers for getting up is difficult because of the varied
configurations a humanoid can end up in after a fall and the
challenging terrains humanoid robots are expected to operate on.
This paper develops a learning framework to produce controllers
that enable humanoid robots to get up from varying configurations
on varying terrains. Unlike previous successful applications of
learning to humanoid locomotion, the getting-up task involves
complex contact patterns (which necessitates accurately modeling
of the collision geometry) and sparser rewards. We address
these challenges through a two-phase approach that induces a
curriculum. The first stage focuses on discovering a good getting-
up trajectory under minimal constraints on smoothness or speed
torque limits. The second stage then refines the discovered
motions into deployable (i.e. smooth and slow) motions that are
Equal contributions.
robust to variations in initial configuration and terrains. We find
these innovations enable a real-world G1 humanoid robot to get
up from two main situations that we considered: a) lying face up
and b) lying face down, both tested on flat, deformable, slippery
surfaces and slopes (e.g., sloppy grass and snowfield). This is
one of the first successful demonstrations of learned getting-up
policies for human-sized humanoid robots in the real world.
Project page:
I. INTRODUCTION
This paper develops learned controllers that enable a hu-
manoid robot to get up from varied fall configurations on varied
terrains. Humanoid robots are susceptible to falls, and their
reliance on humans for fall recovery hinders their deployment.
environments involving complex terrains and tight workspaces
(i.e. challenging scenarios that are too difficult for wheeled
robots), a humanoid robot may end up in an unpredictable
configuration upon a fall, or may be on an unknown terrain. 26
of the 46 trials at the DARPA Robotics Challenge (DRC) had
a fall, and 25 of these falls required human intervention for
recovery . The DRC identified fall prevention and recovery
as a major topic needing more research. This paper pursues
it and proposes a learning-based framework for learning fall
recovery policies for humanoid robots under varying conditions.
The need for recovering from varied initial conditions makes
it hard to design a fall recovery controller by hand and motivates
the need for learning via trial and error in simulation. Such
learning has produced exciting results in recent years for
locomotion problems involving quadrupeds and humanoids,
e.g. [47, 64]. Motivated by these exciting results, we started
with simply applying the Sim-to-Real (Sim2Real) paradigm
for the getting-up problem. However, we quickly realized that
the getting-up problem is different from typical locomotion
problems in the following three significant ways that made a
naive adaptation of previous work inadequate:
a) Non-periodic behavior. In locomotion, contacts with the
environment happen in structured ways: cyclic left-right
stepping pattern. The getting-up problem doesnt have
such a periodic behavior. The contact sequence necessary
for getting up itself needs to be figured out. This makes
optimization harder and may render phase coupling of left
and right feet commonly used in locomotion ineffective.
b) Richness in contact. Different from locomotion, contacts
necessary for getting up are not limited to just the feet.
Many other parts of the robot are likely already in touch
with the terrain. But more importantly, the robot may find
it useful to employ its body, outside of the feet, to exert
forces upon the environment, in order to get up. Freezing
decoupling the upper body, only coarsely modeling the
upper body for collisions, and using a larger simulation
step size: the typical design choices made in locomotion,
are no longer applicable for the getting up task.
c) Reward sparsity. Designing rewards for getting up is
harder than other locomotion tasks. Velocity tracking
offers a dense reward and feedback on whether the robot
is meaningfully walking forward is available within a few
tens of simulation steps. In contrast, many parts of the
body make negative progress, e.g., the torso first needs to
tilt down for seconds before tilting up to finally get up.
We present HUMANUP, a two-stage reinforcement learning
(RL) training framework that circumvents these issues. Stage I
targets solving the task in easier settings (sparse task rewards
with weak regularization), while Stage II makes the learned
motion deployable (i.e., control should be smooth; velocities
and executed torques should be small; etc). Discovering the
getting-up motion is hard because of sparse and underspecified
rewards. Stage I tackles this hard problem without being limited
by smoothness in motion or speed  torque limits. Tracking a
trajectory is easier as it offers dense rewards. Stage II tackles
this easier problem but does it under strict Sim2Real control
regularization and randomization of terrains and initial poses.
curriculum that progresses from simplified full collision
strong control regularization, and domain randomization.
This amounts to a hard-to-easy curriculum on task difficulty
(Stage I: getting-up task; Stage II: motion tracking), and an
easy-to-hard curriculum on regularization and variability (Stage
We conduct experiments in simulation and the real world
with the G1 platform from Unitree. In the real world, we find
our framework enables the G1 robot to get up from two different
poses (supine, i.e. lying face up, and prone, i.e. lying face down)
across six different terrains. This expands the capability of
the G1 robot: the manufacturer-provided hand-crafted getting-
up controller only successfully gets up from supine poses on
a flat surface without bumps. In simulated experiments, our
framework can successfully learn getting-up policies that work
on varied terrains and varied starting poses.
II. RELATED WORK
We review related works on humanoid control, learning for
humanoid control, and work specifically targeted toward fall
recovery for legged robots.
A. Humanoid Control
Controlling a high degree of freedom humanoid robots has
fascinated researchers for the last several decades. Model-based
(ZMP) principle [35, 65, 74, 76], optimization [4, 14, 45],
and Model Predictive Control (MPC) [12, 15, 22, 79], have
demonstrated remarkable success in fundamental locomotion
tasks like walking, running and jumping. However, these
approaches often struggle to generalize or adapt to novel
environments. In contrast, learning-based approaches have
recently made significant strides, continuously expanding the
generalization capabilities of humanoid locomotion controllers.
1) Learning for humanoid control: Learning in simulation
via reinforcement followed by a sim-to-real transfer has led to
many successful locomotion results for quadrupeds [46, 47]
and humanoids [2, 8, 29, 6264]. This has enabled locomotion
on challenging in-the-wild terrain [28, 62], agile motions
like jumping [48, 81], and even locomotion driven by visual
inputs [50, 83]. Researchers have also expanded the repertoire
of humanoid motions to skillful movements like dancing and
naturalistic walking gaits through use of human mocap or
video data [9, 34, 38, 57]. Some works address locomotion and
manipulation problems for humanoids simultaneously to enable
loco-manipulation controllers in an end-to-end fashion facili-
tated by teleportation [20, 32, 52]. Notably, these tasks mostly
involve contact between the feet and the environment, thus
requiring only limited contact reasoning. How to effectively
develop controllers for more contact-rich tasks like crawling,
and unpredictable contacts between the whole body and the
environment remains under-explored.
Discovery
(a) Stage I: Discovery Policy (RL-based Task Learning)
(b) Stage II: Deployable Policy (RL-based Tracking)
Tracking
Real-World Deployment
Discovered
Trajectory
(c) Sim-to-Real Curriculum
Deployable
Curriculum 1: Collision Mesh
(Stage II)
Curriculum 2: Posture Randomization
Curriculum 3: Control Regularization
Regularization
(Stage I)
(Stage II)
(Stage I)
Regularization
Simplified
(Stage II)
(Stage I)
Canonical
(Stage II)
(Stage I)
(Stage II)
(Stage I)
Environment  Domain
constraints
Fig. 2: HUMANUP system overview. Our getting-up policy (Sec. III-A) is trained in simulation using two-stage RL training,
after which it is directly deployed in the real world. (a) Stage I (Sec. III-B1) learns a discovery policy f that figures out a
getting-up trajectory with minimal deployment constraints. (b) Stage II (Sec. III-B2) converts the trajectory discovered by Stage
I into a policy  that is deployable, robust, and generalizable. This policy  is trained by learning to track a slowed down
version of the discovered trajectory under strong control regularization on varied terrains and from varied initial poses. (c)
The two-stage training induces a curriculum (Sec. III-C). Stage I targets motion discovery in easier settings (simpler collision
learned motion deployable and generalizable.
B. Legged robots fall recovery
Humanoid robots are vulnerable to falls due to under-actuated
control dynamics, high-dimensional states, and unstructured
environments [27, 30, 35, 36, 42, 44], making the ability to
recover from falling of great significance. Over the years, this
problem has been tackled in the following ways.
1) Getting up via motion planning: Early work from
Morimoto and Doya  solved the getting-up problem for a
states are used as subgoals to transit via hierarchical RL. This
line of work can be viewed as an application of motion planning
by configuration graph transition learning , where stored
robot states between lying and standing are used as graph
nodes to transit [21, 40, 41, 69]. More recently, some progress
has been made to enable toy-sized humanoid robots to get
up [24, 26, 37, 67]. For example, Gonzalez-Fierro et al.
explores getting up from a canonical sitting posture with motion
planning by imitating human demonstration with ZMP criterion.
To address the high-dimensionality of humanoid configurations,
Jeong and Lee  leverage bilateral symmetry to reduce the
control DoFs by half and a clustering technique is used for
further reducing the complexity of configuration space, thereby
improving getting-up learning efficiency. However, such state
machine learning using predefined configuration graphs may
not be sufficient for generalizing to unpredictable initial and
intermediate states, which happens when the robot operates on
challenging terrains.
2) Hand-designed getting-up trajectories: Another solution,
often adopted by commercial products, is to replay a manually
designed motion trajectory. For example, Unitree  has a
getting-up controller built into G1s default controllers. Booster
Robotics  designed a specific recovery controller for their
robots that can help the robot recover from fallen states.
Concurrent work from Zhuang and Zhao  enables a G1
robot to get up by tracking the getting-up motion of a real
human. The main drawback of such pre-defined trajectory
getting-up controllers is that they may only handle a limited
number of fallen states and lack generalization.
3) Learned getting-up policies for real robots: RL fol-
lowed by sim-to-real has also been successfully applied for
quadruped [39, 47, 55, 77] fall recovery. For example, Lee et al.
explore sim2real RL to achieve real-world quadruped fall
recovery from complex configurations. Ji et al.  train a
recovery policy that enables the quadruped to dribble in snowy
and rough terrains continuously. Wang et al.  develop a
quadruped recovery policy in highly dynamic scenarios.
4) Learned getting-up policies for character animation: A
parallel research effort in character animation, also explores
the design of RL-based motion imitation algorithms: Deep-
controllers in simulation. By tracking user-specified getting-
up curves, Frezzato et al.  enable humanoid characters to
get up by synthesizing physically plausible motion. Without
recourse to mocap data, such naturalistic getting-up controllers
for simulated humanoid characters can also be developed with
careful curriculum designs . Some works explore sampling-
based methods for addressing contact-rich character locomotion,
including getting up [31, 49, 61], while some works have
demonstrated success in humanoid getting up with online
model-predictive control . It is worth noticing, however,
that these works use humanoid characters with larger DoFs
compared to humanoid robots (e.g., 69 DoFs in SMPL )
and use simplified dynamics. As a result, learned policies
operate body parts at high velocities and in infeasible ways,
leading to behavior that cannot be transferred into the real world
directly. Hence, developing generalizable recovery controllers
for humanoid robots remains an open problem.
III. HUMANUP: SIM-TO-REAL HUMANOID GETTING UP
Our goal is to learn a getting-up policy  that enables a
humanoid to get up from arbitrary initial postures. We consider
getting up from two families of lying postures: a) supine poses
(i.e. lying face up) and b) prone poses (i.e. lying face down).
Getting up from these two groups of postures may require
different behaviors, which makes it challenging to learn a
single policy that handles both scenarios. To tackle this issue,
we decompose the getting-up task from a prone pose to first
rolling over and then standing up from the resulting supine
posture. Therefore, we aim to learn policies for rolling over
from a prone pose and getting up from a supine pose separately.
To solve these two tasks, we propose HUMANUP, a general
learning framework for training getting-up and rolling over
policy f is trained to figure out standing-up or rolling-over
motions. f is trained without deployment constraints, and
only the task and symmetry rewards are used. In stage II,
a deployable policy  imitates the rolling-over  getting-
up behaviors obtained from stage I under strong control
regularization. This deployable policy  is transferred from
simulation to the real world as the final policy. We detail the
policy model and two-stage training in Sec. III-B, and then
discuss the induced curriculum in Sec. III-C.
A. Policy Architecture
HUMANUP trains two policy models f and  with RL. Both
policy models take observation ot  [zt, st, st10:t1] R868
as input and output action at R23, where st R74
is the proprioceptive information, st10:t1 is the 10 steps
history states. zt R54 are the encoded environment extrinsic
latents that are predicted from observation history and learned
using regularized online adaptation . The proprioceptive
information st consists of the robots roll and pitch, angular
tive information can be accurately obtained in the real world,
and we find that this is sufficient for the robot to infer the
overall posture. We do not use any linear velocity and yaw
information as it is difficult to reliably estimate them in the
real world [32, 33].
implemented
trained via PPO . The optimization maximizes the ex-
pected -discounted policy return within T episode length:
, where rt is the reward at timestamp t.
B. Two-Stage Policy Learning
1) Stage I: Discovery Policy: This stage discovers getting-
up  rolling-over behavior efficiently without deployment
constraints. We use the following task rewards with very weak
regularization to train this discovery policy f. Timestep t and
reward weight terms are omitted for simplicity. The precise
expressions for each reward term and their weights are provided
in Sec. A.1.
Rewards for Getting Up: rup  rheight rheight ruprightness
rstand on feet  rfeet contact forces  rsymmetry, where
rheight encourages the robots height to be close to a target
height when standing;
rheight encourages the robot to continuously increasing its
ruprightness encourages the robot to increase the z-component
of the projected gravity,1 so that the robot stands upright;
rstand on feet encourages the robot to stand on both feet;
rfeet contact forces encourages the robot to increase contact
forces applied to the feet continuously;
rsymmetry reduces the search space by encouraging (but not
requiring) the robot to output bilaterally symmetric actions.
Past work [37, 68] employed hard symmetry which improves
RL sample efficiency at the cost of limiting robots DoFs and
generalization. Our soft symmetry reward partially leverages
the benefit but mitigates the limitation.
Rewards for Rolling Over: rroll  rgravity, which encourages
the robot to change its body orientation so that its projected
gravity is close to the projected gravity when lying face up.
2) Stage II: Deployable Policy: This stage trains policy
that will be directly deployed in the real world. Policy  is
trained to imitate an 8 slowed-down version of the state
trajectories discovered in Stage I, while also respecting strong
regularization to ensure Sim2Real transferability. We use the
typical regularization rewards and describe them in Sec. A.2.
Tracking Rewards: rtracking encourages the robot to act close
to the given motion trajectory derived from the discovered
motion. rtracking  rtracking DoF  rtracking body, where
rtracking DoF encourages the robot to move to the same DoF
position as the reference motion, and
rtracking body encourages the robot to move the body to
the same position as the reference. Specifically, rtracking body
becomes two different rewards to encourage tracking upright
posture (rhead height) and correct head orientation (rhead gravity)
for getting-up and rolling-over tasks, respectively.
1Projected gravity on a robot part is the gravity vector transformed from
the world frame to the parts local frame.
C. Stage I to Stage II Curriculum
The design of two-stage policy learning induces a hard-to-
easy curriculum . Stage I targets motion discovery in easier
settings (weak regularization, no variations in terrain, same
starting poses, simpler collision geometry). Once motions have
been discovered, Stage II solves the task of making the learned
motion deployable and generalizable. As our experiments will
learning. Specifically, complexity increases from Stage I to
Stage II in the following ways:
1) Collision mesh: As shown in Fig. 2, Stage I uses a
simplified collision mesh for faster motion discovery, while
Stage II uses the full mesh for improved Sim2Real performance.
2) Posture randomization: Stage I learns to get up (and roll
over) from a canonical pose, accelerating learning, while Stage
II starts from arbitrary initial poses, enhancing generalization.
To further speed up Stage I, we mix in standing poses. For
Stage II, we generate a dataset P of 20K supine poses Psupine
and 20K prone poses Pprone by randomizing initial DoFs from
canonical lying poses, dropping the humanoid from 0.5m, and
simulating for 10s to resolve self-collisions. We use 10K poses
from each set for training and the rest for evaluation.
3) Control Regularization and Terrain Randomization: For
Sim2Real transfer, we use the following control regularization
terms and environment randomization in Stage II:
Weak strong control regularization. Weak control
regularization in Stage I enables discovery of getting-up
rolling-over motion, while strong control regularization (via
smoothness rewards, DoF velocity penalties, etc, see the full
list in Sec. A.2) in Stage II encourages more deployable action.
Fast slow motion speed.
Without strong control
up motion (<1s), infeasible for real-world deployment. To
address this, we slow it to 8s via interpolation, providing stable
tracking targets for Stage II, which better aligns with its control
regularization.
Fixed random dynamics and domain parameters. Stage
II also employs domain and dynamics randomization via terrain
randomization and noise injection. Such randomization has
been shown to play a vital role in successful Sim2Real .
IV. IMPLEMENTATION DETAILS
A. Platform Configurations
We use the Unitree G1 platform  in all real-world
and simulation experiments. G1 is a medium-sized humanoid
robot with 29 actuatable degrees of freedom (DoF) in total.
12 DoFs, and the waist has 3 DoFs. As getting up does not
involve object manipulation, we disable the 3 DOFs in the
G1 has waist yaw and roll DoFs, and we find them useful
for our getting-up task. The robot has an IMU sensor for roll
and pitch states, and the joint states can be obtained from the
motor encoders. We use position control where the torque is
derived by a PD controller operating at 50 Hz.
B. Simulation Configurations
We use Isaac Gym  for simulated training and evaluation.
We use a URDF with simplified collision for stage-I training
and the official whole-body URDF from Unitree  for stage-
II. To accurately model the numerous contacts between the
humanoid and the ground, we use a high simulation frequency
of 1000 Hz, while the low-level PD controller frequency
operates at 50 Hz. More details can be found in Sec. C.
V. SIMULATION RESULTS
A. Tasks
We evaluate three tasks involved in the humanoid getting-up
prone to supine poses, and getting up from prone poses which
can be addressed by solving task and task consecutively.
Simulation tests are conducted with the full URDF.
B. Baselines
We compare to the following baselines,
a) RL with Simple Task Rewards (Tao et al. ):
This policy is trained with RL using rewards from Tao
et al.  originally designed for physically animated
characters instead of humanoid robots. Similar to our
weak torque limit and motion speed curriculum for
getting-up policy learning. Because  does not consider
sim2real deployment regularization and requirements (e.g.,
smoothness and collision mesh usage), policies learned
through their scheme arent appropriate for real-world
humanoid deployment.
b) HUMANUP wo Stage II: Our policy trained with only
stage I, where no deployment constraints are applied.
c) HUMANUP wo Full URDF: Our policy trained with
two stages, but stage II uses the simplified collision mesh.
d) HUMANUP wo Posture Randomization: Our policy
trained on a single canonical lying posture without any
randomization of initialization postures.
e) HUMANUP w Hard Symmetry: Our policy trained using
a humanoid with a symmetric controller. This symmetric
controller follows the symmetry control principle of
the manufacturer-provided controller baseline described
in real-world experiments, which leads to bilaterally
symmetric control. We set all pitch DoFs actions to
be the same between the left and the right DoFs, while
flipping the directions of all the roll and yaw actions.
f) HUMANUP wo Two-Stage Learning: Our policy trained
in a single stage with the full collision mesh, posture
applied at the same time.
C. Metrics
Task Success. i) Task success rate Success (): For the
getting-up task, the robots head height must be 1.1m at
success. For the rolling-over task, the cosine between the robots
TABLE I: Simulation results. We compare HUMANUP with several baselines on the held-out split of our curated posture set
Psupine and Pprone using full URDF. All methods are trained on the training split of our posture set P, except for methods
HUMANUP wo Stage II and wo posture randomization. HUMANUP solves task by solving task and task consecutively.
We do not include the results of baseline 6 (HUMANUP wo Two-Stage Learning) as it cannot solve the task.  Tao et al.
is trained to directly solving task as it does not have a rolling over policy. SIM2REAL column indicates whether the method
can transfer to the real world successfully. We tested all methods in the real world for which the SMOOTHNESS and SAFETY
metrics are reasonable, and SIM2REAL is false if deployment wasnt successful. Metrics are introduced in Sec. V-C.
SIM2REAL
SMOOTHNESS
Task Metric
Action Jitter
DoF Pos Jitter
Getting Up from Supine Poses
Tao et al.
HUMANUP wo Stage II
HUMANUP wo Full URDF
HUMANUP wo Posture Rand.
HUMANUP w Hard Symmetry
Rolling Over from Prone to Supine Poses
HUMANUP wo Stage II
HUMANUP wo Full URDF
HUMANUP wo Posture Rand.
HUMANUP w Hard Symmetry
Getting Up from Prone Poses
Tao et al.
HUMANUP wo Stage II
HUMANUP wo Full URDF
HUMANUP wo Posture Rand.
HUMANUP w Hard Symmetry
lying face up should be 0.9, thus, the robot needs to move
until lying face up. ii) Task Metrics: the head height (m) for
the getting-up task, and the cosine of the angle between the
robots torso X-axis (sticking out to the front from the torso)
and the gravity, for the rolling-over task.
Smoothness. We measure the Action Jitter (rads3), DoF
Pos Jitter (rads3), and mean Energy (N mrads ) for action
smoothness evaluation . The jitter metrics are computed as
the third derivative values , which indicate the coordination
stability of body movements.
Safety. We introduce safety scores STorque
[0, 1] and SDoF
[0, 1] that measure the relative magnitude of commanded torque
DoF compared to the torque and DoF limits, where  is a
safety threshold. This is essential for robotic safety during
issues and cause mechanical and motor damage. Formally,
these scores are defined as:
where t,j and qt,j denote the applied torque and joint
displacement at time step t for joint j, respectively.  max
represent their respective limits, T is the total number of
time steps, and J is the total number of joints. The threshold
determines when a torque or displacement is considered
excessive. The indicator function 1() returns 1 if the condition
is met and 0 otherwise. The parameters ,  [0, 1] control
the trade-off between peak and prolonged violations, ensuring
a balanced assessment of safety risks. In this paper, we use
0.8,   0.5,   0.5 as default during evaluation.
D. Results and Analysis
Tab. I presents results based on policies tested on held-out
subsets of our curated initial posture set P, i.e. 10K val samples
each from Psupine and Pprone. Fig. 4 shows the learning curve
for the getting-up task, where the termination base height
reflects the robots ability to lift its body, and body uprightness
indicates whether it achieves a stable standing posture.
1) Ignoring Torque  Control Limits Leads to Undeployable
cantly worse than HUMANUP. For example, the average action
jitter metric is nearly 18 higher than HUMANUP. Actions
from  are highly unstable and unsafe and thus cannot be
safely deployed to the real robot. Furthermore,  learns a
very fast getting-up motion that keeps jumping after getting
up. See visualization s getting up motion in Sec. B.1.
A similar trend can be seen when comparing HUMANUP to
HUMANUP wo Stage II. While HUMANUP wo Stage II
also solves the task to some extent, it achieves unsatisfying
smoothness and safety metrics similar making it inappropriate
for real-world deployment. Thus, the regularization imposed
in Stage II is essential to making policies more amenable to
Sim2Real transfer.
Getting Up
From Supine Poses
Roll Over
From Prone Pose
Snow Field
Conc. Path
Brick Surface
Stone Tile
Muddy Grass Grass Slope
Snow Field
Manufacturer
HumanUP wo PR
(a) Getting Up from Supine Poses (10 Trials)
(b) Rolling Over from Prone Poses (10 Trials)
Brick Surface
Concrete Path
Muddy Grass
Stone Tile
Grass Slope (10)
Snow Field
Brick Surface
Concrete Path
Muddy Grass
Stone Tile
Grass Slope (10)
Conc. Path
Brick Surface
Stone Tile
Muddy Grass Grass Slope
Snow Field
Manufacturer (NA)
HumanUP wo PR
Fig. 3: Real-world results. We evaluate HUMANUP (ours) in several real setups that span diverse surface properties, including
both man-made and natural surfaces, and cover a wide range of roughness (rough concrete to slippery snow), bumpiness
(flat concrete to tiles), ground compliance (completely firm concrete to being swampy muddy grass), and slope (flat to about
10). We compare HUMANUP with G1s manufacturer-provided controller and HUMANUP wo posture randomization (PR).
HUMANUP succeeds more consistently (78.3 vs 41.7) and can solve terrains that the manufacturer-provided controller cant.
2) Importance of Learning via a Curriculum: So, while it is
clear that we need to incorporate strong control regularization
for good safety metrics and Sim2Real transfer, our 2 stage
process is better than doing it in a single stage. In fact, as plotted
in Fig. 4, HUMANUP wo Two Stage Learning where the policy
is trained in a single stage using all sim2real regularization
fails to solve the task. This is because the strict Sim2Real
regularization makes task learning extremely challenging. Our
two-stage curriculum successfully incorporates both aspects: it
learns to solve the task, but the policy also operates safely.
3) Full URDF vs. Simplified URDF: Somewhat surprisingly,
even though HUMANUP wo Full URDF was trained without
the full URDF mesh, it generalizes fine when tested with the
full URDF in simulation, as reported in Tab. I. However, we
found poor transfer of this policy to the real world. It failed on
all 5 trials on a simple flat terrain. We believe the poor real-
world performance was because of the mismatch between the
contact it was expecting and the contact that actually happened.
4) Posture randomization helps: HUMANUP wo posture
randomization (PR) works much worse than HUMANUP,
suggesting that PR is necessary for generalizable control.
5) Soft symmetry vs. hard symmetry: Compared to HU-
MANUP w Hard Symmetry, HUMANUP achieves better task
success in Tab. I, particularly for the rolling-over task, which
is very difficult with symmetric commands.
VI. REAL WORLD RESULTS
We also tested HUMANUP policies in the real world on G1
robot. Our real-world test bed consists of 6 different terrains
shown in Fig. 3: concrete, brick, stone tiles, muddy grass, grassy
Ours wo Two-Stage Learning
Fig. 4: Learning curve. (a) Termination height of the torso,
indicating whether the robot can lift the body. (b) Body
normalized to [0, 1] for better comparison. The overall number
of simulation sampling steps is about 5B, normalized to [0, 1].
cover a wide range of roughness (rough concrete to slippery
snow), bumpiness (flat concrete to tiles), ground compliance
(completely firm concrete to being swampy muddy grasp), and
slope (flat to 10). We tested two tasks: a) getting up from
supine poses, and b) rolling over from prone to supine poses.
We compare our policy with 1) Manufacturer-provided
Controller and 2) a high-performing ablation of HUMANUP
(HUMANUP wo posture randomization). The manufacturer-
provided controller, which comes with the robot G1, tracks a
hand-crafted trajectory in three phases shown in Fig. 5: Phase
Phase 0: Reset (1s) Phase 1: Canonical Lying to Squatting (6s)
Phase 2: Squatting to Standing (4s)
Manufacturer-
Provided
HumanUP (Ours)
Learned Continuous Whole-Body Getting Up Motion (6s)
Standing
Standing
Squatting
HumanUP (Ours)
Manufacturer-Provided
Fig. 5: Getting up execution comparison with G1s manufacturer-provided controller. The manufacturer-provided controller
uses a handcrafted motion trajectory, which can be divided into three phases, while our HUMANUP learns a continuous and
more efficient whole-body getting-up motion. Our HUMANUP enables the humanoid to get up within 6 seconds, half of the
manufacturer-provided controllers 11 seconds of control. (a), (b), and (c) record the corresponding mean motor temperature of
the upper body, lower body, and waist, respectively. G1s manufacturer-provided controllers execution causes the arm motors
to heat up significantly, whereas our policy makes more use of the leg motors that are stronger (higher torque limit of 83N as
opposed to 25N for the arm motors) and thus able to take more load.
0 brings the robot to a canonical lying pose; Phase 1 first props
up and then slides the torso forward using hands, followed by
bending legs to squat; Phase 2 uses its waist to tilt up the torso
to stand up from squatting. Motions in phase 1 and phase 2
are symmetric, and this controller only works for supine poses.
A. Results
Fig. 3 presents experimental results. Overall, we find that HU-
MANUP policies perform better than the manufacturer-provided
controller and HUMANUP without posture randomization (PR).
We discuss the results and observed behavior further.
1) Getting up from supine poses: The manufacturer-provided
controller works under nominal conditions, i.e., firm, flat
concrete ground with a reasonable friction value. However,
it starts to fail on more challenging terrains. For the bumpier
and rougher terrains (brick surface and stone tiles), the arms
may get stuck between bumps, causing failures. On slopes, the
robot fails to squat or hoist itself up due to both the resistance of
the grassland and the unstable squatting pose prone to falling
caused by slopes. On the compliant ground, the robot gets
destabilized. On slippery snow, the robot slips.
Both versions of HUMANUP outperform the manufacturer-
provided controller. Trained with terrain and domain ran-
as slipperiness, bumps, and slopes. Dynamics randomization
further enhances resilience to minor perturbations like slippage
or ground compliance. The full method, incorporating posture
it is specifically trained to handle diverse initial configurations.
2) Rolling over from prone to supine poses: Findings are
similar for the rolling over task. As noted, the manufacturer-
provided controller cant handle this situation. The full model
exhibits more robust performance than the model trained
without posture randomization. Rolling over seems to be easier
than getting up, HUMANUP achieves a 98.3 success rate.
B. Motion analysis
Fig. 5 shows the motion and motor temperatures for the
manufacturer-provided controller and HUMANUP policy.
1) Motor temperature: The manufacturer-provided controller
uses the arms during Phase 1 of getting up. Fig. 5 (a) shows
that the default controllers execution causes the arm motors
Manufacturer-
Provided
HumanUP (Ours)
Manufacturer-
Provided
HumanUP (Ours)
(a) Failure Example on Grass Slope
(b) Failure Example on Snow Field
Fig. 6: Qualitative examples of failure modes on grass slope and snow field. G1s manufacturer-provided controller isnt
able to squat on the sloping grass and slips on the slope. HUMANUP policy can partially get up on both the slope and the
to heat up significantly more when compared to HUMANUP
execution. Our policy makes more use of the leg motors that
are stronger (higher torque limit of 83N as opposed to 25N
for the arm motors) and thus able to take more load.
2) Efficiency: HUMANUP gets the robot to stand success-
fully within about 6 seconds through a smooth and continuous
provided controller, which takes nearly 11 seconds.
C. Failure Mode Analysis
Fig. 6 shows example failure modes for the manufacturer-
provided controller and HUMANUP on challenging terrains.
Fig. 6 (a) shows that the manufacturer-provided controller
tries to utilize the robots hands to squat, while the sloping
ground prevents it from getting to the full squatting pose
due to high friction and weak waist torques to move against
the dumping tendency. In contrast, HUMANUP manages to
lift the body, while the sloping ground sometimes causes
an unstable foot orientation. Fig. 6 (b) shows that on even
more challenging terrains like snow fields, both manufacturer-
provided and HUMANUP controllers may fail due to the
slippery and deformable ground.
VII. LIMITATIONS
HUMANUP has several limitations: 1) Motions discovered
in Stage I could be incompatible with stronger control regu-
larization used in Stage II. We didnt encounter this issue
in our experiments, possibly because of the weak control
regularization used in Stage I and the use of 8 slower
motion in Stage II. 2) HUMANUP depends on high-performance
physics simulators (IsaacGym ) running at high frequency
(e.g., 1 kHz). Simulation speed and fidelity for more complex
tasks involving perception and contacts remain a challenge.
Recent advances such as Genesis , Mujoco Playground ,
and Roboverse  could help address these limitations. 3)
The RL formulation in HUMANUP is under-specified
and may lead to reward hacking , complicating precise
alignment with natural human behaviors. For instance, our
learned motions sometimes include unnatural hand raising for
balance. 4) Extending HUMANUP to handle more complex
terrains like stairs or uneven surfaces remains under-explored,
while humanoid robots may fall more easily on such terrains.
Encouraging adaptive behaviors involving strong-arm usage
on more powerful platforms may be useful to properly handle
such situations.
VIII. DISCUSSION
In this paper, we tackle the problem of learning getting-
up controllers for real-world humanoid robots. Different from
locomotion tasks, getting up involves complex contact patterns
that are not known apriori. We develop a two-stage solution for
this problem based on reinforcement learning and sim-to-real.
Stage I finds a solution under minimal constraints, while Stage
II learns to track the trajectory discovered in Stage I under
regularization on control and from varied starting poses and on
varied terrains. We found this two stage strategy to be effective
both in simulation and the real world. Specifically, it enabled
us to get a real-world G1 humanoid to stand up from a supine
pose and roll over from a supine pose to a prone pose on
different terrains and from different starting poses. HUMANUP
achieves a higher success rate than G1s manufacturer-provided
controller and expands the capabilities of the G1 robot.
We hope our learned policies for automatic fall recovery will
be useful to researchers and practitioners, while our two-stage
learning framework may be helpful for other problems that
require figuring out complex contact patterns.
ACKNOWLEDGMENTS
This material is based upon work supported by an NSF
CAREER Award (IIS2143873) and a DURIP grant (N00014-
23-1-2166). We also thank the Coordinated Science Laboratory
for providing experimental space.
REFERENCES
Booster robotics. URL
Alphonsus Adu-Bredu, Grant Gibson, and Jessy Grizzle.
Exploring kinodynamic fabrics for reactive whole-body
control of underactuated humanoid robots.
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), pages 1039710404. IEEE, 2023. 2
Pulkit Agrawal.
The task specification problem.
Conference on Robot Learning, pages 17451751. PMLR,
Min Sung Ahn. Development and Real-Time Optimization-
based Control of a Full-sized Humanoid for Dynamic
Walking and Running.
University of California, Los
Anonymous. Hierarchical world models as visual whole-
body humanoid controllers. In The Thirteenth Interna-
tional Conference on Learning Representations, 2025.
Genesis Authors. Genesis: A universal and generative
physics engine for robotics and beyond, December 2024.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and
Jason Weston. Curriculum learning. In Proceedings of
the 26th Annual International Conference on Machine
2009. Association for Computing Machinery. 5
Zixuan Chen, Xialin He, Yen-Jen Wang, Qiayuan Liao,
Yanjie Ze, Zhongyu Li, S Shankar Sastry, Jiajun Wu,
Koushil Sreenath, Saurabh Gupta, et al. Learning smooth
humanoid locomotion through lipschitz-constrained poli-
cies. arXiv preprint arXiv:2410.11825, 2024. 2
Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang,
Ge Yang, and Xiaolong Wang. Expressive Whole-Body
Control for Humanoid Robots. In Proceedings of Robotics:
Science and Systems, Delft, Netherlands, July 2024. 2
Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak
Pathak. Extreme parkour with legged robots. In IEEE
International Conference on Robotics and Automation,
ICRA 2024, Yokohama, Japan, May 13-17, 2024, pages
Nuttapong Chentanez, Matthias Muller, Miles Macklin,
Viktor Makoviychuk, and Stefan Jeschke. Physics-based
motion capture imitation with deep reinforcement learning.
In Panayiotis Charalambous, Yiorgos Chrysanthou, Ben
Annual International Conference on Motion, Interaction,
and Games, MIG 2018, Limassol, Cyprus, November
Matthew Chignoli, Donghyun Kim, Elijah Stanger-Jones,
and Sangbae Kim. The mit humanoid robot: Design,
motion planning, and control for acrobatic behaviors.
In 2020 IEEE-RAS 20th International Conference on
Humanoid Robots (Humanoids), pages 18, 2021. 2
Jack Clark and Dario Amodei. Faulty reward functions
in the wild.
faulty-reward-functions. 9
Devin Crowley, Jeremy Dao, Helei Duan, Kevin Green,
Jonathan Hurst, and Alan Fern.
Optimizing bipedal
locomotion for the 100m dash with comparison to human
running.
In 2023 IEEE International Conference on
Robotics and Automation (ICRA), pages 1220512211,
Jared Di Carlo, Patrick M. Wensing, Benjamin Katz,
Gerardo Bledt, and Sangbae Kim. Dynamic locomotion
in the mit cheetah 3 through convex model-predictive
control. In 2018 IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 19, 2018.
Tamar Flash and Neville Hogan. The coordination of arm
model. Journal of neuroscience, 5(7):16881703, 1985.
Anthony Frezzato, Arsh Tangri, and Sheldon Andrews.
Synthesizing get-up motions for physics-based characters.
In Computer Graphics Forum, volume 41, pages 207218.
Wiley Online Library, 2022. 3
Zipeng Fu, Ashish Kumar, Jitendra Malik, and Deepak
Pathak. Minimizing energy consumption leads to the
emergence of gaits in legged robots. In Aleksandra Faust,
David Hsu, and Gerhard Neumann, editors, Conference on
Robot Learning, 8-11 November 2021, London, UK, vol-
ume 164 of Proceedings of Machine Learning Research,
pages 928937. PMLR, 2021. 6
Zipeng Fu, Xuxin Cheng, and Deepak Pathak.
whole-body control: Learning a unified policy for manip-
ulation and locomotion. In Karen Liu and Dana Kulic
andD Jeffrey Ichnowski, editors, Conference on Robot
New Zealand, volume 205 of Proceedings of Machine
Learning Research, pages 138149. PMLR, 2022. 4
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein,
and Chelsea Finn. Humanplus: Humanoid shadowing and
imitation from humans. In 8th Annual Conference on
Robot Learning, 2024. 2
Kiyoshi Fujiwara, Fumio Kanehiro, Shuuji Kajita, Kenji
Falling motion control to minimize damage to biped
humanoid robot. In IEEERSJ international conference
on Intelligent robots and systems, volume 3, pages 2521
Manuel Y Galliker, Noel Csomay-Shanklin, Ruben
with nonlinear model predictive control: Online gait
generation using whole-body dynamics. In 2022 IEEE-
RAS 21st International Conference on Humanoid Robots
(Humanoids), pages 622629. IEEE, 2022. 2
Jiawei Gao, Ziqin Wang, Zeqi Xiao, Jingbo Wang, Tai
Jiangmiao Pang. Coohoi: Learning cooperative human-
object interaction with manipulated object dynamics.
arXiv preprint arXiv:2406.14558, 2024. 3
Clement Gaspard, Marc Duclusaud, Gregoire Passault,
Melodie Daniel, and Olivier Ly. Frasa: An end-to-end
reinforcement learning agent for fall recovery and stand
up of humanoid robots. arXiv preprint arXiv:2410.08655,
Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li,
Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe
Jiangran Lyu, Siheng Zhao, Jiazhao Zhang, Jialiang Zhang,
Chengyang Zhao, Haoran Lu, Yufei Ding, Ran Gong, Yu-
ran Wang, Yuxuan Kuang, Ruihai Wu, Baoxiong Jia, Carlo
Yue Wang, Jitendra Malik, and Pieter Abbeel. Roboverse:
Towards a unified platform, dataset and benchmark for
scalable and generalizable robot learning, April 2025.
Miguel Gonzalez-Fierro, Carlos Balaguer, Nicola Swann,
and Thrishantha Nanayakkara. A humanoid robot standing
up through learning from demonstration using a multi-
modal reward function. In 2013 13th IEEE-RAS Inter-
national Conference on Humanoid Robots (Humanoids),
pages 7479. IEEE, 2013. 3
J. W. Grizzle, Jonathan W. Hurst, Benjamin Morris,
Hae-Won Park, and Koushil Sreenath.
robotic bipedal walker and runner. In American Control
Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi,
Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing
Humanoid Locomotion: Mastering Challenging Terrains
with Denoising World Model Learning. In Proceedings of
Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi,
Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing
humanoid locomotion: Mastering challenging terrains
with denoising world model learning.
arXiv preprint
Zhaoyuan Gu, Junheng Li, Wenlan Shen, Wenhao Yu,
Zhaoming Xie, Stephen McCrory, Xianyi Cheng, Ab-
dulaziz Shamsah, Robert Griffin, C Karen Liu, et al.
Humanoid locomotion and manipulation: Current progress
and challenges in control, planning, and learning. arXiv
preprint arXiv:2501.02116, 2025. 3
Perttu Hamalainen, Sebastian Eriksson, Esa Tanskanen,
Ville Kyrki, and Jaakko Lehtinen. Online motion synthesis
using sequential monte carlo.
ACM Transactions on
Graphics (TOG), 33(4):112, 2014. 4
Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong
Guanya Shi. Omnih2o: Universal and dexterous human-
to-humanoid whole-body teleoperation and learning. In
8th Annual Conference on Robot Learning, 2024. 2, 4,
Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang,
Kris Kitani, Changliu Liu, and Guanya Shi. Learning
human-to-humanoid real-time whole-body teleoperation.
In IEEERSJ International Conference on Intelligent
Robots and Systems, IROS 2024, Abu Dhabi, United Arab
Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia
Xiaolong Wang, et al. Hover: Versatile neural whole-
body controller for humanoid robots.
arXiv preprint
Kazuo Hirai, Masato Hirose, Yuji Haikawa, and Toru
Takenaka. The development of honda humanoid robot.
In Proceedings. 1998 IEEE international conference on
robotics and automation (Cat. No. 98CH36146), volume 2,
Masayuki Inaba, Takashi Igarashi, Satoshi Kagami, and
Hirochika Inoue. A 35 dof humanoid that can coordinate
arms and legs in standing up, reaching and grasping
an object.
In Proceedings of IEEERSJ International
Conference on Intelligent Robots and Systems. IROS96,
volume 1, pages 2936. IEEE, 1996. 3
Heejin Jeong and Daniel D. Lee.
Efficient learning
of stand-up motion for humanoid robots with bilateral
symmetry. In 2016 IEEERSJ International Conference
on Intelligent Robots and Systems, IROS 2016, Daejeon,
South Korea, October 9-14, 2016, pages 15441549. IEEE,
Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li,
Ge Yang, Xuxin Cheng, and Xiaolong Wang. Exbody2:
Advanced expressive humanoid whole-body control. arXiv
preprint arXiv:2412.13196, 2024. 2
Yandong Ji, Gabriel B Margolis, and Pulkit Agrawal.
In 2023 IEEE International Conference on Robotics and
Automation (ICRA), pages 51555162. IEEE, 2023. 3
Fumio Kanehiro, Kenji Kaneko, Kiyoshi Fujiwara, Ken-
suke Harada, Shuuji Kajita, Kazuhito Yokoi, Hirohisa
The first humanoid robot that has the same size as a
human and that can lie down and get up. In 2003 IEEE
International Conference on Robotics and Automation
(Cat. No. 03CH37422), volume 2, pages 16331639.
Fumio Kanehiro, Kiyoshi Fujiwara, Hirohisa Hirukawa,
Shinichiro Nakaoka, and Mitsuharu Morisawa. Getting
up motion planning using mahalanobis distance.
Proceedings 2007 IEEE International Conference on
Robotics and Automation, pages 25402545. IEEE, 2007.
Ichiro Kato. Development of wabot 1. Biomechanism, 2:
Lydia E. Kavraki, Petr Svestka, Jean-Claude Latombe,
and Mark H. Overmars. Probabilistic roadmaps for path
planning in high-dimensional configuration spaces. IEEE
Trans. Robotics Autom., 12(4):566580, 1996. 3
Eric Krotkov, Douglas Hackett, Larry Jackel, Michael
and Christopher Orlowski. The darpa robotics challenge
challenge finals: Humanoid robots to the rescue, pages
Scott Kuindersma, Robin Deits, Maurice Fallon, Andres
Pat Marion, and Russ Tedrake.
Optimization-based
locomotion planning, estimation, and control design for
the atlas humanoid robot. Autonomous robots, 40:429
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra
Malik. RMA: rapid motor adaptation for legged robots.
In Robotics: Science and Systems XVII, Virtual Event,
Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassil-
ios Tsounis, Vladlen Koltun, and Marco Hutter. Learning
agile and dynamic motor skills for legged robots. Sci.
Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine,
Glen Berseth, and Koushil Sreenath. Robust and versatile
bipedal jumping control through reinforcement learning.
In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and
Jingjin Yu, editors, Robotics: Science and Systems XIX,
Libin Liu, KangKang Yin, Michiel van de Panne, Tianjia
Sampling-based contact-rich
motion control.
In ACM SIGGRAPH 2010 Papers,
SIGGRAPH 10, New York, NY, USA, 2010. Association
for Computing Machinery. ISBN 9781450302104. 4
Junfeng Long, Junli Ren, Moji Shi, Zirui Wang, Tao
manoid locomotion with perceptive internal model. arXiv
preprint arXiv:2411.14386, 2024. 2
Matthew Loper, Naureen Mahmood, Javier Romero,
Gerard Pons-Moll, and Michael J. Black.
skinned multi-person linear model. ACM Trans. Graph.,
Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang,
Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, and
Xiaolong Wang. Mobile-television: Predictive motion
priors for humanoid whole-body control. arXiv preprint
Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani,
and Weipeng Xu. Perpetual humanoid control for real-
time simulated avatars.
In IEEECVF International
Conference on Computer Vision, ICCV 2023, Paris,
Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler,
Jing Huang, Kris M. Kitani, and Weipeng Xu. Universal
humanoid motion representations for physics-based con-
trol. In The Twelfth International Conference on Learning
2024. OpenReview.net, 2024. 3
Yuntao Ma, Farbod Farshidian, and Marco Hutter. Learn-
ing arm-assisted fall damage reduction and recovery for
legged mobile manipulators. In 2023 IEEE International
Conference on Robotics and Automation (ICRA), pages
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,
Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,
Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel
State. Isaac gym: High performance GPU based physics
simulation for robot learning. In Joaquin Vanschoren
and Sai-Kit Yeung, editors, Proceedings of the Neural
Information Processing Systems Track on Datasets and
Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,
December 2021, virtual, 2021. 5, 9, 15
Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi,
Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik,
Vitor Guizilini, and Yue Wang. Learning from massive
human videos for universal humanoid pose control. arXiv
preprint arXiv:2412.14172, 2024. 2
Jun Morimoto and Kenji Doya. Reinforcement learning
of dynamic motor sequence: Learning to stand up. In
Proceedings. 1998 IEEERSJ International Conference
on Intelligent Robots and Systems. Innovations in The-
volume 3, pages 17211726. IEEE, 1998. 3
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel
van de Panne. Deepmimic: example-guided deep rein-
forcement learning of physics-based character skills. ACM
Trans. Graph., 37(4):143, 2018. 3
Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine,
and Angjoo Kanazawa. Amp: Adversarial motion priors
for stylized physics-based character control.
Transactions on Graphics (ToG), 40(4):120, 2021. 3
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes,
Jan Achterhold, Joerg Stueckler, Michal Rolinek, and
Georg Martius. Sample-efficient cross-entropy method
for real-time planning. In Conference on Robot Learning,
pages 10491065. PMLR, 2021. 4
Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, and
Jitendra Malik.
Learning humanoid locomotion over
challenging terrain.
arXiv preprint arXiv:2410.03654,
Ilija Radosavovic, Jathushan Rajasegaran, Baifeng Shi,
Bike Zhang, Sarthak Kamat, Koushil Sreenath, Trevor
token prediction. In The Thirty-eighth Annual Conference
on Neural Information Processing Systems, 2024.
Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Dar-
humanoid locomotion with reinforcement learning. Sci.
Y. Sakagami, R. Watanabe, C. Aoyama, S. Matsunaga,
N. Higaki, and K. Fujimura. The intelligent asimo: system
overview and integration.
In IEEERSJ International
Conference on Intelligent Robots and Systems, volume 3,
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
algorithms. arXiv preprint arXiv:1707.06347, 2017. 4
Sebastian Stelter, Marc Bestmann, Norman Hendrich,
and Jianwei Zhang. Fast and reliable stand-up motions
for humanoid robots using spline interpolation and pa-
rameter optimization. In 20th International Conference
on Advanced Robotics, ICAR 2021, Ljubljana, Slovenia,
December 6-10, 2021, pages 253260. IEEE, 2021. 3
Zhi Su, Xiaoyu Huang, Daniel Felipe Ordonez Apraez,
Yunfei Li, Zhongyu Li, Qiayuan Liao, Giulio Turrisi,
Massimiliano Pontil, Claudio Semini, Yi Wu, and Koushil
Sreenath. Leveraging symmetry in rl-based legged loco-
motion control. In IEEERSJ International Conference on
Intelligent Robots and Systems, IROS 2024, Abu Dhabi,
United Arab Emirates, October 14-18, 2024, pages 6899
Jie Tan, Zhaoming Xie, Byron Boots, and C Karen
Simulation-based design of dynamic controllers
for humanoid balancing. In 2016 IEEERSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 27292736. IEEE, 2016. 3
Tianxin Tao, Matthew Wilson, Ruiyu Gou, and Michiel
van de Panne. Learning to get up. In Munkhtsetseg
SIGGRAPH 22: Special Interest Group on Computer
Graphics and Interactive Techniques Conference, Vancou-
Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis
and stabilization of complex behaviors through online
trajectory optimization. In 2012 IEEERSJ International
Conference on Intelligent Robots and Systems, pages 4906
Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik,
and Xue Bin Peng. Maskedmimic: Unified physics-based
character control through masked motion inpainting. ACM
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-
ciech Zaremba, and Pieter Abbeel. Domain randomization
for transferring deep neural networks from simulation
to the real world.
In 2017 IEEERSJ International
Conference on Intelligent Robots and Systems (IROS),
Nikolaos G Tsagarakis, Darwin G Caldwell, Francesca
manoid platform for realistic environments. Journal of
Field Robotics, 34(7):12251259, 2017. 2
Unitree. Unitree G1: Humanoid Agent AI Avatar. 2024.
Miomir Vukobratovic and Branislav Borovac.
moment pointthirty five years of its life. International
journal of humanoid robotics, 1(01):157173, 2004. 2
Yikai Wang, Mengdi Xu, Guanya Shi, and Ding Zhao.
Guardians as you fall: Active mode transition for safe
falling. In 2024 IEEE International Automated Vehicle
Validation Conference (IAVVC), pages 18. IEEE, 2024.
Zhen Wu, Jiaman Li, and C Karen Liu. Human-object
interaction from human-level instructions. arXiv preprint
Haoru Xue, Chaoyi Pan, Zeji Yi, Guannan Qu, and
Guanya Shi. Full-order sampling-based mpc for torque-
level locomotion control via diffusion-style annealing.
arXiv preprint arXiv:2409.15610, 2024. 2
Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa
Erik Frey, Koushil Sreenath, Lueder A Kahrs, et al.
Mujoco playground. arXiv preprint arXiv:2502.08844,
Chong Zhang, Wenli Xiao, Tairan He, and Guanya Shi.
sequential contacts. In 8th Annual Conference on Robot
Ziwen Zhuang and Hang Zhao.
Embrace collisions:
Humanoid shadowing for deployable contact-agnostics
motions. arXiv preprint arXiv:2502.01465, 2025. 3
Ziwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid
parkour learning. arXiv preprint arXiv:2406.10759, 2024.
APPENDIX
A Rewards
Rewards Components in Stage I . . . . . . . .
Rewards Components in Stage II
B Additional Results
Additional Baseline Result Visualization
Additional Results on Getting Up From Sitting
On Stairs  Leaning Against Walls . . . . . .
Additional Robustness Test Against External
Turbulence
C Training Details
TABLE II: Reward components and weights in Stage I.
Penalty rewards prevent undesired behaviors for sim-to-real
successful getting up or rolling over.
EXPRESSION
Torque limits
1( t [ min,  max])
DoF position limits
1(dt [qmin, qmax])
Termination
1termination
DoF acceleration
DoF velocity
Action rate
DoF position error
1(hbase 0.8)  exp
0.05dt dtdefault
Angular velocity
Base velocity
Foot slip
1(F feet
Feet distance reward
exp(100 max(dfeet dmin, 0.5))
exp(100 max(dfeet dmax, 0))
Feet orientation
Feet height reward
exp(10  hfeet)
Getting-Up Task Rewards:
Base height exp
exp(hbase) 1
Head height exp
exp(hhead) 1
base height
Feet contact forces reward
1(F feet
> F feet
Standing on feet reward
(F feet> 0
hfeet < 0.2)
Body upright reward
exp(gbase
Soft body symmetry penalty
aleft aright
Soft waist symmetry penalty
Rolling-Over Task Rewards:
Base Gravity Error
1 cos base
cos base
gbasegtarget
gbasegtarget
Torso Gravity Error
1 cos torso
cos torso
gtorsogtarget
gtorsogtarget
Knee Gravity Error
(1 cos knee
left )  (1 cos knee
cos knee
gkneegtarget
gkneegtarget
Rewards Components in Stage I
Detailed reward components used in Stage I are summarized
in Tab. II.
Rewards Components in Stage II
Detailed reward components used in Stage II are summarized
in Tab. III.
TABLE III: Reward components and weights in Stage II.
Penalty rewards prevent undesired behaviors for sim-to-real
successful whole-body tracking in real time.
EXPRESSION
Torque limits
1( t [ min,  max])
Ankle torque limits
1( tankle [ ankle
Upper torque limits
1( tupper [ upper
DoF position limits
1(dt [qmin, qmax])
Ankle DoF position limits
1(dtankle [qankle
Upper DoF position limits
1(dtupper [qupper
Termination
1termination
DoF acceleration
DoF velocity
Action rate
Ankle torque
Upper torque
Angular velocity
Base velocity
Feet distance reward
exp(100 max(dfeet dmin, 0.5))
exp(100 max(dfeet dmax, 0))
Foot orientation
Tracking Rewards:
Tracking DoF position
(dtdttarget)2
ADDITIONAL RESULTS
Additional Baseline Result Visualization
Fig. 7 showcases a visualization of getting up from a prone
pose generated by the baseline method . This method
generates motion that is highly unstable and unsafe to deploy
in the real world. For example, its joints continuously jitter,
the feet are stumbling and the body keeps jumping up. This
indicates that this baseline  cannot be Sim2Real.
Additional Results on Getting Up From Sitting On Stairs
Leaning Against Walls
Fig. 8(a) and (b) show the simulation results of getting
up from additional initial postures: sitting on the stairs and
leaning against the wall. The results show that HUMANUP can
be generalized to more diverse initial postures in addition to
lying on the ground. Besides, we find that the convergence is
4 faster than getting up from lying. We argue that getting
Fig. 7: Getting-up from prone pose result visualization of Tao et al. . The motion generated by method  is highly
unstable and unsafe, and it keeps jittering and jumping during the getting-up phase.
(a-b) Getting Up From (a) Leaning Against Walls  (b) Sitting On Stairs
(c-d) Robustness Against (c) Thumps From A Hard Stick  (d) Object Hurling
Fig. 8: Getting up from additional initial poses and against real-world external turbulence. Getting up from (a) leaning
against walls, (b) sitting on the stairs. Robustness to external turbulence: (c) thumps from a hard stick, and (d) object hurling.
HUMANUP enables the robot to get up from initial poses other than lying, and is robust against external turbulence.
up from sitting or leaning is easier because of the additional
support from the ground, chairs, or walls.
Additional Robustness Test Against External Turbulence
Fig. 8(c) and (d) show the robustness test against external
results show that HUMANUP getting-up policy is practically
robust against certain external turbulence in the real world.
TRAINING DETAILS
In Stage I, we train the discovery policy f for overall 5B
simulation steps, and 20K simulation steps for the Stage II de-
ployable policy . Each stage uses a regularization curriculum
legged locomotion literature. All training is conducted on Isaac
environments on a single NVIDIA RTX 4090 or L40S GPU.
For the getting-up task, we slow down the discovered trajectory
to 8 seconds (8). We also tried 4 and 10. 4 leads to
large torques and DoF velocities, and 10 does not converge.
For the rolling-over task, the trajectory is slowed down to 4
seconds (selected through trials similar to the getting-up task).
We use flat terrains in Stage I and varied terrains during Stage
works [10, 32, 83] to apply varied dynamics randomization,
such as base center of mass (CoM) offset and control delay.
