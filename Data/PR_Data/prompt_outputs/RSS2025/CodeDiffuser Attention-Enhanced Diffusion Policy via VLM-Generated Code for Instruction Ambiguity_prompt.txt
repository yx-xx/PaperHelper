=== PDF文件: CodeDiffuser Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity.pdf ===
=== 时间: 2025-07-22 15:46:20.946525 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：via VLM-Generated Code for Instruction Ambiguity
Guang Yin3
Yitong Li4
Yixuan Wang1
Dale McConachie2
Paarth Shah2
Kunimatsu Hashimoto2
Huan Zhang3
Katherine Liu2
Yunzhu Li1
1Columbia University
2Toyota Research Institute
3University of Illinois Urbana-Champaign
4Tsinghua University
3D Attention Map
Hang a mug on a branch
Place the DUBAI somewhere
before or after the Istanbul
Put a battery on a slot
Hang a mug on a branch
Hang the red mug on the top
branch. Sorry, the green mug
Instructions with Different Ambiguity
Challenging Tasks Involving Language Ambiguity and Contact-Rich Manipulation
Language
Foundation
Low-Level
Fig. 1: CodeDiffuser leverages the code generated by Vision-Language Models (VLMs) as an interpretable and executable representation to
understand abstract and ambiguous language instructions. This generated code interfaces with Visual Foundation Models (VFMs) to compute
3D attention maps, serving as an intermediate representation that highlights task-relevant areas and communicates with the low-level policy.
Through extensive evaluations in both simulation and real-world settings, we demonstrate our methods effectiveness in challenging language-
conditioned robotic tasks involving language ambiguity, contact-rich manipulation, and multi-object interactions.
AbstractNatural language instructions for robotic manipula-
tion tasks often exhibit ambiguity and vagueness. For instance,
the instruction Hang a mug on the mug tree may involve
multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically
rely on end-to-end models that jointly handle high-level semantic
understanding and low-level action generation, which can result
in suboptimal performance due to their lack of modularity
and interpretability. To address these challenges, we introduce
a novel robotic manipulation framework that can accomplish
tasks specified by potentially ambiguous natural language. This
framework employs a Vision-Language Model (VLM) to interpret
abstract concepts in natural language instructions and generates
task-specific code  an interpretable and executable intermediate
representation. The generated code interfaces with the perception
Denotes equal contribution.
module to produce 3D attention maps that highlight task-relevant
regions by integrating spatial and semantic information, effec-
tively resolving ambiguities in instructions. Through extensive
learning methods, such as poor adaptation to language and envi-
ronmental variations. We show that our approach excels across
challenging manipulation tasks involving language ambiguity,
contact-rich manipulation, and multi-object interactions.
I. INTRODUCTION
Natural language instructions are often vague and ambigu-
ous when used to specify robotic tasks. As shown in Figure 2,
the instruction Pack the battery into the tray could involve
multiple feasible execution paths. However, current language-
conditioned imitation learning methods typically deploy end-
to-end models to jointly handle high-level semantic under-
Put one battery into
Put the right battery
into a slot
Put the right battery
into the left slot
Hang one mug on
one branch
Hang the red mug
on a branch
Hang the red mug
on the right branch
Pack Battery Task
Hang Mug Task
Fig. 2: Language Ambiguity for Task Specification. Natural language instructions for robotic tasks often exhibit inherent ambiguity and
vagueness. Consider the instruction Put one battery into a slota common task in factory settings. In the given scenario, this instruction
can be executed through multiple possible actions; the robot can choose from three available batteries and place one into any of the six
potential slots in the tray, resulting in a total of eighteen possible choices. Furthermore, language instructions can vary in ambiguity, ranging
from highly ambiguous directives to those that explicitly specify the target instance and goal location.
standing and low-level action prediction, which can lead to
suboptimal performance. In our experiments, we found that
existing diffusion policies converge to a notably low success
ratewell below practical usabilityon challenging tasks
involving language ambiguity, even with extensive amounts
of data [1, 2].
In this work, we propose a novel robotic manipulation
framework to accomplish tasks with potential language ambi-
guities. Building upon the key technical insight that code gen-
eration provides an interpretable and executable interface be-
tween the visual-semantic understanding of Vision-Language
Models (VLMs) and the dexterous physical capabilities of
visuomotor policies, we introduce CodeDiffuser in this work
to effectively handle language ambiguity and enhance seman-
tic understanding capabilities. Our system consists of three
major components: code generation using VLMs, 3D attention
computing using Visual Foundation Models (VFMs), and low-
level visuomotor policy. The VLM first processes both visual
observations and language descriptions and uses the provided
perception API to generate code that produces a 3D attention
features from foundational vision models such as DINOv2
and SAM [3, 4]. The 3D point cloud and attention map are
then fed into the visuomotor policy, which generates an action
trajectory to complete the task.
We conduct comprehensive experiments to analyze the
limitations of state-of-the-art imitation learning algorithms
when handling language ambiguities. Our results show that
performance declines as language ambiguity increases. Fur-
of demonstrations does not significantly enhance the ability to
address language ambiguity and vagueness.
To gain deeper insights into attention map conditioning, we
evaluate the language-conditioned 3D attention map and the
visuomotor policy separately. Both quantitative and qualitative
results show that 3D attention maps generated by VLM-based
code accurately align with human instructions and effectively
highlight task-relevant locations. Furthermore, extensive quan-
titative analysis demonstrates that the 3D attention map serves
as an effective representation, improving the visuomotor pol-
icys ability to handle language ambiguity. Finally, we evaluate
the entire system and show that CodeDiffuser successfully
performs challenging robotic manipulation tasks with language
ambiguity and vagueness.
In summary, our contributions are threefold: 1) We system-
atically evaluate the key limitations of state-of-the-art imitation
learning frameworks in scenarios with language ambiguity
and vagueness, such as pick-and-place tasks with multiple
possible objects and destinations. 2) We propose CodeDiffuser,
a novel robotic manipulation framework that addresses these
challenges using VLM-generated code as an interpretable and
executable intermediate representation. By interfacing with
perception APIs, it generates 3D attention maps to bridge
visual-semantic reasoning and low-level trajectory prediction.
3) We conduct extensive evaluations of individual modules
and the full system in both simulation and real-world tasks,
including contact-rich 6-DoF manipulation with multi-object
in handling language ambiguity.
II. RELATED WORKS
A. Imitation Learning for Robotic Manipulation
Imitation learning has achieved impressive results in dex-
terous robotic manipulation tasks, such as spreading sauces,
flipping mugs [1, 2], tying shoelaces , and more [635].
While existing imitation learning frameworks model policies
in an end-to-end mannerrequiring them to jointly under-
stand high-level semantics and predict low-level skillsthis
often results in suboptimal performance in complex scenarios
where instructions lack specificity. In contrast, our framework
leverages the visual-semantic reasoning capabilities of VLMs
to interpret potentially vague or abstract natural language in-
structions and generate codean interpretable and executable
intermediate representationthat interfaces with perception
APIs to provide structured inputs to the low-level policy.
B. Foundational Vision Model for Robotics
Foundational vision models have demonstrated impressive
zero-shot generalization capabilities in 2D vision tasks such
as detection, segmentation, and visual representation [3, 36
38]. Many robotic systems have adopted these models due
to their strong generalization ability [20, 21, 35, 3958].
Among these, GenDP and 3D Diffuser Actor are most relevant
to our work [40, 45]. While GenDP demonstrates category-
level generalization, it lacks the ability to understand natural
language instructions. In contrast, our framework is capable
of understanding potentially ambiguous natural language in-
structions by using visual-semantic reasoning capabilities of
VLM and generated code as an intermediate representation.
The 3D Diffuser Actor introduces a diffusion model that
predicts end-effector keyposes from point clouds augmented
with visual features from foundational vision models. In
3D attention map, which highlights task-relevant regions and
possesses much lower dimension compared to 3D Diffuser
Actor for easier visuo-motor policy training. Furthermore, 3D
Diffuser Actor predicts end-effector keyposes rather than a
continuous trajectory, which requires manual keypose defini-
tion for each task and is insufficient for tasks requiring smooth
trajectory control, such as stowing books. In contrast, our low-
level visuomotor policy predicts a smooth trajectory, providing
greater flexibility for diverse tasks.
C. Code Generation for Robotics
Due to their advanced visual-semantic reasoning and code
generation capabilities, LLMs and VLMs have been widely
applied to generate code for various robotic tasks, including
manipulation [41, 42, 5973], navigation [58, 74], and plan-
ning [75, 76], as reviewed in several surveys [7780]. Existing
works typically employ motion planning to generate trajec-
tories [41, 42]. While these methods demonstrate impressive
zero-shot performance, they lack the ability to learn skills from
human demonstrations. In contrast, our framework connects a
VLM to the low-level visuomotor policy via VLM-generated
capabilities of VLM with the smooth low-level control enabled
by learned policies.
Among LLM- and VLM-based approaches applied to
our work. They use a 3D voxel heatmap and 3D relational
code both geometric and semantic information, incorporating
various optimization methods to generate action trajectories
for downstream manipulation tasks. However, for certain
taskssuch as stowing books, which involve multi-object
interactionsinstantiating an optimization problem can be
challenging due to the complex dynamics involved. In contrast,
our approach learns a visuomotor policy from demonstrations,
offering greater flexibility in handling a wide range of tasks.
III. METHOD
A. Problem Statement
Our learning goal is to match the distribution given in a
dataset of collected demonstrations, i.e.,
L(ppred(aot, l), pgt(aot, l)),
with observations o O, task descriptions l L, and
actions a A. ppred(aot, l) represents the predicted action
distribution and pgt(aot, l) denotes the ground truth action
distribution provided by a human demonstration dataset. The
action distribution p(aot, l) can be decomposed as follows:
p(aot, l)
p(aot, l, z  zt)p(z  ztot, l)
p(az  zt)p(z  ztot, l),
where zt is a task-relevant latent representation of the state
such that p(aot, l, z  zt)  p(az  zt), i.e., zt contains
enough information about the observation and instruction to
predict the action.
We observe that in the presence of language ambiguity, the
distribution p(aot, l) is highly multi-modal. While single-task
imitation learning policies have recently shown great success
in modeling the multi-modal action distributions , ambigu-
ity in the instruction introduces an additional complexity. For
if the instruction is Hang one mug on one branch without
specifying the mug or branch instance, the probability of
each battery-slot pair is 118, imposing an additional axis
of multi-modality in the language instructions. The highly
multimodal distribution poses challenges for training an end-
to-end policy to model p(aot, l). In practice, we find that
learning a single end-to-end policy is suboptimal when dealing
with task ambiguity. Notably, we show in Section IV-B that
the current state-of-the-art methods can fail to achieve a high
success rate even with extensive training demonstrations.
Under this framework, we model p(az  zt) with modern
generative imitation learning methods and p(z  ztot, l) with
a VLM. We adopt a latent representation indicating task-
relevant areas to accomplish the task, i.e., a 3D attention
Hang the blue mug
on the left branch
Vision Language Model
Observation
find blue mug
mugsdetect("mug")
mugselname(mugs,"blue mug)
find left branch
branchesdetect("branch")
branchselpos(branches,"left)
Visual Foundation Model
Low-Level Policy
(a) Task-Specific Code Generation from VLM
(b) 3D Attention Map
from Perception APIs
(c) Low-Level Trajectory
Prediction
Fig. 3: Method Overview. CodeDiffuser consists of three primary components: code generation, 3D attention map computation, and low-
level policy. (a) CodeDiffuser first leverages the VLM semantic reasoning and code generation capabilities to understand human instructions
with potentially ambiguity and environmental observations, generating task-specific code. (b) This generated code interfaces with perception
low-level policy to generate actions accomplishing tasks with multi-object interaction, contact-rich manipulation, and language ambiguity.
map highlighting a specific mug-branch pair. We show that
this representation can be used to condition the imitation
learning algorithms, and can also be generated from language
by leveraging the reasoning capabilities of VLMs. We first
generate intermediate code from the instruction l and multi-
view RGBD observations ot RKHW 4, where K is the
number of camera views, and H and W represent the image
height and width, respectively, as described in Section III-B.
In Section III-C, we describe the API provided to the code
generation process used to construct our state representation
which predicts a sequence of 6D end-effector poses a for the
B. Code Generation
CodeDiffuser uses a VLM to translate a natural language
instruction into executable code that generates 3D attention
maps for downstream consumption by the visuomotor policy.
In our work, we leverage the VLMs few-shot generalization
and commonsense reasoning capabilities to generate percep-
tion code from a few provided examples. We first provide
a VLM, such as ChatGPT-4o , with several in-context
example code snippets for constructing 3D attention maps
using our perception APIs. At inference time, we input a task
description into the VLM, which then produces the code to
create the 3D attention map. The generated code takes as
input the RGB observations and outputs a 3D attention map.
to further invoke VLMs, enabling more advanced semantic
reasoning. For instance, as shown in Figure 3 (a), if the
instruction is Hang the blue mug on the left branch, the
generated code first detects all instances of mugs. It then
selects the blue mug instance from the detected mugs using
another call to the VLM. A similar process is applied for
for branches relies more on spatial relations, such as left
rather than semantic attributes like blue. The implementation
of perception APIs is detailed in Section III-C.
C. API for 3D Attention Map Generation
To provide the VLM with an API to bridge the semantic
meaning of an instruction and the corresponding 3D obser-
functions detect, selname, and selpos. Example us-
age of these functions is provided in-context during inference.
observations from cameras with known intrinsics and extrin-
sics and an object name as input and outputs a list of object
instances belonging to the object category. The detection
pipeline includes 3D feature extraction and object clustering.
We first extract 2D feature maps Wi RHW F of each
view using the DINOv2 model , where F is the semantic
feature dimension. Utilizing the depth images and camera
parameters from all K viewpoints, we fuse the 2D DINOv2
features into 3D space to obtain 3D point clouds P RM3
and associated 3D features F RMF , where M is the
number of points. In practice, we follow the implementation
introduced in D3Fields  to obtain the 3D point clouds and
their associated semantic features.
From a reference object, we annotate a set of relevant im-
RF . We observe that the annotation process is lightweight and
is only required when first setting up a task for training. By
comparing reference feature Fref with 3D semantic features
element in S represents how likely the point belongs to the
object category. Given the similarity S, we can cluster the
point clouds from the same object category into different
object instances using the density-based spatial clustering of
applications with noise (DBSCAN). Each clustering centroid
represents one object instance.
an input object list (obtained via detect) based on spatial
instructions requiring geometric understanding. To implement
this function, we invoke a VLM, which generates code to
compute distances or compare coordinates to identify the
target instance.
Fig. 4: Image Input to selname. After segmenting 3D instances and projecting them into 2D images, we concatenate the masked images
and overlay instance labels. The selname function takes this composite image as input and outputs the selected instance ID. This example
shows the input image for the book stowing task.
quiring semantic understanding, such as Bobs mug or blue
mug. Inside this function, it passes the current observations
into a VLM for instance selection. Concretely, we project 3D
instances to 2D images and plot their corresponding bounding
boxes. Then we feed the current observation overlayed with
instance bounding boxes into VLM for instance selection.
Given the selected object instances, we project them onto
2D images and prompt Segment Anything  using the pro-
jected 2D bounding boxes. Next, we fuse the 2D segmentations
from multiple views into 3D space to generate a 3D attention
map I RM, where M is the number of points. Each element
in I indicates whether the corresponding point belongs to
the task-relevant object instances. By concatenating the 3D
attention map I with the 3D point cloud P, we construct the
state representation z for the downstream visuomotor policy.
Since the generated code selects one instance from all valid
detected instances (e.g., all mugs), the computed 3D attention
map helps resolve the language ambiguity.
D. Visuomotor Policy Learning
Similar to Diffusion Policy [1, 82], we model our policy as
Denoising Diffusion Probabilistic Models (DDPMs). Instead
of predicting the action directly, we train a noise prediction
network  conditioned on state representation z:
bk  (ak, z, k),
where inputs are noisy actions ak, current state representations
z  (P, I), and denoising iterations k and outputs are the
noise bk. During training, we sample denoising step k and
noise k added to the unmodified sample a0. Our training loss
is defined as the Mean Square Error (MSE) between the added
noise k and the predicted noise:
L  MSELoss(k, bk).
At inference time, the policy begins with random actions aK
and denoises them over K iterations to generate the final action
predictions. At each iteration, the action is updated according
to the following equation:
ak (ak, z, k)  N(0, 2I)
In contrast to the original variant of Diffusion Policy, where
z is the RGB observation, our z is a state representation,
concatenating 3D point clouds P and 3D attention map I.
Since the 3D attention map can highlight object instances
relevant to the task instruction l, we adopt a 3D attention map
as an intermediate representation to bridge a high-level VLM
and a low-level visuomotor policy. Therefore, our framework
can leverage a VLMs reasoning capability and generate low-
level actions to accomplish the task following the language
instruction. We adopt PointNet  to process point cloud
To train the policy, we collect the human demonstrations
using a teleoperation interface in the real world or scripted
policy in the simulation. In addition, we include a lightweight
annotation process to label reference DINOv2 feature for
reference objects and to generate ground-truth 3D attention
maps. At inference time, our system predicts actions given
the current observations, instruction, and a list of reference
object features in the scene.
IV. EXPERIMENTS
In this section, we aim to answer four questions: (1) How
does the current imitation learning policy perform as task
ambiguity increases, and can more data alone resolve this
issue? (Section IV-B) (2) Does the 3D attention map generated
by VLM-generated code align with the language instruction?
(Section IV-C) (3) Is the 3D attention map a suitable rep-
resentation for the downstream visuomotor policy to handle
task ambiguity? (Section IV-D) (4) How well does the entire
system perform in both comprehensive simulation and real-
world evaluations? (Section IV-E.)
A. Experiment Setup
We use SAPIEN as the platform for large-scale simulation
experiments . For real-world robot experiments, we use the
ALOHA system for data collection and evaluation , along
with four RealSense cameras positioned around the workspace
to capture multi-view RGB-D observations.
We consider three practical tasks: Pack Battery, Hang
complex ambiguities, such as multiple picking and placing
options in the battery packing task. In addition to collecting
real-world demonstrations, we design a lightweight labeling
process to generate language instructions and 3D attention
maps for training the low-level policy.
Success Rate
(i) Hang Mug
Placing Options
Picking Options
DP (PCD)
DP (RGB)
(a) Success Rate under Increasing Task Ambiguity
(b) Failure Breakdown of Two Special Scenarios
(c) Success Rate Under
Increasing Demonstrations
(ii) Pack Battery
DP (RGB)
DP (PCD)
Success Rate
DP (RGB)
DP (PCD)
4x1 Options
1x12 Options
pick failure
transport
pick failure
transport
Fig. 5: Analysis of Existing Imitation Learning Algorithms. (a) We first evaluate three SOTA imitation learning algorithmsACT, DP
(RGB), and DP (PCD)on the Pack Battery task with increasing task ambiguity. Each element in the matrix represents the methods
success rate under the corresponding task setting, where columns indicate the number of empty slots and rows indicate the number of
batteries to pick. A rollout is considered successful if a battery is placed into any slot. All policies are trained on 180 demonstrations. For
simple tasks with no ambiguity (top-left entry), the high success rate confirms the validity of the baseline methods. However, as the number
of picking and placing options increases, success rates decline, highlighting the vulnerability of existing methods to task ambiguity. (b) We
further analyze failure patterns in cases with more picking options (41, bottom-left entry of (a)) and more placing options (112, top-right
entry of (a)) respectively. We observe that failure primarily occurs at the task stage with the highest ambiguity, demonstrating a strong
correlation between policy failure and task ambiguity. (c) While (a) and (b) examine existing imitation learning algorithms trained on 180
train and test on a mixture of 1 to 4 picking options with 1 placing option. For the Hang Mug task, we train and test on the scene with
2 mugs for picking and 3 branches for placing. We find that adding additional demonstrations in these settings often shows diminishing
returns at low success rates even with extensive demonstrations, indicating that additional training data alone may not resolve the problem.
For method evaluation, we report the task success rate,
where the success criteria are determined by the information
provided to the method. For methods conditioned on language
or attention, we consider a rollout successful if the task
is completed in the desired manner, such as successfully
following the language instruction or picking the highlighted
mug and placing it on the highlighted branch. For baseline
imitation learning methods without additional conditioning, a
rollout is considered successful if the base task is completed,
regardless of the specific route or mode of execution (e.g.,
as long as a mug is placed on a branch). We note that this
distinction results in a stricter evaluation metric for methods
conditioned on additional instructional information.
B. Analysis of Existing Imitation Learning Algorithm
In this section, we aim to study how well current imitation
learning algorithms can handle task ambiguities. Specifically,
we consider two state-of-the-art methods, Action Chunking
Transformer (ACT)  and Diffusion Policy (DP)  in
comprehensive simulation evaluations. For DP, we consider
two variants - DP with RGB inputs, denoted as DP (RGB),
and DP with point cloud inputs, denoted as DP (PCD). We
note that here we do not consider language inputs and focus
on studying the low-level policy capabilities.
In our first experiment, we evaluate ACT, DP (RGB), and
DP (PCD) on the Pack Battery task with increasing task
represents the methods success rate under the corresponding
task setting, where columns indicate the number of empty slots
to place the battery and rows indicate the number of batteries
to pick. A rollout is considered successful if a battery is placed
into any slot. All policies are trained on 180 demonstrations.
For the simplest task (i.e., top-left entry), there is only one
battery to pick and one fixed placing slot, meaning a single
picking option and a single placing option. For the most
ambiguous task (i.e., bottom-right entry), there are up to four
batteries and twelve empty slots to choose from, corresponding
to four picking options and twelve placing options. Task-level
ambiguity was gradually increased from the top left to the
bottom right.
We observe that all baseline methods perform well in the
simple task setting, demonstrating that the policy can success-
fully accomplish the task when no task ambiguity is present.
an increasing number of possible choices, the performance of
existing imitation learning algorithms degrades significantly.
This suggests that current imitation learning algorithms strug-
gle to handle task ambiguity.
casesfour picking options with one placing option (i.e., 41
options) and one picking option with twelve placing options
(i.e., 112 options)as shown in Figure 5 (b). As the number
of picking options increases, ACT and DP struggle to execute
the picking skill. Similarly, as the number of placement options
the task. The observed correlation between (i) increased task
ambiguity and (ii) declining task success rates further under-
scores the limitations of existing imitation learning algorithms
in handling task ambiguity.
In addition, we investigate whether increasing the number of
demonstrations can help existing imitation learning algorithms
handle task ambiguity. We conduct experiments on the Pack
Battery and Hang Mug tasks in simulation. For the Pack
Battery task, we train and test on a mixture of 1 to 4
picking options with 1 placing option. For the Hang Mug
and 3 branches for placing. The number of demonstrations is
increased from 30 to 540 episodes. While the performance of
ACT and DP initially improves, they generally show diminish-
ing returns while success rate is still low, and in some cases
plateaus as the number of demonstrations further increases,
suggesting that additional demonstrations may not effectively
resolve task ambiguity.
C. Evaluation of 3D Attention Maps
In this section, we evaluate the pipeline from language
instructions to 3D attention maps across different scenes and
instructions. Figure 6 illustrates our systems performance in
various scenarios. We observe that for a simple instruction
such as Put the rightmost battery in the slot on the left
tended battery instance and slot position. The VLM-generated
code can also perform zero-shot interpretation of language
exhibiting more complicated logical structures, such as self-
repairing phrases such as Hang the red mug on the top branch.
correctly highlight the green mug and top branch. Furthermore,
even with ambiguous commands like Hang a mug on a
branch (without specifying a particular mug or branch),
our system autonomously selects and highlights appropriate
objects. These results demonstrate the systems ability to
handle ambiguous or vague instructions while highlighting its
semantic understanding and capability to generate accurate 3D
attention maps across diverse instructions and scenarios.
In addition, we build a benchmark in the simulation to quan-
titatively evaluate the language-to-3D attention pipeline, which
can automatically generate scenes, prompts, and corresponding
ground truth 3D attention maps. We measure the distance
between ground truth 3D attention maps and generated 3D
Hang Mug
Pack Battery
Table I: Attention Quantitative Evaluation. We quantitatively
evaluate the pipeline from language instructions to 3D attention maps
in simulation. Our results demonstrate that our pipeline effectively
attends to task-relevant areas.
attention maps, and a test is considered successful if they are
close enough.
Table I summarizes our quantitative evaluation of the
pipeline from language to attention. Our method effectively
leverages the powerful visual-semantic understanding capa-
bilities of VLMs and benefits from explicit spatial relation
reasoning using 3D representations. Additional analysis and
visualizations of 3D attention failure cases are provided in the
supplementary materials.
D. Evaluation of Attention-Conditioned Diffusion Policy
In this section, we investigate whether 3D attention is a
suitable representation for visuomotor policy learning and
evaluate the pipeline from 3D attention maps to low-level
actions. We first evaluate our method by varying the number of
demonstrations on the Pack Battery task in simulation, as
shown in Figure 7 (a). We train and test on a scene consisting
of a mixture of 1 to 4 picking options and 1 placing option.
A policy rollout is considered successful only if it completes
the task as specified by the given groundtruth 3D attention
map. Figure 7 (a) shows that our systems success rate reaches
(>90) at approximately 120 demos, indicating that our
method can effectively handle task ambiguity. Additionally,
we increase task ambiguity and observe its effect on the
success rate, as shown in Figure 7 (b). Each entry in the
matrix represents the success rate under the corresponding task
and columns indicate the number of placing options. Figure 7
(b) demonstrates that our system is not significantly affected
by task ambiguity and performs well across both simple
and complex task settings. These results confirm that the
3D attention map is a robust representation for downstream
visuomotor policy learning in ambiguous task scenarios.
Pack Battery task across unseen scenarios in simulation.
In Figure 7 (c), each entry represents the success rate of
the trained policy in different testing environments, where
rows indicate training environments and columns indicate
testing environments. For example, the second row (i.e., 12)
corresponds to the training scenario with one picking option
and two placing options, while the third column (i.e., 13)
represents the testing scenario with one picking option and
three placing options. First, we observe that the success rate
along the diagonal is high, validating the expected perfor-
mance pattern where policies perform well on their original
training scenarios. Second, while training on a 1x1 scenario
does not generalize to scenarios with multiple placing options,
the generalization of CodeDiffuser quickly improves after
seeing more than one placing option at training time. The 3D
Hang a mug on a branch
Hang the red mug on the top
branch. Sorry, the green mug.
Put the rightmost battery in
the slot on the left column.
Put a battery on a slot.
Top-Down
Top-Down
Fig. 6: 3D Attention Maps Visualization. We visualize the 3D attention maps for corresponding instructions and scenarios. First, our 3D
attention maps successfully highlight the correct object instances even under ambiguous instructions, such as Hang a mug on a branch
or Put a battery on a slot, without specifying the instance. Furthermore, as instructions become more complex andor specific, the 3D
attention maps continue to attend to the correct instances, accurately matching the given instructions.
attention module enables the policy to focus on task-relevant
visual regions, thereby enhancing its ability to generalize
across diverse and previously unseen task settings.
E. Evaluation of the Entire System
In this section, we evaluate the entire robotic manipulation
quantitative and qualitative analyses. In the simulation, we
collect 180 training demonstrations for each task. We con-
sider two simulation tasks: Hang Mug and Pack Battery,
which involve language ambiguity, contact-rich manipulation,
and multi-object interactions. The Hang Mug task requires
picking between two mugs and placing them on one of four
from four batteries and placing them into one of twelve
slots. For the simulation experiments, we compare our method
against the following baselines:
Ours with 2D Attention: Instead of mapping the multi-
view RGBD observation into 3D space, this baseline uses a
2D attention mechanism to segment objects of interest. The
masked observation is then input into visuomotor policy.
Ours without Residual Connection: In our policy, we in-
clude a residual connection in PointNet for visual feature
extraction. This baseline ablates the residual connection to
evaluate its contribution to performance.
Lang-DP (RGB): This baseline extends DP (RGB) by
conditioning the policy on language using a frozen CLIP
encoder. The extracted language features are concatenated
with visual features to condition the diffusion policy.
Lang-DP (PCD): Similar to Lang-DP (RGB), this baseline
adds language conditioning to DP (PCD) using a CLIP-
based language encoder.
framework with language features, similar to Lang-DP.
Lang-ACT with 3D Attention: Unlike the original ACT,
which uses multi-view RGB observations, this baseline
inputs 3D attention maps into Lang-ACT to assess whether
the attention module consistently improves the performance
of base imitation learning algorithms.
The prompts are generated from randomly selected descrip-
tive components, such as right, furthest, and blue. These
components are incorporated into templates such as Put the
blue mug into the furthest slot. More specifically, all prompts
can be categorized into four types:
Prompt without slackness: All objects are strictly specified,
such as Hang the left-most mug on the top branch.
Prompt with full slackness: No objects are strictly speci-
Prompt with picked-object slackness: The placement lo-
cations (e.g., branch or slot) are strictly specified, while the
objects being placed are not, such as Hang a mug on the
left-most branch.
Prompt with placed-object slackness: The objects being
placed (e.g., mug or battery) are strictly specified, while the
placement locations are not, such as Hang the blue mug
on a branch.
Figure 8 (b) summarizes the quantitative results from the
simulation experiments. From this table, we draw the follow-
ing conclusions: (1) Adding the attention module significantly
enhances the policys ability to accomplish tasks involving lin-
guistic ambiguity. Compared to Lang-DP (PCD), our method
leverages VLM to interpret language instructions and compute
(i) Hang Mug
(ii) Pack Battery
Success Rate
(a) Success Rate Under Increasing Demonstrations
(b) Success Rate Under
Increasing Ambiguity
(c) Success Rate Under
Unseen Scenarios
Testing Scenarios
Training Scenarios
Placing Options
Picking Options
Fig. 7: Analysis of Attention-Conditioned Policy. (a) We first
examine the performance of the attention-conditioned policy under
varying numbers of demonstrations. A rollout is considered as suc-
cessful if the policy accomplishes the task as specified by the 3D
attention maps, a stricter criteria than that of Fig. 5. The training and
testing scenarios consist of a mixture of 1 to 4 picking options with 1
placing option. The success rate curve indicates that, given a sufficient
number of demonstrations, our attention-conditioned policy converges
to a high success rate. (b) We then analyze the performance of the
attention-conditioned policy under increasing task ambiguity. Similar
to Figure 5, each element represents the success rate, with columns
indicating the number of placing options and rows indicating the
number of picking options. The success rate remains consistently high
as task ambiguity increases, demonstrating that 3D attention serves
as an effective representation for the downstream visuomotor policy.
(c) Additionally, we find 3D attention improves policy generalization.
When trained on scenarios with lower ambiguity, such as 1 picking
option with 2 placing options (i.e., 12 in the row), the policy
generalizes well to scenarios with greater ambiguity, such as those
involving 3 or 4 placing options (i.e., 13 and 14 in the column).
3D attention maps, resulting in a substantial performance
improvement from 5.5 to 86.5. (2) The 3D attention
maps can also be integrated into other base imitation learning
algorithms to improve performance. For instance, comparing
Lang-ACT with 3D Attention to Lang-ACT, we observe a
significant performance increase from 6 to 61. (3) The
attention mechanism is also effective for 2D representations. In
contrast to Lang-DP (RGB), our method, which incorporates
a similar pipeline from language instructions to 2D attention,
achieves a performance improvement from 12 to 84.5. (4)
Our ablation study demonstrates that incorporating the residual
connection into PointNet improves performance from 61
to 86.5. This enhancement is attributed to the residual con-
nections ability to better propagate attention information into
the visuomotor policy, thereby improving trajectory prediction.
In addition, we evaluate the entire system on the Hang
world. We collect 150 demonstrations for each real-world task.
For the Stow Book taska challenging task that is difficult to
simulate due to its contact-rich naturewe test on scenes with
two available slots for placement. We note that while using 2D
attention achieves similar performance to using 3D attention
their robustness to environmental factors, as observed in DP3
and GenDP [34, 40]. We compare our method to Lang-DP
(RGB) and Lang-DP (Colored PCD), where the point cloud is
colored based on RGB observations.
Figure 8 (c) summarizes the quantitative evaluation results
from the real-world experiments. We find that our policy
consistently outperforms the baselines by leveraging VLM-
generated code as an interpretable and executable intermediate
ing capabilities of the VLM.
Figure 8 (a) presents our qualitative evaluation. Our policy
effectively handles task instructions with varying degrees of
specificity. For instance, the instruction Put a battery on a
slot. is ambiguous regarding the target battery and slot. Our
system interprets this ambiguous instruction, selects a spe-
cific instance, and successfully executes the task. In contrast,
another instruction with a similar initial object configuration
explicitly specifies the target battery and slot. Thanks to
the VLMs visual-semantic reasoning capabilities, our system
correctly interprets this precise instruction, identifies the ap-
propriate task-relevant locations, and successfully completes
the task.
In addition, we analyze the common failure cases of our
tern according to the pipeline modules, including code gener-
ation failure, perception failure, and task execution failure. We
observe that the majority of failure focuses on task execution,
while the code generation and perception are relatively stable.
V. CONCLUSION
Language ambiguity is a common challenge in robotic
where to place it for the instruction Hang a mug on
a branch. Existing imitation learning algorithms typically
employ end-to-end models that jointly interpret high-level
semantic information and generate low-level actions, often
resulting in suboptimal performance. In this work, we address
this challenge by introducing a novel robotic manipulation
framework that utilizes VLM-generated code as an executable
and interpretable intermediate representation. The generated
code interfaces with perception APIs to compute 3D attention
maps using VFMs, which are then used for downstream
visuomotor policy execution. Our modular design leverages
both the visual-semantic understanding capabilities of VLMs
and the smooth trajectory prediction of low-level policies.
In our experiments, we first identify the key limitations of
existing imitation learning algorithms. We then conduct a
comprehensive evaluation of our method in both simulation
and real world, and study the pipeline from language to 3D
attention maps, the pipeline from 3D attention maps to low-
level actions, and the entire system respectively. We demon-
strate CodeDiffusers effectiveness in challenging robotic tasks
(a) Qualitative Results
Ours w 2D Attn
Ours wo Res
Lang-DP (RGB)
Lang-DP (PCD)
Lang-ACT
Lang-ACT w 2D Attn
Hang Mug
Pack Battery
(b) Quantitative Results in Simulation
Total Suc
