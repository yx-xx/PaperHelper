=== PDF文件: You Only Teach Once Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations.pdf ===
=== 时间: 2025-07-22 16:07:02.833912 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：You Only Teach Once: Learn One-Shot Bimanual
Robotic Manipulation from Video Demonstrations
Huayi Zhou, Ruixiang Wang, Yunxin Tai, Yueci Deng, Guiliang Liuand Kui Jia
The Chinese University of Hong Kong, Shenzhen.
Harbin Institute of Technology, Weihai.
The Corresponding Author.
pullpush drawer  pick-place
pour water into the mug cup
pick up  unscrew the bottle
uncover the box with lid
open the delivery box
contact-rich long-horizon complex bimanual tasks (asynchronous)
non-prehensile bimanual tasks (synchronous)
Right Hand Moiton Trajectory
Left Hand Moiton Trajectory
Left Cam
Observation
Right Cam
Observation
Binocular
Left Arm
Right Arm
Parallel
Grippers
Fig. 1: Our proposed YOTO (You Only Teach Once) facilitates various complex long-horizon bimanual tasks. It needs only
a one-shot observation of a single third-person binocular camera to extract the fine-grained motion trajectory of human hands,
which can then be utilized for the dual-arm coordinated action injection and rapid proliferation of training demonstrations.
AbstractBimanual robotic manipulation is a long-standing
challenge of embodied intelligence due to its characteristics of
dual-arm spatial-temporal coordination and high-dimensional
action spaces. Previous studies rely on pre-defined action tax-
onomies or direct teleoperation to alleviate or circumvent these is-
teaching bimanual manipulation is learning from human demon-
strated videos, where rich features such as spatial-temporal
transitions are available almost for free. In this work, we propose
the YOTO (You Only Teach Once), which can extract and then
inject patterns of bimanual actions from as few as a single
binocular observation of hand movements, and teach dual robot
arms various complex tasks. Furthermore, based on keyframes-
based motion trajectories, we devise a subtle solution for rapidly
generating training demonstrations with diverse variations of
manipulated objects and their locations. These data can then
be used to learn a customized bimanual diffusion policy (BiDP)
across diverse scenes. In experiments, YOTO achieves impressive
performance in mimicking 5 intricate long-horizon bimanual
spatial conditions, and outperforms existing visuomotor imitation
learning methods in accuracy and efficiency. Our project link is
I. INTRODUCTION
Bimanual manipulation is an enduring topic in the robotics
community [5, 68, 82, 35, 91, 16, 21]. It has been widely
involved in many other fields such as bionics, high-end
computer vision. Despite this, achieving efficient, precise and
robust manipulation of dual-arm robots to accomplish various
daily tasks remains a difficult research area. Generally, there
are two main challenges: coordination and state complexity
[32, 51]. On the one hand, the two arms working together
need to move alternately or simultaneously in a coordinated,
non-procrastinated manner and avoid collisions with the scene
or each other. This places stringent demands on the control
and scheduling scheme. On the other hand, the total degrees
of freedom of two arms and their respective end effectors
are distributed in a higher-dimensional space than a single
arm. This makes the design of motion planning and action
prediction more challenging. Given these difficulties, it is no
small feat to drive two robot arms to perform tasks that human
toddlers can do with ease, such as uncovering lids, assembling
blocks and lifting large-size objects, let alone mastering many
more complex long-horizon skills.
The mainstream bimanual manipulation research includes
two major branches: explicitly classifying tasks based on pre-
defined taxonomy [82, 45, 32, 51] and implicitly learning from
demonstrations collected by teleoperation [104, 65, 62, 53].
The former often fails to uniformly cover arbitrary tasks
and also limits the flexibility of the robot arm. While the
latter requires substantial training data which is inconvenient
to scale up. And collected demonstrations are intrinsically
non-stationary and despatialized, which is not conducive to
training robust and generalizable action policies. In addition
to taxonomy and teleoperation, an indirect but more plausible
and interpretable route is to learn from human action videos
[4, 107, 25, 13, 47, 69, 42]. This route is based on relatively
mature vision techniques to process human demonstrations and
extract high-level features for generating robot manipulation-
relevant elements. In this paper, we also follow this promising
path. Our dual-arm workbench, hardware settings, and selected
bimanual tasks are shown in Fig. 1. And the overall framework
is shown in Fig. 2.
cluding their location, left-rightness, 3D shape, joints, pose,
using hand-related vision methods [71, 78, 67]. After extract-
ing hand motion trajectories, we do not simply inject step-
wise actions into robots. Because visual perception results
are inevitably erroneous, and real hand motions are jittery
and discontinuous. We thus simplify the consecutive trajectory
into discrete keyframes [38, 80], and assign the corresponding
keyposes to two arms to execute by applying inverse kine-
matics interpolation. Besides, we also record and replay the
order of dual-hand movements (termed as motion mask), which
can help to address the dual-arm coordination issue in long-
horizon bimanual tasks. Now, we successfully obtain a stable
and refined manipulation motion exemplar.
More than that, thanks to the editability of obtained single
demonstrations. First, we change the 6-DoF pose of task-
related objects and adjust corresponding keyposes to let real
robots replay similar actions. Objects can also be replaced with
other ones of analogous shape and size. This auto-rollout oper-
ation is stable and much faster than teleoperation [104, 86]. For
based on a well-taught task. On the other hand, after knowing
the reachable area of manipulators, we can perform geometric
transformation on segmented object point clouds, which can
be extracted by using open vocabulary segmentation [90, 73]
and binocular stereo matching [92, 93]. Such augmentation is
more reliable and efficient than rollout. Therefore, mixing the
above two data expansion schemes, we call it proliferation,
just like the generation of cells.
With sufficient training data, we follow diffusion-based
visuomotor imitation methods [15, 98, 95] and propose a
specialized bimanual diffusion policy (BiDP), which is cus-
tomized for learning long-horizon dual-arm tasks. It has three
major improvements. First, we reduce observations (e.g., 3D
point clouds) from the entire scene to manipulated objects to
accelerate training convergence and eliminate irrelevant terms
[26, 51]. Then, instead of modeling continuous actions, we
choose to predict essential keyposes [55, 89, 41, 99], which
can greatly decrease the diffusion space dimensionality. Third,
we utilize the motion mask to determine the alternating or
synchronous dual-arm moving, and reorganize the bimanual
action space to train a unified action policy. In experiments,
we have verified the high efficiency and effectiveness of BiDP
on challenging bimanual tasks.
We present a paradigm for extracting and injecting dual-
arm movements from a one-shot observation of human
hands demonstration, which supports the fast transfer of
bimanual manipulation skills to two robotic arms.
We develop a solution for rapidly proliferating training
demonstrations based on one-shot teaching, which is
more convenient and reliable than teleoperation.
We propose a dedicated bimanual diffusion policy (BiDP)
algorithm that can efficiently and effectively assist dual-
arm manipulators in imitating complex skills.
Our framework YOTO is compatible with most bimanual
tasks. We verified its effectiveness and superiority on
5 complex long-horizon manipulation tasks (including
synchronous and asynchronous).
II. RELATED WORKS
A. Bimanual Robotic Manipulation
Many bimanual manipulation methods focus on specialized
tasks or primitive skills, such as cloth-folding [56, 6, 19, 88,
[49, 4], throwingcatching [37, 94, 48], scooping , carrying
and dressing . For general bimanual manipulation,
typical research [82, 35, 61, 45, 33, 106, 97] tends to explicitly
classify them into uncoordinated and coordinated, or symmet-
rical and asymmetrical according to task characteristics. Some
homologous approaches assume that two arms form a leader-
follower [52, 32] or stabilizer-actor [29, 51] pair. Most re-
bimanual manipulation by dexterous teleoperating and upgrad-
ing low-cost hardwares of real-world robotics. These similar
works [104, 84, 43, 62, 53, 7] implicitly train an end-to-end
imitation network using massive and diverse teleoperated data,
expecting to get generalized large robotic models. To further
improve dual-arm reachability and dexterity, some studies have
equipped multi-finger hands [50, 86, 79, 20, 14, 23], mobile
footplates [96, 95, 102, 24], tactile feedbacks [50, 20, 12] or
active cameras [17, 14]. In contrast to them, our manipulators
are two fixed-base robot arms with parallel-jaw grippers. We
propose an universal framework that learns bimanual policies
with considering the dual-arm coordination. And the training
data is not collected via teleoperation but proliferated from a
single-shot demonstration.
B. Learn from Human Hand Videos
Human hand videos are valuable resources for learning
complex manipulation behaviors [30, 22, 100, 54, 31]. Ex-
tensive research has leveraged human demonstrations to learn
robot manipulation by extracting rich non-privileged features,
such as keypoints [66, 25, 87], affordances [46, 101, 63], 3D
hand poses [47, 42, 4], motion trajectories [47, 42, 13, 101]
and invariant correspondences [69, 44, 103]. These features
can be tailored to robot-specific variables to alleviate mor-
phology gaps, such as manipulation plans, retargeted motions
Demonstration
(Binocular
Stereo Cam)
segmented
manipulated
Extracted
High-Level
Features
(on Left Cam)
3d hand meshes
hand moiton trajectories
Injected
Keyframes-
based Robot
Components
Refined    Post-Processing
Extraction
Injection
Vision    Techniques
timeline
move move hold move
hold move
hold move hold move move move move move
Auto-Rollout
Verification in
Real-World
Geometric
Transformation
of Point Clouds
Rotation
Translation
Augmentation
Variations
in Location
Category
One-shot
Teaching
Objects Point
Robot State
Bimanual
Diffusion
Predicted
Training
Deployment
Binocular
Vision       Techniques
Objects Point
Predicted
Robot State
Timestep
Fig. 2: The overview of our proposed YOTO. It is a general framework consists of three main modules: (a) the human hand
motion extraction and injection, (b) the training demonstration proliferation from one-shot teaching, and (c) the training and
deployment of a customized bimanual diffusion policy (BiDP). It is best to zoom in to view the details.
and precise actions. Two contemporary works [42, 47] also
propose to use a single human demonstration to learn bimanual
manipulation similar to us. RSRD  roughly recovers 3D
part motion of articulated objects from a monocular RGB
capture arbitrary object in 3D space. OKAMI  applies
the object-aware motion retargeting which is noisy and non-
scheme which is more robust and versatile.
C. Visuomotor Imitation Learning
Visuomotor imitation learning aims to train action predic-
tion policies based on visual observations by exploiting labeled
demonstrations [57, 40, 58, 39, 85, 38, 80]. These learned
policies can drive robots to complete various manipulation
with just dozens of demonstrations, covering long-horizon
[57, 85], dexterous [98, 86] and bimanual [104, 79] tasks.
transformers (ACT) to learn high-frequency controls with
closed-loop feedback in an end-to-end manner. Chi et al.
adopted conditional denoising diffusion models [36, 83, 64]
to represent visuomotor policies in robotics, exhibiting im-
pressive training stability in modeling high-dimensional action
distributions. Ze et al.  incorporated 3D conditioning into
the original diffusion policy , rather than focusing on RGB
images and states as conditions. Yang et al.  combined
SIM(3)-equivariance [96, 76, 8] with diffusion policy, ac-
quiring a more generalizable and sample-efficient visuomotor
policy than [15, 98]. Inspired by them, we propose a bimanual
diffusion policy (BiDP), which adds motion mask as a new
diffusion condition and simplifies visual observations to task-
related object point clouds, making it suitable for learning
bimanual manipulation tasks.
III. HARDWARE SYSTEM
Dual-Arm Placement: Most human video-inspired biman-
ual manipulation works apply humanoid robots [4, 25, 47, 69,
42] or two ipsilateral arms [107, 25] to build workstations.
Some bimanual teleoperations also tend to be anthropopathic
[50, 79, 14, 23] or ipsilateral [86, 12, 20]. Despite the
similarity to human morphology, they are not necessarily op-
timal. Comparatively, it is possible to place two manipulators
opposite each other, as in ALOHA series [104, 24, 1, 105] and
its followers [17, 53, 7]. This heterolateral setup minimizes
the overlap of accessible space and is thus compatible with a
wider range of bimanual tasks. We also adopt the contralateral
placement as shown in the left of Fig. 1, where each arm (Aubo
i51) has a span of approximately 880 mm.
End Effector Selection: Although some methods utilize
multi-fingered dexterous hands as end effectors [86, 79, 14, 23]
and even add tactile sensors [50, 20, 12] to the hands, we still
use two parallel-jaw grippers (with max opening distance 80
mm of each DH-Robotics2), which are easier to control and
interpret. We will show that it is sufficient to complete complex
tasks that are non-prehensile or synchronous.
Camera Observation: Many previous methods adopt the
multi-view RGB observations [104, 53, 7], mainly including
the global third-person camera and the local eye-in-hand
camera. Other works have shown that a single third-person
RGB-D camera [86, 4, 25, 47] is also acceptable. We use a
binocular stereo camera (the DexSense 3D industrial camera3),
similar to commercial RGB-D cameras, but providing raw left
and right images to enable flexible post-processing.
IV. METHOD
In this part, we introduce in detail the proposed framework
in Fig. 2. We firstly give a basic definition of the problem
in Sec. IV-A. Then, a detailed explanation of the three core
modules is presented, which includes the standardized hand
motion extraction and injection process in Sec. IV-B, the
demonstration proliferation solution from one teaching in
Sec. IV-C and the proposed visuomotor bimanual diffusion
policy (BiDP) method in Sec. IV-D.
A. Problem Formulation
In this paper, we mainly consider bimanual robot manipula-
tion tasks, where the agent (e.g., dual manipulators equipped
with parallel-jaw grippers) does not have access to the ground-
truth state of the environment, but visual observations O from
a binocular camera and robots proprioception states S. As for
the action space A  {ap R3, ar SO(3), ag {0, 1}},
it includes the target 6-DoF pose of each robot arm and
the binary openclosed state of the gripper. Note, we focus
on bimanual tasks sharing the same observations O. For the
difference between left and right hands below.
For imitation learning, the agent mimics manipulation plans
from labeled demonstrations D  {(O, A)i}N
is the number of trajectories, O  {Ot, SL
observations of all T steps, and A{AL
t1 are actions
to complete the task. The learning objective can be simply
concluded as a maximum likelihood observation-conditioned
imitation objective to learn the policy :
E(O,A)iD
t0 log (A
strations proliferated from only a single-shot human teaching
and how to improve existing diffusion-based imitation policies
for addressing the bimanual manipulation problem.
B. Hand Motion Extraction and Injection
This part corresponds to the module in Fig. 2 (a).We first
manually demonstrate a long-horizon bimanual task using two
hands on the dual-arm accessible operating table. Then, we
leverage favourable vision techniques to extract rich manip-
ulation features from recorded videos by a single binocular
camera. Extracted features will be post-processed to obtain
keyframes-based motion variables (such as 6-DoF poses and
gripper states) that can drive dual arms.
1) Human Demonstration Capturing: By default, we cap-
ture dual-stream synchronized RGB videos with slight nec-
essary visual difference between left and right cameras to
estimate disparity and depth map. We mainly observe the left
RGB view to extract a series of hand-related features, and
thus always keep both hands visible to the left camera. The
right view is only awakened when accurate 3D information is
needed in a particular frame. This reduces the computational
burden of stereo matching  by at least half.
Algorithm 1 3D Hand Pose Calculation.
hand joints Ihand, index numbers of wrist joint iwri  index-fingertip
iind  ring-fingertip iring, the given chirality  L or  R.
either L or R
Initialize P
j MANO(H
3D hand joints indexing
j[iwri], pind P
j[iind], pring P
j[iring];
liw (pind pwri), lrw (pring pwri);
two 3D lines
vz CROSS PRODUCT(liw, lrw);
Z-axis direction
vz vz(NORMALIZE(vz)  1e-8);
vector normalization
vy  lmid (liw  lrw)2.0;
middle line (Y-axis direction)
vy vy(NORMALIZE(vy)  1e-8);
vector normalization
vx CROSS PRODUCT(vy, vz);
X-axis direction
vrot CONCATENATE([vx, vy, vz]);  final 33 rotation matrix
return vrot;
2) High-Level Features Extraction: Given a video demon-
stration (the left stream) of one specified bimanual task, we
run our vision perception pipeline to obtain the 3D point
trajectories and status of two hands.
3D point trajectories. We first use WiLoR  to detect
bounding boxes of left and right hands in each frame and
then estimate their 3D shapes HL and HR represented by
MANO . Then, we simply track the center point hp,
j ) of each hand and obtain the 3D hands sequence
index among all J frames. The hp,
can be calculated by
averaging several selected points (e.g., five finger tips) from
21 pre-defined joints of the 3D hand model H
As of here, many similar works [42, 47, 14, 23] choose to
retarget the produced continuous trajectories {hp,
j1 to their
end effectors through estimated 3D geometric transformations.
algorithms in left-right classification and 3D shape regression,
we cannot fully trust trajectories directly derived from them. In
achieve continuous and consistent prediction in a given camera
space. This is also pointed out and verified by DexCap .
More examples can be found in Fig. 5. As an alternative, we
propose to project all 3D points {hp,
j1 onto the 2D image,
and then lift these points to 3D by applying the stereo matching
algorithm . The final back-projected 3D point trajectories
are {bhp,
given camera space.
States of two hands. In order to fully map hand movements
to two-fingered grippers, we also need to determine the 3D ori-
entations hr,
and openclosed states hg,
by further observing
3D hands H
j. Here, we can estimate the openclosed state by
detecting if the hand is in contact with an object . If there
is contact, the hand is considered closed (hg,
0), otherwise
open (hg,
1). This is more trustworthy than relying solely
on hands to estimate status. For calculating 3D hand poses
We summarize this process in Alg. 1. To this point, we have
Fig. 3: A detailed example of extracted motion trajectories
with corresponding keyframes of both left hand and right hand.
It is best to zoom in to view the details.
obtained the rough motion trajectories purely based on human
hand videos {(bhp,
cluding the vision-language model Florence-2  and SAM2
) to extract segmented manipulated objects from the left
initial image as our disturbance-free visual observations bO,
which will be further lifted to 3D point clouds eO by applying
stereo matching approaches [92, 93].
3) Robot Actions Injection: Although we have obtained
robot-oriented motion trajectories, their validity and usability
are still concerns. For example, some target poses may be
unreachable for the failed inverse kinematics. Due to agnostic
approach is to replay and verify the rationality of each action
step by step directly on real robots, but this choice is unsafe
and inefficient, considering that the total number of frames J
is usually about 100 to 200.
Keyframes-based motion actions. To this end, we turn to a
more reasonable and safer post-processing, namely keyframes-
based motion simplification and injection. Specifically, we
inherit the abstraction of a consequent demonstration into
discrete keyframes (a.k.a. keyposes) as in C2FARM
and PerAct . Keyframes are important intermediate end-
effector poses that summarize a demonstration and can be
auto-extracted using simple heuristics, such as a change in
the openclose end-effector state or local extrema of veloc-
ityacceleration. This concept is widely used in long-horizon
manipulation studies [55, 89, 41, 99]. Accordingly, we can
just learn to predict the next best keyframe, and use a
sampling-based motion planner to reach it during inference.
We thus simplify trajectories {(bhp,
j1 into a set
of keyframes {(ehp,
K keyframes. K is around 10 in our tasks (K J), which
makes it much more easier to quickly verify and correct errors.
To inject these keyposes into the dual-arm robot, we need
to transform them from the camera coordinate to the robot
coordinate using the pre-measured hand-eye calibration trans-
formation matrix. Usually, a real-robot verification takes about
three minutes. We finally update the verified trajectories into
eA{(eap,
injected K robot actions. An elaborate example of extracted
keyframes is shown in Fig. 3.
Derivation of motion mask. Additionally, we should al-
ways care about the dual-arm spatial-temporal coordination,
which is one of the core issues of bimanual manipulation.
have a time record in every frame, which represents the refined
keyframes-based set eA naturally contains detailed timestamps.
Based on it, we can thus derive the corresponding coordination
strategy C  {(CL
the motion state of a robot arm at the k-th keyframe. The
binary value 0 means holding on, 1 means moving on. Given
this particularity, we name it motion mask to schedule robot
motion. A specific illustration of C for the pull drawer task
can be found in the down-left corner of Fig. 2. This example
is broadly applicable to strictly asynchronous bimanual tasks
(e.g., CL
k ). While, for fully synchronous manipulation
k and CR
k in C keep the same. Currently, we
do not consider those long-horizon tasks where synchronized
and asynchronized keyframes are mixed.
In the following, we show that the extracted fine-grained
keyframes-based motion actions eA along with the correspond-
ing motion mask C will continue to play a vital role.
C. Demonstration Proliferation from One Teaching
Based on the one-shot teaching, we propose two demonstra-
tion proliferation schemes, the automatic rollout verification
of real robots and point cloud-level geometry augmentation of
manipulated objects. This solution is an efficient and reliable
route to quickly produce training data for imitation learning.
An example is shown in Fig. 2 (b).
1) Auto-Rollout Verification in Real-World: Formally, our
refined keyframes-based robot actions eA are interpretable
and editable. These properties assist us to conduct automated
demonstration rollout verification and collection on real robots.
and eAR belonging to the left and right robotic arms based
on the motion mask C. Below is for decomposing strictly
asynchronous tasks.
k  1, CR
k  0, CR
eAL   eAR
where we actually eliminate K redundant keyposes for unilat-
eral arm waiting (holding on actions). For synchronous tasks
( eAL   eAR  K), we always have to drive both arms, so
there is no need to apply the motion mask.
The above allows two arms to disengage smoothly. Then,
we can precisely edit any keyframe in eAL or eAR closely
related to the manipulated object to align with its changed
TABLE I: The time comparison of different data collection
or expansion methods. We report the average completion time
for 3 tasks, 10 valid trials in total for each task. The  means
it can be achieved by directly modifying the script.
Operators
Long-Horizon Bimanual Tasks
drawer (s)
water (s)
bottle (s)
Master-Slave
DragDrop
Auto-Rollout
Geo-Trans
keypose in real-world. We still take the pull drawer task (with
10 keyframes) as an example. When moving the object picked
up by the left arm, we need to adjust the 6-th keypose eaL
). For example, if we move the object 5 cm
along the X-axis positive direction, we then just add an offset
(0.05, 0.00, 0.00) to the position part eap,L
. Moreover, we can
also replace objects with similar shapes in the same position
to expand category diversity. Finally, we conduct the rollout
to get a new demonstration. The same is true for adjusting the
drawer manipulated by the right arm. Regardless of simplicity,
we compared auto-rollout with two popular data collection
the comparison. The other two ways are hampered by multi-
operators and higher failure rates.
2) Geometric Transformation of Point Clouds: Regarding
the above expansion of object positions and categories in real-
to reliably augment visual observations of manipulated objects
(the extracted 3D point clouds eO) any number of times, so
that theoretically infinite demonstrations can be obtained. In
the auto-rollout stage, we have initially figured out the cor-
respondence between manipulated objects and their relevant
keyframes. Now, we can perform geometric transformations
(mainly controlled rotations and translations) on the objects
at the point cloud level, and update the 6-DoF values in
the corresponding keyframes. In this way, matching pairs of
visual observations eO and keyframes-based actions eA can be
generated in batches, forming a series of new training data,
which no longer need to be verified in real robots. It should be
noted that the geometric transformation of eO is restricted, that
rational moving range of manipulated objects can be measured
during the auto-rollout phase incidentally. In Tab. I, we have
added the time comparison of this data proliferation, which
maintains the highest efficiency.
D. Bimanual Diffusion Policy Learning
In this part, we adapt popular visuomotor diffusion policies
[15, 98, 95], and propose a customized bimanual diffusion
policy (BiDP) to enable fast and robust imitation of long-
horizon tasks. We firstly shrink the input observations into
task-relevant object point clouds, allowing the policy model
to converge quickly and resistant to interference. Additionally,
we devise a motion mask to unify the action prediction and
address the dual-arm coordination problem.
Bimanual dataset composition. According to the defi-
nition in Sec. IV-A, we rewrite the training set as eD
{( eO, eA, C)i}N
C is the motion mask containing coordination strategies. eD is
generated by applying our proposed data proliferation solution
to expand the seeding one-shot teaching to get a large dataset
with hundreds or thousands of trajectories. Here, we update
eO{ eOk, SL
k1 and eA{(eap,
eOk is the observation containing 3D point clouds of manip-
ulated objects instead of the entire RGB image  or point
clouds scene [98, 95]. SL
k and SR
k are robot proprioception
states with similar formats as actions S
have discrete keyposes, rather than continuous and dense robot
states. Learning to predict keyposes is common in robotic
manipulation [55, 89, 41, 99]. The policy needs to learn a
mapping from the initial observation eO1 to all subsequent
keyposes eA for two arms. The history horizon and prediction
horizon is 1 and K, respectively. In evaluation, the policy
predicts all actions to be executed conditioned only on an one-
shot observation { eO1, SL
1 } at first sight.
Diffusion-based policy representation. Similar to [15, 98],
we utilize Denoising Diffusion Probabilistic Models (DDPMs)
to model the conditional distribution p( eAk eOk). Starting
from the random Gaussian noise eAT
actions with decreasing levels of noise, gradually from eAT 1
k. This process follows:
k ( eOk, eAt
The policy finally outputs eA0
k. Because point clouds are
used as the visual input instead of RGB images, we adopt
more robust SIM(3)-equivariant architectures [96, 95], rather
than policies based on CNNs  or transformers .
predicts the gradient E( eAk) for denoising the noisy action
input. It first uses a modified PointNet-based  encoder
with SIM(3)-equivariance to encode visual observations. The
encoded visual features and positional embeddings of t are
passed to FiLM layers . Then, the policy network applies a
convolutional U-Net  to process eAk, t and the conditioned
observations to predict denoising gradients. Note that eOk, eAk
k are processed to be invariant to scale and position.
Above-mentioned FiLM layers, convolutional U-net, and other
connecting layers are also modulated to be SO(3)-equivariant.
Please refer to [96, 95] for more details.
Customized bimanual diffusion policy. Since eAk and
k contain dual-arm actions in our task, it is important to
preprocess them appropriately. A vanilla approach is to pre-
dict all actions in each keyframe, including (eap,L
and (eap,R
). This not only needs to re-splice the
based policy network accordingly, but also learns redundant
actions for asynchronous tasks (as pointed out in Sec. IV-C),
3 drawers
9 daily objects
6 bottles
6 bottles  6 caps
4 delivery boxes
5 covered boxes
pull drawer
pour water
unscrew bottle
uncover lid
open box
Fig. 4: We collected a variety of manipulated objects in instance-level for each of five bimanual tasks to improve and verify
the generalizability of trained policies. All of these objects are from everyday life, not intentionally customized.
which is inefficient and error-prone. To this end, we reorganize
the action space into A  { eAL, eAR} based on the motion
mask C according to Eqn. 2. A contains a series of time-
ordered single-arm actions, which is a mixture of the left
and right with removing potential redundancy. Taking the pull
drawer task as an example, a demonstration consists of 10
keyframes { eAR
10}. For
synchronous tasks, the left and right sides appear alternately.
In this way, we unify the policy network form of bimanual
mentation details are in supplementary materials.
V. EXPERIMENTS
We aim to answer the following research questions. Q1:
What is the quality of our extracted hand motions? Q2: Can the
various strategies introduced in the YOTO framework enable it
to better learn bimanual manipulation policies? Q3: Do trained
BiDP models generalize outside of the in-distribution domain?
variety of long-horizon complex tasks?
A. Experiment Setups
1) Tasks: We evaluate YOTO on five real-world bimanual
lectively encompass two types of dual-arm collaborations:
strictly asynchronous and synchronous. The manipulated ob-
jects in these tasks might be rigid, articulated, deformable or
non-prehensile. They also involve many primitive skills such
as pullpush, pickplace, re-orient, unscrew, revolve and lift
up. Some skills must require both arms to complete. More
are quite complex due to containing multiple substeps. In the
pull drawer: A drawer and a daily pocketed object.
It consists of 6 substeps including stable the drawer (L), pull
the drawer (R), pick up the object (L), place the object into
the drawer (L), stable the drawer (L), and push the drawer (R).
TABLE II: Detailed statistics of five bimanual tasks. The
means we only count these auto-rollout demonstrations.
Task Names
Is Synchronous?
Manipulated Objects
Substeps
Keyframes
Avg. Duration (s)
Categories
Demonstrations
pour water: A capless bottle with water and an empty
mug. It consists of 6 substeps including pick up the mug (R),
pick up the bottle (L), bring the mug close to the bottle (R),
pour water in bottle into the mug (L), put down the bottle (L),
and put down the mug (R).
unscrew bottle: A capped bottle with water. It
consists of 5 substeps including pick up the bottle (L), bring
the bottle close to the right arm (L), unscrew the cap (R), put
down the cap (R), and put down the bottle (L).
uncover lid: A rectangular box with a top covered
lid and no handles. It consists of 3 substeps including go to
the lower middle part of the lid (LR), lift up the lid (LR), and
put down the lid to one side (LR).
open box: A delivery box with four handleable wings.
It consists of 4 substeps including go close to the two vertical
wings (LR), flick open two wings (LR), go close to the two
horizontal wings (LR), and flick open two wings (LR).
The statistics of these tasks are in Tab. II, where the number
of keyframes is counted based on the one-shot teaching.
Examples of each task are shown in Fig. 1 and Fig. 7.
2) Demonstrations: Imitation learning requires sufficient
training data, including diverse verified task trajectories, to
learn a closed-loop action prediction policy. To this end, as
described in Sec. IV-C, we start from a single-shot teaching
of every task and collect a considerable number of demonstra-
tions via the proposed rapid proliferation solution. Moreover,
to improve and evaluate the generalization of learned policies,
we have collected multiple objects within each task. All related
assets are shown in Fig. 4.
collect real robot data. We set 3 (for tasks pull drawer
and pour water) or 9 (for the other three tasks) position
variations for each manipulated object, and replace all al-
ternatives from the assets in each position. In this way, we
get training data with diverse positions and categories. The
demonstration number of every task is in the last row of
Tab. II, where we added statistics on their average duration.
We then processed these data into the form suitable for BiDP,
including extracting 3D point clouds of manipulated objects
and saving the corresponding multi-step end-effector keyposes.
Note that we also recorded the complete binocular video
observation and continuous robot actions during each auto-
methods [104, 15, 98, 95] for comparison. Next, we applied
3D geometric transformations to each demonstration, acting
only on task-relevant object point clouds. These synthetically
augmented data are only applicable to our proposed BiDP
algorithm. After formulating the script, we finally expanded
the data volume by 100 times, which results in 5K24K
trajectories per task. This magnitude is comparable to existing
large-scale bimanual teleoperation methods such as RDT
(6K self-created episodes) and 0  (5100 hours post-
training data), but our cost is extremely low.
3) Baselines: We compare our method to four strong base-
lines. (1) Action Chunking Transformers (ACT) . It is
proposed by ALOHA and uses a well-designed transformer
structure as the visual encoder. (2) Diffusion Policy (DP) .
The vanilla diffusion policy uses RGB images as inputs and
ResNet  as the visual encoder. We modified it by using
point cloud scenes as observations and a PointNet encoder
. (3) 3D Diffusion Policy (DP3) . It is a variant of
diffusion policy with a simpler point cloud encoder. It also
designs a two-layer MLP to encode robot proprioceptive states
before concatenating with the observation representation. (4)
EquiBot . It takes the point cloud scene as observation,
and learns to predict continuous undecomposed 7-DoF actions
of dual arms. Note that these baselines, including our BiDP,
are designed to learn task-independent policies, and do not
consider the multi-task model currently.
4) Metrics: We train all methods for 500 or 1,000 epochs
and only save the last checkpoint for testing. We evaluate
each model with 5 trials for each single object (last three
tasks with 30, 25 and 20 trials, respectively) or 2 trials
for paired objects (first two tasks with 54 and 36 trials,
respectively) in every task. These objects have randomized
initial placements. For a more detailed comparison, we report
the average length (following CLAVIN ) in each substep
for a sequenced long-horizon task, where the last substep
indicates the final success rate. Although above tests have new
variations in object placements, we choose two tasks pull
drawer and uncover lid to perform more challenging
out-of-distribution (OOD) evaluations on novel objects. We
omit the last object or paired objects from the training set
(a) Raw 3D Hand Points
(b) Projected 2D Hand Points
(c) Lifted Points in Keyframes
Fig. 5: Illustrations of extracted hand motion trajectories by
using (a) unhandled raw 3D hand center points, (b) projected
hand center points on the 2D image, and (c) lifted 3D points
in simplified keyframes. The first and second line represents
the task pull drawer and uncover lid, respectively.
TABLE III: Ablation studies of proposed strategies in YOTO
and the bimanual diffusion policy (BiDP). The task pull
drawer with 243 episodes is used to train all models.
observation
keyframes
reorganize
geometric
transforms
Not Expanded
Success Rate
Success Rate
Not Expanded
Average Length
Average Length
Fig. 6: Ablation studies on expanded training data at different
scales using geometric transformations. The task pull drawer
with 243 episodes is treated as the not expanded version.
and treat them as unseen objects to evaluate the final trained
model. The number of all OOD trials is quadrupled.
B. Results Comparison
by one, including basic in-distribution results and generaliza-
tions to out-of-distribution settings.
(Q1) Our extracted hand motions have good continuity
and consistency. We first discuss the quality of the extracted
motion trajectories, which is the core concept of this paper and
extremely important for the various strategies developed next.
As shown in Fig. 5, we compared the general effect of 3D hand
TABLE IV: Quantitative results of detailed long-horizon performance comparisons (in-distribution evaluations). The step-
wise success rates and average length of completed task sequences are reported. We use different colors such as teal, olive
and purple to indicate that each substep corresponds to the left arm, right arm and both arms, respectively.
pull drawer (243 episodes)
pour water (162 episodes)
close to
BiDP (Ours)
unscrew bottle (54 episodes)
uncover lid (45 episodes)
open box (36 episodes)
close to
close to
close to
BiDP (Ours)
TABLE V: Comparison of the average success rate of various
methods on all five tasks (in-distribution evaluations).
BiDP (Ours)
Success Rate
TABLE VI: Quantitative results of detailed long-horizon per-
formance comparisons (out-of-distribution evaluations). The
substeps are abbreviated as sequential numbers.
pull drawer
(144 episodes)
uncover lid
(36 episodes)
S1 S2 S3 S4 S5 S6 Avg.
motion trajectories extracted using different methods in two
different long-horizon bimanual tasks. Firstly, when directly
applying advanced 3D hand mesh reconstruction methods (ei-
ther HaMeR  or WiLoR ), the resulting hand trajectory
is always unstable and difficult to parse (see Fig. 5 (a)).
This is mainly because most of these methods are based on
monocular images, and the preset camera parameters such as
focus and focal length are directly calculated using the center
and size of each image. This makes the estimation results for
consecutive frames in the video not in a unified and invariant
camera space, and therefore unreliable and ambiguous in
depth. Nevertheless, this intuitive but sub-optimal approach
is still widely used by mainstream methods for learning from
human videos [42, 47, 23]. In comparison, after projecting
these 3D points onto a 2D image plane (with the Z-axis set to
0 for ease of visualization), it is clear that the trajectory trends
and estimated motion flow are improved (see Fig. 5 (b)). This
conclusion is generally applicable, for tasks like ours where the
camera is stationary and its intrinsic and extrinsic parameters
are known. Finally, as described in Sec IV-B, we filter out
sparse keyframes from these continuous points and lift the
corresponding position components into 3D points to obtain
the keyposes suitable for the end-effector (see Fig. 5 (c)). We
thus claim that our extracted hand motion trajectory based on
an one-shot human teaching has a more guaranteed quality.
And we expect that this motion extraction technology will
be used for retargeting to other more dexterous end-effectors,
such as multi-fingered hands.
(Q2) The various strategies we propose in YOTO are
effective. After extracting primary keyposes that could be
successfully injected into the robot, we continue to explore
YOTO including other strategies, which are closely related
to the visuomotor policy learning. As shown in Tab. III,
we quantitatively illustrate the effectiveness of each strategy
one by one through many ablation studies. We experimented
with task pull drawer which has 243 training trajectories.
regarded as the vanilla EquiBot , which takes the entire
point cloud scene as observation, learns to predict continuous
augmented training demonstrations. Despite being a solid
horizon task. Next, we replaced the input with point clouds
containing only manipulated objects (id-2) or predicted sim-
plified sparse keyposes (id-3), and the success rate and average
execution length of the task were improved. These results
suggest that reducing unnecessary distractions in the input
and learning fewer simplified actions are the right direction.
When both are used together (id-4), better performance can
be achieved. Based on these two strategies, we decoupled
the output action space and reconstructed it into a single-arm
format (id-5), the policy could also be superior, indicating the
importance of eliminating redundant actions. Alternately, if
3D geometric transformations were applied to further expand
training demonstrations (id-6), the resulting model effect was
much better, with the most prominent growth. This proves
that our developed demonstration proliferation is simple yet
efficient. We accordingly show in Fig. 6 the typical trend that
using more extended training data leads to better performance,
which is consistent with our consensus. Finally, combining the
above strategies together (id-7), our BiDP takes full advantage
pull drawer
pour water
unscrew bottle
uncover lid
open box
Fig. 7: Visualization of five bimanual tasks performed on real robots. We use different colors such as teal, olive and purple to
distinguish frames of left arm, right arm and both arms, respectively. Arrows are artificially added to show movement trends.
of all the strengths and has achieved the best results.
On the other hand, we need to compare and explain whether
BiDP is better than other visuomotor imitation methods [104,
following the mainstream in-distribution setting, we performed
extensive policies training and real robot evaluations on five
long-horizon tasks, and reported a detailed performance com-
parison of various methods. Generally speaking, we can draw
three conclusions from these quantitative data. (1) First, the
diffusion-based strategy always performed better than the
transformer-based ACT. This is mainly because the diffusion
model can model a higher-dimensional action space and is
highly malleable, while tranformer architectures usually do
not have these characteristics and require a large amount of
data to achieve scale effects and gain advantages. In addition,
ACT utilizes 2D images as observations instead of 3D input,
which also makes it achieve inferior results. (2) Second, a more
advanced and sophisticated 3D observation perception archi-
tecture can lead to higher policy performance. For example,
compared to the modified DP that directly uses PointNet to
process 3D point cloud input, DP3 and EquiBot adopt a self-
designed lightweight MLP encoder and SIM(3)-equivariant
backbone to extract point cloud features, respectively, and
always achieved better results. (3) Finally, for more complex
long-horizon bimanual manipulation tasks, the existing state-
of-the-art methods still have a lot of room for improvement,
such as the gradually decaying effect over multiple substeps
and less exploration of efficient utilization of training data.
Thanks to the proposed multiple strategies, our BiDP can
better cope with bimanual tasks, significantly better than all
compared policies. We summarized the average success rate
of each method on all five tasks in Tab V, where our method
BiDP achieved a success rate of nearly 60, demonstrating
good potential for practical robotic applications. To sum up,
it can be concluded that the various strategies we proposed in
YOTO are quite effective.
(Q3) BiDP has satisfactory out-of-domain generalization
ability. To further illustrate the superiority of BiDP, we de-
signed tests under out-of-distribution (OOD) settings. Results
are shown in Tab VI. From it, we can see that, except for
our method and EquiBot, the performance of the other three
methods has dropped significantly when it comes to OOD
paring to EquiBot, our BiDP still has a clear advantage, thanks
to the fact that we use explicit 3D geometric transformations
for expanding the training demonstrations instead of SIM(3)-
equivariant augmentation of the entire point cloud input in
EquiBot. In addition, using pure object point clouds as input
reorient pen  pick-place
flip basket upside down
contact-rich long-horizon bimanual task (asynchronous)
non-prehensile bimanual task (synchronous)
Fig. 8: Illustrations of another two bimanual tasks. Top: the visualization of hand motions extraction. Bottom: the corresponding
rollout examples by injecting actions on real robots. Refer to Fig. 1 and Fig. 7 for notes on different colors and curves.
also makes our model more robust compared to all baselines.
The core idea here is to rely on the still rapidly developing
capabilities of vision foundation models, such as the open vo-
cabulary detection  and segmentation , to more reliably
perceive various unseen scenes and objects. In summary, these
results verify that our BiDP indeed outperforms prior methods
with the least amount of performance degradation in OOD
generalization.
(Q4) YOTO is widely applicable to diverse bimanual
tasks. Our proposed framework YOTO is compatible with
most bimanual tasks, such as the selected five representative
long-horizon tasks, covering a variety of skills, multi-object
the above-mentioned quantitative results, we also qualitatively
demonstrate the visual effects of real robot execution on five
tasks in Fig. 7, mainly showing the sparse keyframes contained
in them. We can see that the two robot arms have learned the
movements demonstrated by human hands and complete these
complex tasks in an orderly manner.
ulation tasks and enabled the dual-arm robot to learn new given
tasks quickly and easily through one-shot human teaching. Due
to space limitations, we did not continue the demonstration
proliferation and policy training. The illustrations of extracted
actions that can be injected into real robots are shown in
Fig. 8. These results further reveal the simplicity, versatility
and scalability of YOTO. In the future, we will explore using
YOTO to handle more intricate, valuable, but less researched
bimanual manipulation tasks.
VI. CONCLUSION AND LIMITATION
In this paper, we propose a novel framework named YOTO
to address the challenge of efficient and robust bimanual
manipulation. Our approach learns from one-shot human video
and consecutive hand features like pose, joints, and states.
To ensure stable and precise manipulation, we simplify noisy
hand motion trajectories into discrete keyframes and introduce
a motion mask for better dual-arm coordination. Based on
the refined one-shot teaching, we develop a scalable data
proliferation solution using auto-rollout verification and 3D
geometric transformations to rapidly create diverse training
examples. With this enriched dataset, we design a dedicated
bimanual diffusion policy (BiDP) that simplifies observations,
predicts keyposes, and reorganizes action spaces for efficient
training. Validated on five complex bimanual tasks, our frame-
work demonstrates superior performance in both synchronous
and asynchronous scenarios. These contributions provide a
standardized method for transferring human motions to robots,
a scalable approach for data generation, and an effective
algorithm for mastering intricate dual-arm tasks, advancing
the field of bimanual manipulation.
formance on various long-horizon bimanual manipulation
(1) Our vision-based hand trajectory extraction schemes have
inherent errors. This means that we have to check carefully
and verify on the real robot whether the extracted position and
posture information is reliable, which still requires additional
manpower. (2) The primary version of YOTO adopts a fixed
carts or multi-legged robots. (3) The equipped parallel gripper
is not flexible enough and has limited functionality. Upgrading
the end-effector to a multi-fingered dexterous hand or equip-
ping it with force-tactile sensors can make the robot more
versatile and powerful. (4) More ultra-difficult bimanual tasks
are still under-explored, such as the specialized tool-based
manipulation (e.g., picking up a hammer to pound a nail or
twisting a screwdriver to tighten a screw), highly dynamic non-
quasi-stationary tasks, and friendly interactive collaboration
with people. In short, these limitations highlight the need for
further innovations to enhance robustness, generalization, and
scalability in bimanual robot manipulation.
ACKNOWLEDGMENTS
This work was supported by the Guangdong Provincial
Key Field RD Program (No. 20240104, the project name:
Research and Application of Common Key Technologies of
Robot Embodied Intelligence Based on AI Large Model),
and also received funding from the 2024 Shenzhen Science
and Technology Major Project (No. 202402002, the project
Robots to Learn Human Skills).
REFERENCES
Jeff Bingham, Sanky Chan, Kenneth Draper, De-
bidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer
Aloha 2: An enhanced low-cost
hardware for bimanual teleoperation.
arXiv preprint
Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten
ficient bimanual folding of garments. In 2022 IEEERSJ
International Conference on Intelligent Robots and Sys-
tems (IROS), pages 18. IEEE, 2022.
Arpit Bahety, Shreeya Jain, Huy Ha, Nathalie Hager,
Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, and
Shuran Song. Bag all you need: Learning a general-
izable bagging strategy for heterogeneous objects. In
2023 IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 960967. IEEE,
Arpit Bahety, Priyanka Mandikal, Ben Abbatematteo,
and Roberto Martn-Martn. Screwmimic: Bimanual im-
itation from human videos with screw space projection.
In Proceedings of Robotics: Science and Systems (RSS),
Ravin Balakrishnan and Gordon Kurtenbach. Exploring
bimanual camera control and object manipulation in
3d graphics interfaces. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
Christian Bersch, Benjamin Pitzer, and Soren Kam-
mel. Bimanual robotic cloth manipulation for laundry
folding.
In 2011 IEEERSJ International Conference
on Intelligent Robots and Systems, pages 14131419.
Kevin Black, Noah Brown, Danny Driess, Adnan Es-
vision-language-action flow model for general robot
control. arXiv preprint arXiv:2410.24164, 2024.
Johann Brehmer, Joey Bose, Pim De Haan, and Taco S
Cohen. Edgi: Equivariant diffusion for planning with
embodied agents.
Advances in Neural Information
Processing Systems, 36, 2024.
Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burch-
Canonicalized-alignment
purpose garment manipulation. In 2023 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 58725879. IEEE, 2023.
Lawrence Yunliang Chen, Baiyu Shi, Roy Lin, Daniel
David Held, and Ken Goldberg. Bagging by learning
to singulate layers using interactive perception.
2023 IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 31763183. IEEE,
Lawrence Yunliang Chen, Baiyu Shi, Daniel Seita,
Richard Cheng, Thomas Kollar, David Held, and Ken
Goldberg. Autobag: Learning to open plastic bags and
insert objects. In 2023 IEEE International Conference
on Robotics and Automation (ICRA), pages 39183925.
Sirui Chen, Chen Wang, Kaden Nguyen, Li Fei-Fei, and
C Karen Liu.
demonstrations for robot learning with augmented real-
ity feedback. arXiv preprint arXiv:2410.08464, 2024.
Yuanpei Chen, Chen Wang, Yaodong Yang, and Karen
Liu. Object-centric dexterous manipulation from human
motion data.
In 8th Annual Conference on Robot
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback. In 8th Annual Con-
ference on Robot Learning, 2024.
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shu-
ran Song. Diffusion policy: Visuomotor policy learning
via action diffusion.
The International Journal of
Robotics Research, page 02783649241273668, 2023.
Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, and
Abhinav Gupta. Efficient bimanual manipulation using
learned task schemas.
In 2020 IEEE International
Conference on Robotics and Automation (ICRA), pages
Ian Chuang, Andrew Lee, Dechen Gao, and Iman
Soltani. Active vision might be all you need: Exploring
active vision in bimanual robotic manipulation. arXiv
preprint arXiv:2409.17435, 2024.
Matei Ciocarlie, Corey Goldfeder, and Peter Allen.
Dexterous grasping via eigengrasps: A low-dimensional
approach to a high-complexity problem. In Proceedings
of Robotics: Science and Systems (RSS), 2007.
Adria Colome and Carme Torras. Dimensionality reduc-
tion for dynamic movement primitives and application
to bimanual manipulation of clothes. IEEE Transactions
on Robotics, 34(3):602615, 2018.
Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia,
Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong
Wang. Bunny-visionpro: Real-time bimanual dexterous
teleoperation for imitation learning.
arXiv preprint
Michael Drolet, Simon Stepputtis, Siva Kailas, Ajinkya
A comparison of imitation learning algorithms for bi-
manual manipulation. IEEE Robotics and Automation
Dimitrios
Muhammed Kocabas, Manuel Kaufmann, Michael J
Hilliges.
for dexterous bimanual hand-object manipulation.
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 1294312954,
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wet-
owing and imitation from humans.
arXiv preprint
Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mobile
low-cost whole-body teleoperation.
arXiv preprint
Jianfeng Gao, Xiaoshu Jin, Franziska Krebs, Noemie
visual imitation learning of bimanual manipulation
In 2024 IEEE International Conference on
Robotics and Automation (ICRA), pages 1685016857.
Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-
Wei Chao, and Dieter Fox.
manipulation from few demonstrations. In Proceedings
of Robotics: Science and Systems (RSS), 2024.
Jennifer Grannen, Priya Sundaresan, Brijen Thanan-
Goldberg.
Untangling dense knots by learning task-
relevant keypoints. In Conference on Robot Learning,
pages 782800. PMLR, 2021.
Jennifer Grannen, Yilin Wu, Suneel Belkhale, and Dorsa
Sadigh. Learning bimanual scooping policies for food
acquisition. In Conference on Robot Learning, pages
Jennifer Grannen, Yilin Wu, Brandon Vu, and Dorsa
Stabilize to act: Learning to coordinate for
bimanual manipulation. In Conference on Robot Learn-
Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.
In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Kristen Grauman, Andrew Westbury, Lorenzo Torre-
Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal,
Bikram Boote, et al. Ego-exo4d: Understanding skilled
human activity from first-and third-person perspectives.
In Proceedings of the IEEECVF Conference on Com-
puter Vision and Pattern Recognition, pages 19383
Markus Grotz, Mohit Shridhar, Tamim Asfour, and
Dieter Fox.
robotic bimanual manipulation tasks.
arXiv preprint
Valentin N Hartmann, Andreas Orthey, Danny Driess,
Ozgur S Oguz, and Marc Toussaint.
Long-horizon
multi-robot rearrangement planning for construction as-
sembly. IEEE Transactions on Robotics, 39(1):239252,
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 770778, 2016.
Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W
Burdick. Dual arm estimation for coordinated bimanual
manipulation. In 2013 IEEE International Conference
on Robotics and Automation, pages 120125. IEEE,
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models.
Advances in Neural
Information Processing Systems, 33:68406851, 2020.
Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe
Dynamic handover: Throw and catch with
bimanual hands.
In Conference on Robot Learning,
pages 18871902. PMLR, 2023.
Stephen James, Kentaro Wada, Tristan Laidlow, and
Andrew J Davison. Coarse-to-fine q-attention: Efficient
learning for visual robotic manipulation via discreti-
In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kap-
Chelsea Finn. Bc-z: Zero-shot task generalization with
robotic imitation learning.
In Conference on Robot
Edward Johns. Coarse-to-fine imitation learning: Robot
manipulation from a single demonstration.
IEEE international conference on robotics and automa-
tion (ICRA), pages 46134619. IEEE, 2021.
Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina
Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d
scene representations. arXiv preprint arXiv:2402.10885,
Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi,
Qianqian Wang, Ken Goldberg, and Angjoo Kanazawa.
Robot see robot do: Imitating articulated object manipu-
lation with monocular 4d reconstruction. In 8th Annual
Conference on Robot Learning, 2024.
Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti,
Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael
et al. Openvla: An open-source vision-language-action
model. arXiv preprint arXiv:2406.09246, 2024.
Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun,
and Joshua B Tenenbaum.
Learning to act from ac-
tionless videos through dense correspondences. In The
Twelfth International Conference on Learning Repre-
Franziska Krebs and Tamim Asfour.
A bimanual
manipulation taxonomy. IEEE Robotics and Automation
Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-
Sevilla-Lara. Learning precise affordances from ego-
centric videos for robotic manipulation. arXiv preprint
Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo
ing humanoid robots manipulation skills through single
video imitation. In 8th Annual Conference on Robot
Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, and
Yi Wu. Efficient bimanual handover and rearrangement
via symmetry-aware actor-critic learning. In 2023 IEEE
International Conference on Robotics and Automation
(ICRA), pages 38673874. IEEE, 2023.
Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel,
and Jitendra Malik. Twisting lids off with two hands.
arXiv preprint arXiv:2403.02338, 2024.
Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi,
Sergey Levine, and Jitendra Malik. Learning visuotac-
tile skills with two multifingered hands. arXiv preprint
I-Chun Arthur Liu, Sicheng He, Daniel Seita, and
Gaurav S Sukhatme. Voxact-b: Voxel-based acting and
stabilizing policy for bimanual manipulation.
Annual Conference on Robot Learning, 2024.
Junjia Liu, Yiting Chen, Zhipeng Dong, Shixiong Wang,
Sylvain Calinon, Miao Li, a
