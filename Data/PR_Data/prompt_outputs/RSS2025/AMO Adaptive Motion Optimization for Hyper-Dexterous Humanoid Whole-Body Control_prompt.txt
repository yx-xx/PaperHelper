=== PDF文件: AMO Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control.pdf ===
=== 时间: 2025-07-21 13:45:50.776840 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Hyper-Dexterous Humanoid Whole-Body Control
Jialong Li
Xuxin Cheng
Tianshu Huang
Shiqi Yang
Ri-Zhao Qiu
Xiaolong Wang
UC San Diego
Fig. 1: AMO enables hyper-dexterous whole-body movements for humanoid robots. (a): The robot picks and places a can on
platforms of different heights. (b): The robot picks a bottle from the higher shelf on the left and puts it on the lower table on
the right. (c): The robot stretches its legs to put the bottle on a high shelf. (d-g): The robot demonstrates a wide range of torso
roll rotation.
AbstractHumanoid robots derive much of their dexterity
from hyper-dexterous whole-body movements, enabling tasks that
require a large operational workspacesuch as picking objects
off the ground. However, achieving these capabilities on real
humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive
Motion Optimization (AMO), a framework that integrates sim-to-
real reinforcement learning (RL) with trajectory optimization for
bias in motion imitation RL, we construct a hybrid AMO
dataset and train a network capable of robust, on-demand
Equal contributions.
adaptation to potentially O.O.D. commands. We validate AMO
in simulation and on a 29-DoF Unitree G1 humanoid robot,
demonstrating superior stability and an expanded workspace
compared to strong baselines. Finally, we show that AMOs
consistent performance supports autonomous task execution via
imitation learning, underscoring the systems versatility and
robustness.
I. INTRODUCTION
Humans can expand their workspace of hands using whole-
body movements. The joint congurations of humanoid robots
closely mimic humans functionality and degree of freedom
while facing challenges of achieving similar movements with
Opt2Skill
Ref. Type
Traj. Opt.
SO(3)  Height Dex. Torso
SE(3) Task Space
No Ref. Deploy
TABLE I: Comparisons with two recent representative hu-
manoid motion imitation works. Dex. Torso means if the robot
is able to adjust its torsos orientation and height to expand the
workspace. Task Space means the end effector. No Ref. Deploy
means if the robot needs reference motion during deployment.
O.O.D. means if the work has evaluated O.O.D. performance,
which is a typical case when the robot is controlled by a human
operator and the control signal is highly unpredictable.
real-time control. This is due to the dynamic humanoid
whole-body controls high-dimensional, highly non-linear, and
contact-rich nature. Traditional model-based optimal control
methods require precise modeling of the robot and environ-
for realizable computation results, which is not feasible for
the problem of utilizing all DoFs (29) of an overactuated
humanoid robot in the real world.
Recent advances in reinforcement learning (RL) combined
with sim-to-real have demonstrated signicant potential in
enabling humanoid loco-manipulation tasks in real-world set-
tings . While these approaches achieve robust real-time
control for high-degree-of-freedom (DoF) humanoid robots,
they often rely on extensive human expertise and manual
tuning of reward functions to ensure stability and performance.
To address this limitation, researchers have integrated motion
imitation frameworks with RL, leveraging retargeted human
motion capture (MoCap) trajectories to dene reward objec-
tives that guide policy learning [10, 28]. However, such tra-
jectories are typically kinematically viable but fail to account
for the dynamic constraints of target humanoid platforms,
introducing an embodiment gap between simulated motions
and hardware-executable behaviors. An alternative approach
combines trajectory optimization (TO) with RL to bridge this
methodologies
humanoid
manipulation capabilities, current approaches remain con-
strained to simplied locomotion patterns rather than achieving
true whole-body dexterity. Motion capture-driven methods
suffer from inherent kinematic bias: their reference datasets
predominantly feature bipedal locomotion sequences (e.g.,
ments essential for hyper-dexterous manipulation. Conversely,
trajectory optimization TO-based techniques face complemen-
tary limitationstheir reliance on a limited repertoire of
motion primitives and computational inefciency in real-time
applications precludes policy generalization. This critically
hinders deployment in dynamic scenarios requiring rapid adap-
tation to unstructured inputs, such as reactive teleoperation or
environmental perturbations.
To bridge this gap, we present Adaptive Motion Optimiza-
tion (AMO)a hierarchical framework for real-time whole-
body control of humanoid robots through two synergistic
brid upper-body command sets by fusing arm trajectories
from motion capture data with probabilistically sampled torso
training distribution. These commands drive a dynamics-
aware trajectory optimizer to produce whole-body reference
motions that satisfy both kinematic feasibility and dynamical
humanoid motion repository explicitly designed for dexterous
loco-manipulation. (ii) Generalizable Policy Training: While a
straightforward solution would map commands to motions via
discrete look-up tables, such methods remain fundamentally
constrained to discrete, in-distribution scenarios. Our AMO
network instead learns continuous mappings, enabling robust
interpolation across both continuous input spaces and out-of-
distribution (O.O.D.) teleoperation commands while maintain-
ing real-time responsiveness.
During deployment we rst extract the sparse poses from
a VR teleoperation system and output upper-body goals with
multi-target inverse kinematics. The trained AMO network and
RL policy together output the robots control signal. We list a
brief comparison in Table. I to show the major advantages
of our method with two recent representative works. To
A novel adaptive control method AMO that substantially
expands the workspace of humanoid robots. AMO works
in real-time with sparse task-space targets and shows
O.O.D. performance previous methods have not achieved.
A new landmark of humanoid hyper-dexterous WBC
controller with orders larger workspace that enables a
humanoid robot to pick up objects from the ground.
Comprehensive experiments both in simulation and the
real world with teleoperation and autonomous results
showing the effectiveness of our method and ablations
of core components.
II. RELATED WORK
Humanoid Whole-body Control. Whole-body control for
humanoid robots remains a challenging problem due to their
high DoFs and non-linearity. This is previously primarily
achieved by dynamic modeling and model-based control [14,
deep reinforcement learning methods have shown promise in
achieving robust locomotion performance for legged robots [3,
from high-dimensional inputs for quadrupeds [7, 8, 22, 27]
and humanoids [9, 24, 29, 33].
trains one transformer
for control and another transformer for imitation learning.
only encourages the upper body to imitate the motion,
while the lower body control is decoupled.  trains goal-
conditioned policies for downstream tasks. All
show only limited whole-body control ability, which enforcing
AMO Dataset
AMO Training
Mocap Dataset
RL Teacher
Lower Goal
Proprio.  Priv.
Traj. Opt.
Upper Goal
Supervised        Learning
WBC Training
Proprio.
WBC Policy
Multi-Task IK
Deployment
Teleop Sys.
Real Robot
Autonomy
WBC Policy
Proprio.
Transformer
Torso Commands:
RPY  Height
Fig. 2: System overview. The system is decomposed into four stages: 1. AMO module training by collecting AMO dataset
using trajectory optimization; 2. RL policy training by teacher-student distillation in simulation; 3. real robot teleoperation by
IK and retargeting; 4. real robot autonomous policy training by imitation learning (IL) with a transformer.
the torso and pelvis of humanoid robots to stay motionless.
shows expressive whole-body action for humanoid robots,
but it does not emphasize leveraging whole-body control to
extend robots loco-manipulation task space.
Teleoperation of Humanoids. Teleoperation of humanoids
is crucial for real-time control and robot data collection. Prior
efforts in humanoid teleoperation include [11, 24, 28, 29, 42,
64]. For example, [24, 29] uses a third-person RGB camera
to obtain key points of the human teleoperator. Some works
use VR to provide ego-centric observation for teleoperators.
uses Apple VisionPro to control the active head and
upper body with dexterous hands.
uses Vision Pro for
the head and upper body while using pedals for locomotion
control. Humanoid whole-body control requires a teleoperator
to provide physically achievable whole-body coordinates for
the robots.
Loco-manipulation Imitation Learning. Imitation learn-
ing has been studied to help the robot complete the task
autonomously. Identifying existing work with demonstration
sources can be classied as learning from real robot expert
play data [15, 50, 70], and learning from human demonstra-
learning studies are limited to manipulation skills, while there
are very few imitation learning studies for loco-manipulation.
studies imitation learning for loco-manipulation, using
a wheel-based robot. This paper uses imitation learning to
enable humanoid robots to complete loco-manipulation tasks
autonomously.
III. ADAPTIVE MOTION OPTIMIZATION
We present AMO: Adaptive Motion Optimization, a frame-
work that achieves seamless whole-body control as illustrated
in Figure 2. Our system is decomposed into four different com-
ponents. We introduce the notations and overall framework at
components in the following sections separately.
A. Problem Formulation and Notations
We address the problem of humanoid whole-body control
and focus on two distinct settings: teleoperation and au-
tonomous.
In the teleoperation setting, the whole-body control problem
is formulated as learning a goal-conditioned policy 0 : G
S ! A, where G represents the goal space, S the observation
learned policy : S ! A generates actions solely based on
The goal-conditioned teleoperation policy receives a con-
trol signal g
G from the teleoperator, where g
[phead, pleft, pright, v]. phead, pleft, pright represent the oper-
ators head and hand keypoint poses, while v  [vx, vy, vyaw]
species the base velocity. Observations s 2 S include
visual and proprioceptive data: s  [imgleft, imgright, sproprio].
Actions a 2 A consist of joint angle commands for the upper
and lower body: a  [qupper, qlower].
Goal-conditioned
teleoperation
goal-conditioned
hierarchical
upper(phead, pleft, pright)
[qupper, g0] outputs actions
for the upper body and an intermediate control signal
g0  [rpy, h], where rpy command the torso orientation
commands
lower(v, g0, sproprio)
qlower generates actions for the
lower body using this intermediate control signal, the velocity
b) Autonomous policy: The autonomous policy
[upper, lower]
hierarchical
the teleoperation policy. The lower policy is identical:
lower  0
and intermediate control independently from human input:
upper(imgleft, imgright, sproprio)  [qupper, v, g0].
B. Adaptation Module Pre-Training
In our system specication, the lower policy follows com-
mands in the form of [vx, vy, vyaw, rpy, h]. The locomotion
ability of following velocity commands [vx, vy, vyaw] can be
easily learned by randomly sampling directed vectors in the
simulation environment following the same strategy as [8, 9].
skills as they require whole-body coordination. Unlike in the
locomotion task, where we can design feet-tracking rewards
based on Raibert Heuristic
to facilitate skill learning,
there lacks such a heuristic to guide the robot to complete
whole-body control. Some works [28, 33] train such policies
by tracking human references. However, their strategy does
not build a connection between human poses and whole-body
control directives.
To address this issue, we propose an adaptive motion
optimization (AMO) module. The AMO module is repre-
sented as (qupper, rpy, h)  qref
lower. Upon receiving whole-
body control commands rpy, h from the upper, it converts
these commands into joint angle references for all lower-body
actuators for the lower policy to track explicitly. To train
this adaptive module, rst, we collect an AMO Dataset by
randomly sampling upper commands and performing model-
based trajectory optimization to acquire lower body joint an-
gles. The trajectory optimization can be formulated as a multi-
contact optimal control problem (MCOP) with the following
cost fuction:
L  Lx  Lu  LCoM  Lrpy  Lh
Lx  kxt xrefk2
Lu  kutk2
LCoM  kct crefk2
Lrpy  kRtorso Rref(rpy)k2
Lh  wh(ht h)2
Which includes regularization for both state x and control
mass (CoM) regularization term that ensures balance when
performing whole-body control. Upon collecting dataset, we
rst randomly select upper body motions from the AMASS
and sample random torso commands. Then,
we perform trajectory optimizations to track torso objectives
while maintaining a stable CoM and adhering to wrench
cone constraints to generate dynamically feasible reference
joint angles. Since we do not take the walking scenario into
making contact with the ground. The references are generated
PD Controller
qleftarm
qrightarm
WBC Policy
Fig. 3: Teleoperation system overview. The operator provides
three end-effector targets: head, left wrist, and right wrist
poses. A multi-target IK computes upper goals and intermedi-
ate goals by matching three weighted targets simultaneously.
The intermediate goals (rpy, h) are fed to AMO and converted
to lower goals.
using control-limited feasibility-driven differential dynamic
programming (BoxFDDP) through Crocoddyl [48, 49]. These
data are collected to train an AMO module that converts torso
commands into reference lower poses. The AMO module is a
three-layer multi-layer perceptron (MLP) and is frozen during
the later stage of lower policy training.
C. Lower Policy Training
We use massively parallel simulation to train our lower
policy with IsaacGym. The lower policy aims at tracking
g0 and v, while utilizing proprioceptive observations sproprio,
which is dened as:
[t, !t, qwholebody
, qwholebody
, awholebody
, t, qref
The above formulation contains base orientation t, base
angular velocity !t, current position, velocity, and last position
targets. It is noticeable that the lower policys observation
includes the states of upper body actuators for better upper-
lower body coordination. t is the gait cycling signal dened
in similar ways as [45, 77]. qref
lower is the reference lower body
joint angles generated by the AMO module. The lower action
space qlower 2 R15 is a vector of dimension 15, which consists
of 2 6 target joint positions for both legs and 3 target joint
positions for the waist motors.
We opt to use a teacher-student framework to train our
lower policy. We rst train a teacher policy that can ob-
serve privileged information in simulation, using off-the-shelf
PPO. We then distill the teacher policy into a student
policy using supervised learning. The student policy only
observes information available in real and can be deployed
for teleoperation and autonomous tasks.
formulated
teacher(v, g0, sproprio, spriv)
additional
privilege observation spriv is dened as:
Which includes ground-truth values of: base velocity vgt
torso orientation rpygt
readily available when tracking their corresponding targets
in the real world. ct is the contact signal between feet and
the ground. The teacher RL training process is detailed in
Appendix.B.
student(v, g0, sproprio, shist)
compensate
sproprio
observations
accessible
proprioceptive observations as an additional input signal:
D. Teleoperation Upper Policy Implementation
The teleoperation upper policy generates a series of com-
mands for whole-body control, including arm and hand move-
this policy using optimization-based techniques to achieve
the precision required for manipulation tasks. Specically,
hand movements are generated via retargeting, while other
control signals are computed using inverse kinematics (IK).
Our implementation of hand retargeting is based on dex-
retargeting . More details about our retargeting formula-
tion are presented in Appendix.A.
In our whole body control framework, we extend the
conventional IK to multi-target weighted IK, minimizing the
6D distances to three key targets: the head, the left wrist, and
the right wrist. The robot mobilizes all upper-body actuators
to match these three targets simultaneously. Formally, our
objective is:
Lhead  Lleft  Lright
Lhead  kphead pheadlinkk2  kRhead Rheadlinkk2
Lleft  kpleft pleftlinkk2  kRleft Rleftlinkk2
Lright  kpright prightlinkk2  kRright Rrightlinkk2
q  [qhead, qleftarm, qrightarm, rpy, h]
As illustrated in Fig. 3. The optimization variable q in-
cludes all actuated degrees of freedom (DoFs) in the robots
upper body: qhead, qleftarm, and qrightarm. In addition
to the motor commands, it also solves for an intermediate
command to enable whole-body coordination: rpy and h
for torso orientation and height control. To ensure smooth
upper body control, posture costs are weighted differently
across different components of the optimization variable q:
policy to prioritize upper-body actuators for simpler tasks.
bending over to pick up or reaching distant targets, additional
control signals [rpy, h] are generated and sent to the lower
policy. The lower policy coordinates its motor angles to
fulll the upper policys requirements, enabling whole-body
target reaching. Our IK implementation employs Levenberg-
Marquardt (LM) algorithm  and is based on Pink .
E. Autonomous Upper Policy Training
We learn the autonomous upper policy via imitation learn-
ing. First, the human operators teleoperate with the robot
Metrics ()
Ours(AMO)
w rand arms
Ours(AMO)
w rand arms
TABLE II: Comparison of tracking errors with baselines.
Each tracking error is averaged over 4096 environments and
500 steps. Ey, Ep, Er, Eh, Ev represent tracking errors in
torso yaw, torso pitch, torso roll, base height, and base linear
using our goal-conditioned policy, recording observations and
actions as demonstrations. We then employ ACT  with
a DinoV2 [17, 53] visual encoder as the policy backbone.
The visual observation includes two stereo images imgleft and
imgright. DinoV2 divides each image into 16 22 patches
and produces a 384-dimensional visual token for each patch,
yielding a combined visual token of shape 21622384.
This visual token is concatenated with a state token obtained
by projecting ot
[vt1, rpyt1, ht1] constitutes the last command sent to the
lower policy. Due to our decoupled system design, the upper
policy observes these lower-policy commands instead of direct
lower-body proprioception. The policys output is represented
, qdualarm
, qdualhand
, vt, rpyt, ht]
comprising all upper-body joint angles and the intermediate
control signals for the lower policy.
IV. EVALUATION
In this section, we aim to address the following questions
by conducting experiments in both simulation and in the real
How well does AMO performs on tracking locomotion
commands v and torso commands rpy, h?
How does AMO compare to other WBC strategies?
How well does the AMO system perform in the real-
world setting?
We conduct our sim experiments in IsaacGym simulator
. Our real robot setup is as shown in 3, which is modied
from Unitree G1  with two Dex3-1 dexterous hands. This
platform features 29 whole-body DoFs and 7 DoFs on each
hand. We customized an active head with three actuated DoFs
to map the human operators head movement and mounted a
ZED Mini  camera for stereo streaming.
Ours(AMO)
Waist Tracking
TABLE III: Comparison of maximum torso control ranges.
following in-distribution (I.D.) tracking commands in yaw,
A. How well does AMO perform on tracking locomotion
commands v and torso commands rpy, h?
Table II presents the evaluation of AMOs performance by
comparing it against the following baselines:
wo AMO: This baseline follows the same RL training
recipe as Ours(AMO), with two key modications. First,
it excludes the AMO output qref
lower from the observation
space. Second, instead of penalizing deviations from
viations from a default stance pose.
wo priv: This baseline is trained without additional
privileged observations spriv.
w rand arms: In this baseline, arm joint angles are
not set using human references sampled from a MoCap
dataset. Instead, they are randomly assigned by uniformly
sampling values within their respective joint limits.
The performance is evaluated using the following metrics:
1) Torso Orientation Tracking Accuracy: Torso orienta-
tion tracking is measured by Ey, Ep, Er. The results indi-
cate that AMO achieves superior tracking accuracy in roll
and pitch directions. The most notable improvement is in
pitch tracking, where other baselines struggle to maintain
ing error. w rand arms exhibits the lowest yaw tracking
the robot to explore a broader range of postures. However,
AMO is not necessarily expected to excel in yaw tracking,
as torso yaw rotation induces minimal CoM displace-
ment compared to roll and pitch. Consequently, yaw
tracking accuracy may not fully capture AMOs capacity
for generating adaptive and stable poses. Nonetheless,
it is worth noting that wo AMO struggles with yaw
information for achieving stable yaw control.
2) Height Tracking Accuracy: The results show that AMO
achieves the lowest height tracking error. Notably, wo
AMO reports a signicantly higher error than all other
Unlike torso tracking, where at least one waist motor
angle is directly proportional to the command, height
tracking requires coordinated adjustments across multiple
lower-body joints. Without reference information from
ship between height commands and corresponding motor
Ours(AMO)
Torso Yaw ()
Torso Yaw (-)
Torso Pitch ()
Torso Pitch (-)
Torso Roll ()
Torso Roll (-)
Waist Tracking
Fig. 4: Comparison of torso orientation ranges.
3) Linear Velocity Tracking Accuracy: The AMO module
generates reference poses based on whole-body control
in a double-support stance, meaning it does not account
for pose variations due to foot swing during locomo-
tion. Despite this limitation, AMO remains capable of
performing stable locomotion with a low tracking error,
demonstrating its robustness.
B. How does AMO compare to other WBC strategies?
To address this question, we compare the range of torso
control across the following baselines:
wo AMO: This baseline is the same as described in the
previous section.
Waist Tracking: This baseline explicitly commands the
controlling the torso orientation.
is a representative work in leveraging
human reference motions to guide robot whole-body
control in RL. It achieves torso orientation control by
modifying the reference joint angles of the waist.
Table. III presents the quantitative measurements of torso
control ranges, while Fig. 4 illustrates the qualitative differ-
ences. For ExBody2 and other methods that rely on human
motion tracking, the range of torso control is inherently
constrained by the diversity of the human motion dataset
used for training. If a particular movement direction is un-
derrepresented in the dataset, the learned policy struggles
to generalize beyond those sparse examples. As shown in
our results, ExBody2 exhibits only minor control over torso
pitch and is largely incapable of tracking torso yaw and roll
movements.
Waist Tracking is fundamentally restricted by the robots
waist joint limits, as it relies exclusively on waist motors for
Fig. 5: Evaluation of in-distribution (I.D.) and out-of-
distribution (O.O.D.) tracking results. Each gure shows the
target vs. the actual commanded direction. The white area
indicates I.D., meaning the command is used in both trajectory
optimization and RL training. The grey area indicates O.O.D.,
meaning the command is not used in either trajectory opti-
mization or RL training. The red and blue curves represent
w.o. AMO and AMO, respectively.
torso orientation control rather than utilizing the entire lower
body. For example, Unitree G1s waist pitch motors positional
limit is only 0.52 radians. In contrast, AMO achieves a
signicantly larger range of torso motion compared to other
to bend its upper body completely at. Moreover, the policy
demonstrates adaptive behaviours by leveraging leg motors
to adjust the lower posture for stability. This is evident in
torso roll control, where the robot slightly bends one leg
to tilt its pelvis. Such adaptive behaviour is not observed
in wo AMO. Overall, by incorporating AMO, the policy
not only expands the operational range of torso control but
also improves torso stability by dynamically adapting to the
commanded orientation.
AMOs advantage lies not only in accurately tracking in-
distribution (I.D.) torso commands but also in its ability to
effectively adapt to out-of-distribution (O.O.D.) commands.
In Fig.5, we compare AMO and wo AMO by evaluating
their performance on both I.D. and O.O.D. commands. It
is evident that wo AMO struggles with O.O.D. commands:
it fails to track torso pitch and yaw commands before they
reach the sampled training ranges, and it does not track height
commands at all, as discussed in the previous section. In
O.O.D. commands. It successfully tracks torso yaw command
(a) Paper Bag Picking: The task begins with the robot adjusting
its torso to align its rubber hand with the handle. Then, the robot
should turn and move to an appropriate distance to the destination
the hand out of the handle.
(b) Trash Bottle Throwing: The task begins with the robot stooping
and turning its upper body to the left to grab the trash bottle. The
robot then turns its waist about 90 degrees to the right and throws
the trash bottle into the trash bin.
Fig. 6: Autonomous tasks performed in the real-world setting.
For each task, we collect 50 episodes using the teleoperation
system and train an ACT to complete it autonomously.
up to 2, despite being trained only within the range of 1.57.
was limited to a range of 0.5m to 0.8m, the policy generalizes
well and accurately tracks height as low as 0.4m. These results
indicate that both the AMO module trained via imitation
and the RL policy exhibit strong generalization capabilities,
demonstrating AMOs robustness in adapting to whole-body
commands beyond the training distribution.
C. How well does the AMO system perform in the real-world
setting?
To showcase our policys capability in torso orientation and
base height control, we have performed a series of hyper-
dexterous manipulation tasks using the AMO teleoperation
framework. These teleoperation experiments are presented in
Fig.1 and the supplementary videos.
To further highlight the robustness and hyper-dexterity of
the AMO system, We select several challenging tasks that
require adaptive whole-body control and perform imitation
Paper Bag Picking
Stereo Input; Chunk Size 120
Mono Input; Chunk Size 120
Stereo Input; Chunk Size 60
Trash Bottle Throwing
Stereo Input; Chunk Size 120
Mono Input; Chunk Size 120
Stereo Input; Chunk Size 60
TABLE IV: Training settings and success rate of individual
stages of each task. Each training setting includes: using stereo
(both left-right-eye images) or mono (single image) visual
input; chunk size used in training action-chunking-transformer.
learning by collecting demonstrations, as illustrated in Fig.6.
The performance and evaluations of these tasks are detailed
Paper Bag Picking: This task requires the robot to perform
a loco-manipulation task in the absence of an end-effector,
making it particularly demanding in terms of precision. We
evaluate the impact of various training settings on task perfor-
the policy achieves a near-perfect success rate. While using
only a single image slightly reduces the success rate, this task
is particularly susceptible to shorter chunk sizes. When the
robot places its hand under the bag handle and attempts to
Trash Bottle Throwing: This task involves no locomotion;
a bin positioned at another angle, the robot must execute
extensive torso movements. The evaluation results, as shown
in Table.IV, indicate that our system can learn to complete
this task autonomously. Both stereo vision and longer action
chunks enhance task performance. The benets of stereo vision
likely stem from an increased eld-of-view (FoV) and implicit
depth information, which are essential for grasping.
To demonstrate the effectiveness of our system and its
potential in executing complex loco-manipulation tasks, we
conduct an IL experiment on a long-horizon task that demands
hyper-dexterous whole-body control: Basket Picking. In this
adjust its torso orientation to grasp two baskets positioned on
either side, close to the ground. A recorded autonomous rollout
is presented as a case study in Fig.7, showcasing the learned
policys execution in real-world conditions.
To illustrate how the robot coordinates its whole-body
and the left knee motor. The left knee motor is selected as a
representative joint for height changing and walking. As shown
in Fig.7, the task begins with the robot crouching and bending
forward to align its hands with the baskets height. This motion
is reected by an increase in the knee and waist pitch motor
angles. Next, the robot tilts left and right to grasp the baskets,
as indicated by the variations in the waist roll curve. After
successfully retrieving the targets, the robot stands up, which is
Fig. 7: Basket Picking: A complicated loco-manipulation task
that also requires whole-body coordination. The task begins
with the robot picking two baskets from left(1) and right(2),
which are placed at low heights and close to the ground. Then,
the robot stands up, moves forward(3), and puts two baskets on
the shelf at eye level(4,5). The motor angle trajectories along
the autonomous rollout are displayed and labeled to match the
corresponding phases.
marked by a decrease in knee angle. The periodic uctuations
in the knee motor reading conrm that the robot is walking.
Once stationary, the robot reaches to the left and right to place
the basket on the shelf, with the waist yaw motor rotating back
and forth to facilitate lateral torso movements. This case study
clearly demonstrates our systems ability to leverage whole-
body coordination to accomplish complex tasks effectively.
V. CONCLUSIONS AND LIMITATIONS
We present AMO, a framework that integrates model-
based trajectory optimization with model-free reinforcement
learning to achieve whole-body control. Through extensive
experiments in both simulation and real-robot platforms, we
demonstrate the effectiveness of our approach. AMO enables
real-world humanoid control with an unprecedented level of
dexterity and precision. We hope that our framework provides
a novel pathway for achieving humanoid whole-body control
beyond conventional model-based whole-body methods and
RL policies that rely on tracking human motion references.
While our decoupled approach introduces a new paradigm
for whole-body control, its separated nature inherently limits
the level of whole-body coordination it can achieve. For
not only their lower body and waist but also their arms to
maintain balance. However, in our current setup, arm control is
independent of the robots base state. A balance-aware upper-
body control mechanism could be explored by incorporating
base state information into upper-limb joint-angle acquisition,
potentially enhancing overall stability and adaptability.
VI. ACKNOWLEDGEMENTS
This project was supported, in part, by gifts from Amazon,
Qualcomm and Meta.
REFERENCES
Unitree Robotics, G1, 2024, www.unitree.comg1, [On-
line; accessed Jan. 2025].
www.robotis.us
Ananye Agarwal, Ashish Kumar, Jitendra Malik, and
Deepak Pathak. Legged locomotion in challenging ter-
rains using egocentric vision. In Conference on Robot
Anthony Brohan, Noah Brown, Justice Carbajal, Yev-
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
control at scale. arXiv preprint arXiv:2212.06817, 2022.
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2:
Vision-language-action models transfer web knowledge
to robotic control.
arXiv preprint arXiv:2307.15818,
Stephane Caron, Yann De Mont-Marin, Rohan Bud-
Simeon Nedelchev.
based on Pinocchio, 2024.
stephane-caronpink.
Xuxin Cheng, Ashish Kumar, and Deepak Pathak. Legs
as manipulator: Pushing quadrupedal agility beyond lo-
comotion.
In 2023 IEEE International Conference on
Robotics and Automation (ICRA), 2023.
Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak
Pathak. Extreme parkour with legged robots, 2023. URL
Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang,
Ge Yang, and Xiaolong Wang. Expressive whole-body
control for humanoid robots, 2024. URL
orgabs2402.16796.
Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang,
Ge Yang, and Xiaolong Wang.
Expressive whole-
body control for humanoid robots.
arXiv preprint
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback. CoRL, 2024.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
fusion policy: Visuomotor policy learning via action
diffusion.
In Proceedings of Robotics: Science and
Systems (RSS), 2023.
Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau,
Benjamin Burchel, Siyuan Feng, Russ Tedrake, and
Shuran Song.
Universal manipulation interface: In-
the-wild robot teaching without in-the-wild robots.
Proceedings of Robotics: Science and Systems (RSS),
Matthew Chignoli, Donghyun Kim, Elijah Stanger-Jones,
and Sangbae Kim.
The mit humanoid robot: Design,
motion planning, and control for acrobatic behaviors.
In 2020 IEEE-RAS 20th International Conference on
Humanoid Robots (Humanoids), pages 18. IEEE, 2021.
Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi
From play to policy:
Conditional behavior generation from uncurated robot
data. arXiv preprint arXiv:2210.10047, 2022.
Antonin Dallard, Mehdi Benallegue, Fumio Kanehiro,
and Abderrahmane Kheddar.
Synchronized human-
humanoid motion imitation. IEEE Robotics and Automa-
tion Letters, 8(7):41554162, 2023. doi: 10.1109LRA.
Timothee Darcet, Maxime Oquab, Julien Mairal, and
Piotr Bojanowski.
Vision transformers need registers,
Behzad Dariush, Michael Gienger, Bing Jian, Christian
control from human motion descriptors. In 2008 IEEE
International Conference on Robotics and Automation,
pages 26772684. IEEE, 2008.
Kourosh Darvish, Yeshasvi Tirupachuri, Giulio Ro-
Francisco
Javier Andrade Chavez, and Daniele Pucci.
body geometric retargeting for humanoid robots.
2019 IEEE-RAS 19th International Conference on Hu-
manoid Robots (Humanoids), pages 679686, 2019. doi:
Helei Duan, Bikram Pandit, Mohitvishnu S Gadde,
Bart Jaap van Marum, Jeremy Dao, Chanho Kim, and
Alan Fern.
Learning vision-based bipedal locomotion
for challenging terrain. arXiv preprint arXiv:2309.14594,
Alejandro Escontrela, Xue Bin Peng, Wenhao Yu,
Tingnan Zhang, Atil Iscen, Ken Goldberg, and Pieter
Abbeel. Adversarial motion priors make good substitutes
for complex reward functions. In 2022 IEEERSJ Inter-
national Conference on Intelligent Robots and Systems
(IROS), pages 2532. IEEE, 2022.
Jiawei Fu, Yunlong Song, Yan Wu, Fisher Yu, and Davide
Scaramuzza.
Learning deep sensorimotor policies for
vision-based autonomous drone racing, 2022.
Zipeng Fu, Ashish Kumar, Jitendra Malik, and Deepak
Minimizing energy consumption leads to the
emergence of gaits in legged robots.
Conference on
Robot Learning (CoRL), 2021.
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein,
and Chelsea Finn. Humanplus: Humanoid shadowing and
imitation from humans, 2024. URL
Zipeng Fu, Tony Z Zhao, and Chelsea Finn.
bile aloha: Learning bimanual mobile manipulation with
low-cost whole-body teleoperation.
arXiv preprint
Yuni Fuchioka, Zhaoming Xie, and Michiel Van de
for dynamic quadruped behaviors. In 2023 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 50925098. IEEE, 2023.
Huy Ha, Yihuai Gao, Zipeng Fu, Jie Tan, and Shuran
Song. Umi on legs: Making manipulation policies mobile
with manipulation-centric whole-body controllers. arXiv
preprint arXiv:2407.10353, 2024.
Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong
Guanya Shi. Omnih2o: Universal and dexterous human-
to-humanoid whole-body teleoperation and learning,
2024. URL
Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris
to-humanoid real-time whole-body teleoperation. arXiv
preprint arXiv:2403.04436, 2024.
Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia
neural whole-body controller for humanoid robots. arXiv
preprint arXiv:2410.21229, 2024.
Kazuo Hirai, Masato Hirose, Yuji Haikawa, and Toru
Takenaka. The development of honda humanoid robot.
In Proceedings. 1998 IEEE international conference on
robotics and automation (Cat. No. 98CH36146), vol-
ume 2, pages 13211326. IEEE, 1998.
Marco Hutter, Christian Gehring, Dominic Jud, Andreas
Anymal-a highly mobile and dynamic
quadrupedal robot. In IROS, 2016.
Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li,
Ge Yang, Xuxin Cheng, and Xiaolong Wang.
trol. arXiv preprint arXiv:2412.13196, 2024.
Shuuji Kajita. A simple modeling for a biped walking
pattern generation. In Proceedings of the International
Conference on Intelligent Robotics and Systems, Maui,
Ichiro Kato. Development of wabot 1. Biomechanism,
Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ash-
win Balakrishna, Sudeep Dasari, Siddharth Karam-
Lawrence Yunliang Chen, Kirsty Ellis, Peter David
Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu,
Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abra-
ham Lee, Youngwoon Lee, Marius Memmel, Sungjae
Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan
nag R Sanketi, Archit Sharma, Cody Simpson, Quan
Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao,
Christopher Agia, Rohan Baijal, Mateo Guaman Cas-
Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario
Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Ji-
tendra Malik, Roberto Martn-Martn, Subramanian Ra-
Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey
wild robot manipulation dataset. 2024.
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra
Malik. Rma: Rapid motor adaptation for legged robots.
arXiv preprint arXiv:2107.04034, 2021.
Zhongyu Li, Xuxin Cheng, Xue Bin Peng, Pieter Abbeel,
Sergey Levine, Glen Berseth, and Koushil Sreenath.
Reinforcement learning for robust parameterized loco-
motion control of bipedal robots. In 2021 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 28112817. IEEE, 2021.
Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey
ment learning for versatile, dynamic, and robust bipedal
locomotion control.
arXiv preprint arXiv:2401.16889,
Qiayuan Liao, Bike Zhang, Xuanyu Huang, Xiaoyu
arXiv preprint arXiv:2407.21781, 2024.
Fukang Liu, Zhaoyuan Gu, Yilin Cai, Ziyi Zhou, Shijie
feasible whole-body trajectories for versatile humanoid
loco-manipulation.
arXiv preprint arXiv:2409.20514,
Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang,
Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, and
Xiaolong Wang.
priors for humanoid whole-body control. arXiv preprint
Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje,
Gerard Pons-Moll, and Michael J. Black. Amass: Archive
of motion capture as surface shapes.
In The IEEE
International Conference on Computer Vision (ICCV),
Oct 2019. URL
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong
performance
gpu-based
physics simulation for robot learning.
arXiv preprint
Gabriel B Margolis and Pulkit Agrawal.
Walk these
plicity of behavior. Conference on Robot Learning, 2022.
Gabriel B Margolis, Tao Chen, Kartik Paigwar, Xi-
ang Fu, Donghyun Kim, Sangbae Kim, and Pulkit
Agrawal. Learning to jump from pixels. arXiv preprint
Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen,
and Pulkit Agrawal. Rapid locomotion via reinforcement
learning. arXiv preprint arXiv:2205.02824, 2022.
Carlos Mastalli, Rohan Budhiraja, Wolfgang Merkt,
Guilhem Saurel, Bilal Hammoud, Maximilien Naveau,
Justin Carpentier, Ludovic Righetti, Sethu Vijayakumar,
and Nicolas Mansard.
Versatile Framework for Multi-Contact Optimal Control.
In IEEE International Conference on Robotics and Au-
tomation (ICRA), 2020.
Carlos Mastalli, Wolfgang Merkt, Josep Marti-Saumell,
Henrique Ferrolho, Joan Sola, Nicolas Mansard, and
Sethu Vijayakumar.
A feasibility-driven approach to
control-limited ddp. Autonomous Robots, 2022.
Russell Mendonca, Shikhar Bahl, and Deepak Pathak.
world. ICRA, 2023.
Hirofumi Miura and Isao Shimoyama. Dynamic walk of
a biped. IJRR, 1984.
Federico L Moro and Luis Sentis. Whole-body control
of humanoid robots. Humanoid Robotics: A reference,
Maxime Oquab, Timothee Darcet, Theo Moutakanni,
Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre
Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan
mand Joulin, and Piotr Bojanowski. Dinov2: Learning
robust visual features without supervision, 2023.
Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex
Anant Rai, Anikait Singh, Anthony Brohan, et al. Open
els. arXiv preprint arXiv:2310.08864, 2023.
Jyothish Pari, Nur Muhammad Shaullah, Sridhar Pan-
dian Arunachalam, and Lerrel Pinto. The surprising ef-
fectiveness of representation learning for visual imitation.
arXiv preprint arXiv:2112.01511, 2021.
Leonardo Lanari, Giuseppe Oriolo, and Serena Ivaldi.
A multimode teleoperation framework for humanoid
IEEE Robotics and Automation Magazine, 26(4):7382,
Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-
Wei Lee, Jie Tan, and Sergey Levine.
Learning agile
robotic locomotion skills by imitating animals.
Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine,
and Angjoo Kanazawa. AMP: adversarial motion priors
for stylized physics-based character control. ACM Trans.
Graph., 40(4):120, July 2021.
Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and
Angjoo Kanazawa. Amp: Adversarial motion priors for
stylized physics-based character control. ACM Transac-
tions on Graphics (ToG), 40(4):120, 2021.
Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk,
Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dieter Fox.
hand teleoperation system.
In Robotics: Science and
Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Dar-
Learning
humanoid locomotion with transformers. arXiv preprint
Marc H Raibert. Legged robots that balance. MIT press,
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Mingyo Seo, Steve Han, Kyutae Sim, Seung Hyeon
Deep imitation learning for humanoid loco-manipulation
through human teleoperation. In 2023 IEEE-RAS 22nd
International Conference on Humanoid Robots (Hu-
manoids), pages 18. IEEE, 2023.
Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit
Chelsea Finn.
Yell at your robot: Improving on-the-
y from language corrections.
arXiv preprint arXiv:
nipulation. In Conference on Robot Learning, pages 785
Jonah Siekmann, Kevin Green, John Warila, Alan Fern,
and Jonathan Hurst.
Blind bipedal stair traversal
via sim-to-real reinforcement learning.
arXiv preprint
Tomomichi Sugihara.
Solvability-unconcerned inverse
kinematics by the levenbergmarquardt method. IEEE
Transactions on Robotics, 27(5):984991, 2011.
ieee.orgdocument5784347.
Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor,
Gal Chechik, and Xue Bin Peng.
adversarial latent models for directable virtual characters.
In ACM SIGGRAPH 2023 Conference Proceedings, SIG-
GRAPH 23, New York, NY, USA, 2023. Association
for Computing Machinery. ISBN 9798400701597. doi:
Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang,
Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anand-
kumar. Mimicplay: Long-horizon imitation learning by
watching human play. arXiv preprint arXiv:2302.12422,
Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja
Fidler. Unicon: Universal neural controller for physics-
based character motion, 2020.
Eric R Westervelt, Jessy W Grizzle, and Daniel E
Koditschek.
Hybrid zero dynamics of planar biped
walkers. IEEE transactions on automatic control, 48(1):
Ruihan Yang, Zhuoqun Chen, Jianhan Ma, Chongyi
Generalized animal imitator: Agile locomotion with ver-
satile motion prior.
arXiv preprint arXiv:2310.01408,
Ruihan Yang, Ge Yang, and Xiaolong Wang.
volumetric memory for visual locomotion control.
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 14301440, 2023.
KangKang Yin, Kevin Loken, and Michiel Van de Panne.
actions on Graphics, 2007.
Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu,
Muhan Wang, and Huazhe Xu.
3d diffusion policy:
Generalizable visuomotor policy learning via simple 3d
representations. In Proceedings of Robotics: Science and
Systems (RSS), 2024.
Qiang Zhang, Peter Cui, David Yan, Jingkai Sun, Yiqun
Whole-body
humanoid robot locomotion with human reference. arXiv
preprint arXiv:2402.18294, 2024.
Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea
Finn. Learning ne-grained bimanual manipulation with
low-cost hardware.
arXiv preprint arXiv:2304.13705,
Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher
Zhao. Robot parkour learning. In Conference on Robot
Learning (CoRL), 2023.
