=== PDF文件: Dexonomy Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy.pdf ===
=== 时间: 2025-07-22 16:07:39.691465 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Grasp Types in a Grasp Taxonomy
Jiayi Chen1,2, Yubin Ke1,2, Lin Peng2, He Wang1,2,3
1Peking University, 2Galbot, 3Beijing Academy of Artificial Intelligence
Equal contribution, Corresponding author
Fig. 1: For any grasp type in the GRASP taxonomy , any object, and any articulated hand, our pipeline efficiently
synthesizes contact-rich, penetration-free, and physically plausible dexterous grasps, starting from only one human-annotated
grasp template to specify an initial hand pose and contact information per hand and grasp type.
AbstractGeneralizable
dexterous
grasping
suitable
grasp types is a fundamental skill for intelligent robots. Devel-
oping such skills requires a large-scale and high-quality dataset
that covers numerous grasp types (i.e., at least those categorized
by the GRASP taxonomy), but collecting such data is extremely
challenging. Existing automatic grasp synthesis methods are often
limited to specific grasp types or object categories, hindering
scalability. This work proposes an efficient pipeline capable
of synthesizing contact-rich, penetration-free, and physically
plausible grasps for any grasp type, object, and articulated hand.
Starting from a single human-annotated template for each hand
and grasp type, our pipeline tackles the complicated synthesis
problem with two stages: optimize the object to fit the hand
template first, and then locally refine the hand to fit the object
in simulation. To validate the synthesized grasps, we introduce
a contact-aware control strategy that allows the hand to apply
the appropriate force at each contact point to the object. Those
validated grasps can also be used as new grasp templates to
facilitate future synthesis. Experiments show that our method
significantly outperforms previous type-unaware grasp synthesis
baselines in simulation. Using our algorithm, we construct a
dataset containing 10.7k objects and 9.5M grasps, covering
31 grasp types in the GRASP taxonomy. Finally, we train a
type-conditional generative model that successfully performs the
desired grasp type from single-view object point clouds, achieving
an 82.3 success rate in real-world experiments. Project page:
I. INTRODUCTION
Dexterous grasping is a fundamental skill for intelligent
though dexterous grasping has received increasing attention,
most prior work focuses on whether a dexterous hand can
successfully grasp an object, rather than considering how to
grasp it. As a result, the dexterous hand loses its dexterity and
becomes functionally similar to a large parallel gripper. True
dexterous grasping is not merely about grasping with dexter-
ous hands, but about grasping dexterously with appropriate
grasp types based on the task requirement. For example, when
a robot needs to securely grasp an apple or hold a knife to cut,
it should use a power grasp to envelop the object. Conversely,
when grasping a lightweight or flat object from the table, a
precision grasp using the fingertips would be more suitable.
To develop such intelligent skills, there are two key chal-
task and (2) generating high-quality grasps for specified types
and objects. The first challenge is a high-level decision-making
problem and can take advantage of recent advances in large
vision-language models, e.g., GPT-4o , as a temporary
solution. However, the second challenge is less studied and
represents a significant bottleneck, which is the main focus
of this paper. To address the problem of type-aware grasp
synthesis with data-driven methods, the first step is to build a
large-scale grasp dataset that at least includes most of the grasp
types in the GRASP taxonomy . However, collecting and
annotating grasp data, particularly for multi-fingered hands in
contact-rich scenarios, remains a big challenge.
Several approaches have been explored for automatically
synthesizing a large-scale dexterous grasp dataset, but most
of them suffer from various limitations. Analytical grasp
synthesis methods [47, 44, 25, 8, 7] are often applicable
to any object, but most of them are type-unaware and the
synthesized grasps only belong to limited types. This is
because specifying flexible grasp types solely through ana-
lytical metrics is challenging. Moreover, these methods often
produce unnatural hand poses, as they prioritize force closure,
which does not always align with human habits. Another line
of research [55, 48, 52] focuses on transferring functional
dexterous grasps by mapping object contact regions. While
these methods generate more human-like grasps and support
a wider range of grasp types, they are limited to objects
that are geometrically similar or axis-aligned with the initial
In this work, we propose a novel pipeline based on sampling
and optimization to address these challenges. As shown in
Figure 1, our algorithm can efficiently synthesize high-quality
dexterous grasps for any grasp type, object, and articulated
hand and grasp type. Our synthesized grasps achieve rich
hand-object contact (e.g., > 10 hand links within 2 mm
of the object for power grasps), guarantee penetration-free
poses via collision mesh verification, and satisfy force closure
under six-axis testing in MuJoCo   all with shared
hyperparameters across grasp types, objects, and hands.
Our key insight is that grasping can be framed as a geomet-
ric matching problem, where the hand and object should align
through contact points. We begin by introducing a human-
annotated grasp template that specifies the initial hand pose
and contact information (i.e., points and normals). Unlike
previous methods that directly adjust the hand pose to fit
the object, we first use a lightweight stage that samples and
optimizes the object pose to match the hand contacts defined in
the grasp template. This stage supports hundreds of thousands
of initial samples processed in parallel on a single GPU and
leaves only a small number of promising results for the next
stage. The combination of dense sampling and optimization
helps avoid local optima without sacrificing efficiency.
After aligning the object pose, the hand only needs a slight
refinement to get a good grasp. This dual-stage design not
only eases the hand refinement stage, but also ensures that the
final grasp remains similar to the initial grasp template and
thus remains natural. In contrast to most prior work [24, 47,
to refine the hand pose, we propose a novel method based on
the transposed Jacobian control in MuJoCo. This approach is
key to achieving rich contacts while ensuring no penetration,
with minimal coding effort and parameter tuning.
assess their ability to withstand external forces applied to the
object. Unlike previous work [47, 60] that designs heuristics to
squeeze the hand for applying force on the object, which is not
suitable for all grasp types, we design a contact-aware control
strategy that computes the desired forces for each contact point
and controls the hand to apply them approximately, also based
on the transposed Jacobian control. Finally, high-quality grasps
that pass the simulation tests can be used to construct new
grasp templates, reducing the need for human annotations and
broadening the range of objects that can be grasped.
Experiments show that our method greatly outperforms
previous type-unaware grasp synthesis baselines in simulation,
especially under more challenging testing conditions (i.e.,
smaller friction coefficients) and with a wider variety of ob-
jects (i.e., from Objaverse ). Using our proposed pipeline,
we also build a large-scale dataset covering different grasp
types. This dataset further enables training a type-conditional
generative model that generates desired grasp types for novel
objects from single-view point clouds, achieving a success
rate of 82.3 on the Shadow hand in real-world experiments.
annotation UI for collecting semantic grasps on the specified
object regions with only a few mouse clicks.
In summary, our main contributions are:
An efficient pipeline to synthesize high-quality grasps for
any grasp type, object, and articulated hand, starting from
one human-annotated template per hand and grasp type.
A large-scale dataset containing 9.5M grasps and 10.7k
A type-conditional generative model that can use the
specified grasp types to grasp novel objects in the real
An annotation UI for collecting semantic grasps with only
a few mouse clicks.
II. RELATED WORK
A. Analytical Grasp Synthesis
Analytical grasp synthesis aims to find good grasps, typ-
ically measured by wrench-based force closure metrics [21,
40]. These methods generally assume complete knowledge
of the objects geometry for the metric calculation, making
them more suitable for data preparation than for real-world
For parallel grippers and suction cups, where hand-object
contact patterns are simple and the dimension of the hand pose
is low (around 7), many work [32, 33, 19, 4] randomly sample
a large number of grasps and select those with better metrics
as the final results. For dexterous hands, which have more
complex contact patterns and higher pose dimensions (often
exceeding 20), random sampling is insufficient to find good
sampling-based optimization, such as simulated annealing,
since the commonly used metrics are non-differentiable. More
recent approaches [27, 43, 44, 47, 6, 25, 8, 7] introduce
differentiable energies to leverage gradient-based optimization.
requiring many iterations to adjust the hand pose, which can be
poses. Our approach benefits from both random sampling and
gradient-based optimization, greatly reducing the complexity.
B. Grasp Taxonomy
The GRASP taxonomy  categorizes 33 distinct grasp
types based on human daily activities. Many previous analyt-
ical methods [6, 7, 25, 8] can only synthesize limited grasp
47] supports contact beyond the fingertip, they can only syn-
thesize some simple grasp types, e.g. warping fingers around
the object, and typically have high randomness and cannot
specify a desired type. Some work on functional dexterous
grasping [23, 48, 1, 52, 61] tackles more complex, human-
like grasp types, but is often limited to axis-aligned objects
with similar geometries, hindering scalability. In contrast, our
method can synthesize all 33 grasp types without assumptions
about the object, requiring only one template per grasp type.
C. Data Collection for Learning-based Grasp Synthesis
Learning-based dexterous grasp synthesis [53, 51, 50] is
often built on generative models such as CVAE, diffusion
observation and be deployed in the real world, their per-
formance is highly dependent on the quality of the training
dataset. To build dexterous grasp datasets, several approaches
are explored, including teleoperation [38, 11, 56], transferring
grasps from human hands [3, 37, 30, 5, 55, 10], and rein-
forcement learning (RL) [59, 46, 13]. Our work also presents a
possible approach for scaling up grasp data collection, because
our method requires less human effort than teleoperation,
avoids the morphological gap seen with human hands, and
is often more efficient than RL.
D. Physics Simulation for Robotic Grasping
Rigid-body physics simulators, such as Isaac PhysX ,
or perform RL for dexterous hands. However, prior work
rarely uses simulators for local refinement of the hand pose to
improve contact with the object. Most prior work [24, 47, 8, 7]
develops custom energy functions and optimizers to refine
hand-object contact, but they often lead to centimeter-level
penetrations and require careful hyperparameter tuning. In
achieving submillimeter-level contact convergence with mini-
mal parameter tuning in different experiment settings. This is
made possible by MuJoCos highly optimized framework and
its second-order Newton optimizer.
III. PRELIMINARY OF GRASP QUALITY METRIC
In this section, we summarize a unified formulation of grasp
quality metrics widely used in recent work [27, 47, 51, 7].
While prior work focuses on their own formulations, our
summary provides a cohesive view across them. Although our
pipeline does not directly optimize this metricunlike prior
analytical methodsit plays an important role in several stages
of our pipeline, such as post-filtering (Sections IV-B and IV-C)
and the contact-aware control strategy (Section IV-D).
We assume that the object O is grasped by a robot hand
with m contact points. For each contact i, let pi R3 denote
the contact position, ni R3 the inward-pointing surface unit
satisfying ni  di  ci. The Coulomb friction cone Fi and
the contact Jacobian Jo,i for the object at contact i are defined
as follows:
xi R3  0 xi,1 1, x2
where  is the friction coefficient. The friction cone Fi
represents all feasible contact forces at contact i, and Jo,i
maps a contact force xi to a wrench wi  JT
To balance an external wrench g R6 (e.g., object gravity),
the optimal contact forces {fi}m
i1 are obtained by solving the
following quadratic program (QP):
(f1, ..., fm)  arg min
where  is a hyperparameter enforcing a minimum total nor-
mal force to avoid trivial solutions. To reduce computational
converting the problem into a linearly-constrained quadratic
program (LCQP) that can be efficiently solved .
where a lower e indicates a more stable or robust grasp.
The formulation in Eq. 3 is intuitive for testing one external
wrench on the object. However, to test force-closure grasps,
we typically need to consider six orthogonal gravities . This
requires solving the LCQP six times for each grasp and thus is
inefficient. The energy function used in DexGraspNet [27, 47]
assumes equal contact forces and zero friction, simplifying
the problem by setting fi  [1, 0, 0] and g  0. While this
approach eliminates the need for optimization and is extremely
providing a better balance between computational cost and
accuracy. Therefore, in this paper, we adopt the variation
proposed by  as the grasp quality metric.
Fig. 2: The pipeline of Dexonomy. (1) Grasp Template Library initially requires one human-annotated template. (2) Lightweight
Global Alignment stage samples and optimizes the object poses in parallel on a GPU, to match the contact points and normals
of the selected grasp templates. (3) Simulation-based Local Refinement stage adjusts the hand pose to improve hand-object
contacts. (4) Simulation Validation tests force-closure grasps using our proposed contact-aware control strategy. (5) New
templates are constructed from successful grasps and added to the Grasp Template Library, used in the following iterations.
IV. GRASP SYNTHESIS METHOD
In this section, we present our proposed dexterous grasp
synthesis pipeline, with an overview shown in Figure 2.
A. Grasp Template Definition
A grasp template consists of several components: the hand
joint configuration q Rq, hand contact points ph
corresponding normals nh
i R3, and the link name for each
contact point (i  1, 2, . . . , m). Our algorithm requires a single
human-annotated grasp template for each hand and grasp type
as initialization.
B. Lightweight Global Alignment of Object Pose
In this stage, we simultaneously sample and optimize the
object pose to align with the selected templates hand contacts
while keeping the hand pose fixed. The optimization variable
is the objects transformation, parameterized by its (optional)
scale so R, rotation Ro S3, and translation to R3.
Before optimization, we begin with dense sampling. First, a
random grasp template is selected from the Grasp Template Li-
on the object is chosen. The object is initialized by aligning
the sampled hand and object contacts, where contact points
are matched and the contact normal directions are set opposite.
The objects scale and in-plane rotation perpendicular to the
normal direction are randomly sampled. Our pipeline supports
parallelizing massive samples of different contacts, objects,
and grasp templates on a single GPU.
During each optimization iteration, each hand contact point
i calculates the nearest point po
i on the objects surface using
the differentiable library Warp . To penalize the mismatch
between hand and object contacts, we optimize the object pose
by minimizing the following energy function:
where kp and kn are hyperparameters. There is no other energy
used for optimization except Eq. 7.
After optimization, results are filtered using four criteria.
ensure a good match between hand and object contacts. Sec-
collision skeletons parameterized by line segments (details in
Appendix A). Third, the object contact quality, as measured
by Eq. 6, must exceed a threshold. Finally, we apply a process
similar to farthest point sampling to filter out duplicate object
Our design, using only one energy during optimization
and leaving other checks for post-filtering, provides several
advantages. First, it reduces computational costs, enabling
maximized parallelization to benefit from dense sampling to
avoid local optimum traps. Second, it reduces sensitivity to
C. Simulation-based Local Refinement of Hand Pose
In this stage, the object is fixed, and the hand pose is locally
refined to improve the hand-object contact. A virtual force fi is
needed at each hand point ph
i toward the corresponding nearest
object point po
i . To apply these virtual forces in MuJoCo,
they are transferred to the hands joint torque via simplified
transposed Jacobian control:
fi  kf(ph
where kf is a hyperparameter and JT
of the hand contact Jacobian that maps force vectors from the
world to joint coordinates.
While Eq. 8 is a simplified control strategy with many as-
sumptions (e.g., no dynamics or gravity; joint torques mapped
from each contact force are independent and additive), it serves
Sim.Real
Grasp Types
Force Closure
Data Type
DexGraspNet
IsaacGym
Grasp pose
Optimization
Teleoperation
Multiple
Fingertip
Optimization
Dexonomy (Ours)
31 types
Samplingopt.
TABLE I: Dexterous Grasp Dataset Comparison. Our large-scale dataset aims to support the study of data-driven methods
for type-aware grasp synthesis.
our need for synthesizing contact-rich grasps in simulation.
This is easy to implement and theoretically works for other
physics simulators. Eq. 8 is iteratively applied for 200 steps,
where po
i is fixed in the world frame to avoid drift, while ph
fixed in the hand link frame (and moves in the world frame).
To ensure the hand remains strictly penetration-free with
respect to the object, we apply a 1mm contact margin in Mu-
JoComeaning the hand is repelled if it comes within 1mm of
the object surface. Since MuJoCo reliably resolves penetration
at the millimeter scale, the resulting contact distance between
the hand and the object typically falls within the range of
[0, 2mm]. This behavior motivates our use of a 2mm contact
distance to the object is less than 2mm.
After optimization, we filter the results based on three
criteria. First, there should be no penetration between the hand
and the object, measured using collision meshes. Second, those
fingers with at least one annotated contact are required to
be in contact with the object. This prevents the synthesized
grasps from deviating significantly from the initial template.
a threshold, which is needed again because the final contact
points may slightly differ from the ones in the previous stage.
D. Simulation Validation with Contact-Aware Control
To validate the synthesized grasps in MuJoCo, the hand
should squeeze to hold the object stably, controlled by a
control signal of joint torques. Our contact-aware control
strategy first calculates the desired forces on each contact using
Eq. 3 where g  0, and then converts these forces into joint
torques using the same transposed Jacobian control as in Eq. 8.
The g is set to zero to reduce computational cost and
standardize the control signals across different external forces.
Despite the resultant force of all contacts being approximately
Given the large strength of the normal force (i.e., around
1 10N in our experiment for a 100g object), constrained
by Eq. 5, there is typically sufficient room for friction to
considered to succeed only if the object remains stable under
all six orthogonal external forces for 2 seconds in simulation.
E. Construction of New Grasp Templates
Once a grasp successfully passes the simulation validation,
a new grasp template is constructed and added to the template
library. The joint configuration of the new template is taken
from the successful grasp, while the contact information (i.e.,
Fig. 3: Dexonomy Dataset Visualization. Each color corre-
sponds to a different grasp type.
points and normals) is updated only if an actual contact is
detected near the original contact on the same hand link. This
strategy prevents the new templates contact information from
deviating too much from the original. Newly added templates
can be randomly selected during the global alignment stage in
subsequent iterations of the whole pipeline.
V. DEXONOMY DATASET
Using our proposed grasp synthesis pipeline, we construct
a large-scale dataset for Shadow hand covering 31 grasp types
from the GRASP taxonomy . This dataset is designed
to support research on data-driven methods for type-aware
grasp synthesis. Two grasp types in the taxonomy, Distal
Type (19) and Tripod Variation (21), are excluded due
to their specificity to object categories, namely scissors and
As shown in Table I, our dataset comprises 10.7k object
objects are normalized such that the diagonal of their axis-
aligned bounding box is 2 meters, with scales ranging between
[0.05, 0.2]. Only successful grasps are retained, resulting in
9.5M data points. The entire dataset was synthesized in less
than 3 days on a server with 8 NVIDIA RTX 3090 GPUs.
Additional statistics are provided in Table IV and Appendix B.
Each data point includes three key poses:
Grasp pose, obtained via local refinement (Section IV-C).
Pre-grasp pose for collision-free motion planning, gen-
erated after the grasp pose by enforcing a 2cm contact
margin in MuJoCopushing the hand away if it is within
2cm of the object.
Squeeze pose, derived from the control signal used
for simulation validation (Section IV-D), to apply force
through hand-object contacts.
Normalizing Flow
Grasp-type
codebook
Single-view
point cloud
Grasp pose,
probability
Condition
Pregrasp pose,
three hand qpos
Random sample in a
base distribution
Fig. 4: Type-Conditional Grasp Generative Model. Without
the grasp-type codebook in the red dashed box, the model
becomes type-unconditional and is similar to BODex .
These poses provide the minimal requirements for generating a
complete grasping trajectory (including reaching and squeez-
ing) and are compatible with diverse robot arms and initial
hand configurations.
VI. TYPE-CONDITIONAL GRASP GENERATIVE MODEL
To generate grasps from partial observations for real-
world deployment, data-driven methods are essential. Al-
though learning is not the main focus of this paper, we present
a simple model as an initial try. The model architecture,
illustrated in Figure 4, is similar to previous work [60, 7],
with the key difference being the grasp-type codebook added
as a conditional input to specify a grasp type.
The input to the model consists of a single-view object
point cloud and a type feature f i
t selected from the grasp-
type codebook. The point cloud is encoded into a feature fv
using a Sparse3DConv network with MinkowskiEngine .
This vision feature fv, along with the type feature ft, are
concatenated to form a conditional feature fc. Conditioned on
a base distribution to a grasp pose Rg and Tg, and calculates
a probability p indicating the pose quality. The predicted grasp
pose is then concatenated with fc and passed through an MLP
to predict a pre-grasp pose Rp, Tp, and three hand qpos
respectively. The whole model is trained end-to-end and the
type feature f i
t is also optimizable.
VII. EXPERIMENT
A. Evaluation Metrics
The following metrics are used for a comprehensive evalu-
ation of the synthesis pipeline and grasp quality. All distances
are measured using collision meshes in MuJoCo.
Grasp Success Rate (GSR) (unit: ): The percentage of
successful grasps relative to the attempt number. For our
by the global alignment stage. A grasp succeeds only if it
resists six external forces in MuJoCo and does not have
severe penetrations (> 1 cm), since the penetration may cause
simulation failure and prevent the object from moving. The
object mass is 100g, and the success criteria for the object
pose are 5cm and 15.
Object Success Rate (OSR) (unit: ): The percentage of
objects that have at least one successful grasp. If the object
scales are fixed, as in Sections VII-B and VII-E, different
scales of the same object are treated as separate objects.
Speed (S) (unit: second1): The maximum number of
attempts completed per second on a server with 8 NVIDIA
RTX 3090 GPUs and 2 Intel Xeon Platinum 8255C CPUs
(48 cores, 96 threads). We report the time running on a server
because our method utilizes both GPUs and CPUs. This metric
excludes simulation validation.
Contact Link Number (CLN): The number of hand links
whose distance to the object surface is within 2 mm.
Contact Distance Consistency (CDC) (unit: mm): The
delta between the maximum and minimum signed distances
across all fingers. This metric quantifies the variation in contact
distance across different fingers and is invariant to penetration.
Penetration Depth (PD) (unit: mm): The maximum inter-
section distance between the hand and object for each grasp.
Self-Penetration Depth (SPD) (unit: mm): The maximum
self-intersection distance among different hand links.
Diversity (D) (unit: ): The proportion of total variance
explained by the first principal component in PCA, computed
as the ratio of the first eigenvalue to the sum of all eigenvalues.
PCA is performed on data points that include grasp translation
angles qg.
B. Type-Unaware Grasp Synthesis
1) Comparison with analytical methods: Four open-source
baselines are compared: DexGraspNet , FRoGGeR ,
Experiment settings. Most of the settings follow the bench-
mark provided by BODex , with some modifications on the
object set to increase difficulty. The Allegro hand is used, as it
is the only hand type supported by all baselines. 5697 object
assets from DexGraspNet are used, with six scales applied to
each normalized object: 0.05, 0.08, 0.11, 0.14, 0.17, and 0.20.
Although our method supports optimizing object scales, we
fix the object scale to ensure a fair comparison. The attempt
number for each method is set to 20, and we manually annotate
two grasp templates, each with 10 attempts.
Quantitative result analysis. As shown in Table II, our
method achieves the highest grasp success rate and best perfor-
mance on contact and penetration. Our speed is slightly lower
than that of BODex, as their pipeline is highly optimized for
version. Detailed time analysis of our pipeline is available in
Appendix F. Our diversity is somewhat lower, as we use only
two initial templates and our hand refinement stage makes
only local adjustments. However, the overall diversity of our
Dexonomy dataset is much better, as reported in Table IV.
The success rates of baselines are lower than those reported
in BODex , primarily because our objects have a higher
mass (100g vs. 30g), a larger scale range ([0.05, 0.2] vs.
[0.06, 0.12]), and more diverse shapes (5697 instances vs.
CDC (mm)
SPD (mm)
DexGraspNet
SpringGrasp
TABLE II: Comparison with Type-Unaware Grasp Synthesis Baselines for Allegro Hand. Most baselines, except
Fig. 5: Visualization of Type-Unaware Grasps. Our method
synthesizes human-like and stable grasps, even for objects with
complex geometries (e.g., object scales  0.11, 0.17, and 0.20).
2397). More details about the performance drop of baselines
are provided in Appendix C.
Visualization analysis. Figure 5 illustrates the initial hand
pose and some synthesized grasps for each method. Our
method consistently synthesizes human-like and stable grasps,
even for objects with complex geometries (e.g., for scales
0.11, 0.17, and 0.20). Notably, the synthesized grasp for
template 1 and object scale 0.05 requires high precision and is
challenging for previous methods. Furthermore, the grasp for
template 2 and object scale 0.14 shows a much larger thumb-
to-other-tip distance than the initial human-annotated template,
demonstrating our methods ability to adjust hand joint angles
across a large range.
For baseline methods, DexGraspNet  shows high un-
points. While it occasionally generates good grasps (e.g.,
for scales 0.08 and 0.14), it often results in twisted fingers
DGN object
Objaverse
TABLE III: A Harder Benchmark for Fingertip Grasp Syn-
thesis. This benchmark uses smaller friction coefficients and
more diverse objects, and our method consistently outperforms
the baseline. DGN indicates DexGraspNet.
(e.g., for scales 0.17 and 0.20) or large thumb-to-object
distance (e.g., for scale 0.05). FRoGGeR  performs well
on simple objects but almost always fails on objects with
complex geometries. It also tends to generate grasps with
different contact normals for each fingertip (e.g., for scales
0.05 and 0.08), an issue encouraged by many previous force
closure metrics. SpringGrasp  suffers from severe pene-
tration and inconsistent contact distances, especially for the
thumb. Additionally, their grasps lack diversity, and their
thumb joint frequently exceeds the feasible range, which is
not executable in both MuJoCo and the real world. Although
BODex  demonstrates high success rates in simulation, their
synthesized grasps rarely involve finger bending, resulting in
unnatural poses.
2) Comparison using a harder benchmark: The benchmark
in Table II uses large friction coefficients and many simple
to synthesize very high-quality grasps in more complex sce-
narios. To address this, we introduce a more challenging
benchmark by reducing the tangential and torsional friction
coefficients from 0.6 and 0.02 to 0.3 and 0.002, respectively,
and randomly selecting 5000 additional objects from Obja-
verse  for testing. To mitigate the increased difficulty,
we allow each method more attempts per object (from 20 to
100). We compare only with BODex, as other baselines exhibit
significantly lower success rates and slower speeds.
As shown in Table III, our method significantly outperforms
BODex. Notably, our method achieves an object success
rate exceeding 94, successfully grasping nearly all scaled
objects. This highlights our methods stronger generalizability
to complex in-the-wild objects. Additionally, our grasp success
rate continues to improve with more attempts, benefiting
from continuous updates to our template repository, whereas
the performance of BODex remains unchanged. This further
demonstrates the adaptability of our approach.
Fig. 6: Comparison with Functional Grasp Transfer Base-
lines. Our grasps involve more contact points without any
Intermediate
Precision
TABLE IV: Statistics of Grasp Synthesis for the GRASP
Taxonomy. The success rate is lower than fingertip grasps
because many flexible grasp types are suitable only for specific
C. Type-Aware Grasp Synthesis
1) Comparison with functional grasp transfer baselines:
lines for comparison, as existing methods either do not support
robotic hands (e.g., Oakink ) or have not made their
code publicly available (e.g., LHFG  and CCFG ).
using figures from their papers. As shown in Figure 6, previous
baselines mainly use fingertips to grasp the object, particularly
for scissors. In contrast, our method achieves significantly
more contact points (approximately 10 links in contact for
scissors and 7 for mugs), resulting in more stable and human-
like grasps. Additionally, CCFGs grasps show noticeable
maximum penetration of around 1 cm in their paper. In
2) Statistics analysis of our pipeline: In the absence of a
suitable baseline for comparison, we provide some quantitative
results in Table IV, which were gathered while synthesizing
our Dexonomy dataset in Section V. The grasp types are cat-
egorized into three large groups, namely power, intermediate,
and precision grasps, according to the GRASP taxonomy .
The overall success rate is considerably lower than that
of fingertip grasp synthesis, as many flexible grasp types are
designed for specific object shapes. For instance, the Lateral
(16) grasp is only used for flat and small objects. Among
different grasp types, precision grasps exhibit the highest
success rate under normal test conditions (i.e., with friction
coefficients of 0.6 and 0.02), since these grasps typically
involve only the fingertips and suit more objects. However,
the success rate of precision grasps drops more rapidly than
Global stage
wo filter
Local stage
wo filter
Template library
wo update
TABLE V: Ablation Study on Pipeline Modules for Fin-
gertip Grasp Synthesis of Allegro Hand.
Intermediate
Precision
Fingertip
Grasp only
w pre-grasp
TABLE VI: Ablation on Control Strategy for Simulation
Validation. Power grasps are robust likely due to rich contacts.
that of power grasps when the friction coefficients are reduced
to 0.3 and 0.002 (i.e., the hard test conditions), indicating that
power grasps offer higher stability due to more contact with
the object. Additionally, the overall diversity of grasps is better
than previous work reported in Table II, owing to the inclusion
of many distinct grasp types.
D. Ablation Study
1) Modular ablation: Table V shows the impact of each
module on fingertip grasp synthesis for the Allegro hand.
For the global alignment stage, optimization provides a slight
We also take a deeper look at the post-filtering of this stage,
and find that a significant portion (> 50k out of 80k) is filtered
out due to a high loss (Eq. 7). In contrast, collision and grasp
quality filters only remove fewer than 10k grasps, indicating
that using them as new losses for optimization is not effective.
In the local refinement stage, optimization is crucial for
improving grasp quality, while post-filtering primarily ensures
penetration-free grasps and negatively affects the success rate.
enhances the synthesis success rate.
2) Control strategy ablation: In Table VI, we evaluate the
impact of different control strategies during simulation testing.
The Grasp only method computes the pre-grasp and squeeze
joint angles by scaling the grasp joint angles with fixed factors:
0.9 for the pre-grasp and 1.1 for the squeeze pose. The
w pre-grasp method calculates the squeeze joint angles as
squeeze  2grasppregrasp, following BODex . Note
that these two strategies only calculate the joint angles, while
the 6DoF hand root pose is the same as the one of the grasp
pose. During execution, the hand transitions sequentially from
the pre-grasp to the grasp pose, and then to the squeeze pose.
The results demonstrate that our contact-aware control
strategy consistently improves the grasp success rate, except
the power grasps. Power grasps are particularly robust to the
control strategy, likely due to the rich contacts.
3) Robustness to initial human-annotated templates: To
investigate the robustness of our pipeline to different initial
Grasp Success Rate()
6.Prismatic 4 Finger
1.Large Diameter
Template ID
wo new templates
w new templates (Ours)
TABLE VII: Robustness to Initial Templates. Our strategy,
adding new templates from successful grasps, is the key to
robustness.
Template 1
(Default)
Template 3
6. Prismatic 4 Finger
1. Large Diameter
Template 2
Initialization
Exemplar Result
Template 1
(Default)
Template 3
Template 2
(Noisy contact annotations)
(Different hand joint angles)
Fig. 7: Robustness to Initial Templates. Our method gener-
ates good grasps even for very noisy contact annotations as in
columns 2 and 3.
common grasp types of the Shadow Hand and evaluated on
1000 random objects, with 100 attempts per object. As shown
in Table VII and Figure 7, our algorithm is robust to both
noisy contact annotations and variations in hand joint angles,
thanks to our template-adding strategy. As long as the grasp
success rate is not 0, the template-adding strategy will
construct high-quality templates from successful grasps and
greatly reduce the impact of bad initial templates.
A more challenging case is using random initialization.
high dimensionality of our template (including hand pose and
contact annotations), where valid grasps are extremely sparse.
This highlights the usage of human-annotated templates.
E. Learning-based Grasp Synthesis from Partial Observation
In this section, we compare the influence of both the
grasping dataset and the learning method in simulation. The
10.7k objects in our Dexonomy dataset are randomly split into
training and test sets with a 4:1 ratio. While the object scales
used for training vary, we fix the scales during testing, using
the same six scale levels as described in Section VII-B. To
ensure a fair comparison, we also regenerate a dataset for
BODex using our objects and scales, resulting in 0.7M valid
grasps. Ours-type1 includes only the Large Diameter (1)
grasp type from the Dexonomy dataset and contains 0.4M
data points, while Ours-all uses the full 9.5M dataset. For
the type-conditional model, we additionally train a classifier
to select the best grasp type based on each objects point cloud;
more details are provided in Appendix D. For each object, 100
candidate grasps are predicted and ranked by their associated
As shown in Table VIII, our type-conditional model trained
on the Dexonomy dataset significantly outperforms the BODex
baseline by around 10, further highlighting the value of
Type-uncond.
Ours-type1
Ours-all
Type-cond.
Ours-all
TABLE VIII: Learning-based Grasp Synthesis from Single-
Simulation. Our type-
conditional model trained on our Dexonomy dataset signifi-
cantly outperforms baselines.
Azure Kinect
Fig. 8: Real-World Experiment. (Left) Hardware setup.
(Right) 13 varied objects for testing.
our dataset. Notably, even when using only a single grasp
type with less data, the learned model still outperforms its
counterpart trained on BODex. Without type-conditional fea-
and performs poorly. In contrast, the type-conditional model
successfully synthesizes the intended grasp types, as visualized
in Figure 9. The model trained on our dataset exhibits slightly
higher penetration, likely due to the fact that our grasps are
more contact-rich. Contact distance consistency is also higher
for Ours-all, as this metric considers all fingers, while some
grasp types do not involve every finger.
F. Real-World Experiment
The experimental setup and 13 unseen objects for testing are
shown in Figure 8. To simplify the testing process, we select
12 grasp types from the taxonomy that are distinct and suitable
for grasping from a table. For 3 simple objects, namely the
red cylinder, the white box, and the red apple, we use all 12
grasp types to grasp them. For the other 10 objects, we let
GPT-4o choose 3 suitable grasp types by providing pictures
of the grasp types and the objects. Each object and grasp type
combination is tested in 3 trials, resulting in around 200 total
testing trials.
To perform a grasp, a single-view object point cloud seg-
mented by SAM2  and the specified grasp type are taken
as input to the trained type-conditional generative model. The
model generates 100 candidates and we use the pre-grasp
poses as the target for collision-free motion planning with
are ordered by the output probability of the normalizing flow,
and the top 3 are executed. In this way, we prevent the success
rate of motion planning from affecting the results, since it is
not the focus of this paper. After reaching the pre-grasp pose,
Fig. 9: Real-World Gallery. Our trained type-conditional generative model successfully synthesizes desired grasp types from
single-view object point clouds. All grasps succeed in lifting the object except the one in the red box, where the grasp type
is unsuitable for the object.
Fig. 10: An Annotation UI based on Our Algorithm for
Collecting Functional Grasp. (Left) The user clicks twice to
specify a contact point on the object and a grasp type. (Right)
A high-quality grasp is synthesized according to the users
needs within seconds.
the hand moves to the grasp pose and then the squeeze pose
to grasp the object stably, and finally lifts it.
As shown in Figure 9, our model can correctly generate
physically plausible grasps for the specified types and achieves
an overall success rate of 82.3. The most common failure
mode is the unsuitable grasp types for the object, especially
for Lateral (16). Moreover, some grasp types, such as Tip
Pinch (24), are not very robust and easily fail due to real-
world noise and prediction errors. More details are provided
in Appendix E.
VIII. APPLICATION
Although our algorithm is semantic-unaware and cannot
directly synthesize grasps to touch object regions specified
by human language commands, it can be used to develop an
efficient annotation system for collecting semantic dexterous
grasp data. Unlike widely used teleoperation methods, which
often require well-trained annotators and hardware depen-
dencies like data gloves, our annotation system has minimal
As shown in Figure 10, the annotator only needs to click
select a desired grasp type. Our algorithm will automatically
sample nearby object points and grasp templates from existing
displayed in the GUI within seconds. For a full demonstration,
please refer to our supplementary video. We plan to continue
improving this tool and hope it facilitates future research on
semantic grasping.
IX. LIMITATIONS AND FUTURE WORK
suitable or unstable grasp types, as shown in Figure 9. Future
work could explore a more suitable taxonomy for robotic
lection. Second, our method focuses on synthesizing static
grasp poses rather than generating sequential trajectories that
involve contact changes. A promising direction for future
research is to incorporate trajectory generation for dynamic
and differentiable physics simulators [16, 17, 54]. Finally,
our work focuses solely on grasping a single object, and the
extension to cluttered scenes remains an open challenge for
future exploration.
X. CONCLUSION
In this work, we present a novel pipeline to efficiently
synthesize high-quality dexterous grasps for any grasp type,
human-annotated template. Our pipeline greatly outperforms
previous type-unaware grasp synthesis baselines in the simu-
lation benchmark. Using our proposed method, a large-scale
dataset covering 31 grasp types is constructed, enabling the
training of a type-conditional grasp generative model. The
trained model successfully generates desired grasp types from
single-view object point clouds, achieving a real-world success
rate of 82.3.
ACKNOWLEDGMENTS
We thank Danshi Li for assisting with the integration of the
Inspire Hand and the Unitree G1 Hand.
REFERENCES
Ananye Agarwal, Shagun Uppal, Kenneth Shaw, and
Deepak Pathak. Dexterous functional grasping. In 7th
Annual Conference on Robot Learning, 2023.
Arun L Bishop, John Z Zhang, Swaminathan Guru-
model-predictive control.
In 2024 IEEE International
Conference on Robotics and Automation (ICRA), pages
Samarth Brahmbhatt, Chengcheng Tang, Christopher D
A dataset of grasps with object contact and hand pose.
In Computer VisionECCV 2020: 16th European Confer-
Part XIII 16, pages 361378. Springer, 2020.
Hanwen Cao, Hao-Shu Fang, Wenhai Liu, and Cewu Lu.
grasping. IEEE Robotics and Automation Letters, 6(4):
Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,
Ankur Handa, Jonathan Tremblay, Yashraj S Narang,
Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al.
objects. In Proceedings of the IEEECVF Conference on
Computer Vision and Pattern Recognition, pages 9044
Jiayi Chen, Yuxing Chen, Jialiang Zhang, and He Wang.
Task-oriented dexterous grasp synthesis via differen-
tiable grasp wrench boundary estimator. arXiv preprint
Jiayi Chen, Yubin Ke, and He Wang. Bodex: Scalable and
efficient robotic dexterous grasp synthesis using bilevel
optimization. arXiv preprint arXiv:2412.16490, 2024.
Sirui Chen, Jeannette Bohg, and C Karen Liu. Spring-
pliant dexterous pre-grasp synthesis.
arXiv preprint
Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Ed-
ward Adelson, and Pulkit Agrawal. Visual dexterity: In-
hand reorientation of novel and complex object shapes.
Science Robotics, 8(84):eadc9244, 2023.
Zerui Chen, Shizhe Chen, Etienne Arlaud, Ivan Laptev,
and Cordelia Schmid.
dexterous manipulation from human videos.
preprint arXiv:2404.15709, 2024.
X Cheng, J Li, S Yang, G Yang, and X Wang. Open-
Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4d spatio-temporal convnets: Minkowski convolutional
neural networks.
In Proceedings of the IEEECVF
conference on computer vision and pattern recognition,
Sammy Christen, Muhammed Kocabas, Emre Aksan,
Jemin Hwangbo, Jie Song, and Otmar Hilliges. D-grasp:
Physically plausible dynamic grasp synthesis for hand-
object interactions.
In Proceedings of t
