=== PDF文件: Building Rome with Convex Optimization.pdf ===
=== 时间: 2025-07-22 15:49:09.417886 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Building Rome with Convex Optimization
Haoyu Han and Heng Yang
School of Engineering and Applied Sciences, Harvard University
(a) BAL Dataset. From left to right: 10155, 1934, and 392 camera frames.
(b) Replica Dataset. All three cases have 2000 camera frames.
(c) Mip-Nerf Dataset (left: reconstruction, right: novel view synthesis). 185 camera frames.
(d) IMC Dataset. From left to right: 3765, 2063, and 904 camera frames.
Left (e) TUM Dataset (798 and 613 camera frames); Right (f) C3VD Dataset (370 and 613 camera frames).
Fig. 1: Faster, scalable, and initialization-free 3D reconstruction powered by conveX bundle adjustMent (XM).
AbstractGlobal bundle adjustment is made easy by depth
prediction and convex optimization. We (i) propose a scaled
bundle adjustment (SBA) formulation that lifts 2D keypoint
measurements to 3D with learned depth, (ii) design an empirically
tight convex semidenite programming (SDP) relaxation that
solves SBA to certiable global optimality, (iii) solve the SDP
relaxation at extreme scale with Burer-Monteiro factorization
and a CUDA-based trust-region Riemannian optimizer (dubbed
XM), (iv) build a structure from motion pipeline with XM as the
optimization engine and show that XM-SfM compares favorably
with existing pipelines in terms of reconstruction quality while
being signicantly faster, more scalable, and initialization-free.
I. INTRODUCTION
At the heart of modern structure from motion (SfM) and
simultaneous localization and mapping (SLAM) sits bundle
adjustment (BA), the procedure of reconstructing camera poses
and 3D landmarks from 2D image keypoints.
Classical BA formulation. Consider a so-called view graph
illustrated in Fig. 2 with two types of nodes: 3D points pk
An edge (i, k) E indicates the k-th 3D point is visible to
the i-th camera, and a 2D keypoint measurement uik R2
has been obtained regarding the projection of the point onto
the camera frame.1 The bundle adjustment problem consists
of estimating points and poses (i.e., pks and (Ri, ti)s on the
nodes) using the 2D keypoint measurements (i.e., uiks on the
edges). This is often formulated as an optimization problem:
(Ri,ti)SE(3),i1,...,N
uik (Ripk  ti)2 ,
where Ripk  ti transforms the point pk to the i-th camera
by the depth and projects the 3D point to 2D, and the
sum of squared errors evaluates how well the reprojected 2D
points agree with the measurements uik for all (i, k) E.
Problem (1) assumes the availability of a view graph, which
is often obtained through feature detection and matching in
SfM and SLAM pipelines (to be detailed in IV). An important
observation is that problem (1) can only be solved up to scale.
This is because (Ripk  ti) (Ri(pk)  (ti)) for any
scalar  > 0, i.e., scaling the points and translations by a factor
of  does not change the objective value of problem (1).
Optimization challenges. Problem (1) is intuitive to formu-
late but extremely difcult to optimize. The difculty comes
from two challenges. First, problem (1) is highly nonconvex.
The nonconvexity comes from both the nonconvex feasible
set SE(3) and the nonconvex objective function due to the 2D
reprojection function (). Second, problem (1) can have an
extremely large scale. Both the number of camera poses N
and the number of points M can range from hundreds to tens
of thousands (cf. examples in Fig. 1). Due to these challenges,
BA solvers such as CERES , GTSAM , and accelerated
variants [19, 36] require good initializations and can often get
stuck in poor local minima (cf. V-A). To address this, the
popular SfM pipeline COLMAP  employs an incremental
strategy which starts by reconstructing only two views (i.e., (1)
with N  2) and then sequentially registers additional camera
images and associated 3D structure. Incremental SfM ensures
stable initialization but makes the pipeline slow and hard to
scale to large datasets (cf. V where COLMAP requires several
hours runtime). The recent global SfM pipeline GLOMAP
replaces the incremental process with rotation averaging and
global positioning, but such initialization can still be time-
consuming (see results in V).
1We consider BA with calibrated cameras, i.e., uik has been normalized
by camera intrinsics.
Landmarks
(RN, tN)
Fig. 2: A view graph for the bundle adjustment formulation (1).
We propose a scaled bundle adjustment formulation (3) by
lifting the 2D keypoints to 3D with learned depth.
Can we design an algorithm that solves the global bundle
adjustment problem (1) without initialization and at scale?
BA with learned depth. We start by designing an approx-
imate formulation to problem (1) that is simpler to optimize.
The key insight is to lift the 2D keypoints uik as approxi-
mate (and noisy) 3D keypoints leveraging off-the-shelf large-
scale pretrained monocular depth prediction models, such
as [35, 49, 10]. Formally, let dik > 0 be the predicted depth of
the 2D keypoint uik, we generate a 3D keypoint measurement
uik  dik
(i, k) E.
In our work, we select a metric depth model, which is designed
to produce depth estimates that match the true depth when the
model is perfectly trained. Further, due to imperfect depth pre-
diction caused by potentially out-of-distribution test images,
we incorporate a scaling parameter that allows subsequent
optimization to correct erroneous predictions, leading to the
following scaled bundle adjustment (SBA) formulation
si>0,i1,...,N
(Ri,ti)SE(3),i1,...,N
wik Ri(siuik)  ti pk2 ,
where si scales the predicted 3D keypoint uik in (2), (Ri, ti)
transforms the scaled 3D keypoint to the global frame,2 and the
squared errors in the objective computes 3D distances to the
landmarks pk without the reprojection (). In (3), we allow
weighting the squared errors by condence values (wik > 0)
from the depth prediction model.
Remark 1 (Connection to Other Perception Problems). It is
worth noting that (i) without the per-frame scaling, prob-
lem (3) recovers the multiple point cloud registration prob-
lem [15, 26]; (ii) when N  2 (two frames), problem (3)
reduces to (scaled) point cloud registration [25, 48].
Through depth prediction, we removed the reprojection
function () from (1) and resolved one challenge. However,
nonconvexity and large scale remain in problem (3).
2Although we used the same notation (Ri, ti) in both problems (1) and (3),
the two transformations are inverse to each other. (Ri, ti) in (1) transforms
landmarks from global frame to camera frame, while (Ri, ti) in (3) transforms
landmarks from camera frame to global frame.
Contributions. First, we show the remaining nonconvexity
in problem (3) is benign. Our strategy is to rst rewrite (3)
as a quadratically constrained quadratic program (QCQP), and
then design a convex semidenite program (SDP) relaxation,
in the spirit of a growing family of SDP-enabled certiable
algorithms [47, 38, 52, 28, 6, 33, 44]. We show the SDP relax-
ation is empirically tight, i.e., globally optimal solutions of the
nonconvex (3) can be computed from the convex SDP relax-
ation with optimality certicates. Second, we show the convex
SDP relaxations can be solved at extreme scale and speed
faster and more scalable than even the best local solvers
such as CERES. The enabling technique is to exploit the low-
rankness of (tight) SDP optimal solutions via Burer-Monteiro
(BM) factorization  and solve the resulting Riemannian
optimization using a trust-region algorithm . For the rst
directly in CCUDA and show the GPU implementation
is up to 100 times faster than the state-of-the-art CPU-based
MANOPT package  and can solve extreme-scale problems
beyond the reach of MANOPT (e.g., N > 10, 000 camera
bundle adjustMent). Third, we build a full SfM pipeline
with XM as the optimization engine and various techniques
from prior work, such as feature matching from COLMAP,
view graph creation from GLOMAP, CERES renement (i.e.,
warmstart CERES with XMs solutions for solving (1)), and
outlier-robust ltering and estimation schemes [4, 20, 31, 40].
We test XM-SfM across six popular datasets and demonstrate
that XM-SfM compares favorably with existing SfM pipelines
in terms of reconstruction quality while being signicantly
optimization and GPU-based implementation.
In summary, our contributions are:
designing an empirically tight convex SDP relaxation for
the scaled bundle adjustment problem (3);
solving the convex SDP at extreme scales using BM
factorization paired with a trust-region Riemannian op-
timizer directly implemented in CCUDA, i.e., XM;
creating a full SfM pipeline called XM-SfM that builds
Rome with convex optimization.
Paper organization. We derive the QCQP formulation
for (3) and design the SDP relaxation in II. We present BM
factorization and the CUDA-based trust-region Riemannian
optimizer in III. We describe the XM-SfM pipeline in IV and
present experimental results in V. We conclude in VI.
II. QCQP AND CONVEX SDP RELAXATION
In this section, let us focus on solving the SBA problem (3)
to certiable global optimality. We proceed in two steps. In
constrained quadratic program (QCQP) through a sequence
of mathematical manipulations. In II-B, we apply Shors
semidenite relaxation to convexify the nonconvex QCQP.
Before we get started, we remove the ambiguity of prob-
lem (3) through anchoring.
Anchoring. Observe that one can choose si 0, i
p1      pM
and the objective value of (3) can be set arbitrarily close
to zero. Additionally, multiplying an arbitary rotation matrix
on Ri, ti, pi does not change the objective of (3), leading to
innitely many solutions. To resolve these issues, we anchor
the rst frame and set R1  I3, t1  0, s1  1.
A. QCQP Formulation
We rst show that the unconstrained variables in (3), namely
the translations ti and the 3D landmarks pk, can be marginal-
ized out, leading to an optimization problem only concerning
the scaling factors and 3D rotations.
Proposition 2 (Scaled-Rotation-Only Formulation). Problem
(3) is equivalent to the following optimization
subject to
si > 0, Ri SO(3),
where Q S3N is a constant and symmetric data matrix
whose expression is given in Appendix A-A.
Let U represent the optimal solution of (4) and let
t  [t1; . . . ; tN] R3N be the concatenation of translations,
p  [p1; . . . ; pM] R3M be the concatenation of landmark
positions. The optimal translations tand landmark positions
pof problem (3) can be recovered from U as follows:
(A I3)vec (U ) .
where the expression of A can be found in Appendix A-A.
Proposition 2 reformulates the SBA problem (3) as a lower-
dimensional problem (4). However, problem (4) is not a QCQP
because the objective function is a degree-four polynomial in
si and Ri. In the next step, we show that it is possible to
combine the scaled rotation siRi together as a new variable,
effectively reducing the degree of the polynomial.
Proposition 3 (QCQP Formulation). Dene the set of scaled
orthogonal group as
The set sO(3) can be described by quadratic constraints
1 c1  cT
2 c2  cT
1 c2  cT
2 c3  cT
Consider the following QCQP
QCQP  min tr
subject to
and let U  [I3, R
N] be a global optimizer. If
then U is a global minimizer to problem (4).
It is clear that
because from (4) to (8) we have relaxed the SO(3) constraint
to O(3). After solving (8), if there exists some index i such
that det R
i < 0, we can extract a feasible solution to (4) by
projecting onto SO(3) after extracting the scaling from R
Remark 4 (Bad Local Minimum). Problem (8) is a nonconvex
QCQP. In fact, it is a smooth Riemannian optimization by
identifying sO(3) as a product manifold of the positive man-
ifold and the orthogonal group. Therefore, one can directly
we will show in V on the Mip-Nerf 360 dataset , directly
solving (8) can get stuck in bad local minima.
This motivates and necessitates convex relaxation.
B. Convex SDP Relaxation
For any QCQP, there exists a convex relaxation known as
Shors semidenite relaxation [46, Chapter 3]. The basic idea
is fairly simple: by creating a matrix variable X : U TU
that is quadratic in the original variable U, problem (8)
becomes linear in X. The convex relaxation proceeds by using
convex positive semidenite constraints and linear constraints
to properly enforce the matrix variable X.
Proposition 5 (SDP Relaxation). The following semidenite
program (SDP)
SDP  min
subject to
is a convex relaxation to (8), i.e.,
Let Xbe a global minimizer of (11).
If rank (X)  3, then Xcan be factorized as X
( U )T U , and it holds3
then U is a global optimizer to (8). In this case, we say the
relaxation is tight or exact.
If rank (X) > 3, then we can extract feasible solutions
to (8) by taking the top three eigenvectors of Xand project
the corresponding entries to sO(3) and further to scaled
1 is in O(3) because we restrict the scale of the rst frame to be 1.
the rounded solution that is feasible for (4), we can evaluate
the objective of (4) at U and denote it . Combining the chain
of inequalities from (10) and (12), we get
where the last inequality follows from U is a feasible solution
for the minimization problem (4). Assuming we can solve the
convex SDP, we compute
SDP and  at both ends of the in-
equality (15), allowing us to evaluate a relative suboptimality:
0 certies global optimality of the rounded solution U,
and tightness of the SDP relaxation.
From (3) to (4), we rst eliminated translations and
landmark positions. From (4) to (8), we formulated
a QCQP by creating the new constraint set sO(3).
From (8) to (11), we applied Shors semidenite
relaxation. This sequence of manipulations and relax-
ations allows us to focus on solving the convex SDP
problem (11) while maintaining the ability to certify
(sub)optimality of the original nonconvex problem (3),
through the inequalities established in (15).
Remark 6 (Connection to Prior Work). For readers familiar
with SDP relaxations, this section should not be surprising at
allthat is the reason why we kept this section very brief and
only focus on milestone results listed in the propositions. The
closest two works to ours are SE-Sync  and SIM-Sync ,
and the SDP relaxation technique traces back to at least the
work by Carlone et al. . The novelty of our formulation are
to correct learned depth while SE-Sync estimates rotations
and translations only; (b) we jointly estimate 3D landmarks
and (scaled) camera poses while SIM-Sync does not estimate
3D landmarks. These differences allow us to solve the long-
standing bundle adjustment problem with convex optimization.
We shall focus on how to solve the convex SDP (11).
III. BURER-MONTEIRO FACTORIZATION AND
CUDA-BASED RIEMANNIAN OPTIMIZER
Let us rst write the SDP (11) in standard primal form:
subject to
where we have rewritten the constraint (11b) as a positive
semidenite (PSD) constraint (17c) and m  5N  1 linear
equality constraints (17b). To see why this reformulation is
scaled I3the former can be enforced using 6 linear equalities
and the latter can be enforced using 5, summing up to 5N 1
linear equalities. Associated with the primal standard SDP (17)
is the following dual standard SDP:
subject to
Z(y) : Q
Since Slaters condition holds (X  I3N 0 is feasible
for the primal (17)), we know strong duality holds between
primal (17) and dual (18), i.e., their optimal values are equal
to each other [46, 45].
Scalability. Once the SDP (11) is formulated in standard
SDP relaxation leads to a matrix variable with size 3N  3N.
This means that when N is in the order of hundreds, MOSEK
can solve the SDP without any problem. However, when N
is in the order of thousands or tens of thousands, which
is not uncommon in bundle adjustment problems, MOSEK
will become very slow or even runs out of memory (e.g.,
MOSEK becomes unresponsive when N > 2000). Therefore,
we decided to customize a solver for our SDP relaxation.
A. Burer-Monteiro Factorization
The key structure we will leverage is, as stated in Propo-
sition 5, the optimal solution Xhas its rank equal to three
when the SDP relaxation is tight. In other words, the effective
dimension of Xcan be 3  3N instead of 3N  3N.
To exploit the low-rank structure, we will leverage the
famous Burer-Monteiro (BM) factorization .
Proposition 7 (BM Factorization). For a xed rank r 3,
the Burer-Monteiro factorization of (11) and (17) reads:
subject to
A few comments are in order. First, by factorizing X
U TU, X 0 holds by construction. Second, note that when
r  3, problem (19) is exactly the same as the original
QCQP (8) (up to the difference in R1), and thus is NOT
convex. Third, as long as r rwhere ris the minimum
rank of all optimal solutions of the SDP (11), the nonconvex
BM factorization has the same global minimum as the convex
SDP (11) [46, 13]. Note that the factor U is a matrix of size
r  3N, i.e., a at matrix as shown in Fig. 3 bottom right.
Counterintuitive? The reader might nd this confusing. In
optimization problem into a convex SDP, enabling us to solve
it to global optimality. Surprisingly, the BM factorization
appears to undo this effort, pulling us back into nonconvex
optimization. While the low-rank factorization offers a clear
scalability advantage, it remains uncertain whether this benet
outweighs the drawbacks of reintroducing nonconvexity.
Rank staircase. The secret ingredient of BM factoriza-
tion to tackle nonconvexity is that we will solve the factorized
problem (19) at increasing ranks, like stepping up a staircase
(cf. Fig. 3 bottom right). We will start with the lowest rank
r  3 and solve problem (19) using local optimization. One
of two cases will happen. (a) The local optimizer is the same
as the global optimizer of the SDP, in which case we can
leverage the dual SDP (18) to certify global optimality and
declare victory against the SDP. (b) The local optimizer is not
the same as the global optimizer of the SDP, in which case we
can again leverage the dual SDP (18) to escape the bad local
minimum via increasing the rank, i.e., going up the staircase.
We formalize this in Algorithm 1 and prove its correctness.
Algorithm 1: Riemannian Staircase
Local optimization of (19)
r  LOCALOPTIMIZER(U 0
Compute dual certificate
Certify global optimality
if Z(yr) 0 then
return Ur
Escape local minimum
v LEASTEIGENVECTOR(Z(yr))
T with   1
Line search
while tr
Certify global optimality. At every iteration of Algo-
rithm 1, it rst computes a local optimizer of the BM factoriza-
tion problem (19) in line 7 using an algorithm to be described
in III-B. Denote the local optimizer as U
r . To check whether
r is the optimal solution to the convex SDP (11), we
need to compute the dual optimality certicate.
Theorem 8 (Dual Optimality Certicate). Let U
r be a lo-
cally optimal solution of (19). Then, the linear independence
constraint qualication (LICQ) must hold at U
U(Ai, (U
r bi)  2Ai(U
are linearly independent. Hence, there must exist a unique dual
variable ysuch that
If further,
r is a global optimizer of (19), and (U )TU ,y, Z(y)
are optimal for the SDP (17) and its dual (18).
solution of the BM factorization. However, in Appendix A-D,
we show that the unique structure of our XM SDP relaxation
leads to guaranteed satisfaction of LICQ. The rest of the
Theorem is standard and follows from  or .
Theorem 8 provides a simple recipe to certify global opti-
mality by solving the linear system of equations in (21) (recall
Z(y) is linear in yfrom (18b)), forming the dual matrix
Z(y), and checking its positive semidenite-ness.
Escape local minimum. If Z(y) is not PSD, then
r is not optimal for the SDP. In this case, the following
theorem states that the eigenvector of Z(y) corresponding to
the minimum eigenvalue provides a descent direction.
Theorem 9 (Descent Direction). Let U
r be a local minimizer
of problem (19) and ybe the corresponding dual variable.
Suppose Z(y) is not PSD and v is an eigenvector of Z(y)
corresponding to a negative eigenvalue. Then, consider the
BM factorization (19) at rank r  1. The direction
is a descent direction at the point
Theorem 9 states that if Algorithm 1 gets stuck at rank r, it
can escape the local minimum by increasing the rank. Since
D is a descent direction, we perform line search in lines 18-19
until a point with lower objective value is found.
Global convergence. Algorithm 1 is guaranteed to converge
to the optimal solution of the SDP, as stated below.
Theorem 10 (Global Convergence). Algorithm 1 nds a
globally optimal solution of the SDP pair (17)-(18) regardless
of the initialization point U 0
r at r  3.
every local optimizer of the BM factorization (19) corresponds
to a globally optimal solution of the SDP [27, Corollary 8].
To see this, if the locally optimal solution at r  3N is rank-
deciency Lemma [13, Proposition 4]. Conversely, if the
solution is full-rank, then Z(y) must be zerohence also
positive semideniteas a result of solving (21), which again
implies global optimality of the SDP.
This property ensures that our approach is initialization-free.
To verify the correctness of the theory, we conducted experi-
ments on the BAL-93 dataset using random initializations, and
after 1000 trials, our method achieved a 100 success rate.
It is worth noting that, in the worst case when r is increased
to 3N, the BM factorization does not have any scalability
advantage over the original SDP. Fortunately, in almost all
numerical experiments, Algorithm 1 terminates when r  3
or 4, bringing signicant scalability advantage to solving large-
scale SDP relaxations.
B. CCUDA-based Riemannian Optimization
Everything in Algorithm 1 is clear except line 7 where one
needs to locally optimize the nonconvex BM factorization (19).
Riemannian structure. At rst glance, problem (19) looks
like a nonconvex constrained optimization problem. However,
the constraints of (19) indeed dene a smooth manifold.
Proposition 11 (BM Riemannian Optimization). The BM
factorization problem (19) is equivalent to the following un-
constrained Riemannian optimization problem
subject to
where Mp is the positive manifold dened as
and M(r)
is the Stiefel manifold of order r:
{R Rr3  RTR  I3, }.
We solve problem (25) using the Riemannian trust-region
algorithm with truncated conjugate gradient (Rtr-tCG). Details
of this algorithm can be found in [1, 11].
CCUDA implementation. The Rtr-tCG algorithm is
readily available through the MANOPT package . However,
to boost efciency and enable fast solution of the SDP
Conjugate gradient method. The conjugate gradient
method involves only Hessian-vector products and vector
addition. The Hessian-vector product can be decom-
posed into two components: (a) the Euclidean Hessian-
vector product and (b) the Riemannian projection onto
the tangent space. The rst component primarily re-
quires matrix-vector multiplication, which can be ef-
ciently implemented using cuBLAS. The second com-
ponent involves batched small matrix-matrix multiplica-
CUDA kernels. For vector addition, we directly utilize
the cublasDaxpy function from cuBLAS.
Retraction. The retraction operation aims to map a point
from the tangent space back to the manifold. In our
manifold and the Stiefel manifold. The retraction on the
positive manifold is a simple custom kernel, while the
retraction on the Stiefel manifold involves QR decompo-
sition. We directly apply Gram-Schmidt process on every
batch of 3  r matrices, which is implemented using
custom CUDA kernels.
In V, we show our GPU-based implementation achieves up
to 100 times speedup compared to the CPU-based MANOPT.
We focused on developing a customized solver ca-
pable of solving large-scale SDP relaxations in (11)
(and (17)). We applied the Burer-Monteiro factoriza-
tion method to exploit low-rankness of the optimal
SDP solutions (cf. problem (19)), and leveraged the
Staircase Algorithm 1 to solve the nonconvex BM
factorization to global optimality. We pointed out the
BM factorization problem is indeed an unconstrained
Riemannian optimization problem (cf. problem (25))
and developed a CCUDA-based implementation
that is signicantly faster than MANOPT.
Remark 12 (Connection to Prior Work). This is not the rst
time BM factorization has been applied in robotics. SE-Sync
and related works [18, 37] pioneered the application of BM
factorization for solving the pose graph optimization problem.
Several works [22, 21, 23, 24] utilized the dual optimality
certicate result in Theorem 8 to develop fast certiers.
Our novelty lies in developing the rst CCUDA-based
implementation of the Riemannian trust-region algorithm to
push the scalability limitations of BM factorization.
IV. STRUCTURE FROM MOTION WITH XM
In this section, we present our SfM pipeline with XM as the
optimization engine, illustrated in Fig. 3.
View graph. To construct view graph for an image set, we
rst run COLMAPs feature extractor and exhaustive matcher
to extract 2D correspondences. The feature extractor employs
SIFT  for feature detection and description, while the
exhaustive matcher matches every image pair. After matching,
we apply GLOMAPs track establishment to produce a four-
column le where the rst two columns represent feature
point coordinates, the third indicates the image index, and the
fourth corresponds to the 3D landmark indices. Currently, we
use the original implementation from COLMAP and GLOMAP.
processes further using C and GPU implementation. We
leave this improvement for future work.
Depth estimation. We use the depth estimation model
UNIDEPTH  to calculate the metric depth of a given image,
and lift the view graph from 2D to 3D. It is worth emphasizing
that our test datasets do NOT belong to the training dataset of
UNIDEPTH. If given the condence map, we use it to update
the weight of different observations. We also tried other depth
prediction models in Appendix B.
Filter from two-view estimation. Using 2D observations,
we estimate the relative pose between two images. Based on
this pose, we lter out 3D landmarks with large Euclidean
distance errors. Specically, landmarks with distance errors
exceeding three times the median are removed.
XM solver. We then use the lifted 3D measurements and the
view-graph to form a Q matrix as shown in (4). We solve the
SDP problem in (11) using our XM solver. If needed, we also
delete the 10 measurements with the largest residuals and
re-run the XM solver. This corresponds to a greedy heuristic
for outlier removal [4, 40] and we call it XM2 (running twice).
CERES renement. Usually the depth predictions are quite
also feed the estimated poses and landmarks to CERES as a
warmstart to solve the original bundle adjustment problem (1).
As we will show, the solution of XM always provide a strong
warmstart for CERES to quickly optimize (1).
V. EXPERIMENTS
We evaluate the XM solver and the XM-SfM pipeline on
diverse datasets. XM is benchmarked against the leading bundle
adjustment solver CERES and XM-SfM is compared against the
widely adopted SfM pipelines COLMAP and GLOMAP. We
further analyze the convergence rate and evaluate robustness
to depth estimation noise in Appendix F.
Experiments run on a Lambda Vector workstation with 64-
core AMDo Ryzen Threadripper Pro 5975WX CPUs and dual
NVIDIAo RTX 6000 GPUs, using CUDA 12.4 and Python
3.11.10. All dependencies are carefully set up to leverage
multi-CPU and GPU acceleration.
A. BAL Dataset
We rst evaluate on the Bundle Adjustment in the Large
(BAL) dataset . This dataset contains reconstruction results
from Flickr photographs using Bundler. On BAL we focus on
XM solver performance rather than the full XM-SfM pipeline.
Setup and baselines. For input, we use 2D observations
for CERES and 3D observations for XM. The 2D keypoint
measurements come directly from the BAL dataset, while
the 3D keypoint measurements are lifted by appending z-
coordinates to the 2D measurements. Though accurate, these
3D observations incorporate slight noise, making them a re-
ned yet imperfect ground truth. To showcase XMs efciency
in solving the SDP relaxation of the SBA problem (3), we also
evaluate MANOPT using the same input as XM.
Metrics. We evaluate performance based on the runtime and
the median of Absolute Trajectory Error (ATE) and Relative
Pose Error (RPE). More details can be found in Appendix G.
eigenvalue of the Z(y) matrix in (18) to demonstrate that XM
achieves global optimality. Note that the minimum eigenvalue
of Z(y) has been used as a metric for global optimality in
previous works as well . Runtime is split into prepro-
cessing time and solver time. The former primarily involves
constructing the Q matrix in (4), which is implemented in
separate preprocessing time and solver time because there are
still ways to further reduce the preprocessing time (while the
solver time, to the best of our understanding, has been pushed
to the limit). For example, as building the Q matrix requires
large dense matrix multiplications, a GPU implementation is
expected to accelerate this step by 10 to 100 times. However,
we leave this as a future step.
Feature Extracting
Feature Matching
Depth Estimation
Outlier Filter
3D Observations
Lifted View-Graph
Scaled Bundle Adjustment (XM)
Lower-rank Subproblems
Global Optimality
2063 frames 20s
Ceres Renement
Fig. 3: XM-SfM: structure from motion pipeline with XM.
TABLE I: Results on the BAL dataset. We report the ATE, RPE and running time for CERES, MANOPT, and our proposed
XM solver. The evaluation is conducted on four BAL datasets with varying numbers of frames to demonstrate that our method
is both fast and accurate across datasets of different scales (e.g., BAL-10155 indicates there are 10155 camera frames to be
reconstructed). CERES fails at a bad local minimum without a good initial guess, while MANOPT performs signicantly slower
and even fails to solve within ten hours on the largest dataset.
Datasets
BAL-93 (6033 landmarks)
BAL-392 (13902 landmarks)
BAL-1934 (67594 landmarks)
BAL-10155 (33782 landmarks)
Solver Time
Processing Time
Solver Time
Processing Time
Solver Time
Processing Time
Solver Time
Processing Time
CERES-GT
CERES-GT-0.01
CERES-GT-0.1
TABLE II: Results on the BAL dataset (suboptimality and
min-eig). We report the suboptimality gap and minimum
eigenvalue of Z matrix in (18) for our proposed XM method.
Datasets
BAL-1934
BAL-10155
Suboptimality-Gap
Results. Table I summarizes the comparison between XM
and other methods. We tested several versions of CERES.
CERES indicates running CERES without any initialization.
CERES-GT indicates starting CERES at the groundtruth
estimation. CERES-GT-0.01 means adding noise to the
groundtruth with standard deviation 0.01. We make several
observations. (a) Without good initialization, CERES does not
(b) XM is up to 100 times faster than MANOPT, showing the
superior efciency of our GPU implementation. Notably, XMs
solver time is below a second for N in the order of hundreds,
and XM scales to N > 10, 000 camera frames.
Table II presents the suboptimality gap and minium eigen-
value of XM. As we can see, except the largest instance with
N  10155 camera frames, XM solved all the other instances
to certiable global optimality. The reason why XM did not
solve the largest instance to global optimality is because we
restricted its runtime to one hour (XM indeed achieve global
optimality if allowed four hours of runtime).
Fig. 4 visualizes the 3D reconstructions. Since real images
are not available in BAL, all 3D landmarks have the same
purple color. The reconstructed cameras are shown in red.
Fig. 4: Visualization of BAL datasets. Top: Our XM solver. Middle: CERES-GT-0.01. Bottom: CERES-GT-0.1. Both our XM
solver and CERES-GT-0.01 accurately recover the ground truth camera poses and landmarks, whereas CERES-GT-0.1 fails.
Fig. 5: Visualization of Replica datasets. Top: Our XM solver. Middle: GLOMAP. Bottom: COLMAP. All methods achieve
high accuracy, producing nearly identical reconstruction results. GLOMAP sometimes produce outliers (see column 2 and 3).
Takeaway
The SBA problem (3) is easier to solve than the
BA problem (1)we can design efcient convex relax-
ations. While CERES needs good initialization for solv-
ing (1), XM requires no initializations for solving (3).
With GT depth and outlier-free matchings, solving
SBA with XM produces the same result as solving BA
with CERES or COLMAP.
XM is fast and scalable.
B. Replica Dataset
We then test on the Replica dataset [53, 43, 41], which
contains synthetic images of different virtual scenes.
map but employ the full XM-SfM pipeline. We compare XM-SfM,
both with and without two-view ltering, against COLMAP
and GLOMAP. The evaluation metrics remain the same. For
runtime analysis, we categorize all components preceding our
XM solverincluding Matching, Indexing, Depth Estimation,
The indexing, ltering and matrix construction components
can be further accelerated in CUDA. Similarly, for GLOMAP
and COLMAP, all steps prior to global positioning and bundle
adjustment are counted as preprocessing time.
Results. Results are presented in Table III and Table IV.
Each Replica dataset contains 2000 frames, but for a diverse
Fig. 6: Visualization of Mip-Nerf datasets. Top: COLMAP. Bottom: Our XM solver. 3D-gaussian renderings are the same.
Fig. 7: Visualization of IMC2023 datasets. Top: Our XM solver. Bottom: GLOMAP.
Fig. 8: Visualization of TUM datasets using our XM solver.
Fig. 9: Visualization of C3VD medical datasets. Top: With ground truth depth. Bottom: With learned depth.
TABLE III: Results on the Replica dataset. COLMAP, while highly stable, is extremely slow, taking over 20 hours for datasets
with 2000 frames. GLOMAP improves speed but still requires several hours for the solving stage and occasionally produces
outliers. In comparison, our solver achieves similar accuracy in just 10 seconds for the same dataset size.
Datasets
FILTER  XM2
Solver Time
Processing Time
Solver Time
Processing Time
Solver Time
Processing Time
Solver Time
Processing Time
Room0-100
Room0-2000
Room1-100
Room1-2000
Ofce0-100
Ofce0-2000
Ofce1-100
Ofce1-2000
TABLE IV: Results on the Replica dataset (min-eig and
suboptimality). All the min-eig and suboptimality-gap are
FILTER  XM2
Suboptimality-gap
Suboptimality-gap
Room0-100
Room0-2000
Room1-100
Room1-2000
Ofce0-100
Ofce0-2000
Ofce1-100
Ofce1-2000
comparison across different dataset sizes, we sample the
rst 100 frames from each dataset as a separate experiment.
Room0-100 refers to the rst 100 frames, while Room0-
2000 represents the full dataset.
XM consistently outperforms baselines by 100 to 1000 times
in solver speed, solving almost all 2000-frame datasets within
10 seconds. At the same time, XM maintains high accuracy,
achieving a median translation error of just 1. In practice,
a 0.1 and 1 translation error yield nearly identical recon-
struction quality, as illustrated in Fig. 5.
Appendix C. The solver time is negligible, appearing as only
a thin bar in the chart. Matching, indexing, and ltering are the
most time-consuming components, with the latter two planned
for CUDA implementation as future work.
Takeaway
With ground truth depth and COLMAP matchings,
minor lter renement achieves results comparable to
COLMAP and GLOMAP.
XM remains highly efcient and scalable.
TABLE V: Results on the Mip-Nerf and Zip-Nerf datasets.
COLMAP is very slow, while both XM and GLOMAP achieve
comparable accuracy. However, XM is signicantly faster.
Datasets
Kitchen-279
Garden-185
Bicycle-194
Room-311
Alameda-1724
FILTER  XM2
(XM  CERES)
Solver Time
Processing Time
Solver Time
Processing Time
Solver Time
Processing Time
TABLE VI: Results on the Mip-Nerf and Zip-Nerf datasets
(min-eig and suboptimality). All datasets are solved to global
minimum.
Suboptimality-gap
C. Mip-Nerf and Zip-Nerf Dataset
Mip-Nerf and Zip-Nerf [7, 8] are real-world image datasets
around a single object. We evaluate learned depth and down-
stream novel view synthesis tasks on these datasets.
by COLMAP, we use its camera poses as ground truth to
benchmark XM against COLMAP and GLOMAP. To address
inaccuracies in learned depth, we apply CERES renement,
incorporating its runtime into the solver time, denoted as XM
CERES. All other evaluation metrics remain unchanged.
Fig. 10: Illustration of local minimum on the Mip-Nerf dataset.
in a poor local minimum. Right: Solution after increasing the
rank to 4, which successfully escapes the local minimum.
Results. Results are presented in Table V, with suboptimal-
ity detailed in Table VI. While adding CERES increases solver
the baselines. On the garden and room datasets, we achieve a
0.2 error, demonstrating the same accuracy as the baselines,
while on others, the error may be slightly higher. However, as
shown in Fig. 6, this has no noticeable impact on downstream
3D Gaussian Splatting tasks .
A runtime breakdown for XM is provided in Appendix C.
Matching and indexing remain slow, while depth estimation
now takes even longer. This is due to (a) foundation models
requiring much time for estimation and (b) depth estimation
runtime scales linearly with the number of frames, whereas
Mip-Nerf datasets are relatively small.
The need for convex relaxation. In Fig. 10, we demonstrate
that directly solving (19) with rank 3 (equivalently (8)) can
lead to a local minimum. Specically, the solver terminates
with a minimal eigenvalue of Z(y) at 6.2102, resulting in
a messy reconstruction. However, by increasing the rank to 4,
the solver escapes the local minimum and achieves the global
the Burer-Monteiro method requires a higher rank to nd the
global minimum.
Takeaway
With CERES renement to mitigate depth errors,
XM achieves nearly the same accuracy as COLMAP
and GLOMAP. Moreover, this has no impact on down-
stream novel view synthesis tasks.
XM maintains a signicant speed advantage, even
with learned depth and real-world data.
BM factorization and the Riemannian staircase ef-
fectively escape local minimum.
D. IMC, TUM and C3VD Datasets
Followed by Mip-Nerf, we step to the IMC PhotoTourism
TABLE VII: Results on the IMC datasets. XM-SfM achieves
comparable accuracy while being much more scalable.
Datasets
FILTER  XM2  CERES
Solver Time
Processing Time
Solver Time
Processing Time
Rome-2063
Gate-1363
Temple-904
Paris-3765
TABLE VIII: Results on the TUM datasets. The error is
slightly higher due to low-resolution images.
Solver Time  Processing Time
ATE-T  ATE-R
RPE-T  RPE-R
fr1xyz-798-26078
fr1rpy-723-26071
fr1desk-613-38765
fr1room-1362-86634
. These datasets are quite challenging because of varying
environments and low-quality images.
against GLOMAP on IMC datasets. In these three datasets we
add both outlier ltering and XM2. In C3VD we use the ground
truth depth map and the learned depth from a medical-specic
depth prediction model .
Results. The results are presented in Table VII, Table VIII,
and Table IX. The IMC datasets consist of large image collec-
tions capturing some famous landmarks. As a result, GLOMAP
requires an extremely long runtime, exceeding 20 hours for the
largest dataset. Our accuracy is comparable to GLOMAP, with
visualizations provided in Fig. 7. The TUM and C3VD datasets
produce high-quality reconstructions, though accuracy is af-
fected by the complexity of the environment and insufcient
lighting. Visualizations of these reconstructions are provided
in Fig. 8 and Fig. 9.
Takeaway
XM remains efcient and scalable across diverse
while being signicantly faster.
VI. CONCLUSION
We proposed XM, a scalable and initialization-free solver
for global bundle adjustment, leveraging learned depth and
convex optimization. By relaxing scaled bundle adjustment as
a convex SDP and solving it efciently with Burer-Monteiro
factorization and a CUDA-based trust-region Riemannian op-
scales. Integrated into the XM-SfM pipeline, it maintains the
accuracy of existing SfM methods while being signicantly
faster and more scalable.
Limitation and future work. First, while our XM solver
outperforms baselines in speed, it can be sensitive to noise
TABLE IX: Results on the C3VD datasets. Ground truth depth
is signicantly more accurate than learned depth, while they
both fail on the last dataset because of dark environment.
Datasets
FILTER  XM2  GT-DEPTH
FILTER  XM2
Solver Time
Processing Time
Solver Time
Processing Time
medical5
medical6
medical7
medical8
and outliers. Future work includes rening the ltering process
and developing better methods to handle outliers. Second,
our GPU solver is built on the cuBLAS dense matrix-vector
multiplication library, whereas SLAM camera sequences often
have sparse patterns. Extending the XM solver to support sparse
matrix-vector multiplications would enhance its applicability
to SLAM. Third, our XM-SfM pipeline still has components
that potentially can be accelerated 100 to 1000 times through
CUDA implementation, e.g., ltering and matrix construction.
prediction model. While generic models are available, training
effective depth predictors can be particularly challenging in
data-scarce domains such as medical or space applications.
ACKNOWLEDGEMENTS
We thank Shucheng Kang for the help on CUDA program-
ming; Xihang Yu and Luca Carlone for discussions about
depth prediction models; and members of the Harvard Com-
putational Robotics Group for various explorations throughout
the project.
REFERENCES
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
Optimization algorithms on matrix manifolds. Princeton
University Press, 2008. 6, 19
Sameer Agarwal, Noah Snavely, Steven M Seitz, and
Richard Szeliski.
Bundle adjustment in the large.
Computer VisionECCV 2010: 11th European Confer-
ence on Computer Vision, Heraklion, Crete, Greece,
S
