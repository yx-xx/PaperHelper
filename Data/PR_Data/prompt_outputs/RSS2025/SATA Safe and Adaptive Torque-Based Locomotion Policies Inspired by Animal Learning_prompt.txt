=== PDF文件: SATA Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning.pdf ===
=== 时间: 2025-07-22 15:52:14.225983 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Locomotion Policies Inspired by Animal Learning
Peizhuo LI1, Hongyi LI1, Ge SUN1, Jin CHENG2, Xinrong YANG1, Guillaume Bellegarda3, Milad Shafiee3,
Yuhong CAO1, Auke Ijspeert3, Guillaume SARTORETTI1
Equal contribution, Corresponding author,
1MARMot Lab, National University of Singapore 2Computational Robotics Lab, ETH Zurich 3BioRob Lab, EPFL
AbstractDespite recent advances in learning-based con-
trollers for legged robots, deployments in human-centric envi-
ronments remain limited by safety concerns. Most of these ap-
proaches use position-based control, where policies output target
joint angles that must be processed by a low-level controller (e.g.,
PD or impedance controllers) to compute joint torques. Although
impressive results have been achieved in controlled real-world
adaptability when encountering environments or disturbances
unseen during training, potentially resulting in extreme or unsafe
behaviors. Inspired by how animals achieve smooth and adaptive
movements by controlling muscle extension and contraction,
torque-based policies offer a promising alternative by enabling
precise and direct control of the actuators in torque space. In
with the environment, resulting in safer and more adaptable
behaviors. However, challenges such as a highly nonlinear state
space and inefficient exploration during training have hindered
their broader adoption. To address these limitations, we propose
Safe and Adaptive Torque-based locomotion policies inspired by
Animal learning(SATA), a bio-inspired framework that mimics
key biomechanical principles and adaptive learning mechanisms
observed in animal locomotion. Our approach effectively ad-
dresses the inherent challenges of learning torque-based poli-
cies by significantly improving early-stage exploration, leading
to high-performance final policies. Remarkably, our method
achieves zero-shot sim-to-real transfer, eliminating the need for
additional fine-tuning on hardware. Our experimental results
indicate that SATA demonstrates remarkable compliance and
terrain or narrow passages, and under significant external distur-
bances (e.g., pushingpullingpressing on the robot, or manually
moving individual legs). These results highlight its potential
for practical deployments in human-centric and safety-critical
scenarios.
I. INTRODUCTION
Reinforcement learning (RL) has demonstrated significant
potential in the control of legged robots [1, 2]. Compared
to conventional control methods, such as Model Predictive
Control (MPC), RL-based approaches exhibit remarkable ro-
in complex terrain [3, 4, 5].
Most existing RL-based methods rely on position con-
trol [6, 7]. In this approach, neural networks output target
joint positions, which are subsequently translated into joint
torque commands through a low-level proportionalderivative
Inspired by biomechanical principles and the growth mechanisms
of animals in nature, we propose a framework that addresses the challenges
of torque-based locomotion learning, achieving zero-shot sim-to-real transfer
along with exceptional compliance and safety in challenging environments.
(PD) controller. Position-based policies are simple and easy
to train, as they abstract away the complexities of actuation
physics and dynamics. However, this simplicity limits the pol-
icys capacity to explore fine-grained and dynamic behaviors,
thereby reducing its adaptability and generalization to unseen
challenges in real-world environments. For instance, position-
based policies trained on rigid terrains in simulation often
struggle to generalize to deformable environments in the real
world due to their out-of-distribution nature. As a result, these
methods usually rely heavily on terrain randomization, where
the agent trains in large amounts of potential environments to
maximize the chances that the final policy may handle real-
world environments [8, 9]. Furthermore, position-based strate-
gies exhibit notable limitations in compliance. Lacking the
ability to directly regulate joint torques, these methods often
lead to overreactions to external disturbances. For example,
if one of the robots legs suddenly gets stuck, a position-
based policy may aggressively attempt to force the actuator
into the commanded position. This can cause the motors to
generate excessively high torques, potentially destabilizing or
even damaging the robot, or creating safety hazards to its
In contrast, torque-based learning controllers aim to train a
policy network that directly generates torques for all joints,
providing enhanced compliance and adaptability. By directly
controlling actuation in torque space, this approach enables
finer interaction with the environment, leading to more dy-
namic and robust locomotion. Moreover, torque control allows
the robot to explicitly reason about its dynamics, theoretically
offering a greater ability to handle unknown environments
and abrupt disturbances without relying on predefined low-
level controllers . However, torque-based policies come
with their own set of challenges, including a high-dimensional
action space and greater nonlinear transformations from states
to actions. These factors make exploration substantially harder
during training, particularly in the early stages. During this
ration process, leading to premature convergence and unnatural
gait behaviors. Few works have effectively addressed these
challenges. DeCAP, proposed by Sood et al. , mitigates
these issues by leveraging pre-trained position-based policies.
adoption. Similarly, Chen et al.  successfully trained a
torque policy by incorporating additional reward terms and
action scaling. Yet, this approach remains highly sensitive
to hyperparameter tuning and often exhibits low exploration
efficiency during the initial stages of training, preventing it
from consistently yielding high-performance policies.
To overcome these limitations, we propose enhancements
to directly learn torque-based policies by drawing inspiration
from biological systems. In animals, smooth motion actuation
is achieved through the intrinsic biomechanical properties of
the Hill model , which regulates movement commands
and prevents excessive behaviors. Moreover, certain muscle
mechanisms provide feedback that can aid decision-making,
such as muscle fatigue (Feedback of pain due to prolonged ex-
ertion) . Borrowing from these mechanisms, we integrate a
simplified biomechanical model into our torque-based learning
framework. This model retains the functional characteristics
of biological muscles, enabling the robot to perform smoother
actions while mitigating the risk of suboptimal convergence. In
gradual development processes observed when animals learn
to locomote. This mechanism dynamically adjusts key robot
properties during training, such as torque limits and control
the process. This significantly enhances early-stage exploration
and improves the generalizability of the trained policy.
By addressing the inherent challenges in torque-based pol-
icy learning, our approach not only provides a robust and ef-
ficient solution for torque-based control but also demonstrates
high performance in compliance and adaptability to previously
unseen scenarios, such as locomotion on deformable terrains.
These results highlight the potential of torque-based con-
trollers to surpass the limitations of position-based methods,
enabling safe and robust locomotion in complex environments.
The main contributions of this work are as follows:
Stable and Efficient Torque-Based Learning: We pro-
pose a novel framework for learning torque-based loco-
motion policies with a growth mechanism that gradually
unlocks torque limits, control frequency, and reward
in torque space.
Biomechanical Model and Safety: We implement a
simplified biomechanical model for actuators that ensures
smooth and safe behaviors, even under disturbances and
performs robustly in diverse environments.
that SATA generalizes effectively to out-of-distribution
terrains and commands while exhibiting exceptional com-
pliance during human interactions. These findings under-
score the robustness and versatility of our approach.
II. RELATED WORK
Quadruped controllers have greatly benefited from the de-
velopment of deep reinforcement learning (DRL) in recent
of non-linear optimization problems, which often involves
approximations that cannot be neglected in real-world set-
tings [18, 19, 20, 21]. However, the problem of sampling
efficiency still hinders its application. Early approaches em-
ployed by Peng et al.  combined imitation learning with
DRL to solve this problem, demonstrating gaits similar to the
training set while adapting to different terrain or disturbances.
This approach allows for the training of adaptive quadruped
controllers in highly dynamic acrobatic tasks, but its perfor-
mance is limited by the dataset quality and robot deployment
remains nontrivial. Addressing these problems, Hwangbo et
proposed a method to train deployable policies in
simulation by introducing an actuator network and domain
randomization. Their learned policies can yield different gaits
or recover from fallen positions. To help the robot travel
in challenging terrains,
Lee et al.  introduced a teacher-
student framework for quadrupedal robots, allowing them to
traverse complex terrains without any visual feedback. To aid
locomotion with environmental information,  built upon
by integrating additional sensors and employing an attention-
based recurrent encoder to fuse proprioceptive and exterocep-
tive inputs. This resulted in a robust and fast legged motion
controller for navigating challenging terrains. While extero-
ceptive inputs can enable more informed decision-making, a
highly compliant and robust base policycapable of oper-
ating effectively without visionremains crucial for reliable
real-world deployment. Moreover, reliance on exteroception
introduces additional challenges, such as the sim-to-real gap,
where sensor noise, latency, and real-world variations degrade
performance.
Learning-based controllers typically use position-based ac-
tion spaces, where the policy directly outputs position com-
mands for the actuators. These commands are subsequently
converted to torque using a low-level (e.g., PD) controller
Overview of our SATA Framework. Dotted lines indicate parts used only during training, while solid lines indicate those used during both training
and deployment. Our framework is mainly composed of 1) a Biomechanical Model (Orange) to ensure the generation of smooth, practical actuator commands
while informing the policy of the current actuator state, and 2) a Growth Model (Green) to help the neural network train a more robust and generalizable
policy by gradually adapting rewards rgrowth, control frequency fpolicy, and torque limits limit during training. Finally, we train a state estimator for real world
deployment using simulated IMU data and temporal proprioception observations (Grey), to help condition our policy on the (estimated) current robot velocity.
during training [24, 25, 26]. While such low-level controllers
facilitate early-stage exploration for reinforcement learning
successful deployment [27, 28]. Moreover, position control
tends to treat the robot as a rigid system, which can result in
significant joint or structural stress when operating in uncertain
environments (e.g. slipping, collisions), thereby increasing the
risk of system damage .
In contrast, torque-based policies, where the policy directly
outputs motor torques, eliminate the need for tuning low-
level controller parameters. They also take advantage of the
inherent compliance of torque control to reduce structural
stress and impact forces, enhancing the safety of human-robot
interactions . There, Chen et al. achieved the first
successful sim-to-real transfer of end-to-end torque control
for quadrupedal locomotion, enabling RL policies to directly
predict joint torques at high frequencies. However, Kim et
al. highlighted that torque-based state spaces exhibit sig-
nificant non-linearity and the controllers need to operate at
much higher frequencies, which impairs exploration efficiency
during early-stage training.
With the advancement of parallel simulation techniques
and high-performance computing [32, 33], massively parallel
RL environments have significantly improved sampling effi-
This allows training of more complicated frameworks that
can be deployed on-robot easily [35, 36, 37]. To enhance
learning-based approaches with traditional control techniques.
For instance, Gangapurwala et al.  proposed RL2AC,
which combines reinforcement learning policy with an adap-
tive torque compensator to mitigate external disturbances and
model mismatches caused by the sim-to-real gap. Similar
studies have also demonstrated that integrating the adaptability
of learning-based methods with the robustness of traditional
models can significantly improve the locomotion performance
of quadruped robots in complex environments [39, 40, 41].
The combination of bio-inspired control and reinforcement
learning is another major direction to address the adaptability
challenges of legged robots in dynamic and complex envi-
ronments [42, 43]. This hybrid approach integrates structured
prior knowledge from bio-inspired control within reinforce-
ment learning to optimize high-level strategies and key pa-
For instance, Margolis et al. achieved agile locomotion,
including walking, trotting, and pronking, by mimicking natu-
ral gait patterns through carefully designed rewards. Similarly,
Peng et al.  employed imitation learning to teach legged
robots agile skills by mimicking real-world animals, while Fu
et al.  leveraged deep reinforcement learning to minimize
energy consumption and achieve emergent gaits. Inspired by
Central Pattern Generators (CPGs) observed in animals, Bel-
legarda et al. combined CPGs with a DRL framework to
generate robust and omnidirectional locomotion. Building on
this foundation, Sun et al.  introduced the learning-based
hierarchical control framework, utilizing a spinal policy to
adjust CPG frequency and amplitude for rhythmic movement,
and a descending modulation policy to adaptively modify
rhythmic outputs for precise control on challenging terrains.
Similar to this idea, various hierarchical methods have also
been proposed for legged controllers [52, 53, 54]. Although
these studies have achieved promising results, most of them
focus on mimicking animal behaviors or neural structures.
Relatively limited research has explored the effects of biome-
chanical properties and growth processes on the development
of locomotion skills in animals, let alone incorporating them
into the learning framework for legged robots.
III. BIO-INSPIRED NEURAL ARCHITECTURE
To achieve robust and adaptive locomotion control in legged
ulates key principles of biological systems. This architecture
comprises two core modules: the Neural Networks and the
Biomechanical Model, as illustrated in Fig. 2. While the Neu-
ral Networks generate action signals based on proprioceptive
information (see detail in section III-B), the Biomechanical
Model is designed to process these action signals, introduc-
ing activation dynamics and muscle-like force modulation to
ensure smooth and realistic control signals. Additionally, the
Biomechanical Model provides internal states feedback to the
Neural Networks, providing more temporal information and
improved overall performance.
A. Biomechanical Model
Compared to directly using the neural networks output
as joint torques, our approach aims to reduce exploration
difficulty during training and improve motion continuity. To
achieve this, we refine the action signal as generated by the
neural network using a biomechanical model that employs
a biologically inspired two-step process and incorporates a
feedback mechanism, including a fatigue mechanism. This fa-
tigue mechanism, inspired by biological systems, dynamically
quantifies actuator load and recovery, contributing to more
balanced and robust control. The biomechanical model ensures
biologically plausible and stable locomotion through three key
components.
Activation Model: Converts action signals into interme-
diate activation signals, incorporating temporal smoothing
to reflect the sequential nature of biological systems.
This process ensures continuity and prepares signals for
precise torque generation.
Muscle Model: Transforms activation signals into joint
torques by loosely approximating muscle dynamics. This
approach limits torque output to a safe range, preventing
abrupt changes that could destabilize the system.
Internal State Model: Tracks the fatigue state of actua-
This feedback helps to optimize load distribution, enhanc-
ing stability during training and deployment.
By combining these components, the biomechanical model
improves exploration efficiency, reduces the risk of local
commands practical and effective for real-world applications.
1) Activation Model: Output by our policy network, the
action signal as first passes through the activation model .
This model functions similarly to motor neurons [56, 57],
transforming the action signal into the corresponding activa-
tion signal, current:
current  tanh
as  scale
of motor neurons in translating neural commands into muscle
The resulting activation signal, constrained within the range
(1, 1), emulates the antagonistic coordination of joint control
by opposing muscle groups. For example, 1 represents full
contraction of one muscle and complete relaxation of its
exact opposite, enabling precise joint torque modulation.
To ensure smooth and continuous activation dynamics, this
model incorporates a temporal update mechanism inspired by
the Hysteresis Effect . This mechanism accounts for the
influence of prior activations on the current state, ensuring
stability and natural transitions in movement. The simplified
first order temporal evolution of the activation signal t is
governed by:
t  (current  )  (t1  (1 )).
In this formulation,  serves as a smoothing factor. This
ensures a smooth temporal evolution of activation signals,
which promotes continuity and smoothness in movements.
2) Muscle Model: The activation signal is subsequently
passed to the muscle model, where it is transformed into
joint torques. There, we developed a dynamics model loosely
inspired by the classical Hill muscle model to mitigate po-
tential extreme behaviors . Specifically, we focused on
modeling and optimizing the force-velocity relationship in the
Hill model [60, 61]. When the activation signal aligns with the
joint motion direction, the torque output decreases as the joint
velocity increases, effectively suppressing rapid and extreme
torque signals. Conversely, when the joint velocity opposes the
activation signal, the model generates greater torque to drive
the joint towards the desired motion velocity. We believe this
force-velocity relationship enhances the activation networks
sensitivity to dynamic motion states, reducing instability risks
during training and improving exploration efficiency.
limit  t
1 sign(t)
where q represents the joint velocity, and qlimit is the maximum
velocity of each joint. This model integrates the activation sig-
nal with the dynamic characteristics of joint motion, enabling
accurate modeling of joint behavior and ultimately improving
the stability and efficiency of the overall control system.
3) Internal State Model: Our internal state model does not
directly participate in torque calculation but serves as a feed-
back mechanism to provide the robot with more information.
In this module, we simulate the fatigue mechanism observed in
animals to construct a dynamic state indicator that quantifies
the accumulation and recovery of fatigue during the robots
operation [62, 63]. The fatigue indicator is directly influenced
by the joint torque intensity and evolves dynamically over
where  represents the fatigue state, dt the time step, and  the
recovery factor, describing the rate at which fatigue dissipates
over time.
Through this mechanism, the robot can continuously update
its fatigue state, providing the neural network with a dynamic
feedback signal. This fatigue feedback effectively optimizes
the control strategy, preventing certain actuators from operat-
ing under prolonged high loads, which could otherwise lead
to excessive wear or reduced performance. Furthermore, it
enables the robot to distribute motion loads more efficiently
on each leg during task execution, reducing the occurrence of
suboptimal behaviors, such as over-reliance on three-legged or
two-legged motion patterns during exploration.
B. Neural Networks
Our SATA framework comprises three sub-networks: actor,
the Proximal Policy Optimization (PPO) algorithm, while the
estimator is trained separately, using supervised learning.
1) Observation and Action Space for Actor: The proprio-
ceptive observation vector ot R60 serves as the input to
the actor network, encapsulating comprehensive information
about the robots state.
The observation vector is structured as:
ot  [vt, wt, gt, q, q, vcmd, , ]T ,
where vt R3 represents the linear velocity of the robot base.
During training, vt is directly sourced from the simulator,
while during deployment, it is estimated by the estimator
network. The estimator uses historical proprioceptive and
inertial data, structured as:
et  [[oH
[q, q] includes the joint angles and velocities,
and it  [at, t, gt] represents the IMU datafeaturing
linear acceleration at R3, angular velocity t R3, and
gravity direction gt R3. This design enables robust velocity
estimation during deployment.
Other components of ot include wt R3, the angular
velocity of the robot base, and gt R3, the gravity direction
vector in the body frame. These quantities aid in orientation
estimation and maintaining balance. Additionally, vcmd R3
specifies the desired linear and angular velocities, while q
joint velocities, joint torques, and fatigue state, as described
earlier.
Based on ot, the actor policy generates the action as R12,
which represents the desired joint torques. These torques are
further refined by the biomechanical model to ensure smooth
and stable control.
2) Reward Design: In this work, we adopt a relatively
simple reward structure, made up of 9 terms designed to
effectively encourage natural and robust locomotion. These
rewards are categorized into two types: locomotion objectives
and auxiliary posture maintenance rewards. The details are
summarized in Table I.
tion used to penalize deviations between actual and com-
manded values. hb and ht denote the robots base height and
target base height above the ground, respectively. qmin and
qmax define the lower and upper limits of each joint, while q
represents the joint acceleration.
REWARD COMPONENTS AND WEIGHTS (dt  0.005)
Reward Terms
Equation
Locomotion Objectives
Auxiliary Posture Maintenance
rbase height
min(hb, ht)
rjoint limits
(qmin q)  (q qmax)
rfatigue
d  scale
rjoint acceleration
IV. GROWTH-BASED TRAINING
Due to the highly nonlinear nature of the torque space,
training a torque-based policy poses greater challenges than a
position-based one, especially during early-stage exploration.
To address this, we propose a biologically inspired growth
mechanism that mimics animal development by progressively
unlocking the robots physical capabilities, dynamically adapt-
ing reward functions, and gradually increasing control fre-
quency. This process facilitates more efficient policy learning
while preserving stability and promoting generalization.
While related in spirit to curriculum learning [64, 65, 66, 67]
and progressive learning [68, 69] approacheswhich typically
increase task difficulty over timeour method maintains a
fixed task throughout training. Instead of staging increasingly
complex goals, we focus on enhancing the agents embodiment
by gradually expanding what it is physically allowed to do.
This leads to deeper exploration and reduces the risk of sub-
optimal shortcuts, such as exploiting a single powerful joint.
Our strategy also aligns conceptually with reward scheduling
the agents growing capabilities [70, 71]. For instance, as
the agent develops from basic contact with the ground to
full stepping behavior, the reward emphasis naturally shifts
to reflect the current developmental stage.
A. Implementation of the Growth Mechanism
Instead of granting the policy full access to the action space
from the start of training, we propose that partially limiting
the robots abilities can promote more efficient exploration.
plifies exploration and mitigates the problem of delayed reward
during the early stages of training. To unify these components,
we introduce a time-dependent general scale G(t), derived
from the Gompertz model , a well-established framework
for modeling growth:
G(t)  eek(tt0).
training parameters. The parameters k, t, and t0 denote the
growth rate, the current training step, and the step at which
the maximum growth rate occurs, respectively.
1) Torque Limits and Control Frequency Adjustment:
Using G(t), we dynamically update the torque limits (limit)
and control frequency (fpolicy), which are closely linked to
growth [73, 74, 75], during training. These updates enable the
robot to gradually get access to its full operational capabilities:
limit  start  (end start)  G(t),
fpolicy  fstart  (fend fstart)  G(t).
control frequency at the beginning of training, while end
and fend denote their maximum values, which are reached as
training progresses.
2) Dynamic Adjustment of Reward Expressions: We also
leverage G(t) to dynamically adjust certain reward expressions
during training. This approach mirrors how animals maintain a
consistent overall goal while shifting their focus across differ-
ent learning stages. For instance, animals prioritize balance
and upright posture during early locomotion learning, then
focus on stepping, and eventually smoothen their movements.
align with specific training objectives. The adjusted growth-
based reward rgrowth expressions are summarized in Table II:
TABLE II
DETAILS OF OUR GROWTH-BASED REWARD TERMS
Adjusted Rewards
Calculation
rbase height
min(hb, ht)(1  G(t)) max (gx, min (0, 0.2  (1.5 2  G)))
As G(t) evolves, the rewards shift focus from encouraging
basic behaviors, such as forward motion, to more complex
objectives like maintaining body height and tracking precise
velocity commands. This progression enables the robot to
transition from simple motions to refined locomotion and
compliant posture control efficiently.
B. Training Details
We conduct training using Isaac Gym and the Unitree GO2
quadruped robot. This framework enables high-throughput
robot in parallel on a single NVIDIA RTX 4090 GPU. We
utilize Proximal Policy Optimization (PPO) to train the control
policy. The hyperparameters and neural network architecture
are consistent with , including a multilayer perceptron
(MLP) with three hidden layers, whose hidden dimensions
are [512, 256, 128]. Leveraging this framework, we achieve
efficient policy learning within 20 minutes 3000 episodes.
The maximum episode length is set to 10 seconds. The
environment resets when the robot flips over or its joint angles
exceed predefined limits. The terrains include rough surfaces
(with a maximum height variation of 12 cm) and slopes.
After each reset, the robot is repositioned lying flat on
the ground, with varying levels of motor fatigue already
applied. This random initialization enhances the generalization
Ablation study of the proposed framework, showing successful
training in green and failurepremature convergence in red. SATA is compared
with variants that lack the biomechanical model or the growth mechanism.
Notice that without the growth model (SATA wo Growth), the policy
struggles to achieve high commanded velocities (1.5ms), especially above
the range seen during training. Without the biomechanical model (SATA wo
biomechanical model), the robot is completely unable to learn a coherent gait,
instead learning to shift its feet on the floor asymmetrically.
capability of the learned policy. Target velocity commands
yaw] are sampled every 5 seconds, with ranges
set to vcmd
[0.5, 1.5] ms, vcmd
yaw [1.5, 1.5] rads.
Domain randomization is applied during training to simulate
real-world variability. The specific randomization settings are
as follows:
Added base mass: Randomly increased by up to 5 kg.
Ground coefficient of friction: Varied within [0.5, 1.25].
Probability to hold last actions or observations: 10.
Shifted center of mass: Varied within [-0.2, 0.2] m along
the x-axis, and [-0.1, 0.1] m along the y- and z-axes.
During training, control frequency and torque limits are pro-
gressively increased, asymptotically approaching their maxi-
mum values without fully reaching them, as defined in Eqs.6
and7. For deployment, the robots full capacity is restored by
setting these parameters to their respective maximums, fend
and end. All hyperparameters related to the growth schedule
and biomechanical model are summarized in Table III.
TABLE III
GROWTH SCHEDULE AND MODEL HYPERPARAMETERS
Growth Schedule
Biomechanical Model
V. EXPERIMENTS
A. Simulation Experiments
1) Ablation Study: To evaluate the contribution of each
component of our approach, we compare the performance of
the complete framework (SATA) with variants that remove the
biomechanical model (SATA wo biomechanical model) or the
growth mechanism (SATA wo Growth).
As shown in Fig. 3, the biomechanical model plays a
critical role in enabling natural and stable locomotion. When
Response to a sudden torque limitation on the front left leg (at t  0.5 s). During this disturbance (0.5 s < t < 1.5 s), the robot dynamically
compensates using other legs, and rapidly recovers once the torque is restored (1.5 s < t < 1.75 s).
this biomechanical model is removed, the robot converges to
unnatural gaits, such as three-legged support patterns, which
reduce stability and limit energy efficiency. This highlights
the importance of the biomechanical model and feedback
mechanisms in smoothing motion commands and preventing
suboptimal convergence.
On the other hand, the inclusion of the growth mechanism
leads to higher early-stage training efficiency and shows
better generalization when tracking out-of-distribution (OOD)
velocity commands. As demonstrated in Fig. 5a, SATA sig-
nificantly outperforms SATA wo growth in early stages of
stage exploration. Moreover, when comparing the cumulative
reward of both scenarios under OOD velocity commands
(vx  1.8ms) as in Fig. 5b, we can see that our method
outperforms SATA wo growth, demonstrating the impact of
the growth mechanism on policy generalization. Upon closer
is commanded to the highest command seen in training and
in OOD scenarios, leading to unstable tracking of velocity
commands.
In particular, these results suggest that the complementary
roles of the biomechanical model help ensure stable and nat-
ural motion, and the growth mechanism enhance the policys
adaptability and robustness under diverse conditions.
2) Robustness to Single-Leg Failure: In this experiment, we
simulate the failure of a single leg by abruptly reducing the
maximum torque of its motor to 20 of its original capacity
(0.2end). By doing so, we validate the robustness of our policy
during asymmetric conditions.
As shown in Fig. 4, when the front left legs torque output
Comparison of SATA and SATA wo Growth. Training rewards (a),
without G(t) adaptation, and cumulative rewards in simulation test (b), when
commanded to run at 1.8 ms (slightly OOD).
is limited, the other legs adaptively increase their force output
to stabilize the robots posture and prevent a collapse. This
dynamic redistribution of effort ensures continuous and stable
locomotion even under single leg failures. Once the torque
capacity of the front left leg is restored, the robot reactively
transitions back to its normal walking gait, demonstrating the
efficiency of the adaptive feedback mechanism in handling and
recovering from localized perturbations.
TABLE IV
PERFORMANCE COMPARISON BETWEEN OUR METHOD AND BASELINES
ACROSS DIFFERENT ROBUSTNESS TESTS (5 TRIALS PER TEST)
Vertical
SATA (Ours)
WalkTheseWay
Vanilla Position-based Policy
DeCAP (Pure Torque)
DeCAP (Position-assisted)
Unitree Built-in MPC
B. Lab Level Experiments
To validate the effectiveness of our approach, we deployed
it on a Unitree Go2 quadruped robot in real-world scenarios.
We also compared its performance against several baseline
Pushing the robot to its left (a) and right (b), and manually lifting
its legs (c). In all those cases, the controller did not start trotting nor generate
overreactinghazardous motions.
Leg disturbance test with a) Backward sweep of the front legs, b)
Backward sweep of the back legs, and c) Forward sweep of the front legs
right after kicking its body. In a), the rear left leg also lifts up to help balance
the whole body, while in b) the robot simply shifts its weight to the other
three feet in response to the sweep.
a vanilla learning position-based policy, a well-known learning
position-based policy, WalkTheseWay , as well as a torque-
based policy, DeCAP . Experimental results demonstrated
that our approach exhibited robust performance across var-
ious scenarios, including handling unexpected disturbances
and navigating unseen environments not encountered during
training. In multiple evaluations, as summarized in Table IV,
our method consistently outperformed the baseline approaches
across all robustness tests (please refer to our associated video
for details).
Contrary to the common perception that torque-based
methods suffer from a significant sim-to-real gap, our ap-
proach achieved zero-shot transfer without any fine-tuning and
demonstrated highly stable operation over extended deploy-
ment periods. To assess its compliance and adaptability, we
conducted a series of external disturbance tests in a controlled
lab environment to observe and compare the controllers
responses against baseline methods. In the first subsection,
we illustrate the compliance of our method during human-
robot interactions, while Sections V-B2 and V-B3 highlight
its robustness against external disturbances.
1) Safety for Human-Robot Interaction: For robotic ma-
during collaborative tasks. A compliant robotic arm can be
easily pushed by a person and will stop applying excessive
force upon contact, preventing injuries instead of forcefully
moving to a predetermined position. Similarly, compliance
Walking under external downward forcespresses. The blue line is the
actual torque command given to the motors after the processing done by our
biomechanical model, while the orange dotted line is the raw action output of
the policy. The robot can continue progressing forward even when an external
force press it to the ground. Note that the biomechanical model is crucial in
ensuring the safety of the robot especially during these disturbances, limiting
excessive torque output and mitigating oscillations.
in quadrupedal robots is essential for safe interaction with
humans. This section focuses on evaluating the passive com-
pliance of our controller and its interaction capabilities with
In standing posture, our torque-based controller allows the
robot to respond naturally to human-applied forces, adjusting
its body posture without unnecessary stepping motions. The
robot only takes corrective steps when balance is at risk. Fig. 6
illustrates the level of compliance the robot can achieve during
human robot interaction. These results significantly surpass
the performance of position-based controllers, which exhibit
a very stiff behavior and are nearly unable to be displaced
without stepping.
2) Impact on Legs: Compliance also plays a crucial role
in responding to localized external disturbances. To test this,
we placed our robot in a standing posture (vcmd
while its legs were subjected to forward and lateral sweeps
with sufficient force to lift up its foot. As demonstrated in
Fig. 7, the robots controller exhibited robust performance,
successfully resisting these disturbances across all four legs
without overreacting. Furthermore, even when additional ex-
ternal impacts were applied to other parts of the body, the
controller consistently enabled the robot to regain balance
through a series of smooth, adaptive adjustments, quickly
regaining balance with a side step.
3) Kicking and Stomping: Most quadrupedal controllers
demonstrate robust responses to kicks. However, the impact
limit for these controllers varies. Some struggles with recovery
Stability test of our proposed controller over a long (1.2km) route,
which covers tiled floor indoors (a,b), rough hard unstructured road (c,d),
high slopes (e), pedestrian paths (f,g,h), and soft lawn (i). The whole route
was completed without human correction, beyond manual adjustment of the
robots heading to chart its course.
from downward forces applied directly from above, while
others perform poorly from horizontal impacts that can flip
the robot over. In contrast, our controller exhibits remarkable
recovery capabilities to these disturbances. Fig. 8 shows how
our approach handles a downward stomp during locomotion,
with its biomechanical model helping to ensure safe motor
commands. Furthermore, we note that our method allows the
robot to successfully start operating from arbitrary configura-
This capability is rarely observed in position-based methods
or traditional control approaches, which typically require the
robot to stand up first through predefined instructions before
operation.
C. Out-of-distribution Environments
Fig. 9 shows a very long (1.2km) walk using our controller,
passing through different terrains without any human correc-
chart its course. Generally, position-based quadrupedal con-
trollers perform well on various hard, unstructured surfaces.
or slippery terrain. As noted in , additional modifications
to the controller or extensive domain randomization are often
required to ensure robust real-world deployment on such
surfaces. On the other hand, torque-based controllers leverage
a more direct understanding of the robots dynamics and
terrain interactions, enabling a more robust response to those
environments. The following section covers several challeng-
ing and out-of-distribution terrains, including height constraint,
or soft and slippery grounds, showcasing the generalizability
and adaptability of our approach.
1) Squeezing into a Tunnel: To evaluate the performance
of our controller in height-constrained scenarios, we designed
a tunnel traversal experiment in which the minimum tunnel
height was approximately 30 cm. Unlike baseline methods
which can hardly be compressed by vertical force, our ap-
proach is able to passively adapt to this situation. As shown
in Fig. 10, the robot was compressed to approximately half of
Fig. 10.
Locomotion through a height-constrained space. Notably, no height
command or additional modifications were made to the robot or the policy,
except for the addition of passive wheels on top of the robots body to reduce
friction from the tunnel ceiling.
Fig. 11.
Locomotion on wet slippery surfaces, showing both success (a) and
failure (b). Even when the foot of the robot slips and fall down in failure
its normal height after bumping into the tunnel ceiling, and
was able to move forward while staying in this compressed
state. Despite the fact that this scenario was never seen during
controller autonomously adopted a crawling gait, allowing the
robot to continue moving forward without getting stuck. Upon
exiting the tunnel, the robot naturally returned to its upright
posture and resumed its normal walking gait without requiring
any external intervention.
2) Slippery Surfaces: Figure 11 demonstrates the robots
performance on a wet and slippery surface. In Figure 11a,
after experiencing a slip during locomotion, the robot quickly
recovers its standing posture. Our controller dynamically ad-
justs the motor output to stabilize the robot without predefined
recovery motions. In contrast, Figure 11b shows a failure
slippery surface. Note that the controller is still active with
the robot applying torque through the thigh joints in an
attempt to recover balance, yet no abrupt or flailing motion
is produced. This behavior contrasts sharply with position-
based controllers, where the thigh and calf joint would typi-
cally be retracted abruptly, often leading to tipping or other
destabilizing outcomes. The torque-based controllers ability
to exert gradual and compliant force enhances minimizes the
aggressive reactions that could compromise stability.
3) Soft Terrain: In addition to slippery surfaces, deformable
terrain presents significant challenges for position-based con-
trollers. To evaluate performance, we conducted an experiment
Fig. 12.
Locomotion on 10cm thick soft mattress with a velocity command
of 0.8ms. Our robot stops with the right most posture when the velocity
command is finally set to 0.
where the robot traversed a soft mattress approximately
