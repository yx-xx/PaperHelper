2025-07-19 13:49:20,417 - __main__ - INFO - 开始运行PaperCrawler...
2025-07-19 13:49:20,417 - __main__ - INFO - 开始爬取RSS源...
2025-07-19 13:49:20,417 - __main__ - INFO - RSS爬取完成，共获取 0 篇论文信息
2025-07-19 13:49:20,417 - __main__ - INFO - 开始爬取arXiv...
2025-07-19 13:49:20,418 - arxiv_crawler - INFO - arXiv搜索完成，共获取 0 篇论文
2025-07-19 13:49:20,418 - __main__ - INFO - arXiv爬取完成，共获取 0 篇论文信息
2025-07-19 13:49:20,418 - __main__ - INFO - 开始爬取配置的网页源...
2025-07-19 13:49:20,418 - __main__ - INFO - 爬取网页: https://roboticsconference.org/2024/program/papers/
2025-07-19 13:49:20,418 - robotics_conference_crawler - INFO - 开始爬取会议论文: https://roboticsconference.org/2024/program/papers/
2025-07-19 13:49:27,802 - robotics_conference_crawler - INFO - 找到 134 个论文链接
2025-07-19 13:49:27,802 - robotics_conference_crawler - INFO - 处理论文 1/134: https://roboticsconference.org/2024/program/papers/1/
2025-07-19 13:49:29,407 - robotics_conference_crawler - INFO - 处理论文 2/134: https://roboticsconference.org/2024/program/papers/2/
2025-07-19 13:49:30,551 - robotics_conference_crawler - INFO - 处理论文 3/134: https://roboticsconference.org/2024/program/papers/3/
2025-07-19 13:49:32,886 - robotics_conference_crawler - INFO - 处理论文 4/134: https://roboticsconference.org/2024/program/papers/4/
2025-07-19 13:49:34,828 - robotics_conference_crawler - INFO - 处理论文 5/134: https://roboticsconference.org/2024/program/papers/5/
2025-07-19 13:49:38,314 - robotics_conference_crawler - INFO - 处理论文 6/134: https://roboticsconference.org/2024/program/papers/6/
2025-07-19 13:49:39,853 - robotics_conference_crawler - INFO - 处理论文 7/134: https://roboticsconference.org/2024/program/papers/7/
2025-07-19 13:49:43,232 - robotics_conference_crawler - INFO - 处理论文 8/134: https://roboticsconference.org/2024/program/papers/8/
2025-07-19 13:49:46,063 - robotics_conference_crawler - INFO - 处理论文 9/134: https://roboticsconference.org/2024/program/papers/9/
2025-07-19 13:49:48,758 - robotics_conference_crawler - INFO - 处理论文 10/134: https://roboticsconference.org/2024/program/papers/10/
2025-07-19 13:49:50,296 - robotics_conference_crawler - INFO - 处理论文 11/134: https://roboticsconference.org/2024/program/papers/11/
2025-07-19 13:49:52,105 - robotics_conference_crawler - INFO - 处理论文 12/134: https://roboticsconference.org/2024/program/papers/12/
2025-07-19 13:49:54,087 - robotics_conference_crawler - INFO - 处理论文 13/134: https://roboticsconference.org/2024/program/papers/13/
2025-07-19 13:49:56,850 - robotics_conference_crawler - INFO - 处理论文 14/134: https://roboticsconference.org/2024/program/papers/14/
2025-07-19 13:49:58,810 - robotics_conference_crawler - INFO - 处理论文 15/134: https://roboticsconference.org/2024/program/papers/15/
2025-07-19 13:50:01,764 - robotics_conference_crawler - INFO - 处理论文 16/134: https://roboticsconference.org/2024/program/papers/16/
2025-07-19 13:50:03,099 - robotics_conference_crawler - INFO - 处理论文 17/134: https://roboticsconference.org/2024/program/papers/17/
2025-07-19 13:50:05,555 - robotics_conference_crawler - INFO - 处理论文 18/134: https://roboticsconference.org/2024/program/papers/18/
2025-07-19 13:50:08,524 - robotics_conference_crawler - INFO - 处理论文 19/134: https://roboticsconference.org/2024/program/papers/19/
2025-07-19 13:50:10,476 - robotics_conference_crawler - INFO - 处理论文 20/134: https://roboticsconference.org/2024/program/papers/20/
2025-07-19 13:50:13,130 - robotics_conference_crawler - INFO - 处理论文 21/134: https://roboticsconference.org/2024/program/papers/21/
2025-07-19 13:50:16,653 - robotics_conference_crawler - INFO - 处理论文 22/134: https://roboticsconference.org/2024/program/papers/22/
2025-07-19 13:50:18,147 - robotics_conference_crawler - INFO - 处理论文 23/134: https://roboticsconference.org/2024/program/papers/23/
2025-07-19 13:50:22,136 - robotics_conference_crawler - INFO - 处理论文 24/134: https://roboticsconference.org/2024/program/papers/24/
2025-07-19 13:50:25,318 - robotics_conference_crawler - INFO - 处理论文 25/134: https://roboticsconference.org/2024/program/papers/25/
2025-07-19 13:50:27,670 - robotics_conference_crawler - INFO - 处理论文 26/134: https://roboticsconference.org/2024/program/papers/26/
2025-07-19 13:50:29,308 - robotics_conference_crawler - INFO - 处理论文 27/134: https://roboticsconference.org/2024/program/papers/27/
2025-07-19 13:50:30,888 - robotics_conference_crawler - INFO - 处理论文 28/134: https://roboticsconference.org/2024/program/papers/28/
2025-07-19 13:50:32,489 - robotics_conference_crawler - INFO - 处理论文 29/134: https://roboticsconference.org/2024/program/papers/29/
2025-07-19 13:50:34,429 - robotics_conference_crawler - INFO - 处理论文 30/134: https://roboticsconference.org/2024/program/papers/30/
2025-07-19 13:50:35,973 - robotics_conference_crawler - INFO - 处理论文 31/134: https://roboticsconference.org/2024/program/papers/31/
2025-07-19 13:50:38,831 - robotics_conference_crawler - INFO - 处理论文 32/134: https://roboticsconference.org/2024/program/papers/32/
2025-07-19 13:50:40,883 - robotics_conference_crawler - INFO - 处理论文 33/134: https://roboticsconference.org/2024/program/papers/33/
2025-07-19 13:50:42,522 - robotics_conference_crawler - INFO - 处理论文 34/134: https://roboticsconference.org/2024/program/papers/34/
2025-07-19 13:50:45,693 - robotics_conference_crawler - INFO - 处理论文 35/134: https://roboticsconference.org/2024/program/papers/35/
2025-07-19 13:50:50,916 - robotics_conference_crawler - INFO - 处理论文 36/134: https://roboticsconference.org/2024/program/papers/36/
2025-07-19 13:50:53,173 - robotics_conference_crawler - INFO - 处理论文 37/134: https://roboticsconference.org/2024/program/papers/37/
2025-07-19 13:50:56,240 - robotics_conference_crawler - INFO - 处理论文 38/134: https://roboticsconference.org/2024/program/papers/38/
2025-07-19 13:50:57,877 - robotics_conference_crawler - INFO - 处理论文 39/134: https://roboticsconference.org/2024/program/papers/39/
2025-07-19 13:50:59,620 - robotics_conference_crawler - INFO - 处理论文 40/134: https://roboticsconference.org/2024/program/papers/40/
2025-07-19 13:51:01,382 - robotics_conference_crawler - INFO - 处理论文 41/134: https://roboticsconference.org/2024/program/papers/41/
2025-07-19 13:51:04,753 - robotics_conference_crawler - INFO - 处理论文 42/134: https://roboticsconference.org/2024/program/papers/42/
2025-07-19 13:51:08,623 - robotics_conference_crawler - INFO - 处理论文 43/134: https://roboticsconference.org/2024/program/papers/43/
2025-07-19 13:51:12,214 - robotics_conference_crawler - INFO - 处理论文 44/134: https://roboticsconference.org/2024/program/papers/44/
2025-07-19 13:51:14,365 - robotics_conference_crawler - INFO - 处理论文 45/134: https://roboticsconference.org/2024/program/papers/45/
2025-07-19 13:51:17,232 - robotics_conference_crawler - INFO - 处理论文 46/134: https://roboticsconference.org/2024/program/papers/46/
2025-07-19 13:51:20,509 - robotics_conference_crawler - INFO - 处理论文 47/134: https://roboticsconference.org/2024/program/papers/47/
2025-07-19 13:51:22,047 - robotics_conference_crawler - INFO - 处理论文 48/134: https://roboticsconference.org/2024/program/papers/48/
2025-07-19 13:51:24,201 - robotics_conference_crawler - INFO - 处理论文 49/134: https://roboticsconference.org/2024/program/papers/49/
2025-07-19 13:51:27,370 - robotics_conference_crawler - INFO - 处理论文 50/134: https://roboticsconference.org/2024/program/papers/50/
2025-07-19 13:51:35,562 - robotics_conference_crawler - INFO - 处理论文 51/134: https://roboticsconference.org/2024/program/papers/51/
2025-07-19 13:51:37,404 - robotics_conference_crawler - INFO - 处理论文 52/134: https://roboticsconference.org/2024/program/papers/52/
2025-07-19 13:51:38,704 - robotics_conference_crawler - INFO - 处理论文 53/134: https://roboticsconference.org/2024/program/papers/54/
2025-07-19 13:51:40,272 - robotics_conference_crawler - INFO - 处理论文 54/134: https://roboticsconference.org/2024/program/papers/55/
2025-07-19 13:51:43,549 - robotics_conference_crawler - INFO - 处理论文 55/134: https://roboticsconference.org/2024/program/papers/56/
2025-07-19 13:51:45,292 - robotics_conference_crawler - INFO - 处理论文 56/134: https://roboticsconference.org/2024/program/papers/57/
2025-07-19 13:51:46,638 - robotics_conference_crawler - INFO - 处理论文 57/134: https://roboticsconference.org/2024/program/papers/58/
2025-07-19 13:51:48,055 - robotics_conference_crawler - INFO - 处理论文 58/134: https://roboticsconference.org/2024/program/papers/59/
2025-07-19 13:51:49,400 - robotics_conference_crawler - INFO - 处理论文 59/134: https://roboticsconference.org/2024/program/papers/60/
2025-07-19 13:51:50,820 - robotics_conference_crawler - INFO - 处理论文 60/134: https://roboticsconference.org/2024/program/papers/61/
2025-07-19 13:51:53,892 - robotics_conference_crawler - INFO - 处理论文 61/134: https://roboticsconference.org/2024/program/papers/62/
2025-07-19 13:51:55,529 - robotics_conference_crawler - INFO - 处理论文 62/134: https://roboticsconference.org/2024/program/papers/63/
2025-07-19 13:51:57,991 - robotics_conference_crawler - INFO - 处理论文 63/134: https://roboticsconference.org/2024/program/papers/64/
2025-07-19 13:52:00,343 - robotics_conference_crawler - INFO - 处理论文 64/134: https://roboticsconference.org/2024/program/papers/65/
2025-07-19 13:52:02,091 - robotics_conference_crawler - INFO - 处理论文 65/134: https://roboticsconference.org/2024/program/papers/66/
2025-07-19 13:52:06,078 - robotics_conference_crawler - INFO - 处理论文 66/134: https://roboticsconference.org/2024/program/papers/67/
2025-07-19 13:52:07,916 - robotics_conference_crawler - INFO - 处理论文 67/134: https://roboticsconference.org/2024/program/papers/68/
2025-07-19 13:52:09,559 - robotics_conference_crawler - INFO - 处理论文 68/134: https://roboticsconference.org/2024/program/papers/69/
2025-07-19 13:52:11,198 - robotics_conference_crawler - INFO - 处理论文 69/134: https://roboticsconference.org/2024/program/papers/70/
2025-07-19 13:52:12,939 - robotics_conference_crawler - INFO - 处理论文 70/134: https://roboticsconference.org/2024/program/papers/71/
2025-07-19 13:52:16,114 - robotics_conference_crawler - INFO - 处理论文 71/134: https://roboticsconference.org/2024/program/papers/72/
2025-07-19 13:52:19,286 - robotics_conference_crawler - INFO - 处理论文 72/134: https://roboticsconference.org/2024/program/papers/73/
2025-07-19 13:52:21,437 - robotics_conference_crawler - INFO - 处理论文 73/134: https://roboticsconference.org/2024/program/papers/74/
2025-07-19 13:52:24,407 - robotics_conference_crawler - INFO - 处理论文 74/134: https://roboticsconference.org/2024/program/papers/75/
2025-07-19 13:52:26,762 - robotics_conference_crawler - INFO - 处理论文 75/134: https://roboticsconference.org/2024/program/papers/76/
2025-07-19 13:52:29,117 - robotics_conference_crawler - INFO - 处理论文 76/134: https://roboticsconference.org/2024/program/papers/77/
2025-07-19 13:52:31,680 - robotics_conference_crawler - INFO - 处理论文 77/134: https://roboticsconference.org/2024/program/papers/78/
2025-07-19 13:52:33,521 - robotics_conference_crawler - INFO - 处理论文 78/134: https://roboticsconference.org/2024/program/papers/79/
2025-07-19 13:52:43,249 - robotics_conference_crawler - INFO - 处理论文 79/134: https://roboticsconference.org/2024/program/papers/80/
2025-07-19 13:52:44,990 - robotics_conference_crawler - INFO - 处理论文 80/134: https://roboticsconference.org/2024/program/papers/81/
2025-07-19 13:52:46,606 - robotics_conference_crawler - INFO - 处理论文 81/134: https://roboticsconference.org/2024/program/papers/82/
2025-07-19 13:52:48,164 - robotics_conference_crawler - INFO - 处理论文 82/134: https://roboticsconference.org/2024/program/papers/83/
2025-07-19 13:52:51,031 - robotics_conference_crawler - INFO - 处理论文 83/134: https://roboticsconference.org/2024/program/papers/84/
2025-07-19 13:52:52,772 - robotics_conference_crawler - INFO - 处理论文 84/134: https://roboticsconference.org/2024/program/papers/85/
2025-07-19 13:52:54,521 - robotics_conference_crawler - INFO - 处理论文 85/134: https://roboticsconference.org/2024/program/papers/86/
2025-07-19 13:52:57,277 - robotics_conference_crawler - INFO - 处理论文 86/134: https://roboticsconference.org/2024/program/papers/87/
2025-07-19 13:52:59,019 - robotics_conference_crawler - INFO - 处理论文 87/134: https://roboticsconference.org/2024/program/papers/88/
2025-07-19 13:53:01,067 - robotics_conference_crawler - INFO - 处理论文 88/134: https://roboticsconference.org/2024/program/papers/89/
2025-07-19 13:53:05,674 - robotics_conference_crawler - INFO - 处理论文 89/134: https://roboticsconference.org/2024/program/papers/90/
2025-07-19 13:53:08,541 - robotics_conference_crawler - INFO - 处理论文 90/134: https://roboticsconference.org/2024/program/papers/91/
2025-07-19 13:53:10,597 - robotics_conference_crawler - INFO - 处理论文 91/134: https://roboticsconference.org/2024/program/papers/92/
2025-07-19 13:53:12,346 - robotics_conference_crawler - INFO - 处理论文 92/134: https://roboticsconference.org/2024/program/papers/93/
2025-07-19 13:53:14,638 - robotics_conference_crawler - INFO - 处理论文 93/134: https://roboticsconference.org/2024/program/papers/94/
2025-07-19 13:53:16,222 - robotics_conference_crawler - INFO - 处理论文 94/134: https://roboticsconference.org/2024/program/papers/95/
2025-07-19 13:53:17,557 - robotics_conference_crawler - INFO - 处理论文 95/134: https://roboticsconference.org/2024/program/papers/96/
2025-07-19 13:53:18,884 - robotics_conference_crawler - INFO - 处理论文 96/134: https://roboticsconference.org/2024/program/papers/97/
2025-07-19 13:53:20,521 - robotics_conference_crawler - INFO - 处理论文 97/134: https://roboticsconference.org/2024/program/papers/98/
2025-07-19 13:53:21,756 - robotics_conference_crawler - INFO - 处理论文 98/134: https://roboticsconference.org/2024/program/papers/99/
2025-07-19 13:53:23,083 - robotics_conference_crawler - INFO - 处理论文 99/134: https://roboticsconference.org/2024/program/papers/100/
2025-07-19 13:53:24,517 - robotics_conference_crawler - INFO - 处理论文 100/134: https://roboticsconference.org/2024/program/papers/101/
2025-07-19 13:53:25,847 - robotics_conference_crawler - INFO - 处理论文 101/134: https://roboticsconference.org/2024/program/papers/102/
2025-07-19 13:53:27,179 - robotics_conference_crawler - INFO - 处理论文 102/134: https://roboticsconference.org/2024/program/papers/103/
2025-07-19 13:53:28,537 - robotics_conference_crawler - INFO - 处理论文 103/134: https://roboticsconference.org/2024/program/papers/104/
2025-07-19 13:53:31,275 - robotics_conference_crawler - INFO - 处理论文 104/134: https://roboticsconference.org/2024/program/papers/105/
2025-07-19 13:53:33,323 - robotics_conference_crawler - INFO - 处理论文 105/134: https://roboticsconference.org/2024/program/papers/106/
2025-07-19 13:53:35,783 - robotics_conference_crawler - INFO - 处理论文 106/134: https://roboticsconference.org/2024/program/papers/107/
2025-07-19 13:53:39,563 - robotics_conference_crawler - INFO - 处理论文 107/134: https://roboticsconference.org/2024/program/papers/108/
2025-07-19 13:53:45,532 - robotics_conference_crawler - INFO - 处理论文 108/134: https://roboticsconference.org/2024/program/papers/109/
2025-07-19 13:53:50,322 - robotics_conference_crawler - INFO - 处理论文 109/134: https://roboticsconference.org/2024/program/papers/110/
2025-07-19 13:53:52,984 - robotics_conference_crawler - INFO - 处理论文 110/134: https://roboticsconference.org/2024/program/papers/111/
2025-07-19 13:53:55,135 - robotics_conference_crawler - INFO - 处理论文 111/134: https://roboticsconference.org/2024/program/papers/112/
2025-07-19 13:53:56,772 - robotics_conference_crawler - INFO - 处理论文 112/134: https://roboticsconference.org/2024/program/papers/113/
2025-07-19 13:53:59,020 - robotics_conference_crawler - INFO - 处理论文 113/134: https://roboticsconference.org/2024/program/papers/114/
2025-07-19 13:54:02,916 - robotics_conference_crawler - INFO - 处理论文 114/134: https://roboticsconference.org/2024/program/papers/115/
2025-07-19 13:54:05,783 - robotics_conference_crawler - INFO - 处理论文 115/134: https://roboticsconference.org/2024/program/papers/116/
2025-07-19 13:54:08,447 - robotics_conference_crawler - INFO - 处理论文 116/134: https://roboticsconference.org/2024/program/papers/117/
2025-07-19 13:54:13,771 - robotics_conference_crawler - INFO - 处理论文 117/134: https://roboticsconference.org/2024/program/papers/118/
2025-07-19 13:54:15,409 - robotics_conference_crawler - INFO - 处理论文 118/134: https://roboticsconference.org/2024/program/papers/119/
2025-07-19 13:54:17,047 - robotics_conference_crawler - INFO - 处理论文 119/134: https://roboticsconference.org/2024/program/papers/120/
2025-07-19 13:54:18,788 - robotics_conference_crawler - INFO - 处理论文 120/134: https://roboticsconference.org/2024/program/papers/121/
2025-07-19 13:54:20,532 - robotics_conference_crawler - INFO - 处理论文 121/134: https://roboticsconference.org/2024/program/papers/122/
2025-07-19 13:54:22,800 - robotics_conference_crawler - INFO - 处理论文 122/134: https://roboticsconference.org/2024/program/papers/123/
2025-07-19 13:54:25,337 - robotics_conference_crawler - INFO - 处理论文 123/134: https://roboticsconference.org/2024/program/papers/124/
2025-07-19 13:54:26,974 - robotics_conference_crawler - INFO - 处理论文 124/134: https://roboticsconference.org/2024/program/papers/125/
2025-07-19 13:54:28,513 - robotics_conference_crawler - INFO - 处理论文 125/134: https://roboticsconference.org/2024/program/papers/126/
2025-07-19 13:54:30,150 - robotics_conference_crawler - INFO - 处理论文 126/134: https://roboticsconference.org/2024/program/papers/127/
2025-07-19 13:54:33,018 - robotics_conference_crawler - INFO - 处理论文 127/134: https://roboticsconference.org/2024/program/papers/128/
2025-07-19 13:54:34,566 - robotics_conference_crawler - INFO - 处理论文 128/134: https://roboticsconference.org/2024/program/papers/129/
2025-07-19 13:54:36,294 - robotics_conference_crawler - INFO - 处理论文 129/134: https://roboticsconference.org/2024/program/papers/130/
2025-07-19 13:54:37,528 - robotics_conference_crawler - INFO - 处理论文 130/134: https://roboticsconference.org/2024/program/papers/131/
2025-07-19 13:54:38,858 - robotics_conference_crawler - INFO - 处理论文 131/134: https://roboticsconference.org/2024/program/papers/132/
2025-07-19 13:54:41,931 - robotics_conference_crawler - INFO - 处理论文 132/134: https://roboticsconference.org/2024/program/papers/133/
2025-07-19 13:54:44,793 - robotics_conference_crawler - INFO - 处理论文 133/134: https://roboticsconference.org/2024/program/papers/134/
2025-07-19 13:54:47,666 - robotics_conference_crawler - INFO - 处理论文 134/134: https://roboticsconference.org/2024/program/papers/135/
2025-07-19 13:54:49,509 - robotics_conference_crawler - INFO - 会议论文爬取完成，获取 131 篇论文
2025-07-19 13:54:49,509 - __main__ - INFO - 网页源爬取完成，共获取 131 篇论文信息
2025-07-19 13:54:49,510 - __main__ - INFO - 去重后共有 131 篇论文
2025-07-19 13:54:49,514 - __main__ - INFO - 论文信息已保存到: /home/yx_xx/myProject/PaperRead/Data/PC_Data/output/crawled_papers.json
2025-07-19 13:54:49,514 - __main__ - INFO - 开始下载论文PDF...
2025-07-19 13:54:49,514 - paper_downloader - INFO - 开始下载: Stein Variational Ergodic SearchDarrick Lee, Cameron Lerch, Fabio Ramos, Ian AbrahamPaper ID 1Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Exploration requires that robots reason about numerous ways to cover a space in response to dynamically changing conditions. However, in continuous domains there are potentially infinitely many options for robots to explore which can prove computationally challenging. How then should a robot efficiently optimize and choose exploration strategies to adopt? In this work, we explore this question through the use of variational inference to efficiently solve for distributions of coverage trajectories. Our approach leverages ergodic search methods to optimize coverage trajectories in continuous time and space. In order to reason about distributions of trajectories, we formulate ergodic search as a probabilistic inference problem. We propose to leverage Stein variational methods to approximate a posterior distribution over ergodic trajectories through parallel computation. As a result, it becomes possible to efficiently optimize distributions of feasible coverage trajectories for which robots can adapt exploration. We demonstrate that the proposed Stein variational ergodic search approach facilitates efficient identification of multiple coverage strategies and show online adaptation in a model-predictive control formulation. Simulated and physical experiments demonstrate adaptability and diversity in exploration strategies online.
2025-07-19 13:54:55,685 - paper_downloader - INFO - 下载完成: Stein Variational Ergodic SearchDarrick Lee, Cameron Lerch, Fabio Ramos, Ian AbrahamPaper ID 1Sessio.pdf
2025-07-19 13:54:55,685 - __main__ - INFO - 成功下载: Stein Variational Ergodic SearchDarrick Lee, Cameron Lerch, Fabio Ramos, Ian AbrahamPaper ID 1Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Exploration requires that robots reason about numerous ways to cover a space in response to dynamically changing conditions. However, in continuous domains there are potentially infinitely many options for robots to explore which can prove computationally challenging. How then should a robot efficiently optimize and choose exploration strategies to adopt? In this work, we explore this question through the use of variational inference to efficiently solve for distributions of coverage trajectories. Our approach leverages ergodic search methods to optimize coverage trajectories in continuous time and space. In order to reason about distributions of trajectories, we formulate ergodic search as a probabilistic inference problem. We propose to leverage Stein variational methods to approximate a posterior distribution over ergodic trajectories through parallel computation. As a result, it becomes possible to efficiently optimize distributions of feasible coverage trajectories for which robots can adapt exploration. We demonstrate that the proposed Stein variational ergodic search approach facilitates efficient identification of multiple coverage strategies and show online adaptation in a model-predictive control formulation. Simulated and physical experiments demonstrate adaptability and diversity in exploration strategies online.
2025-07-19 13:54:55,685 - paper_downloader - INFO - 开始下载: Parallel and Proximal Linear-Quadratic Methods for Real-Time Constrained Model-Predictive ControlWilson Jallet, Ewen Dantec, Etienne Arlaud, Nicolas Mansard, Justin CarpentierPaper ID 2Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Recent strides in model predictive control (MPC)
 underscore a dependence on numerical advancements to efficiently
 and accurately solve large-scale problems. Given the substantial
 number of variables characterizing typical whole-body optimal
 control problems —often numbering in the thousands— exploiting
 the sparse structure of the numerical problem becomes crucial
 to meet computational demands, typically in the range of a
 few milliseconds. A fundamental building block for computing
 Newton or Sequential Quadratic Programming steps in direct
 optimal control methods involves addressing the linear-quadratic
 regulator (LQR) problem. This paper concentrates on equality-
 constrained problems featuring implicit system dynamics and
 dual regularization, a characteristic found in advanced interior-
 point or augmented-Lagrangian solvers. Here, we introduce a
 parallel algorithm designed for solving an LQR problem with
 dual regularization. Leveraging a rewriting of the LQR recursion
 through block elimination, we first enhanced the efficiency of
 the serial algorithm, then subsequently generalized to handle
 parametric LQR. This extension enables us to split decision
 variables and to solve several subproblems concurrently. Our
 algorithm is implemented (and will be released in open source) in a
 nonlinear constrained implicit optimal control solver. It showcases
 improved performance over previous serial formulations and we
 validate its efficacy by deploying it in the model predictive control
 of a real quadruped robot.
2025-07-19 13:54:55,961 - paper_downloader - INFO - 下载完成: Parallel and Proximal Linear-Quadratic Methods for Real-Time Constrained Model-Predictive ControlWil.pdf
2025-07-19 13:54:55,961 - __main__ - INFO - 成功下载: Parallel and Proximal Linear-Quadratic Methods for Real-Time Constrained Model-Predictive ControlWilson Jallet, Ewen Dantec, Etienne Arlaud, Nicolas Mansard, Justin CarpentierPaper ID 2Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Recent strides in model predictive control (MPC)
 underscore a dependence on numerical advancements to efficiently
 and accurately solve large-scale problems. Given the substantial
 number of variables characterizing typical whole-body optimal
 control problems —often numbering in the thousands— exploiting
 the sparse structure of the numerical problem becomes crucial
 to meet computational demands, typically in the range of a
 few milliseconds. A fundamental building block for computing
 Newton or Sequential Quadratic Programming steps in direct
 optimal control methods involves addressing the linear-quadratic
 regulator (LQR) problem. This paper concentrates on equality-
 constrained problems featuring implicit system dynamics and
 dual regularization, a characteristic found in advanced interior-
 point or augmented-Lagrangian solvers. Here, we introduce a
 parallel algorithm designed for solving an LQR problem with
 dual regularization. Leveraging a rewriting of the LQR recursion
 through block elimination, we first enhanced the efficiency of
 the serial algorithm, then subsequently generalized to handle
 parametric LQR. This extension enables us to split decision
 variables and to solve several subproblems concurrently. Our
 algorithm is implemented (and will be released in open source) in a
 nonlinear constrained implicit optimal control solver. It showcases
 improved performance over previous serial formulations and we
 validate its efficacy by deploying it in the model predictive control
 of a real quadruped robot.
2025-07-19 13:54:55,961 - paper_downloader - INFO - 开始下载: Differentiable Robust Model Predictive ControlAlex Oshin, Hassan Almubarak, Evangelos TheodorouPaper ID 3Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Deterministic model predictive control (MPC), while powerful, is often insufficient for effectively controlling autonomous systems in the real-world. Factors such as environmental noise and model error can cause deviations from the expected nominal performance. Robust MPC algorithms aim to bridge this gap between deterministic and uncertain control. However, these methods are often excessively difficult to tune for robustness due to the nonlinear and non-intuitive effects that controller parameters have on performance. To address this challenge, we first present a unifying perspective on differentiable optimization for control using the implicit function theorem (IFT), from which existing state-of-the art methods can be derived. Drawing parallels with differential dynamic programming, the IFT enables the derivation of an efficient differentiable optimal control framework. The derived scheme is subsequently paired with a tube-based MPC architecture to facilitate the automatic and real-time tuning of robust controllers in the presence of large uncertainties and disturbances. The proposed algorithm is benchmarked on multiple nonlinear robotic systems, including two systems in the MuJoCo simulator environment and one hardware experiment on the Robotarium testbed, to demonstrate its efficacy.
2025-07-19 13:54:58,019 - paper_downloader - INFO - 下载完成: Differentiable Robust Model Predictive ControlAlex Oshin, Hassan Almubarak, Evangelos TheodorouPaper.pdf
2025-07-19 13:54:58,019 - __main__ - INFO - 成功下载: Differentiable Robust Model Predictive ControlAlex Oshin, Hassan Almubarak, Evangelos TheodorouPaper ID 3Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Deterministic model predictive control (MPC), while powerful, is often insufficient for effectively controlling autonomous systems in the real-world. Factors such as environmental noise and model error can cause deviations from the expected nominal performance. Robust MPC algorithms aim to bridge this gap between deterministic and uncertain control. However, these methods are often excessively difficult to tune for robustness due to the nonlinear and non-intuitive effects that controller parameters have on performance. To address this challenge, we first present a unifying perspective on differentiable optimization for control using the implicit function theorem (IFT), from which existing state-of-the art methods can be derived. Drawing parallels with differential dynamic programming, the IFT enables the derivation of an efficient differentiable optimal control framework. The derived scheme is subsequently paired with a tube-based MPC architecture to facilitate the automatic and real-time tuning of robust controllers in the presence of large uncertainties and disturbances. The proposed algorithm is benchmarked on multiple nonlinear robotic systems, including two systems in the MuJoCo simulator environment and one hardware experiment on the Robotarium testbed, to demonstrate its efficacy.
2025-07-19 13:54:58,019 - paper_downloader - INFO - 开始下载: Computation-Aware Learning for Stable Control with Gaussian ProcessWenhan Cao, Alexandre Capone, Rishabh Yadav, Sandra Hirche, Wei PanPaper ID 4Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:In Gaussian Process (GP) dynamical model learning for robot control, particularly for systems constrained by computational resources like small quadrotors equipped with low-end processors, analyzing stability and designing a stable controller present significant challenges. This paper distinguishes between two types of uncertainty within the posteriors of GP dynamical models: the well-documented mathematical uncertainty stemming from limited data and computational uncertainty arising from constrained computational capabilities, which has been largely overlooked in prior research. Our work demonstrates that computational uncertainty, quantified through a probabilistic approximation of the inverse covariance matrix in GP dynamical models, is essential for stable control under computational constraints. We show that incorporating computational uncertainty can prevent overestimating the region of attraction, a safe subset of the state space with asymptotic stability, thus improving system safety. Building on these insights, we propose an innovative controller design methodology that integrates computational uncertainty within a second-order cone programming framework. Simulations of canonical stable control tasks and experiments of quadrotor tracking exhibit the effectiveness of our method under computational constraints.
2025-07-19 13:55:00,369 - paper_downloader - INFO - 下载完成: Computation-Aware Learning for Stable Control with Gaussian ProcessWenhan Cao, Alexandre Capone, Ris.pdf
2025-07-19 13:55:00,369 - __main__ - INFO - 成功下载: Computation-Aware Learning for Stable Control with Gaussian ProcessWenhan Cao, Alexandre Capone, Rishabh Yadav, Sandra Hirche, Wei PanPaper ID 4Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:In Gaussian Process (GP) dynamical model learning for robot control, particularly for systems constrained by computational resources like small quadrotors equipped with low-end processors, analyzing stability and designing a stable controller present significant challenges. This paper distinguishes between two types of uncertainty within the posteriors of GP dynamical models: the well-documented mathematical uncertainty stemming from limited data and computational uncertainty arising from constrained computational capabilities, which has been largely overlooked in prior research. Our work demonstrates that computational uncertainty, quantified through a probabilistic approximation of the inverse covariance matrix in GP dynamical models, is essential for stable control under computational constraints. We show that incorporating computational uncertainty can prevent overestimating the region of attraction, a safe subset of the state space with asymptotic stability, thus improving system safety. Building on these insights, we propose an innovative controller design methodology that integrates computational uncertainty within a second-order cone programming framework. Simulations of canonical stable control tasks and experiments of quadrotor tracking exhibit the effectiveness of our method under computational constraints.
2025-07-19 13:55:00,370 - paper_downloader - INFO - 开始下载: Decentralized Multi-Robot Line-of-Sight Connectivity Maintenance under UncertaintyYupeng Yang, Yiwei Lyu, Yanze Zhang, Sha Yi, Wenhao LuoPaper ID 5Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:In this paper, we propose a novel decentralized control method to maintain Line-of-Sight connectivity for multi-robot networks in the presence of Guassian-distributed localization uncertainty. In contrast to most existing work that assumes perfect positional information about robots or enforces overly restrictive rigid formation against uncertainty, our method enables robots to preserve Line-of-Sight connectivity with high probability under unbounded Gaussian-like positional noises while remaining minimally intrusive to the original robots’ tasks. This is achieved by a motion coordination framework that jointly optimizes the set of existing Line-of-Sight edges to preserve and control revisions to the nominal task-related controllers, subject to the safety constraints and the corresponding composition of uncertainty-aware Line-of-Sight control constraints. Such compositional control constraints, expressed by our novel notion of probabilistic Line-of-Sight connectivity barrier certificates (PrLOS-CBC) for pairwise robots using control barrier functions, explicitly characterize the deterministic admissible control space for the two robots. The resulting motion ensures Line-of-Sight connectedness for the robot team with high probability. Furthermore, we propose a fully decentralized algorithm that decomposes the motion coordination framework by interleaving the composite constraint specification and solving for the resulting optimization-based controllers. The optimality of our approach is justified by the theoretical proofs. Simulation and real-world experiments results are given to demonstrate the effectiveness of our method.
2025-07-19 13:55:03,966 - paper_downloader - INFO - 下载完成: Decentralized Multi-Robot Line-of-Sight Connectivity Maintenance under UncertaintyYupeng Yang, Yiwei.pdf
2025-07-19 13:55:03,966 - __main__ - INFO - 成功下载: Decentralized Multi-Robot Line-of-Sight Connectivity Maintenance under UncertaintyYupeng Yang, Yiwei Lyu, Yanze Zhang, Sha Yi, Wenhao LuoPaper ID 5Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:In this paper, we propose a novel decentralized control method to maintain Line-of-Sight connectivity for multi-robot networks in the presence of Guassian-distributed localization uncertainty. In contrast to most existing work that assumes perfect positional information about robots or enforces overly restrictive rigid formation against uncertainty, our method enables robots to preserve Line-of-Sight connectivity with high probability under unbounded Gaussian-like positional noises while remaining minimally intrusive to the original robots’ tasks. This is achieved by a motion coordination framework that jointly optimizes the set of existing Line-of-Sight edges to preserve and control revisions to the nominal task-related controllers, subject to the safety constraints and the corresponding composition of uncertainty-aware Line-of-Sight control constraints. Such compositional control constraints, expressed by our novel notion of probabilistic Line-of-Sight connectivity barrier certificates (PrLOS-CBC) for pairwise robots using control barrier functions, explicitly characterize the deterministic admissible control space for the two robots. The resulting motion ensures Line-of-Sight connectedness for the robot team with high probability. Furthermore, we propose a fully decentralized algorithm that decomposes the motion coordination framework by interleaving the composite constraint specification and solving for the resulting optimization-based controllers. The optimality of our approach is justified by the theoretical proofs. Simulation and real-world experiments results are given to demonstrate the effectiveness of our method.
2025-07-19 13:55:03,966 - paper_downloader - INFO - 开始下载: Hamilton-Jacobi Reachability Analysis for Hybrid Systems with Controlled and Forced TransitionsJavier Borquez, Shuang Peng, Yiyu Chen, Quan Nguyen, Somil BansalPaper ID 6Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Hybrid dynamical systems with nonlinear dynamics are one of the most general modeling tools for representing robotic systems, especially contact-rich systems. 
 However, providing guarantees regarding the safety or performance of nonlinear hybrid systems remains a challenging problem because it requires simultaneous reasoning about continuous state evolution and discrete mode switching. 
 In this work, we address this problem by extending classical Hamilton-Jacobi (HJ) reachability analysis, a formal verification method for continuous-time nonlinear dynamical systems, to hybrid dynamical systems. 
 We characterize the reachable sets for hybrid systems through a generalized value function defined over discrete and continuous states of the hybrid system.
 We also provide a numerical algorithm to compute this value function and obtain the reachable set.
 Our framework can compute reachable sets for hybrid systems consisting of multiple discrete modes, each with its own set of nonlinear continuous dynamics, discrete transitions that can be directly commanded or forced by a discrete control input, while still accounting for control bounds and adversarial disturbances in the state evolution. 
 Along with the reachable set, the proposed framework also provides an optimal continuous and discrete controller to ensure system safety.
 We demonstrate our framework in several simulation case studies, as well as on a real-world testbed to solve the optimal mode planning problem for a quadruped with multiple gaits.
2025-07-19 13:55:06,278 - paper_downloader - INFO - 下载完成: Hamilton-Jacobi Reachability Analysis for Hybrid Systems with Controlled and Forced TransitionsJavie.pdf
2025-07-19 13:55:06,278 - __main__ - INFO - 成功下载: Hamilton-Jacobi Reachability Analysis for Hybrid Systems with Controlled and Forced TransitionsJavier Borquez, Shuang Peng, Yiyu Chen, Quan Nguyen, Somil BansalPaper ID 6Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Hybrid dynamical systems with nonlinear dynamics are one of the most general modeling tools for representing robotic systems, especially contact-rich systems. 
 However, providing guarantees regarding the safety or performance of nonlinear hybrid systems remains a challenging problem because it requires simultaneous reasoning about continuous state evolution and discrete mode switching. 
 In this work, we address this problem by extending classical Hamilton-Jacobi (HJ) reachability analysis, a formal verification method for continuous-time nonlinear dynamical systems, to hybrid dynamical systems. 
 We characterize the reachable sets for hybrid systems through a generalized value function defined over discrete and continuous states of the hybrid system.
 We also provide a numerical algorithm to compute this value function and obtain the reachable set.
 Our framework can compute reachable sets for hybrid systems consisting of multiple discrete modes, each with its own set of nonlinear continuous dynamics, discrete transitions that can be directly commanded or forced by a discrete control input, while still accounting for control bounds and adversarial disturbances in the state evolution. 
 Along with the reachable set, the proposed framework also provides an optimal continuous and discrete controller to ensure system safety.
 We demonstrate our framework in several simulation case studies, as well as on a real-world testbed to solve the optimal mode planning problem for a quadruped with multiple gaits.
2025-07-19 13:55:06,278 - paper_downloader - INFO - 开始下载: JIGGLE: An Active Sensing Framework for Boundary Parameters Estimation in Deformable Surgical EnvironmentsNikhil Uday Shinde, Xiao Liang, Fei Liu, Yutong Zhang, Florian Richter, Sylvia Lee Herbert, Michael C. YipPaper ID 7Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Surgical automation can improve the accessibility and consistency of life-saving procedures. Most surgeries require separating layers of tissue to access the surgical site, and suturing to re-attach incisions. These tasks involve deformable manipula- tion to safely identify and alter tissue attachment (boundary) topology. Due to poor visual acuity and frequent occlusions, surgeons tend to carefully manipulate the tissue in ways that enable inference of the tissue’s attachment points without causing unsafe tearing. In a similar fashion, we propose JIGGLE, a framework for estimation and interactive sensing of unknown boundary parameters in deformable surgical environments. This framework has two key components: (1) a probabilistic estimation to identify the current attachment points, achieved by integrating a differentiable soft-body simulator with an extended Kalman filter (EKF), and (2) an optimization-based active control pipeline that generates actions to maximize information gain of the tissue attachments, while simultaneously minimizing safety costs. The robustness of our estimation approach is demonstrated through experiments with real animal tissue, where we infer sutured attachment points using stereo endoscope observations. We also demonstrate the capabilities of our method in handling complex topological changes such as cutting and suturing.
2025-07-19 13:55:25,655 - paper_downloader - INFO - 下载完成: JIGGLE An Active Sensing Framework for Boundary Parameters Estimation in Deformable Surgical Environ.pdf
2025-07-19 13:55:25,655 - __main__ - INFO - 成功下载: JIGGLE: An Active Sensing Framework for Boundary Parameters Estimation in Deformable Surgical EnvironmentsNikhil Uday Shinde, Xiao Liang, Fei Liu, Yutong Zhang, Florian Richter, Sylvia Lee Herbert, Michael C. YipPaper ID 7Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Surgical automation can improve the accessibility and consistency of life-saving procedures. Most surgeries require separating layers of tissue to access the surgical site, and suturing to re-attach incisions. These tasks involve deformable manipula- tion to safely identify and alter tissue attachment (boundary) topology. Due to poor visual acuity and frequent occlusions, surgeons tend to carefully manipulate the tissue in ways that enable inference of the tissue’s attachment points without causing unsafe tearing. In a similar fashion, we propose JIGGLE, a framework for estimation and interactive sensing of unknown boundary parameters in deformable surgical environments. This framework has two key components: (1) a probabilistic estimation to identify the current attachment points, achieved by integrating a differentiable soft-body simulator with an extended Kalman filter (EKF), and (2) an optimization-based active control pipeline that generates actions to maximize information gain of the tissue attachments, while simultaneously minimizing safety costs. The robustness of our estimation approach is demonstrated through experiments with real animal tissue, where we infer sutured attachment points using stereo endoscope observations. We also demonstrate the capabilities of our method in handling complex topological changes such as cutting and suturing.
2025-07-19 13:55:25,655 - paper_downloader - INFO - 开始下载: Conformalized Teleoperation: Confidently Mapping Human Inputs to High-Dimensional Robot ActionsMichelle D Zhao, Reid Simmons, Henny Admoni, Andrea BajcsyPaper ID 8Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with a low-dimensional input, like a joystick. To overcome this challenge, existing approaches use data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional robot actions. However, determining if such a black-box mapping can confidently infer a user’s intended high-dimensional action from low-dimensional inputs remains an open problem. Our key idea is to adapt the assistive map at training time to additionally estimate high-dimensional action quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods. Specifically, we leverage adaptive conformal prediction which adjusts the intervals over time, reducing the uncertainty bounds when the mapping is performant and increasing the bounds when the mapping consistently mis-predicts. Furthermore, we propose an uncertainty-interval-based mechanism for detecting high-uncertainty user inputs and robot states. We evaluate the efficacy of our proposed approach in a 2D assistive navigation task and two 7DOF Kinova Jaco tasks involving assistive cup grasping and goal reaching. Our findings demonstrate that conformalized assistive teleoperation manages to detect (but not differentiate between) high uncertainty induced by diverse preferences and induced by low-precision trajectories in the mapping’s training dataset. On the whole, we see this work as a key step towards enabling robots to quantify their own uncertainty and proactively seek intervention when needed.
2025-07-19 13:55:27,699 - paper_downloader - INFO - 下载完成: Conformalized Teleoperation Confidently Mapping Human Inputs to High-Dimensional Robot ActionsMichel.pdf
2025-07-19 13:55:27,699 - __main__ - INFO - 成功下载: Conformalized Teleoperation: Confidently Mapping Human Inputs to High-Dimensional Robot ActionsMichelle D Zhao, Reid Simmons, Henny Admoni, Andrea BajcsyPaper ID 8Session 1. ControlPoster Session day 1 (Tuesday, July 16)Abstract:Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with a low-dimensional input, like a joystick. To overcome this challenge, existing approaches use data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional robot actions. However, determining if such a black-box mapping can confidently infer a user’s intended high-dimensional action from low-dimensional inputs remains an open problem. Our key idea is to adapt the assistive map at training time to additionally estimate high-dimensional action quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods. Specifically, we leverage adaptive conformal prediction which adjusts the intervals over time, reducing the uncertainty bounds when the mapping is performant and increasing the bounds when the mapping consistently mis-predicts. Furthermore, we propose an uncertainty-interval-based mechanism for detecting high-uncertainty user inputs and robot states. We evaluate the efficacy of our proposed approach in a 2D assistive navigation task and two 7DOF Kinova Jaco tasks involving assistive cup grasping and goal reaching. Our findings demonstrate that conformalized assistive teleoperation manages to detect (but not differentiate between) high uncertainty induced by diverse preferences and induced by low-precision trajectories in the mapping’s training dataset. On the whole, we see this work as a key step towards enabling robots to quantify their own uncertainty and proactively seek intervention when needed.
2025-07-19 13:55:27,699 - paper_downloader - INFO - 开始下载: Optimal Non-Redundant Manipulator Surface Coverage with Rank-Deficient Manipulability ConstraintsTong Yang, Li Huang, Jaime Valls Miro, Yue Wang, Rong XiongPaper ID 9Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:A generalised solver for the manipulator non-revisiting coverage path planning (NCPP) problem is proposed in this paper. Nonlinear manipulator kinematics and the imposition of task-specific constraints dictate that applying conventional coverage path planning (CPP) solutions based on 2D template matching or cellular decomposition schemes on the target surface invariably results in truncated end-effector motions. Likewise, coverage paths designed directly in joint-space cannot ensure re-occurrences will not arise. More recent SOTA works have proposed finite-step optimal NCPP solutions where singularities are however expressly disregarded. Directly incorporating singular configurations violates the local bijectivity and finite-to-one property in the kinematic mapping, and cannot be properly modelled within existing schemes. This work leverages “valid’’ singularities, those that exhibit sufficient manoeuvrability in suitable dimensions to allow continuation of the tracking motion, thus further reducing the number of posture reconfigurations. The scheme assumes a generic representation of surfaces as discrete meshes, symbolising a null probability to locate the points corresponding to valid but singular inverse kinematic configurations, and constructs a practical method to traverse a singularity without explicitly calculating it. Simulated and realistic experiments are carried out where the suitability of the scheme to reduce posture reconfigurations and achieve continuous coverage motions are compared with existing methods. Three scenarios have been examined whereby the planner is able to fulfil motions without discontinuities in all instances.
2025-07-19 13:55:34,755 - paper_downloader - INFO - 下载完成: Optimal Non-Redundant Manipulator Surface Coverage with Rank-Deficient Manipulability ConstraintsTon.pdf
2025-07-19 13:55:34,755 - __main__ - INFO - 成功下载: Optimal Non-Redundant Manipulator Surface Coverage with Rank-Deficient Manipulability ConstraintsTong Yang, Li Huang, Jaime Valls Miro, Yue Wang, Rong XiongPaper ID 9Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:A generalised solver for the manipulator non-revisiting coverage path planning (NCPP) problem is proposed in this paper. Nonlinear manipulator kinematics and the imposition of task-specific constraints dictate that applying conventional coverage path planning (CPP) solutions based on 2D template matching or cellular decomposition schemes on the target surface invariably results in truncated end-effector motions. Likewise, coverage paths designed directly in joint-space cannot ensure re-occurrences will not arise. More recent SOTA works have proposed finite-step optimal NCPP solutions where singularities are however expressly disregarded. Directly incorporating singular configurations violates the local bijectivity and finite-to-one property in the kinematic mapping, and cannot be properly modelled within existing schemes. This work leverages “valid’’ singularities, those that exhibit sufficient manoeuvrability in suitable dimensions to allow continuation of the tracking motion, thus further reducing the number of posture reconfigurations. The scheme assumes a generic representation of surfaces as discrete meshes, symbolising a null probability to locate the points corresponding to valid but singular inverse kinematic configurations, and constructs a practical method to traverse a singularity without explicitly calculating it. Simulated and realistic experiments are carried out where the suitability of the scheme to reduce posture reconfigurations and achieve continuous coverage motions are compared with existing methods. Three scenarios have been examined whereby the planner is able to fulfil motions without discontinuities in all instances.
2025-07-19 13:55:34,755 - paper_downloader - INFO - 开始下载: AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic ManipulationKaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu LiPaper ID 10Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Predictive models are a crucial component of many robotic systems. Yet, constructing accurate predictive models for a variety of deformable objects, especially those with unknown physical properties, remains a significant challenge. This paper introduces AdaptiGraph, a learning-based dynamics modeling approach that enables robots to predict, adapt to, and control a wide array of challenging deformable materials with unknown physical properties. AdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND) framework, which represents material bits as particles and employs a graph neural network (GNN) to predict particle motion. Its key innovation is a unified physical property-conditioned GBND model capable of predicting the motions of diverse materials with varying physical properties without retraining. Upon encountering new materials during online deployment, AdaptiGraph utilizes a physical property optimization process for a few-shot adaptation of the model, enhancing its fit to the observed interaction data. The adapted models can precisely simulate the dynamics and predict the motion of various deformable materials, such as ropes, granular media, rigid boxes, and cloth, while adapting to different physical properties, including stiffness, granular size, and center of pressure. On prediction and manipulation tasks involving a diverse set of real-world deformable objects, our method exhibits superior prediction accuracy and task proficiency over non-material-conditioned and non-adaptive models.
2025-07-19 13:55:54,700 - paper_downloader - INFO - 下载完成: AdaptiGraph Material-Adaptive Graph-Based Neural Dynamics for Robotic ManipulationKaifeng Zhang, Bao.pdf
2025-07-19 13:55:54,701 - __main__ - INFO - 成功下载: AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic ManipulationKaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu LiPaper ID 10Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Predictive models are a crucial component of many robotic systems. Yet, constructing accurate predictive models for a variety of deformable objects, especially those with unknown physical properties, remains a significant challenge. This paper introduces AdaptiGraph, a learning-based dynamics modeling approach that enables robots to predict, adapt to, and control a wide array of challenging deformable materials with unknown physical properties. AdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND) framework, which represents material bits as particles and employs a graph neural network (GNN) to predict particle motion. Its key innovation is a unified physical property-conditioned GBND model capable of predicting the motions of diverse materials with varying physical properties without retraining. Upon encountering new materials during online deployment, AdaptiGraph utilizes a physical property optimization process for a few-shot adaptation of the model, enhancing its fit to the observed interaction data. The adapted models can precisely simulate the dynamics and predict the motion of various deformable materials, such as ropes, granular media, rigid boxes, and cloth, while adapting to different physical properties, including stiffness, granular size, and center of pressure. On prediction and manipulation tasks involving a diverse set of real-world deformable objects, our method exhibits superior prediction accuracy and task proficiency over non-material-conditioned and non-adaptive models.
2025-07-19 13:55:54,701 - paper_downloader - INFO - 开始下载: Human-oriented Representation Learning for Robotic ManipulationMingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei ZhanPaper ID 11Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what’s important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. More demos, datasets, models, and code can be found at «redacted for blind review».
2025-07-19 13:56:03,918 - paper_downloader - INFO - 下载完成: Human-oriented Representation Learning for Robotic ManipulationMingxiao Huo, Mingyu Ding, Chenfeng X.pdf
2025-07-19 13:56:03,918 - __main__ - INFO - 成功下载: Human-oriented Representation Learning for Robotic ManipulationMingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei ZhanPaper ID 11Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what’s important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. More demos, datasets, models, and code can be found at «redacted for blind review».
2025-07-19 13:56:03,918 - paper_downloader - INFO - 开始下载: Dynamic On-Palm Manipulation via Controlled SlidingWilliam Yang, Michael PosaPaper ID 12Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.
2025-07-19 13:56:16,427 - paper_downloader - INFO - 下载完成: Dynamic On-Palm Manipulation via Controlled SlidingWilliam Yang, Michael PosaPaper ID 12Session 3. M.pdf
2025-07-19 13:56:16,427 - __main__ - INFO - 成功下载: Dynamic On-Palm Manipulation via Controlled SlidingWilliam Yang, Michael PosaPaper ID 12Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.
2025-07-19 13:56:16,427 - paper_downloader - INFO - 开始下载: Efficient Data Collection for Robotic Manipulation via Compositional GeneralizationJensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa SadighPaper ID 13Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary many environmental factors of variation (e.g., object types, table textures) during data collection, to cover a diverse range of scenarios. However, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies can compose environmental factors from their data to succeed when encountering unseen factor combinations, we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot that compare data collection strategies and assess whether visual imitation learning policies can compose environmental factors. We find that policies do exhibit composition, although leveraging prior robotic datasets is critical for this on a real robot. We use these insights to propose better in-domain data collection strategies that exploit composition, which can induce better generalization than naive approaches for the same amount of effort during data collection. We further demonstrate that a real robot policy trained on data from such a strategy achieves a success rate of 77.5% when transferred to entirely new environments that encompass unseen combinations of environmental factors, whereas policies trained using data collected without accounting for environmental variation fail to transfer effectively, with a success rate of only 2.5%. We provide videos at our project website http://iliad.stanford.edu/robot-data-comp/.
2025-07-19 13:56:25,261 - paper_downloader - INFO - 下载完成: Efficient Data Collection for Robotic Manipulation via Compositional GeneralizationJensen Gao, Annie.pdf
2025-07-19 13:56:25,261 - __main__ - INFO - 成功下载: Efficient Data Collection for Robotic Manipulation via Compositional GeneralizationJensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa SadighPaper ID 13Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary many environmental factors of variation (e.g., object types, table textures) during data collection, to cover a diverse range of scenarios. However, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies can compose environmental factors from their data to succeed when encountering unseen factor combinations, we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot that compare data collection strategies and assess whether visual imitation learning policies can compose environmental factors. We find that policies do exhibit composition, although leveraging prior robotic datasets is critical for this on a real robot. We use these insights to propose better in-domain data collection strategies that exploit composition, which can induce better generalization than naive approaches for the same amount of effort during data collection. We further demonstrate that a real robot policy trained on data from such a strategy achieves a success rate of 77.5% when transferred to entirely new environments that encompass unseen combinations of environmental factors, whereas policies trained using data collected without accounting for environmental variation fail to transfer effectively, with a success rate of only 2.5%. We provide videos at our project website http://iliad.stanford.edu/robot-data-comp/.
2025-07-19 13:56:25,261 - paper_downloader - INFO - 开始下载: Demonstrating Learning from Humans on Open-Source Dexterous Robot HandsKenneth Shaw, Ananye Agarwal, Shikhar Bahl, Mohan Kumar Srirama, Alexandre Kirchmeyer, Aditya Kannan, Aravind Sivakumar, Deepak PathakPaper ID 14Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Emulating human-like dexterity with robotic hands has been a long-standing challenge in robotics. In recent years, machine learning has demanded robot hands to be reliable, inexpensive and easy-to-reproduce. For the past few years we have been investigating how to address these demands. We will demonstrate our three robot hands that address this problem ranging from rigid easy-to-simulate hand to soft but strong dexterous robot hands performing three different machine learning tasks. Our first machine learning task will be teleoperation, where we will develop a new mobile arm and hand motion capture system that we will bring to RSS 2024. Second, we will demonstrate how to use human-video and human motion to teach robot hands. Finally, we will show how to continually improve these policies using reinforcement learning in both simulation and the real-world. This demo will be engaging, will serve to demystify dexterous manipulation and inspire researchers to bring robot hands into their own projects. Please see our website at https://leaphand.com/rss2024demo for more interactive information.
2025-07-19 13:56:28,390 - paper_downloader - INFO - 下载完成: Demonstrating Learning from Humans on Open-Source Dexterous Robot HandsKenneth Shaw, Ananye Agarwal,.pdf
2025-07-19 13:56:28,390 - __main__ - INFO - 成功下载: Demonstrating Learning from Humans on Open-Source Dexterous Robot HandsKenneth Shaw, Ananye Agarwal, Shikhar Bahl, Mohan Kumar Srirama, Alexandre Kirchmeyer, Aditya Kannan, Aravind Sivakumar, Deepak PathakPaper ID 14Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Emulating human-like dexterity with robotic hands has been a long-standing challenge in robotics. In recent years, machine learning has demanded robot hands to be reliable, inexpensive and easy-to-reproduce. For the past few years we have been investigating how to address these demands. We will demonstrate our three robot hands that address this problem ranging from rigid easy-to-simulate hand to soft but strong dexterous robot hands performing three different machine learning tasks. Our first machine learning task will be teleoperation, where we will develop a new mobile arm and hand motion capture system that we will bring to RSS 2024. Second, we will demonstrate how to use human-video and human motion to teach robot hands. Finally, we will show how to continually improve these policies using reinforcement learning in both simulation and the real-world. This demo will be engaging, will serve to demystify dexterous manipulation and inspire researchers to bring robot hands into their own projects. Please see our website at https://leaphand.com/rss2024demo for more interactive information.
2025-07-19 13:56:28,390 - paper_downloader - INFO - 开始下载: Reconciling Reality through Simulation: A Real-To-Sim-to-Real Approach for Robust ManipulationMarcel Torne Villasevil, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit AgrawalPaper ID 15Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a new system for robustifying real-world imitation learning policies via reinforcement learning in digital twin simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel inverse distillation procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf and four other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.
2025-07-19 13:56:32,803 - paper_downloader - INFO - 下载完成: Reconciling Reality through Simulation A Real-To-Sim-to-Real Approach for Robust ManipulationMarcel .pdf
2025-07-19 13:56:32,803 - __main__ - INFO - 成功下载: Reconciling Reality through Simulation: A Real-To-Sim-to-Real Approach for Robust ManipulationMarcel Torne Villasevil, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit AgrawalPaper ID 15Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a new system for robustifying real-world imitation learning policies via reinforcement learning in digital twin simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel inverse distillation procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf and four other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection.
2025-07-19 13:56:32,803 - paper_downloader - INFO - 开始下载: SAGE: Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under Language InstructionsHaoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas GuibasPaper ID 16Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:To interact with daily-life articulated objects of diverse structures and functionalities, understanding the object parts plays a central role in both user instruction comprehension and task execution.
 However, the possible discordance between the semantic meaning and physics functionalities of the parts poses a challenge for designing a general system.
 To address this problem, we propose SAGE, a novel framework that bridges semantic and actionable parts of articulated objects to achieve generalizable manipulation under natural language instructions.
 More concretely, given an articulated object, we first observe all the semantic parts on it, conditioned on which an instruction interpreter proposes possible action programs that concretize the natural language instruction. Then, a part-grounding module maps the semantic parts into so-called Generalizable Actionable Parts (GAParts), which inherently carry information about part motion. End-effector trajectories are predicted on the GAParts, which, together with the action program, form an executable policy. Additionally, an interactive feedback module is incorporated to respond to failures, which closes the loop and increases the robustness of the overall framework.
 Key to the success of our framework is the joint proposal and knowledge fusion between a large vision-language model (VLM) and a small domain-specific model for both context comprehension and part perception, with the former providing general intuitions and the latter serving as expert facts.
 Both simulation and real-robot experiments show our effectiveness in handling a large variety of articulated objects with diverse language-instructed goals.
2025-07-19 13:56:35,147 - paper_downloader - INFO - 下载完成: SAGE Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under .pdf
2025-07-19 13:56:35,148 - __main__ - INFO - 成功下载: SAGE: Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under Language InstructionsHaoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas GuibasPaper ID 16Session 3. ManipulationPoster Session day 1 (Tuesday, July 16)Abstract:To interact with daily-life articulated objects of diverse structures and functionalities, understanding the object parts plays a central role in both user instruction comprehension and task execution.
 However, the possible discordance between the semantic meaning and physics functionalities of the parts poses a challenge for designing a general system.
 To address this problem, we propose SAGE, a novel framework that bridges semantic and actionable parts of articulated objects to achieve generalizable manipulation under natural language instructions.
 More concretely, given an articulated object, we first observe all the semantic parts on it, conditioned on which an instruction interpreter proposes possible action programs that concretize the natural language instruction. Then, a part-grounding module maps the semantic parts into so-called Generalizable Actionable Parts (GAParts), which inherently carry information about part motion. End-effector trajectories are predicted on the GAParts, which, together with the action program, form an executable policy. Additionally, an interactive feedback module is incorporated to respond to failures, which closes the loop and increases the robustness of the overall framework.
 Key to the success of our framework is the joint proposal and knowledge fusion between a large vision-language model (VLM) and a small domain-specific model for both context comprehension and part perception, with the former providing general intuitions and the latter serving as expert facts.
 Both simulation and real-robot experiments show our effectiveness in handling a large variety of articulated objects with diverse language-instructed goals.
2025-07-19 13:56:35,148 - paper_downloader - INFO - 开始下载: Demonstrating Event-Triggered Investigation and Sample Collection for Human Scientists using Field Robots and Large Foundation ModelsTirthankar Bandyopadhyay, Fletcher Talbot, Callum Bennie, Hashini Senaratne, Xun Li, Brendan Tidd, Mingze Xi, Jan Stiefel, Volkan Dedeoglu, Rod Taylor, Tea Molnar, Ziwei Wang, Josh Pinskier, Feng Xu, Lois Liow, Ben Burgess-Limerick, Jesse Haviland, Pavan Sikka, Simon Murrell, Jane Hodgkinson, Jiajun Liu, Fred Pauling, Stanislav FuniakPaper ID 17Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:In this paper, we introduce a pioneering end-to-end system demonstrated on a team of robots and sensors, designed to augment scientific exploration and discovery for human scientists in remote or inaccessible environments.
 We demonstrate and analyse our system’s capability in a mock-up test-bed scenario. In this futuristic hypothetical scenario human scientists located in a controlled lunar habitat, are assisted by a team of robots in investigating an unknown seismic phenomena like moon-quakes or meteor impact detected by a sensor network deployed on the lunar surface. They do so by autonomously collecting data, providing contextual semantic information and collecting scientific sample for future analysis upon the direction of humans.
 This work is among the earliest to present a feasible way to integration large foundational models (LFMs) into field robotic deployment, enabling easy semantic and contextual understanding of the objects in the environment and natural language-based interactions with the robot for the scientist. In addition we bring together state-of-the-art techniques in mapping, object detection, navigation, mobile manipulation, soft grippers, event detection and present details of the integration, insights and lessons learnt from the deployment.
2025-07-19 13:56:45,630 - paper_downloader - INFO - 下载完成: Demonstrating Event-Triggered Investigation and Sample Collection for Human Scientists using Field R.pdf
2025-07-19 13:56:45,630 - __main__ - INFO - 成功下载: Demonstrating Event-Triggered Investigation and Sample Collection for Human Scientists using Field Robots and Large Foundation ModelsTirthankar Bandyopadhyay, Fletcher Talbot, Callum Bennie, Hashini Senaratne, Xun Li, Brendan Tidd, Mingze Xi, Jan Stiefel, Volkan Dedeoglu, Rod Taylor, Tea Molnar, Ziwei Wang, Josh Pinskier, Feng Xu, Lois Liow, Ben Burgess-Limerick, Jesse Haviland, Pavan Sikka, Simon Murrell, Jane Hodgkinson, Jiajun Liu, Fred Pauling, Stanislav FuniakPaper ID 17Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:In this paper, we introduce a pioneering end-to-end system demonstrated on a team of robots and sensors, designed to augment scientific exploration and discovery for human scientists in remote or inaccessible environments.
 We demonstrate and analyse our system’s capability in a mock-up test-bed scenario. In this futuristic hypothetical scenario human scientists located in a controlled lunar habitat, are assisted by a team of robots in investigating an unknown seismic phenomena like moon-quakes or meteor impact detected by a sensor network deployed on the lunar surface. They do so by autonomously collecting data, providing contextual semantic information and collecting scientific sample for future analysis upon the direction of humans.
 This work is among the earliest to present a feasible way to integration large foundational models (LFMs) into field robotic deployment, enabling easy semantic and contextual understanding of the objects in the environment and natural language-based interactions with the robot for the scientist. In addition we bring together state-of-the-art techniques in mapping, object detection, navigation, mobile manipulation, soft grippers, event detection and present details of the integration, insights and lessons learnt from the deployment.
2025-07-19 13:56:45,631 - paper_downloader - INFO - 开始下载: CraterGrader: Autonomous Robotic Terrain Manipulation for Lunar Site Preparation and EarthmovingRyan Lee, Benjamin Younes, Alexander Pletta, John Harrington, Russell Q. Wong, William WhittakerPaper ID 18Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Establishing lunar infrastructure is paramount to long-term habitation on the Moon. To meet the demand for future lunar infrastructure development, we present CraterGrader, a novel system for autonomous robotic earthmoving tasks within lunar constraints. In contrast to the current approaches to construction autonomy, CraterGrader uses online perception for dynamic mapping of deformable terrain, devises an energy-efficient material movement plan using an optimization-based transport planner, precisely localizes without GPS, and uses integrated drive and tool control to manipulate regolith with unknown and non-constant geotechnical parameters. We demonstrate CraterGrader’s ability to achieve unprecedented performance in autonomous smoothing and grading within a lunar-like environment, showing that this framework is capable, robust, and a benchmark for future planetary site preparation robotics.
2025-07-19 13:56:48,974 - paper_downloader - INFO - 下载完成: CraterGrader Autonomous Robotic Terrain Manipulation for Lunar Site Preparation and EarthmovingRyan .pdf
2025-07-19 13:56:48,974 - __main__ - INFO - 成功下载: CraterGrader: Autonomous Robotic Terrain Manipulation for Lunar Site Preparation and EarthmovingRyan Lee, Benjamin Younes, Alexander Pletta, John Harrington, Russell Q. Wong, William WhittakerPaper ID 18Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Establishing lunar infrastructure is paramount to long-term habitation on the Moon. To meet the demand for future lunar infrastructure development, we present CraterGrader, a novel system for autonomous robotic earthmoving tasks within lunar constraints. In contrast to the current approaches to construction autonomy, CraterGrader uses online perception for dynamic mapping of deformable terrain, devises an energy-efficient material movement plan using an optimization-based transport planner, precisely localizes without GPS, and uses integrated drive and tool control to manipulate regolith with unknown and non-constant geotechnical parameters. We demonstrate CraterGrader’s ability to achieve unprecedented performance in autonomous smoothing and grading within a lunar-like environment, showing that this framework is capable, robust, and a benchmark for future planetary site preparation robotics.
2025-07-19 13:56:48,974 - paper_downloader - INFO - 开始下载: POAM: Probabilistic Online Attentive Mapping for Efficient Robotic Information GatheringWeizhe Chen, Lantao Liu, Roni KhardonPaper ID 19Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Gaussian Process (GP) models are widely used for Robotic Information Gathering (RIG) in exploring unknown environments due to their ability to model complex phenomena with non-parametric flexibility and accurately quantify prediction uncertainty. Previous work has developed informative planners and adaptive GP models to enhance the data efficiency of RIG by improving the robot’s sampling strategy to focus on informative regions in non-stationary environments. However, computational efficiency becomes a bottleneck when using GP models in large-scale environments with limited computational resources. We propose a framework – Probabilistic Online Attentive Mapping (POAM) – that leverages the modeling strengths of the non-stationary Attentive Kernel while achieving constant-time computational complexity for online decision-making. POAM guides the optimization process via variational Expectation Maximization, providing constant-time update rules for inducing inputs, variational parameters, and hyperparameters. Extensive experiments in active bathymetric mapping tasks demonstrate that POAM significantly improves computational efficiency, model accuracy, and uncertainty quantification capability compared to existing online sparse GP models.
2025-07-19 13:56:51,265 - paper_downloader - INFO - 下载完成: POAM Probabilistic Online Attentive Mapping for Efficient Robotic Information GatheringWeizhe Chen, .pdf
2025-07-19 13:56:51,266 - __main__ - INFO - 成功下载: POAM: Probabilistic Online Attentive Mapping for Efficient Robotic Information GatheringWeizhe Chen, Lantao Liu, Roni KhardonPaper ID 19Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Gaussian Process (GP) models are widely used for Robotic Information Gathering (RIG) in exploring unknown environments due to their ability to model complex phenomena with non-parametric flexibility and accurately quantify prediction uncertainty. Previous work has developed informative planners and adaptive GP models to enhance the data efficiency of RIG by improving the robot’s sampling strategy to focus on informative regions in non-stationary environments. However, computational efficiency becomes a bottleneck when using GP models in large-scale environments with limited computational resources. We propose a framework – Probabilistic Online Attentive Mapping (POAM) – that leverages the modeling strengths of the non-stationary Attentive Kernel while achieving constant-time computational complexity for online decision-making. POAM guides the optimization process via variational Expectation Maximization, providing constant-time update rules for inducing inputs, variational parameters, and hyperparameters. Extensive experiments in active bathymetric mapping tasks demonstrate that POAM significantly improves computational efficiency, model accuracy, and uncertainty quantification capability compared to existing online sparse GP models.
2025-07-19 13:56:51,266 - paper_downloader - INFO - 开始下载: Blending Data-Driven Priors in Dynamic GamesJustin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Albert Gimo Contreras, Vikash Modi, Jonathan DeCastro, Deepak Gopinath, Guy Rosman, Naomi Leonard, Maria Santos, Jaime Fernández FisacPaper ID 20Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors. We propose an efficient algorithm for computing multimodal approximate feedback Nash equilibrium strategies of KLGame in real time. Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines.
2025-07-19 13:56:54,860 - paper_downloader - INFO - 下载完成: Blending Data-Driven Priors in Dynamic GamesJustin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Alb.pdf
2025-07-19 13:56:54,860 - __main__ - INFO - 成功下载: Blending Data-Driven Priors in Dynamic GamesJustin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Albert Gimo Contreras, Vikash Modi, Jonathan DeCastro, Deepak Gopinath, Guy Rosman, Naomi Leonard, Maria Santos, Jaime Fernández FisacPaper ID 20Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors. We propose an efficient algorithm for computing multimodal approximate feedback Nash equilibrium strategies of KLGame in real time. Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines.
2025-07-19 13:56:54,860 - paper_downloader - INFO - 开始下载: Demonstrating HOUND: A Low-cost Research Platform for High-speed Off-road Underactuated Nonholonomic DrivingSidharth Talia, Matthew Schmittle, Alexander Lambert, Alexander Spitzer, Christoforos Mavrogiannis, Siddhartha SrinivasaPaper ID 21Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Off-road autonomy, crucial for applications such as search-and-rescue, agriculture, and planetary exploration, poses unique problems due to challenging terrains, as well as due to the risk involved in testing or deploying such systems. 
 Accessible platforms have the potential to widen the field to a broader set of researchers and students.
 Existing efforts in making on-road autonomy more accessible have seen success, yet aggressive off-road autonomy remains underserved.
 We seek to fill this gap by introducing HOUND, a 1/10th-scale, inexpensive, off-road autonomous car platform that can handle challenging outdoor terrains at high speeds.
 To aid development speed, we integrate HOUND with BeamNG, a state-of-the-art driving simulator to enable both software in the loop as well as hardware in the loop testing.
 To reduce the extent of ruggedization required, and thus cost, we integrate a rollover prevention system as a safety feature into the platform.
 Real-world trials over 50 kilometers demonstrate the platform’s longevity and effectiveness over varied terrains and speeds.
2025-07-19 13:56:55,666 - paper_downloader - INFO - 下载完成: Demonstrating HOUND A Low-cost Research Platform for High-speed Off-road Underactuated Nonholonomic .pdf
2025-07-19 13:56:55,666 - __main__ - INFO - 成功下载: Demonstrating HOUND: A Low-cost Research Platform for High-speed Off-road Underactuated Nonholonomic DrivingSidharth Talia, Matthew Schmittle, Alexander Lambert, Alexander Spitzer, Christoforos Mavrogiannis, Siddhartha SrinivasaPaper ID 21Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Off-road autonomy, crucial for applications such as search-and-rescue, agriculture, and planetary exploration, poses unique problems due to challenging terrains, as well as due to the risk involved in testing or deploying such systems. 
 Accessible platforms have the potential to widen the field to a broader set of researchers and students.
 Existing efforts in making on-road autonomy more accessible have seen success, yet aggressive off-road autonomy remains underserved.
 We seek to fill this gap by introducing HOUND, a 1/10th-scale, inexpensive, off-road autonomous car platform that can handle challenging outdoor terrains at high speeds.
 To aid development speed, we integrate HOUND with BeamNG, a state-of-the-art driving simulator to enable both software in the loop as well as hardware in the loop testing.
 To reduce the extent of ruggedization required, and thus cost, we integrate a rollover prevention system as a safety feature into the platform.
 Real-world trials over 50 kilometers demonstrate the platform’s longevity and effectiveness over varied terrains and speeds.
2025-07-19 13:56:55,667 - paper_downloader - INFO - 开始下载: Model Predictive Control for Aggressive Driving Over Uneven TerrainTyler Han, Alex Liu, Anqi Li, Alexander Spitzer, Guanya Shi, Byron BootsPaper ID 22Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Terrain traversability in unstructured off-road autonomy has traditionally relied on semantic classification, resource-intensive dynamics models, or purely geometry-based methods to predict vehicle-terrain interactions.
 While inconsequential at low speeds, uneven terrain subjects our full-scale system to safety-critical challenges at operating speeds of 7-10 m/s.
 This study focuses particularly on uneven terrain such as hills, banks, and ditches. These common high-risk geometries are capable of disabling the vehicle and causing severe passenger injuries if poorly traversed.
 We introduce a physics-based framework for identifying traversability constraints on terrain dynamics. Using this framework, we derive two fundamental constraints, each with a focus on mitigating rollover and ditch-crossing failures while being fully parallelizable in the sample-based Model Predictive Control (MPC) framework. In addition, we present the design of our planning and control system, which implements our parallelized constraints in MPC and utilizes a low-level controller to meet the demands of our aggressive driving without prior information about the environment and its dynamics.
 Through real-world experimentation and traversal of hills and ditches,
 we demonstrate that our approach captures fundamental elements of safe and aggressive autonomy over uneven terrain.
 Our approach improves upon geometry-based methods by completing comprehensive off-road courses up to 22\% faster while maintaining safe operation.
2025-07-19 13:56:57,723 - paper_downloader - INFO - 下载完成: Model Predictive Control for Aggressive Driving Over Uneven TerrainTyler Han, Alex Liu, Anqi Li, Ale.pdf
2025-07-19 13:56:57,723 - __main__ - INFO - 成功下载: Model Predictive Control for Aggressive Driving Over Uneven TerrainTyler Han, Alex Liu, Anqi Li, Alexander Spitzer, Guanya Shi, Byron BootsPaper ID 22Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:Terrain traversability in unstructured off-road autonomy has traditionally relied on semantic classification, resource-intensive dynamics models, or purely geometry-based methods to predict vehicle-terrain interactions.
 While inconsequential at low speeds, uneven terrain subjects our full-scale system to safety-critical challenges at operating speeds of 7-10 m/s.
 This study focuses particularly on uneven terrain such as hills, banks, and ditches. These common high-risk geometries are capable of disabling the vehicle and causing severe passenger injuries if poorly traversed.
 We introduce a physics-based framework for identifying traversability constraints on terrain dynamics. Using this framework, we derive two fundamental constraints, each with a focus on mitigating rollover and ditch-crossing failures while being fully parallelizable in the sample-based Model Predictive Control (MPC) framework. In addition, we present the design of our planning and control system, which implements our parallelized constraints in MPC and utilizes a low-level controller to meet the demands of our aggressive driving without prior information about the environment and its dynamics.
 Through real-world experimentation and traversal of hills and ditches,
 we demonstrate that our approach captures fundamental elements of safe and aggressive autonomy over uneven terrain.
 Our approach improves upon geometry-based methods by completing comprehensive off-road courses up to 22\% faster while maintaining safe operation.
2025-07-19 13:56:57,724 - paper_downloader - INFO - 开始下载: Demonstrating CropFollow++: Robust Under-Canopy Navigation with KeypointsArun Narenthiran Sivakumar, Mateus Valverde Gasparino, Michael McGuire, Vitor Akihiro Hisano Higuti, M. Ugur Akcal, Girish ChowdharyPaper ID 23Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:We present an empirically robust vision-based navigation system for under-canopy agricultural robots using semantic keypoints. Autonomous under-canopy navigation is challenging due to the tight spacing between the crop rows (∼ 0.75 m), degradation in RTK-GPS accuracy due to multipath error, and noise in LiDAR measurements from the excessive clutter. Earlier work called CropFollow addressed these challenges by proposing a learning-based visual navigation system with end-to-end perception. However, this approach has the following limitations:
 Lack of interpretable representation, and Sensitivity to outlier predictions during occlusion due to lack of a confidence measure. Our system, CropFollow++, introduces modular perception architecture with a learned semantic keypoint representation. This learned representation is more modular, and more interpretable than CropFollow, and provides a confidence measure to detect occlusions. CropFollow++ significantly outperformed CropFollow in terms of the number of collisions needed (13 vs. 33) in field tests spanning ∼ 1.9km each in challenging late-season fields with significant
 occlusions. We also deployed CropFollow++ in multiple under-canopy cover crop planting robots on a large scale (25 km in total) in various field conditions and we discuss the key lessons learned from this.
2025-07-19 13:57:07,484 - paper_downloader - INFO - 下载完成: Demonstrating CropFollow++ Robust Under-Canopy Navigation with KeypointsArun Narenthiran Sivakumar, .pdf
2025-07-19 13:57:07,484 - __main__ - INFO - 成功下载: Demonstrating CropFollow++: Robust Under-Canopy Navigation with KeypointsArun Narenthiran Sivakumar, Mateus Valverde Gasparino, Michael McGuire, Vitor Akihiro Hisano Higuti, M. Ugur Akcal, Girish ChowdharyPaper ID 23Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:We present an empirically robust vision-based navigation system for under-canopy agricultural robots using semantic keypoints. Autonomous under-canopy navigation is challenging due to the tight spacing between the crop rows (∼ 0.75 m), degradation in RTK-GPS accuracy due to multipath error, and noise in LiDAR measurements from the excessive clutter. Earlier work called CropFollow addressed these challenges by proposing a learning-based visual navigation system with end-to-end perception. However, this approach has the following limitations:
 Lack of interpretable representation, and Sensitivity to outlier predictions during occlusion due to lack of a confidence measure. Our system, CropFollow++, introduces modular perception architecture with a learned semantic keypoint representation. This learned representation is more modular, and more interpretable than CropFollow, and provides a confidence measure to detect occlusions. CropFollow++ significantly outperformed CropFollow in terms of the number of collisions needed (13 vs. 33) in field tests spanning ∼ 1.9km each in challenging late-season fields with significant
 occlusions. We also deployed CropFollow++ in multiple under-canopy cover crop planting robots on a large scale (25 km in total) in various field conditions and we discuss the key lessons learned from this.
2025-07-19 13:57:07,484 - paper_downloader - INFO - 开始下载: SEEK: Semantic Reasoning for Object Goal Navigation in Real World Inspection TasksMuhammad Fadhil Ginting, Sung-Kyun Kim, David Fan, Matteo Palieri, Mykel Kochenderfer, Ali-akbar Agha-mohammadiPaper ID 24Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:This paper addresses the problem of object-goal navigation in autonomous inspections in real-world environments. Object-goal navigation is crucial to enable effective inspections in various settings, often requiring the robot to identify the target object within a large search space. Current object inspection methods fall short of human efficiency because they typically cannot bootstrap prior and common sense knowledge as humans do. In this paper, we introduce a framework that enables robots to use semantic knowledge from prior spatial configurations of the environment and semantic common sense knowledge. We propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines semantic prior knowledge with the robot’s observations to search for and navigate toward target objects more efficiently. SEEK maintains two representations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network (RSN). The RSN is a compact and practical model that estimates the probability of finding the target object across spatial elements in the DSG. We propose a novel probabilistic planning framework to search for the object using relational semantic knowledge. Our simulation analyses demonstrate that SEEK outperforms the classical planning and Large Language Models (LLMs)-based methods that are examined in this study in terms of efficiency for object-goal inspection tasks. We validated our approach on a physical legged robot in urban environments, showcasing its practicality and effectiveness in real-world inspection scenarios.
2025-07-19 13:57:08,016 - paper_downloader - INFO - 下载完成: SEEK Semantic Reasoning for Object Goal Navigation in Real World Inspection TasksMuhammad Fadhil Gin.pdf
2025-07-19 13:57:08,017 - __main__ - INFO - 成功下载: SEEK: Semantic Reasoning for Object Goal Navigation in Real World Inspection TasksMuhammad Fadhil Ginting, Sung-Kyun Kim, David Fan, Matteo Palieri, Mykel Kochenderfer, Ali-akbar Agha-mohammadiPaper ID 24Session 4. Field roboticsPoster Session day 1 (Tuesday, July 16)Abstract:This paper addresses the problem of object-goal navigation in autonomous inspections in real-world environments. Object-goal navigation is crucial to enable effective inspections in various settings, often requiring the robot to identify the target object within a large search space. Current object inspection methods fall short of human efficiency because they typically cannot bootstrap prior and common sense knowledge as humans do. In this paper, we introduce a framework that enables robots to use semantic knowledge from prior spatial configurations of the environment and semantic common sense knowledge. We propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines semantic prior knowledge with the robot’s observations to search for and navigate toward target objects more efficiently. SEEK maintains two representations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network (RSN). The RSN is a compact and practical model that estimates the probability of finding the target object across spatial elements in the DSG. We propose a novel probabilistic planning framework to search for the object using relational semantic knowledge. Our simulation analyses demonstrate that SEEK outperforms the classical planning and Large Language Models (LLMs)-based methods that are examined in this study in terms of efficiency for object-goal inspection tasks. We validated our approach on a physical legged robot in urban environments, showcasing its practicality and effectiveness in real-world inspection scenarios.
2025-07-19 13:57:08,017 - paper_downloader - INFO - 开始下载: Task Adaptation in Industrial Human-Robot Interaction: Leveraging Riemannian Motion PoliciesMike Allenspach, Michael Pantic, Rik Girod, Lionel Ott, Roland SiegwartPaper ID 26Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:In real-world industrial environments, modern
 robots often rely on human operators for crucial decision-
 making and mission synthesis from individual tasks. Effective and
 safe collaboration between humans and robots requires systems
 that can adjust their motion to human intentions, enabling
 dynamic task planning and adaptation. Addressing the needs of
 industrial applications, we propose a motion control framework
 that (i) removes the need for manual control of the robot’s
 movement; (ii) facilitates the formulation and combination of
 complex tasks; and (iii) allows the seamless integration of human
 intent recognition and robot motion planning. For this purpose,
 we leverage a modular and purely reactive approach for task 
 parametrization and motion generation, embodied by Riemannian Motion Policies.The effectiveness of our method is demonstrated, evaluated and compared to a representative state-of-the-art approach in
 experimental scenarios, inspired by realistic industrial Human-
 Robot Interaction settings.
2025-07-19 13:57:10,594 - paper_downloader - INFO - 下载完成: Task Adaptation in Industrial Human-Robot Interaction Leveraging Riemannian Motion PoliciesMike Alle.pdf
2025-07-19 13:57:10,594 - __main__ - INFO - 成功下载: Task Adaptation in Industrial Human-Robot Interaction: Leveraging Riemannian Motion PoliciesMike Allenspach, Michael Pantic, Rik Girod, Lionel Ott, Roland SiegwartPaper ID 26Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:In real-world industrial environments, modern
 robots often rely on human operators for crucial decision-
 making and mission synthesis from individual tasks. Effective and
 safe collaboration between humans and robots requires systems
 that can adjust their motion to human intentions, enabling
 dynamic task planning and adaptation. Addressing the needs of
 industrial applications, we propose a motion control framework
 that (i) removes the need for manual control of the robot’s
 movement; (ii) facilitates the formulation and combination of
 complex tasks; and (iii) allows the seamless integration of human
 intent recognition and robot motion planning. For this purpose,
 we leverage a modular and purely reactive approach for task 
 parametrization and motion generation, embodied by Riemannian Motion Policies.The effectiveness of our method is demonstrated, evaluated and compared to a representative state-of-the-art approach in
 experimental scenarios, inspired by realistic industrial Human-
 Robot Interaction settings.
2025-07-19 13:57:10,595 - paper_downloader - INFO - 开始下载: Risk-Calibrated Human-Robot Interaction via Set-Valued Intent PredictionJustin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha MajumdarPaper ID 27Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human’s intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP’s ability to predict and adapt to a diverse set of dynamic human intents.
2025-07-19 13:57:13,691 - paper_downloader - INFO - 下载完成: Risk-Calibrated Human-Robot Interaction via Set-Valued Intent PredictionJustin Lidard, Hang Pham, Ar.pdf
2025-07-19 13:57:13,691 - __main__ - INFO - 成功下载: Risk-Calibrated Human-Robot Interaction via Set-Valued Intent PredictionJustin Lidard, Hang Pham, Ariel Bachman, Bryan Boateng, Anirudha MajumdarPaper ID 27Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Tasks where robots must anticipate human intent, such as navigating around a cluttered home or sorting everyday items, are challenging because they exhibit a wide range of valid actions that lead to similar outcomes. Moreover, zero-shot cooperation between human-robot partners is an especially challenging problem because it requires the robot to infer and adapt on the fly to a latent human intent, which could vary significantly from human to human. Recently, deep learned motion prediction models have shown promising results in predicting human intent but are prone to being confidently incorrect. In this work, we present Risk-Calibrated Interactive Planning (RCIP), which is a framework for measuring and calibrating risk associated with uncertain action selection in human-robot cooperation, with the fundamental idea that the robot should ask for human clarification when the risk associated with the uncertainty in the human’s intent cannot be controlled. RCIP builds on the theory of set-valued risk calibration to provide a finite-sample statistical guarantee on the cumulative loss incurred by the robot while minimizing the cost of human clarification in complex multi-step settings. Our main insight is to frame the risk control problem as a sequence-level multi-hypothesis testing problem, allowing efficient calibration using a low-dimensional parameter that controls a pre-trained risk-aware policy. Experiments across a variety of simulated and real-world environments demonstrate RCIP’s ability to predict and adapt to a diverse set of dynamic human intents.
2025-07-19 13:57:13,691 - paper_downloader - INFO - 开始下载: Constraint-Aware Intent Estimation for Dynamic Human-Robot Object Co-ManipulationYifei Simon Shao, Tianyu Li, Shafagh Keyvanian, Pratik Chaudhari, Vijay Kumar, Nadia FigueroaPaper ID 28Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Constraint-aware estimation of human intent
 is essential for robots to physically collaborate and interact with humans. Further, to achieve fluid collaboration in dynamic tasks intent estimation should be achieved in real-time. In this paper, we present a framework that combines online estimation and control to facilitate robots in interpreting human intentions, and dynamically adjust their actions to assist in dynamic object co-manipulation tasks while considering both robot and human constraints. Central to our approach is the adoption of a Dynamic Systems (DS) model to represent human intent. Such a low-dimensional parameterized model, along with human manipulability and robot kinematic constraints, enables us to predict intent using a particle filter solely based on past motion data and tracking errors. For safe assistive control, we propose a variable impedance controller that adapts the robot’s impedance to offer assistance based on the intent estimation confidence from the DS particle filter. We validate our framework on a challenging real-world human-robot co-manipulation task and present promising results over baselines. Our framework represents a significant step forward in physical human-robot collaboration (pHRC), ensuring that robot cooperative interactions with humans are both feasible and effective. https://tinyurl.com/intent-capability
2025-07-19 13:57:16,054 - paper_downloader - INFO - 下载完成: Constraint-Aware Intent Estimation for Dynamic Human-Robot Object Co-ManipulationYifei Simon Shao, T.pdf
2025-07-19 13:57:16,054 - __main__ - INFO - 成功下载: Constraint-Aware Intent Estimation for Dynamic Human-Robot Object Co-ManipulationYifei Simon Shao, Tianyu Li, Shafagh Keyvanian, Pratik Chaudhari, Vijay Kumar, Nadia FigueroaPaper ID 28Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Constraint-aware estimation of human intent
 is essential for robots to physically collaborate and interact with humans. Further, to achieve fluid collaboration in dynamic tasks intent estimation should be achieved in real-time. In this paper, we present a framework that combines online estimation and control to facilitate robots in interpreting human intentions, and dynamically adjust their actions to assist in dynamic object co-manipulation tasks while considering both robot and human constraints. Central to our approach is the adoption of a Dynamic Systems (DS) model to represent human intent. Such a low-dimensional parameterized model, along with human manipulability and robot kinematic constraints, enables us to predict intent using a particle filter solely based on past motion data and tracking errors. For safe assistive control, we propose a variable impedance controller that adapts the robot’s impedance to offer assistance based on the intent estimation confidence from the DS particle filter. We validate our framework on a challenging real-world human-robot co-manipulation task and present promising results over baselines. Our framework represents a significant step forward in physical human-robot collaboration (pHRC), ensuring that robot cooperative interactions with humans are both feasible and effective. https://tinyurl.com/intent-capability
2025-07-19 13:57:16,054 - paper_downloader - INFO - 开始下载: Demonstrating HumanTHOR: A Simulation Platform and Benchmark for Human-Robot Collaboration in a Shared WorkspaceChenxu Wang, Boyuan Du, Jiaxin Xu, Huaping LiuPaper ID 29Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest. However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability. To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace. To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms. The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC. The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms. The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace. More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/.
2025-07-19 13:57:19,845 - paper_downloader - INFO - 下载完成: Demonstrating HumanTHOR A Simulation Platform and Benchmark for Human-Robot Collaboration in a Share.pdf
2025-07-19 13:57:19,845 - __main__ - INFO - 成功下载: Demonstrating HumanTHOR: A Simulation Platform and Benchmark for Human-Robot Collaboration in a Shared WorkspaceChenxu Wang, Boyuan Du, Jiaxin Xu, Huaping LiuPaper ID 29Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Human-robot collaboration (HRC) in a shared workspace has become a common pattern in real-world robot applications and has garnered significant research interest. However, most existing studies for human-in-the-loop (HITL) collaboration with robots in a shared workspace evaluate in either simplified game environments or physical platforms, falling short in limited realistic significance or limited scalability. To support future studies, we build an embodied framework named HumanTHOR, which enables humans to act in the simulation environment through VR devices to support HITL collaborations in a shared workspace. To validate our system, we build a benchmark of everyday tasks and conduct a preliminary user study with two baseline algorithms. The results show that the robot can effectively assist humans in collaboration, demonstrating the significance of HRC. The comparison among different levels of baselines affirms that our system can adequately evaluate robot capabilities and serve as a benchmark for different robot algorithms. The experimental results also indicate that there is still much room in the area and our system can provide a preliminary foundation for future HRC research in a shared workspace. More information about the simulation environment, experiment videos, benchmark descriptions, and additional supplementary materials can be found on the website: https://sites.google.com/view/humanthor/.
2025-07-19 13:57:19,846 - paper_downloader - INFO - 开始下载: Developing Design Guidelines for Older Adults with Robot Learning from DemonstrationErin Hedlund-Botti, Lakshmi Seelam, Chuxuan Yang, Nathaniel Belles, Zulfiqar Haider Zaidi, Matthew GombolayPaper ID 30Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Assistive in-home robots have the potential to enable older adults to age in place by offloading mentally or physically demanding tasks to a robot. However, one challenge for in-home robots is that each individual will have differing needs, preferences, and home environments, which can all change over time. Learning from Demonstration (LfD) is one solution to enable non-expert users to communicate their differing and changing preferences to a robot, but LfD has not been evaluated with a population of older adults.
 In a human-subjects experiment where participants teach a robot via LfD, we characterize disparities between older and younger adult participants in terms of robot performance, usability, and participant perceptions. We find that older adults are significantly more critical of the robot’s performance and found the LfD process less usable than younger adults. Based on participant performance and feedback, we present design guidelines that will enable roboticists to increase LfD accessibility across demographics.
2025-07-19 13:57:24,680 - paper_downloader - INFO - 下载完成: Developing Design Guidelines for Older Adults with Robot Learning from DemonstrationErin Hedlund-Bot.pdf
2025-07-19 13:57:24,680 - __main__ - INFO - 成功下载: Developing Design Guidelines for Older Adults with Robot Learning from DemonstrationErin Hedlund-Botti, Lakshmi Seelam, Chuxuan Yang, Nathaniel Belles, Zulfiqar Haider Zaidi, Matthew GombolayPaper ID 30Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Assistive in-home robots have the potential to enable older adults to age in place by offloading mentally or physically demanding tasks to a robot. However, one challenge for in-home robots is that each individual will have differing needs, preferences, and home environments, which can all change over time. Learning from Demonstration (LfD) is one solution to enable non-expert users to communicate their differing and changing preferences to a robot, but LfD has not been evaluated with a population of older adults.
 In a human-subjects experiment where participants teach a robot via LfD, we characterize disparities between older and younger adult participants in terms of robot performance, usability, and participant perceptions. We find that older adults are significantly more critical of the robot’s performance and found the LfD process less usable than younger adults. Based on participant performance and feedback, we present design guidelines that will enable roboticists to increase LfD accessibility across demographics.
2025-07-19 13:57:24,680 - paper_downloader - INFO - 开始下载: FLAIR: Feeding via Long-Horizon AcquIsition of Realistic dishesRajat Kumar Jenamani, Priya Sundaresan, Maram Sakr, Tapomayukh Bhattacharjee, Dorsa SadighPaper ID 31Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Robot-assisted feeding holds immense promise for improving the quality of life for individuals with mobility limitations who are unable to feed themselves independently. However, there exists a large gap between the kinds of homogeneous, curated plates existing assistive feeding systems can handle, and truly in-the-wild meals. Feeding realistic plates is immensely challenging due to the sheer range of food items that a robot may encounter, each requiring specialized manipulation strategies which must be sequenced over a long-horizon to feed an entire meal. An assistive feeding system should not only be able to sequence different strategies efficiently in order to feed an entire meal, but also in a way that is mindful of user preferences given the personalized nature of the task. We address this with FLAIR, a system for long-horizon feeding which leverages the commonsense reasoning capabilities of foundation models, along with a library of parameterized skills, to plan and execute user-preferred and efficient bite sequences. In real-world evaluations across 6 highly realistic plates, we find that FLAIR can effectively tap into a library of dexterous skills for efficient plate clearance, while adhering to the diverse preferences of over 42 as evaluated in a user study. We finally demonstrate the real-world efficacy of our approach by deploying our system with an in-mouth bite transfer framework for successfully feeding a care recipient with mobility limitations.
2025-07-19 13:57:28,220 - paper_downloader - INFO - 下载完成: FLAIR Feeding via Long-Horizon AcquIsition of Realistic dishesRajat Kumar Jenamani, Priya Sundaresan.pdf
2025-07-19 13:57:28,220 - __main__ - INFO - 成功下载: FLAIR: Feeding via Long-Horizon AcquIsition of Realistic dishesRajat Kumar Jenamani, Priya Sundaresan, Maram Sakr, Tapomayukh Bhattacharjee, Dorsa SadighPaper ID 31Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:Robot-assisted feeding holds immense promise for improving the quality of life for individuals with mobility limitations who are unable to feed themselves independently. However, there exists a large gap between the kinds of homogeneous, curated plates existing assistive feeding systems can handle, and truly in-the-wild meals. Feeding realistic plates is immensely challenging due to the sheer range of food items that a robot may encounter, each requiring specialized manipulation strategies which must be sequenced over a long-horizon to feed an entire meal. An assistive feeding system should not only be able to sequence different strategies efficiently in order to feed an entire meal, but also in a way that is mindful of user preferences given the personalized nature of the task. We address this with FLAIR, a system for long-horizon feeding which leverages the commonsense reasoning capabilities of foundation models, along with a library of parameterized skills, to plan and execute user-preferred and efficient bite sequences. In real-world evaluations across 6 highly realistic plates, we find that FLAIR can effectively tap into a library of dexterous skills for efficient plate clearance, while adhering to the diverse preferences of over 42 as evaluated in a user study. We finally demonstrate the real-world efficacy of our approach by deploying our system with an in-mouth bite transfer framework for successfully feeding a care recipient with mobility limitations.
2025-07-19 13:57:28,220 - paper_downloader - INFO - 开始下载: The Benefits of Sound Resound: An In-Person Replication of the Ability of Character-Like Robot Sound to Improve Perceived Social WarmthNnamdi Nwagwu, Adeline Schneider, Ibrahim Syed, Brian J. Zhang, Naomi T. FitterPaper ID 32Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:While robot sound is known to impact perceptions of robots, little research to date has addressed the topic of robot sound. In particular, there are few in-person studies surrounding the topic. To expand existing robotic sound research, we conducted an in-person empirical study with $N=30$ participants, as a partial replication of a past online study. We sought to better understand the effects that character-like and functional sounds have on human teammates’ perceptions of a robot during a joint in-person task. Participants rated the robot with character-like sound as more socially warm compared against a no-added-sound condition; this result was akin to insights from our previous work that showed benefits of transformative robot sound for warmth and other factors across multiple robot platforms. Additional evidence newly presented in this work also suggests the increased localizability of robots with augmented sonic profiles. The partial replication in the results strengthens our lab’s past findings on robot sound, especially as related to character-like robot sound’s ability to improve perceived robot warmth. This work can help to inform designers and researchers who are interested in enhancing robot interactions via nonverbal robot expression.
2025-07-19 13:57:29,263 - paper_downloader - INFO - 下载完成: The Benefits of Sound Resound An In-Person Replication of the Ability of Character-Like Robot Sound .pdf
2025-07-19 13:57:29,263 - __main__ - INFO - 成功下载: The Benefits of Sound Resound: An In-Person Replication of the Ability of Character-Like Robot Sound to Improve Perceived Social WarmthNnamdi Nwagwu, Adeline Schneider, Ibrahim Syed, Brian J. Zhang, Naomi T. FitterPaper ID 32Session 5. HRIPoster Session day 1 (Tuesday, July 16)Abstract:While robot sound is known to impact perceptions of robots, little research to date has addressed the topic of robot sound. In particular, there are few in-person studies surrounding the topic. To expand existing robotic sound research, we conducted an in-person empirical study with $N=30$ participants, as a partial replication of a past online study. We sought to better understand the effects that character-like and functional sounds have on human teammates’ perceptions of a robot during a joint in-person task. Participants rated the robot with character-like sound as more socially warm compared against a no-added-sound condition; this result was akin to insights from our previous work that showed benefits of transformative robot sound for warmth and other factors across multiple robot platforms. Additional evidence newly presented in this work also suggests the increased localizability of robots with augmented sonic profiles. The partial replication in the results strengthens our lab’s past findings on robot sound, especially as related to character-like robot sound’s ability to improve perceived robot warmth. This work can help to inform designers and researchers who are interested in enhancing robot interactions via nonverbal robot expression.
2025-07-19 13:57:29,264 - paper_downloader - INFO - 开始下载: Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork CollaborationXinzhu Liu, Peiyan Li, Wenju Yang, Di Guo, Huaping LiuPaper ID 33Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Compared with the widely investigated homogeneous multi-robot collaboration, heterogeneous robots with different capabilities can provide a more efficient and flexible collaboration for more complex tasks. In this paper, we consider a more challenging heterogeneous ad hoc teamwork collaboration problem where an ad hoc robot joins an existing heterogeneous team for a shared goal. Specifically, the ad hoc robot collaborates with unknown teammates without prior coordination, and it is expected to generate an appropriate cooperation policy to improve the efficiency of the whole team. To solve this challenging problem, we leverage the remarkable potential of the large language model (LLM) to establish a decentralized heterogeneous ad hoc teamwork collaboration framework that focuses on generating reasonable policy for an ad hoc robot to collaborate with original heterogeneous teammates. A training-free hierarchical dynamic planner is developed using the LLM together with the newly proposed Interactive Reflection of Thoughts (IRoT) method for the ad hoc agent to adapt to different teams. We also build a benchmark testing dataset to evaluate the proposed framework in the heterogeneous ad hoc multi-agent tidying-up task. Extensive comparison and ablation experiments are conducted in the benchmark to demonstrate the effectiveness of the proposed framework. We have also employed the proposed framework in physical robots in a real-world scenario. The experimental videos can be found at https://youtu.be/wHYP5T2WIp0.
2025-07-19 13:57:39,963 - paper_downloader - INFO - 下载完成: Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork CollaborationXinzhu Liu, Peiyan Li.pdf
2025-07-19 13:57:39,963 - __main__ - INFO - 成功下载: Leveraging Large Language Model for Heterogeneous Ad Hoc Teamwork CollaborationXinzhu Liu, Peiyan Li, Wenju Yang, Di Guo, Huaping LiuPaper ID 33Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Compared with the widely investigated homogeneous multi-robot collaboration, heterogeneous robots with different capabilities can provide a more efficient and flexible collaboration for more complex tasks. In this paper, we consider a more challenging heterogeneous ad hoc teamwork collaboration problem where an ad hoc robot joins an existing heterogeneous team for a shared goal. Specifically, the ad hoc robot collaborates with unknown teammates without prior coordination, and it is expected to generate an appropriate cooperation policy to improve the efficiency of the whole team. To solve this challenging problem, we leverage the remarkable potential of the large language model (LLM) to establish a decentralized heterogeneous ad hoc teamwork collaboration framework that focuses on generating reasonable policy for an ad hoc robot to collaborate with original heterogeneous teammates. A training-free hierarchical dynamic planner is developed using the LLM together with the newly proposed Interactive Reflection of Thoughts (IRoT) method for the ad hoc agent to adapt to different teams. We also build a benchmark testing dataset to evaluate the proposed framework in the heterogeneous ad hoc multi-agent tidying-up task. Extensive comparison and ablation experiments are conducted in the benchmark to demonstrate the effectiveness of the proposed framework. We have also employed the proposed framework in physical robots in a real-world scenario. The experimental videos can be found at https://youtu.be/wHYP5T2WIp0.
2025-07-19 13:57:39,964 - paper_downloader - INFO - 开始下载: INTERPRET: Interactive Predicate Learning from Language Feedback for Generalizable Task PlanningMuzhi Han, Yifeng Zhu, Song-Chun Zhu, Ying Nian Wu, Yuke ZhuPaper ID 34Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Learning abstract state representations and knowledge is crucial for long-horizon robot planning. We present InterPreT, an LLM-powered framework for robots to learn symbolic predicates from language feedback of human non-experts during embodied interaction. The learned predicates provide relational abstractions of the environment state, facilitating the learning of symbolic operators that capture action preconditions and effects. By compiling the learned predicates and operators into a PDDL domain on-the-fly, InterPreT allows effective planning toward arbitrary in-domain goals using a PDDL planner. In both simulated and real-world robot manipulation domains, we demonstrate that InterPreT reliably uncovers the key predicates and operators governing the environment dynamics. Although learned from simple training tasks, these predicates and operators exhibit strong generalization to novel tasks with significantly higher complexity. In the most challenging generalization setting, InterPreT attains success rates of 73% in simulation and 40% in the real world, substantially outperforming baseline methods.
2025-07-19 13:57:41,559 - paper_downloader - INFO - 下载完成: INTERPRET Interactive Predicate Learning from Language Feedback for Generalizable Task PlanningMuzhi.pdf
2025-07-19 13:57:41,559 - __main__ - INFO - 成功下载: INTERPRET: Interactive Predicate Learning from Language Feedback for Generalizable Task PlanningMuzhi Han, Yifeng Zhu, Song-Chun Zhu, Ying Nian Wu, Yuke ZhuPaper ID 34Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Learning abstract state representations and knowledge is crucial for long-horizon robot planning. We present InterPreT, an LLM-powered framework for robots to learn symbolic predicates from language feedback of human non-experts during embodied interaction. The learned predicates provide relational abstractions of the environment state, facilitating the learning of symbolic operators that capture action preconditions and effects. By compiling the learned predicates and operators into a PDDL domain on-the-fly, InterPreT allows effective planning toward arbitrary in-domain goals using a PDDL planner. In both simulated and real-world robot manipulation domains, we demonstrate that InterPreT reliably uncovers the key predicates and operators governing the environment dynamics. Although learned from simple training tasks, these predicates and operators exhibit strong generalization to novel tasks with significantly higher complexity. In the most challenging generalization setting, InterPreT attains success rates of 73% in simulation and 40% in the real world, substantially outperforming baseline methods.
2025-07-19 13:57:41,559 - paper_downloader - INFO - 开始下载: Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With SpheresJonathan Michaux, Adam Li, Qingyi Chen, Che Chen, Ram VasudevanPaper ID 35Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments. These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged.
 However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment. Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans. This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans. At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot’s motion. SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free. We demonstrate that SPARROWS’ novel reachable set is significantly less conservative than previous approaches. We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments. Code will be released upon acceptance of this manuscript.
2025-07-19 13:57:45,721 - paper_downloader - INFO - 下载完成: Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With SpheresJonatha.pdf
2025-07-19 13:57:45,721 - __main__ - INFO - 成功下载: Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With SpheresJonathan Michaux, Adam Li, Qingyi Chen, Che Chen, Ram VasudevanPaper ID 35Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments. These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged.
 However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment. Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans. This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans. At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot’s motion. SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free. We demonstrate that SPARROWS’ novel reachable set is significantly less conservative than previous approaches. We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments. Code will be released upon acceptance of this manuscript.
2025-07-19 13:57:45,721 - paper_downloader - INFO - 开始下载: Motion Planning in Foliated Manifolds using Repetition RoadmapJiaming hu, Shrutheesh Raman Iyer, Jiawei Wang, Henrik I ChristensenPaper ID 36Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Numerous classes of robotics motion planning problems involve searching in constrained configuration spaces where the constraints change during different stages of the motion, and these kinds of motion planning problems are named multi-modal problems. The most common method to solve these problems is to represent them as a set of manifolds and search for a trajectory across them. Often, instead of using manifolds alone, foliated manifolds, which are a union of disjoint manifolds, are a better way to model the manipulation problem. However, the complexity of planning in foliated manifolds is significant due to the increased number of manifolds, hard task constraints, and complex environments. To tackle these challenges, we propose an efficient planning framework that leverages a dynamic roadmap structure to learn from accumulated experience acquired during previous planning attempts in similar foliated manifolds. When planning in a new foliated manifold, this experience, captured in configuration distributions and an atlas, which are tangential charts approximating the new manifold with constraints, is effectively utilized to guide motion planning. We demonstrate the framework’s performance for manipulation problems with different foliated manifold structures in simulation and real-world scenarios. An open-source will be released soon.
2025-07-19 13:57:48,335 - paper_downloader - INFO - 下载完成: Motion Planning in Foliated Manifolds using Repetition RoadmapJiaming hu, Shrutheesh Raman Iyer, Jia.pdf
2025-07-19 13:57:48,335 - __main__ - INFO - 成功下载: Motion Planning in Foliated Manifolds using Repetition RoadmapJiaming hu, Shrutheesh Raman Iyer, Jiawei Wang, Henrik I ChristensenPaper ID 36Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Numerous classes of robotics motion planning problems involve searching in constrained configuration spaces where the constraints change during different stages of the motion, and these kinds of motion planning problems are named multi-modal problems. The most common method to solve these problems is to represent them as a set of manifolds and search for a trajectory across them. Often, instead of using manifolds alone, foliated manifolds, which are a union of disjoint manifolds, are a better way to model the manipulation problem. However, the complexity of planning in foliated manifolds is significant due to the increased number of manifolds, hard task constraints, and complex environments. To tackle these challenges, we propose an efficient planning framework that leverages a dynamic roadmap structure to learn from accumulated experience acquired during previous planning attempts in similar foliated manifolds. When planning in a new foliated manifold, this experience, captured in configuration distributions and an atlas, which are tangential charts approximating the new manifold with constraints, is effectively utilized to guide motion planning. We demonstrate the framework’s performance for manipulation problems with different foliated manifold structures in simulation and real-world scenarios. An open-source will be released soon.
2025-07-19 13:57:48,336 - paper_downloader - INFO - 开始下载: Language-Augmented Symbolic Planner for Open-World Task PlanningGuanqi Chen, Lei Yang, Ruixing Jia, Zhe Hu, Yizhou Chen, Wei Zhang, Wenping Wang, Jia PanPaper ID 37Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Enabling robotic agents to perform complex long-horizon tasks has been a long-standing goal in robotics and artificial intelligence (AI). Despite the potential shown by large language models (LLMs), their planning capabilities remain limited to short-horizon tasks and they are unable to replace the symbolic planning approach. Symbolic planners, on the other hand, may encounter execution errors due to their common assumption of complete domain knowledge which is hard to manually prepare for an open-world setting. In this paper, we introduce a Language-Augmented Symbolic Planner (LASP) that integrates pre-trained LLMs to enable conventional symbolic planners to operate in an open-world environment where only incomplete knowledge of action preconditions, objects, and properties is initially available. In case of execution errors, LASP can utilize the LLM to diagnose the cause of the error based on the observation and interact with the environment to incrementally build up its knowledge base necessary for accomplishing the given tasks. Experiments demonstrate that LASP is proficient in solving planning problems in the open-world setting, performing well even in situations where there are multiple gaps in the knowledge.
2025-07-19 13:57:48,616 - paper_downloader - INFO - 下载完成: Language-Augmented Symbolic Planner for Open-World Task PlanningGuanqi Chen, Lei Yang, Ruixing Jia, .pdf
2025-07-19 13:57:48,617 - __main__ - INFO - 成功下载: Language-Augmented Symbolic Planner for Open-World Task PlanningGuanqi Chen, Lei Yang, Ruixing Jia, Zhe Hu, Yizhou Chen, Wei Zhang, Wenping Wang, Jia PanPaper ID 37Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Enabling robotic agents to perform complex long-horizon tasks has been a long-standing goal in robotics and artificial intelligence (AI). Despite the potential shown by large language models (LLMs), their planning capabilities remain limited to short-horizon tasks and they are unable to replace the symbolic planning approach. Symbolic planners, on the other hand, may encounter execution errors due to their common assumption of complete domain knowledge which is hard to manually prepare for an open-world setting. In this paper, we introduce a Language-Augmented Symbolic Planner (LASP) that integrates pre-trained LLMs to enable conventional symbolic planners to operate in an open-world environment where only incomplete knowledge of action preconditions, objects, and properties is initially available. In case of execution errors, LASP can utilize the LLM to diagnose the cause of the error based on the observation and interact with the environment to incrementally build up its knowledge base necessary for accomplishing the given tasks. Experiments demonstrate that LASP is proficient in solving planning problems in the open-world setting, performing well even in situations where there are multiple gaps in the knowledge.
2025-07-19 13:57:48,617 - paper_downloader - INFO - 开始下载: Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Motion Planning with PointcloudsClayton Ramsey, Zachary Kingston, Wil Thomason, Lydia E KavrakiPaper ID 38Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Motion planning against sensor data is often a critical bottleneck in real-time robot control. For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking. We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points. With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU. We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure. Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments.
2025-07-19 13:57:49,155 - paper_downloader - INFO - 下载完成: Collision-Affording Point Trees SIMD-Amenable Nearest Neighbors for Fast Motion Planning with Pointc.pdf
2025-07-19 13:57:49,155 - __main__ - INFO - 成功下载: Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Motion Planning with PointcloudsClayton Ramsey, Zachary Kingston, Wil Thomason, Lydia E KavrakiPaper ID 38Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:Motion planning against sensor data is often a critical bottleneck in real-time robot control. For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking. We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points. With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU. We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure. Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments.
2025-07-19 13:57:49,155 - paper_downloader - INFO - 开始下载: Homotopic Path Set Planning for Robot Manipulation and NavigationJing HuangPaper ID 39Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:This paper addresses path set planning that yields important applications in robot manipulation and navigation such as path generation for deformable object keypoints and swarms. A path set refers to the collection of finite agent paths to represent the overall spatial path of a group of keypoints or a swarm, whose collective properties meet spatial and topological constraints. As opposed to planning a single path, simultaneously planning multiple paths with constraints poses nontrivial challenges in complex environments. This paper presents a systematic planning pipeline for homotopic path sets, a widely applicable path set class in robotics. An extended visibility check condition is first proposed to attain a sparse passage distribution amidst dense obstacles. Passage-aware optimal path planning compatible with sampling-based planners is then designed for single path planning with adjustable costs. Large accessible free space for path set accommodation can be achieved by the planned path while having a sufficiently short path length. After specifying the homotopic properties of path sets, path set generation based on deformable path transfer is proposed in an efficient centralized manner. The effectiveness of these methods is validated by extensive simulated and experimental results.
2025-07-19 13:57:50,749 - paper_downloader - INFO - 下载完成: Homotopic Path Set Planning for Robot Manipulation and NavigationJing HuangPaper ID 39Session 2. Pla.pdf
2025-07-19 13:57:50,749 - __main__ - INFO - 成功下载: Homotopic Path Set Planning for Robot Manipulation and NavigationJing HuangPaper ID 39Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:This paper addresses path set planning that yields important applications in robot manipulation and navigation such as path generation for deformable object keypoints and swarms. A path set refers to the collection of finite agent paths to represent the overall spatial path of a group of keypoints or a swarm, whose collective properties meet spatial and topological constraints. As opposed to planning a single path, simultaneously planning multiple paths with constraints poses nontrivial challenges in complex environments. This paper presents a systematic planning pipeline for homotopic path sets, a widely applicable path set class in robotics. An extended visibility check condition is first proposed to attain a sparse passage distribution amidst dense obstacles. Passage-aware optimal path planning compatible with sampling-based planners is then designed for single path planning with adjustable costs. Large accessible free space for path set accommodation can be achieved by the planned path while having a sufficiently short path length. After specifying the homotopic properties of path sets, path set generation based on deformable path transfer is proposed in an efficient centralized manner. The effectiveness of these methods is validated by extensive simulated and experimental results.
2025-07-19 13:57:50,749 - paper_downloader - INFO - 开始下载: Practice Makes Perfect: Planning to Learning Skill Parameter PoliciesNishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Jennifer L. BarryPaper ID 40Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence togetherparameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills topracticeto maximize expected future task success. We propose that the robot shouldestimatethe competence of each skill,extrapolatethe competence (asking: “how much would the competence improve through practice?”), andsituatethe skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach’s ability to handle noise from perception and control and improve the robot’s ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu
2025-07-19 13:57:52,323 - paper_downloader - INFO - 下载完成: Practice Makes Perfect Planning to Learning Skill Parameter PoliciesNishanth Kumar, Tom Silver, Will.pdf
2025-07-19 13:57:52,323 - __main__ - INFO - 成功下载: Practice Makes Perfect: Planning to Learning Skill Parameter PoliciesNishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Jennifer L. BarryPaper ID 40Session 2. PlanningPoster Session day 1 (Tuesday, July 16)Abstract:One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence togetherparameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills topracticeto maximize expected future task success. We propose that the robot shouldestimatethe competence of each skill,extrapolatethe competence (asking: “how much would the competence improve through practice?”), andsituatethe skill in the task distribution through competence-aware planning. This approach is implemented within a fully autonomous system where the robot repeatedly plans, practices, and learns without any environment resets. Through experiments in simulation, we find that our approach learns effective parameter policies more sample-efficiently than several baselines. Experiments in the real-world demonstrate our approach’s ability to handle noise from perception and control and improve the robot’s ability to solve two long-horizon mobile-manipulation tasks after a few hours of autonomous practice. Project website: http://ees.csail.mit.edu
2025-07-19 13:57:52,323 - paper_downloader - INFO - 开始下载: World Models for General Surgical GraspingHongbin LinPaper ID 41Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances. Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking. We propose a world-model-based deep reinforcement learning framework “Grasp Anything for Surgery” (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness. In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object’s inaccurate region based on the empirical prior of the object’s size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss. The learned controller’s effectiveness is extensively evaluated in simulation and in a real robot. Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control. 
  Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%). Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper. Videos and codes can be found on our project page: https://linhongbin.github.io/gas/.
2025-07-19 13:57:54,410 - paper_downloader - INFO - 下载完成: World Models for General Surgical GraspingHongbin LinPaper ID 41Session 6. GraspingPoster Session da.pdf
2025-07-19 13:57:54,410 - __main__ - INFO - 成功下载: World Models for General Surgical GraspingHongbin LinPaper ID 41Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances. Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking. We propose a world-model-based deep reinforcement learning framework “Grasp Anything for Surgery” (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness. In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object’s inaccurate region based on the empirical prior of the object’s size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss. The learned controller’s effectiveness is extensively evaluated in simulation and in a real robot. Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control. 
  Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%). Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper. Videos and codes can be found on our project page: https://linhongbin.github.io/gas/.
2025-07-19 13:57:54,410 - paper_downloader - INFO - 开始下载: SpringGrasp: Synthesizing Compliant, Dexterous Grasps under Shape UncertaintySirui Chen, Jeannette Bohg, Karen LiuPaper ID 42Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Generating stable and robust grasps on arbitrary objects is critical for dexterous robotic hands, marking a significant step towards advanced dexterous manipulation. Previous studies have mostly focused on improving differentiable grasping metrics with the assumption of precisely known object geometry. However, shape uncertainty is ubiquitous due to noisy and partial shape observations, which introduce challenges in grasp planning. 
 We propose SpringGrasp planner, a planner that considers uncertain observations of the object surface for synthesizing compliant dexterous grasps. A compliant dexterous grasp could minimize the effect of unexpected contact with the object, leading to a more stable grasp with shape-uncertain objects. We introduce an analytical and differentiable metric, SpringGrasp metric, that evaluates the dynamic behavior of the entire compliant grasping process. Planning with SpringGrasp planner, our method achieves a grasp success rate of 89% from two viewpoints and 84% from a single viewpoints in experiment with a real robot on 14 common objects. Compared with a force-closure-based planner, our method achieves at least 18% higher grasp success rate.
2025-07-19 13:58:03,642 - paper_downloader - INFO - 下载完成: SpringGrasp Synthesizing Compliant, Dexterous Grasps under Shape UncertaintySirui Chen, Jeannette Bo.pdf
2025-07-19 13:58:03,642 - __main__ - INFO - 成功下载: SpringGrasp: Synthesizing Compliant, Dexterous Grasps under Shape UncertaintySirui Chen, Jeannette Bohg, Karen LiuPaper ID 42Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Generating stable and robust grasps on arbitrary objects is critical for dexterous robotic hands, marking a significant step towards advanced dexterous manipulation. Previous studies have mostly focused on improving differentiable grasping metrics with the assumption of precisely known object geometry. However, shape uncertainty is ubiquitous due to noisy and partial shape observations, which introduce challenges in grasp planning. 
 We propose SpringGrasp planner, a planner that considers uncertain observations of the object surface for synthesizing compliant dexterous grasps. A compliant dexterous grasp could minimize the effect of unexpected contact with the object, leading to a more stable grasp with shape-uncertain objects. We introduce an analytical and differentiable metric, SpringGrasp metric, that evaluates the dynamic behavior of the entire compliant grasping process. Planning with SpringGrasp planner, our method achieves a grasp success rate of 89% from two viewpoints and 84% from a single viewpoints in experiment with a real robot on 14 common objects. Compared with a force-closure-based planner, our method achieves at least 18% higher grasp success rate.
2025-07-19 13:58:03,642 - paper_downloader - INFO - 开始下载: GRaCE: Balancing Multiple Criteria to Achieve Stable, Collision-Free, and Functional GraspsTasbolat Taunyazov, Kelvin Lin, Harold SohPaper ID 44Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Abstract—This paper addresses the multi-faceted problem of robot grasping, where multiple criteria may conflict and differ in importance. We introduce a probabilistic framework, Grasp Ranking and Criteria Evaluation (GRaCE), which employs hierarchical rule-based logic and a rank-preserving utility function for grasps based on various criteria such as stability, kinematic constraints, and goal oriented functionalities. GRaCE’s probabilistic nature means the framework handles uncertainty in a principled manner, i.e., the method is able to leverage the probability that a given criteria is satisfied. Additionally, we propose GRaCE-OPT, a hybrid optimization strategy that combines gradient-based and gradient-free methods to effectively navigate the complex, non-convex utility function. Experimental results in both simulated and real-world scenarios show that GRaCE requires fewer samples to achieve comparable or superior performance relative to existing methods. The modular architecture of GRaCE allows for easy customization and adaptation to specific application needs. Code and implementation details can be found online at https://github.com/clear-nus/GRaCE.
2025-07-19 13:58:12,077 - paper_downloader - INFO - 下载完成: GRaCE Balancing Multiple Criteria to Achieve Stable, Collision-Free, and Functional GraspsTasbolat T.pdf
2025-07-19 13:58:12,078 - __main__ - INFO - 成功下载: GRaCE: Balancing Multiple Criteria to Achieve Stable, Collision-Free, and Functional GraspsTasbolat Taunyazov, Kelvin Lin, Harold SohPaper ID 44Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Abstract—This paper addresses the multi-faceted problem of robot grasping, where multiple criteria may conflict and differ in importance. We introduce a probabilistic framework, Grasp Ranking and Criteria Evaluation (GRaCE), which employs hierarchical rule-based logic and a rank-preserving utility function for grasps based on various criteria such as stability, kinematic constraints, and goal oriented functionalities. GRaCE’s probabilistic nature means the framework handles uncertainty in a principled manner, i.e., the method is able to leverage the probability that a given criteria is satisfied. Additionally, we propose GRaCE-OPT, a hybrid optimization strategy that combines gradient-based and gradient-free methods to effectively navigate the complex, non-convex utility function. Experimental results in both simulated and real-world scenarios show that GRaCE requires fewer samples to achieve comparable or superior performance relative to existing methods. The modular architecture of GRaCE allows for easy customization and adaptation to specific application needs. Code and implementation details can be found online at https://github.com/clear-nus/GRaCE.
2025-07-19 13:58:12,078 - paper_downloader - INFO - 开始下载: Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild RobotsCheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, Shuran SongPaper ID 45Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:We present Universal Manipulation Interface (UMI) – a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI’s hardware and software system along with our in-the-wild dataset will be open-sourced.
2025-07-19 13:58:14,632 - paper_downloader - INFO - 下载完成: Universal Manipulation Interface In-The-Wild Robot Teaching Without In-The-Wild RobotsCheng Chi, Zhe.pdf
2025-07-19 13:58:14,632 - __main__ - INFO - 成功下载: Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild RobotsCheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, Shuran SongPaper ID 45Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:We present Universal Manipulation Interface (UMI) – a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI’s hardware and software system along with our in-the-wild dataset will be open-sourced.
2025-07-19 13:58:14,632 - paper_downloader - INFO - 开始下载: Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface RenderingSnehal Jauhri, Ishikaa Lunawat, Georgia ChalvatzakiPaper ID 46Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:A significant challenge for real-world robotic manipulation is the effective 6DoF grasping of objects in cluttered scenes from any single viewpoint without needing additional scene exploration. This work re-interprets grasping as rendering and introduces NeuGraspNet, a novel method for 6DoF grasp detection that leverages advances in neural volumetric representations and surface rendering. We encode the interaction between a robot’s end-effector and an object’s surface by jointly learning to render the local object surface and learning grasping functions in a shared feature space. Our approach uses global (scene-level) features for grasp generation and local (grasp-level) neural surface features for grasp evaluation. This enables effective, fully implicit 6DoF grasp quality prediction, even in partially observed scenes. NeuGraspNet operates on random viewpoints, common in mobile manipulation scenarios, and outperforms existing implicit and semi-implicit grasping methods. We demonstrate the real-world applicability of the method with a mobile manipulator robot, grasping in open cluttered spaces. Project website at: https://sites.google.com/view/neugraspnet
2025-07-19 13:58:16,167 - paper_downloader - INFO - 下载完成: Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface RenderingSnehal Jauhr.pdf
2025-07-19 13:58:16,167 - __main__ - INFO - 成功下载: Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface RenderingSnehal Jauhri, Ishikaa Lunawat, Georgia ChalvatzakiPaper ID 46Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:A significant challenge for real-world robotic manipulation is the effective 6DoF grasping of objects in cluttered scenes from any single viewpoint without needing additional scene exploration. This work re-interprets grasping as rendering and introduces NeuGraspNet, a novel method for 6DoF grasp detection that leverages advances in neural volumetric representations and surface rendering. We encode the interaction between a robot’s end-effector and an object’s surface by jointly learning to render the local object surface and learning grasping functions in a shared feature space. Our approach uses global (scene-level) features for grasp generation and local (grasp-level) neural surface features for grasp evaluation. This enables effective, fully implicit 6DoF grasp quality prediction, even in partially observed scenes. NeuGraspNet operates on random viewpoints, common in mobile manipulation scenarios, and outperforms existing implicit and semi-implicit grasping methods. We demonstrate the real-world applicability of the method with a mobile manipulator robot, grasping in open cluttered spaces. Project website at: https://sites.google.com/view/neugraspnet
2025-07-19 13:58:16,168 - paper_downloader - INFO - 开始下载: Demonstrating Adaptive Mobile Manipulation in Retail EnvironmentsMax Spahn, Corrado Pezzato, Chadi Salmi, Rick Dekker, Cong Wang, Christian Pek, Jens Kober, Javier Alonso-Mora, Carlos Hernandez Corbato, Martijn WissePaper ID 47Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Although autonomous robots have great potential to
 boost efficiency and throughput across the whole retail chain, they
 are mostly being deployed in large warehouses and distribution
 centers. Deploying robots in stores with customers, such as supermarkets, requires substantially more development efforts since
 they need to safely operate around customers and reliably cope
 with various uncertainties and disturbances, such as misplaced
 products. We present our recent efforts in developing a mobile
 manipulator platform for order picking in realistic supermarket
 settings. Our robot platform uses state-of-the-art perception and
 planning algorithms to robustly pick items in the presence of
 disturbances. In particular, it successfully demonstrates adaptive
 decision making and rapid replanning. Our robot allows adding
 new products and teaching new picking maneuvers from demonstrations. We validated our robot in a recreated supermarket in
 our lab and in a test supermarket of a large Dutch retailer. Our
 results show how our robot successfully recovers from various
 disturbances, including misplaced products, errors in picking,
 and from human interaction. We summarize our lessons learned
 to bring autonomous robots into real retail environments with
 customers.
2025-07-19 13:58:24,844 - paper_downloader - INFO - 下载完成: Demonstrating Adaptive Mobile Manipulation in Retail EnvironmentsMax Spahn, Corrado Pezzato, Chadi S.pdf
2025-07-19 13:58:24,844 - __main__ - INFO - 成功下载: Demonstrating Adaptive Mobile Manipulation in Retail EnvironmentsMax Spahn, Corrado Pezzato, Chadi Salmi, Rick Dekker, Cong Wang, Christian Pek, Jens Kober, Javier Alonso-Mora, Carlos Hernandez Corbato, Martijn WissePaper ID 47Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:Although autonomous robots have great potential to
 boost efficiency and throughput across the whole retail chain, they
 are mostly being deployed in large warehouses and distribution
 centers. Deploying robots in stores with customers, such as supermarkets, requires substantially more development efforts since
 they need to safely operate around customers and reliably cope
 with various uncertainties and disturbances, such as misplaced
 products. We present our recent efforts in developing a mobile
 manipulator platform for order picking in realistic supermarket
 settings. Our robot platform uses state-of-the-art perception and
 planning algorithms to robustly pick items in the presence of
 disturbances. In particular, it successfully demonstrates adaptive
 decision making and rapid replanning. Our robot allows adding
 new products and teaching new picking maneuvers from demonstrations. We validated our robot in a recreated supermarket in
 our lab and in a test supermarket of a large Dutch retailer. Our
 results show how our robot successfully recovers from various
 disturbances, including misplaced products, errors in picking,
 and from human interaction. We summarize our lessons learned
 to bring autonomous robots into real retail environments with
 customers.
2025-07-19 13:58:24,844 - paper_downloader - INFO - 开始下载: Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation LearningXiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh GuptaPaper ID 48Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that are not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method that reaps the benefits of DAgger but without the cost, for eye-in-hand imitation learning problems. Instead ofcollectingnew samples to cover out-of-distribution states, DMD uses recent advances in diffusion models tosynthesizethese samples. This leads to robust performance from few demonstrations. We compare DMD against behavior cloning baseline across four tasks: pushing, stacking, pouring, and hanging a shirt. In pushing, DMD achieves 80% success rate with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. In stacking, DMD succeeds on average 92% of the time across 5 cups, versus 40% for BC. When pouring coffee beans, DMD transfers to another cup successfully 80% of the time. Finally, DMD attains 90% success rate for hanging shirt on a clothing rack.
2025-07-19 13:58:32,229 - paper_downloader - INFO - 下载完成: Diffusion Meets DAgger Supercharging Eye-in-hand Imitation LearningXiaoyu Zhang, Matthew Chang, Pran.pdf
2025-07-19 13:58:32,229 - __main__ - INFO - 成功下载: Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation LearningXiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh GuptaPaper ID 48Session 6. GraspingPoster Session day 1 (Tuesday, July 16)Abstract:A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that are not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method that reaps the benefits of DAgger but without the cost, for eye-in-hand imitation learning problems. Instead ofcollectingnew samples to cover out-of-distribution states, DMD uses recent advances in diffusion models tosynthesizethese samples. This leads to robust performance from few demonstrations. We compare DMD against behavior cloning baseline across four tasks: pushing, stacking, pouring, and hanging a shirt. In pushing, DMD achieves 80% success rate with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. In stacking, DMD succeeds on average 92% of the time across 5 cups, versus 40% for BC. When pouring coffee beans, DMD transfers to another cup successfully 80% of the time. Finally, DMD attains 90% success rate for hanging shirt on a clothing rack.
2025-07-19 13:58:32,229 - paper_downloader - INFO - 开始下载: RT-H: Action Hierarchies using LanguageSuneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa SadighPaper ID 49Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning have proposed learning language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., “pick coke can” and “pick an apple”) in multi-task datasets. However, as tasks become more semantically diverse (e.g., “pick coke can” and “pour cup”), sharing data between tasks becomes harder and thus learning to map high-level tasks to actions requires substantially more demonstration data. To bridge this divide between tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like “move arm forward” or “close gripper”. Predicting theselanguage motionsas an intermediate step between high-level tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily becorrectedduring execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. Our method RT-H builds anaction hierarchyusing language motions: it first learns to predict language motions, and conditioned on this along with the high-level task, it then predicts actions, using visual context at all stages. Experimentally we show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at https://rt-hierarchy.github.io.
2025-07-19 13:58:38,105 - paper_downloader - INFO - 下载完成: RT-H Action Hierarchies using LanguageSuneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan .pdf
2025-07-19 13:58:38,105 - __main__ - INFO - 成功下载: RT-H: Action Hierarchies using LanguageSuneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa SadighPaper ID 49Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning have proposed learning language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., “pick coke can” and “pick an apple”) in multi-task datasets. However, as tasks become more semantically diverse (e.g., “pick coke can” and “pour cup”), sharing data between tasks becomes harder and thus learning to map high-level tasks to actions requires substantially more demonstration data. To bridge this divide between tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like “move arm forward” or “close gripper”. Predicting theselanguage motionsas an intermediate step between high-level tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. Furthermore, a policy that is conditioned on language motions can easily becorrectedduring execution through human-specified language motions. This enables a new paradigm for flexible policies that can learn from human intervention in language. Our method RT-H builds anaction hierarchyusing language motions: it first learns to predict language motions, and conditioned on this along with the high-level task, it then predicts actions, using visual context at all stages. Experimentally we show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. Our website and videos are found at https://rt-hierarchy.github.io.
2025-07-19 13:58:38,105 - paper_downloader - INFO - 开始下载: RoboCasa: Large-Scale Simulation of Household Tasks for Generalist RobotsSoroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, Yuke ZhuPaper ID 50Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/.
2025-07-19 13:58:44,473 - paper_downloader - INFO - 下载完成: RoboCasa Large-Scale Simulation of Household Tasks for Generalist RobotsSoroush Nasiriany, Abhiram M.pdf
2025-07-19 13:58:44,474 - __main__ - INFO - 成功下载: RoboCasa: Large-Scale Simulation of Household Tasks for Generalist RobotsSoroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, Yuke ZhuPaper ID 50Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/.
2025-07-19 13:58:44,474 - paper_downloader - INFO - 开始下载: Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour CloningVitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen JamesPaper ID 51Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot. Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot. This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation. We thoroughly evaluate several variants of R&D in simulation and showcase their applicability on six everyday tasks in the real world. Our results show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods.
2025-07-19 13:58:46,310 - paper_downloader - INFO - 下载完成: Render and Diffuse Aligning Image and Action Spaces for Diffusion-based Behaviour CloningVitalis Vos.pdf
2025-07-19 13:58:46,311 - __main__ - INFO - 成功下载: Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour CloningVitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen JamesPaper ID 51Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot. Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot. This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation. We thoroughly evaluate several variants of R&D in simulation and showcase their applicability on six everyday tasks in the real world. Our results show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods.
2025-07-19 13:58:46,311 - paper_downloader - INFO - 开始下载: Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention TransformersVidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta DwibediPaper ID 52Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Large-scale multi-task robotic manipulation systems often rely on text to specify the task. In this work, we explore whether a robot can learn by observing humans. To do so, the robot must understand a person’s intent and perform the inferred task despite differences in the embodiments and environments. 
 We introduce Vid2Robot, an end-to-end video-conditioned policy that takes
 human videos demonstrating manipulation tasks as input and producing robot actions. Our model is trained with a large dataset of prompt video-robot trajectory pairs to learn unified representations of human and robot actions from videos.
 Vid2Robot uses cross-attention transformer layers 
 between video features and the current robot state to produce the actions and perform the same task as shown in the video. We use auxiliary contrastive losses to align the prompt and robot video representations for better policies.
 We evaluate Vid2Robot on real-world robots and observe over 20% improvement over BC-Z when using human prompt videos. Further, we also show cross-object motion transfer ability that enables video-conditioned policies to transfer a motion observed on one object in the prompt video to another object in the robot’s own environment. 
 Videos available at https://vid2robot.github.io
2025-07-19 13:58:49,962 - paper_downloader - INFO - 下载完成: Vid2Robot End-to-end Video-conditioned Policy Learning with Cross-Attention TransformersVidhi Jain, .pdf
2025-07-19 13:58:49,962 - __main__ - INFO - 成功下载: Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention TransformersVidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta DwibediPaper ID 52Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Large-scale multi-task robotic manipulation systems often rely on text to specify the task. In this work, we explore whether a robot can learn by observing humans. To do so, the robot must understand a person’s intent and perform the inferred task despite differences in the embodiments and environments. 
 We introduce Vid2Robot, an end-to-end video-conditioned policy that takes
 human videos demonstrating manipulation tasks as input and producing robot actions. Our model is trained with a large dataset of prompt video-robot trajectory pairs to learn unified representations of human and robot actions from videos.
 Vid2Robot uses cross-attention transformer layers 
 between video features and the current robot state to produce the actions and perform the same task as shown in the video. We use auxiliary contrastive losses to align the prompt and robot video representations for better policies.
 We evaluate Vid2Robot on real-world robots and observe over 20% improvement over BC-Z when using human prompt videos. Further, we also show cross-object motion transfer ability that enables video-conditioned policies to transfer a motion observed on one object in the prompt video to another object in the robot’s own environment. 
 Videos available at https://vid2robot.github.io
2025-07-19 13:58:49,962 - paper_downloader - INFO - 开始下载: Offline Imitation Learning Through Graph Search and RetrievalZhao-Heng Yin, Pieter AbbeelPaper ID 54Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Imitation learning is a powerful machine learning algorithm for a robot to acquire manipulation skills. Nevertheless, many real-world manipulation tasks involve precise and dexterous robot-object interactions, which make it difficult for humans to collect high-quality expert demonstrations. As a result, a robot has to learn skills from suboptimal demonstrations and unstructured interactions, which remains a key challenge. Existing works typically use offline deep reinforcement learning~(RL) to solve this challenge, but in practice these algorithms are unstable and fragile due to the deadly triad issue. To overcome this problem, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. We first use pretrained representation to organize the interaction experience into a graph and perform a graph search to calculate the values of different behaviors. Then, we apply a retrieval-based procedure to identify the best behavior (actions) on each state and use behavior cloning to learn that behavior. We evaluate our method in both simulation and real-world robotic manipulation tasks with complex visual inputs, covering various precise and dexterous manipulation skills with objects of different physical properties. GSR can achieve a 10% to 30% higher success rate and over 30% higher proficiency compared to baselines.
2025-07-19 13:58:54,831 - paper_downloader - INFO - 下载完成: Offline Imitation Learning Through Graph Search and RetrievalZhao-Heng Yin, Pieter AbbeelPaper ID 54.pdf
2025-07-19 13:58:54,832 - __main__ - INFO - 成功下载: Offline Imitation Learning Through Graph Search and RetrievalZhao-Heng Yin, Pieter AbbeelPaper ID 54Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Imitation learning is a powerful machine learning algorithm for a robot to acquire manipulation skills. Nevertheless, many real-world manipulation tasks involve precise and dexterous robot-object interactions, which make it difficult for humans to collect high-quality expert demonstrations. As a result, a robot has to learn skills from suboptimal demonstrations and unstructured interactions, which remains a key challenge. Existing works typically use offline deep reinforcement learning~(RL) to solve this challenge, but in practice these algorithms are unstable and fragile due to the deadly triad issue. To overcome this problem, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. We first use pretrained representation to organize the interaction experience into a graph and perform a graph search to calculate the values of different behaviors. Then, we apply a retrieval-based procedure to identify the best behavior (actions) on each state and use behavior cloning to learn that behavior. We evaluate our method in both simulation and real-world robotic manipulation tasks with complex visual inputs, covering various precise and dexterous manipulation skills with objects of different physical properties. GSR can achieve a 10% to 30% higher success rate and over 30% higher proficiency compared to baselines.
2025-07-19 13:58:54,832 - paper_downloader - INFO - 开始下载: RVT-2: Learning Precise Manipulation from Few DemonstrationsAnkit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter FoxPaper ID 55Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.
2025-07-19 13:58:56,862 - paper_downloader - INFO - 下载完成: RVT-2 Learning Precise Manipulation from Few DemonstrationsAnkit Goyal, Valts Blukis, Jie Xu, Yijie .pdf
2025-07-19 13:58:56,862 - __main__ - INFO - 成功下载: RVT-2: Learning Precise Manipulation from Few DemonstrationsAnkit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter FoxPaper ID 55Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.
2025-07-19 13:58:56,862 - paper_downloader - INFO - 开始下载: Imitation Bootstrapped Reinforcement LearningHengyuan Hu, Suvir Mirchandani, Dorsa SadighPaper ID 56Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Despite the considerable potential of reinforcement learning (RL), robotics control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We proposeimitation bootstrapped reinforcement learning(IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with additional imitation loss, IBRL is able to utilize high quality actions from IL policies since the beginning of training, which greatly accelerates exploration and training efficiency. We evaluate IBRL on 6 simulation and 3 real-world tasks spanning various difficulty levels. IBRL significantly outperforms prior methods and the improvement is particularly more prominent in harder tasks.
2025-07-19 13:58:58,941 - paper_downloader - INFO - 下载完成: Imitation Bootstrapped Reinforcement LearningHengyuan Hu, Suvir Mirchandani, Dorsa SadighPaper ID 56.pdf
2025-07-19 13:58:58,941 - __main__ - INFO - 成功下载: Imitation Bootstrapped Reinforcement LearningHengyuan Hu, Suvir Mirchandani, Dorsa SadighPaper ID 56Session 7. Imitation learningPoster Session day 2 (Wednesday, July 17)Abstract:Despite the considerable potential of reinforcement learning (RL), robotics control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We proposeimitation bootstrapped reinforcement learning(IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with additional imitation loss, IBRL is able to utilize high quality actions from IL policies since the beginning of training, which greatly accelerates exploration and training efficiency. We evaluate IBRL on 6 simulation and 3 real-world tasks spanning various difficulty levels. IBRL significantly outperforms prior methods and the improvement is particularly more prominent in harder tasks.
2025-07-19 13:58:58,941 - paper_downloader - INFO - 开始下载: Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion ControllersFan Shi, Chong Zhang, Takahiro Miki, Joonho Lee, Marco Hutter, Stelian CorosPaper ID 57Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach’s effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies.
2025-07-19 13:58:59,982 - paper_downloader - INFO - 下载完成: Rethinking Robustness Assessment Adversarial Attacks on Learning-based Quadrupedal Locomotion Contro.pdf
2025-07-19 13:58:59,983 - __main__ - INFO - 成功下载: Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion ControllersFan Shi, Chong Zhang, Takahiro Miki, Joonho Lee, Marco Hutter, Stelian CorosPaper ID 57Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach’s effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies.
2025-07-19 13:58:59,983 - paper_downloader - INFO - 开始下载: Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model LearningXinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, Jianyu ChenPaper ID 58Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world’s first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.
2025-07-19 13:59:06,716 - paper_downloader - INFO - 下载完成: Advancing Humanoid Locomotion Mastering Challenging Terrains with Denoising World Model LearningXiny.pdf
2025-07-19 13:59:06,716 - __main__ - INFO - 成功下载: Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model LearningXinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, Jianyu ChenPaper ID 58Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world’s first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.
2025-07-19 13:59:06,716 - paper_downloader - INFO - 开始下载: Agile But Safe: Learning Collision-Free High-Speed Legged LocomotionTairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya ShiPaper ID 59Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.
2025-07-19 13:59:09,320 - paper_downloader - INFO - 下载完成: Agile But Safe Learning Collision-Free High-Speed Legged LocomotionTairan He, Chong Zhang, Wenli Xia.pdf
2025-07-19 13:59:09,320 - __main__ - INFO - 成功下载: Agile But Safe: Learning Collision-Free High-Speed Legged LocomotionTairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya ShiPaper ID 59Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.
2025-07-19 13:59:09,320 - paper_downloader - INFO - 开始下载: RL2AC: Reinforcement Learning-based Rapid Online Adaptive Control for Legged Robot Robust LocomotionShangke Lyu, Xin Lang, Han Zhao, Hongyin Zhang, Pengxiang Ding, Donglin WangPaper ID 60Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Dynamic fast adaptation is one of the basic capabilities that enables the animals to timely and properly adjust its locomotion reacting to the unpredictable changes. Such capability is also essential for the quadruped robot, when working in the unforseen environment. While reinforcement learning (RL) has achieved a significant progress in locomotion control, rapid adaptation to the model uncertainties remains a challenge. In this paper, we seek to ascertain the control mechanism behind the locomotion RL policy, from which we propose a new RL-based Rapid onLine Adaptive Control (RL2AC) algorithm to complementarily combine the RL policy and the adaptive control together. RL2AC is run at a frequency of 1000Hz without the need for simultaneous training with RL. It presents a strong capability against the external disturbances or the sim-to-real gap, resulting in a robust locomotion, which is achieved through proper torque compensation derived from a novel adaptive controller. Various simulation and experiments have demonstrated the effectiveness of the proposed RL2AC against the heavy load, disturbances acted on one leg, lateral torque, sim-to-real gap and various terrains.
2025-07-19 13:59:14,186 - paper_downloader - INFO - 下载完成: RL2AC Reinforcement Learning-based Rapid Online Adaptive Control for Legged Robot Robust LocomotionS.pdf
2025-07-19 13:59:14,186 - __main__ - INFO - 成功下载: RL2AC: Reinforcement Learning-based Rapid Online Adaptive Control for Legged Robot Robust LocomotionShangke Lyu, Xin Lang, Han Zhao, Hongyin Zhang, Pengxiang Ding, Donglin WangPaper ID 60Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Dynamic fast adaptation is one of the basic capabilities that enables the animals to timely and properly adjust its locomotion reacting to the unpredictable changes. Such capability is also essential for the quadruped robot, when working in the unforseen environment. While reinforcement learning (RL) has achieved a significant progress in locomotion control, rapid adaptation to the model uncertainties remains a challenge. In this paper, we seek to ascertain the control mechanism behind the locomotion RL policy, from which we propose a new RL-based Rapid onLine Adaptive Control (RL2AC) algorithm to complementarily combine the RL policy and the adaptive control together. RL2AC is run at a frequency of 1000Hz without the need for simultaneous training with RL. It presents a strong capability against the external disturbances or the sim-to-real gap, resulting in a robust locomotion, which is achieved through proper torque compensation derived from a novel adaptive controller. Various simulation and experiments have demonstrated the effectiveness of the proposed RL2AC against the heavy load, disturbances acted on one leg, lateral torque, sim-to-real gap and various terrains.
2025-07-19 13:59:14,186 - paper_downloader - INFO - 开始下载: HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and ManipulationCarmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter AbbeelPaper ID 61Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://humanoid- bench.github.io.
2025-07-19 13:59:18,336 - paper_downloader - INFO - 下载完成: HumanoidBench Simulated Humanoid Benchmark for Whole-Body Locomotion and ManipulationCarmelo Sferraz.pdf
2025-07-19 13:59:18,336 - __main__ - INFO - 成功下载: HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and ManipulationCarmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter AbbeelPaper ID 61Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://humanoid- bench.github.io.
2025-07-19 13:59:18,336 - paper_downloader - INFO - 开始下载: MOKA: Open-World Robotic Manipulation through Mark-Based Visual PromptingKuan Fang, Fangchen Liu, Pieter Abbeel, Sergey LevinePaper ID 62Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Open-world generalization requires robotic systems to have a profound understanding of the physical world and the user command to solve diverse and complex tasks. While the recent advancement in vision-language models (VLMs) has offered unprecedented opportunities to solve open-world problems, how to leverage their capabilities to control robots remains a grand challenge. In this paper, we introduce Marking Open-world Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language instructions. Central to our approach is a compact point-based representation of affordance, which bridges the VLM’s predictions on observed images and the robot’s actions in the physical world. By prompting the pre-trained VLM, our approach utilizes the VLM’s commonsense knowledge and concept understanding acquired from broad data sources to predict affordances and generate motions. To facilitate the VLM’s reasoning in zero-shot and few-shot manners, we propose a visual prompting technique that annotates marks on images, converting affordance reasoning into a series of visual question-answering problems that are solvable by the VLM. We further explore methods to enhance performance with robot experiences collected by MOKA through in-context learning and policy distillation. We evaluate and analyze MOKA’s performance on various table-top manipulation tasks including tool use, deformable body manipulation, and object rearrangement.
2025-07-19 13:59:21,444 - paper_downloader - INFO - 下载完成: MOKA Open-World Robotic Manipulation through Mark-Based Visual PromptingKuan Fang, Fangchen Liu, Pie.pdf
2025-07-19 13:59:21,444 - __main__ - INFO - 成功下载: MOKA: Open-World Robotic Manipulation through Mark-Based Visual PromptingKuan Fang, Fangchen Liu, Pieter Abbeel, Sergey LevinePaper ID 62Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Open-world generalization requires robotic systems to have a profound understanding of the physical world and the user command to solve diverse and complex tasks. While the recent advancement in vision-language models (VLMs) has offered unprecedented opportunities to solve open-world problems, how to leverage their capabilities to control robots remains a grand challenge. In this paper, we introduce Marking Open-world Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language instructions. Central to our approach is a compact point-based representation of affordance, which bridges the VLM’s predictions on observed images and the robot’s actions in the physical world. By prompting the pre-trained VLM, our approach utilizes the VLM’s commonsense knowledge and concept understanding acquired from broad data sources to predict affordances and generate motions. To facilitate the VLM’s reasoning in zero-shot and few-shot manners, we propose a visual prompting technique that annotates marks on images, converting affordance reasoning into a series of visual question-answering problems that are solvable by the VLM. We further explore methods to enhance performance with robot experiences collected by MOKA through in-context learning and policy distillation. We evaluate and analyze MOKA’s performance on various table-top manipulation tasks including tool use, deformable body manipulation, and object rearrangement.
2025-07-19 13:59:21,444 - paper_downloader - INFO - 开始下载: Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex ScenesZili Tang, Yuming Feng, Meng GuoPaper ID 63Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arc segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.
2025-07-19 13:59:24,538 - paper_downloader - INFO - 下载完成: Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex ScenesZili Tang, Y.pdf
2025-07-19 13:59:24,538 - __main__ - INFO - 成功下载: Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex ScenesZili Tang, Yuming Feng, Meng GuoPaper ID 63Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arc segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.
2025-07-19 13:59:24,538 - paper_downloader - INFO - 开始下载: AutoMate: Specialist and Generalist Assembly Policies over Diverse GeometriesBingjie Tang, Iretiayo Akinola, Jie Xu, Bowen Wen, Ankur Handa, Karl Van Wyk, Dieter Fox, Gaurav S. Sukhatme, Fabio Ramos, Yashraj NarangPaper ID 64Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Robotic assembly for high-mixture settings requires adaptivity to diverse parts and poses, which is an open challenge. Meanwhile, in other areas of robotics, large models and sim-to-real have led to tremendous progress. Inspired by such work, we present AutoMate, a learning framework and system that consists of 4 parts: 1) a dataset of 100 assemblies compatible with simulation and the real world, along with parallelized simulation environments for policy learning, 2) a novel simulation-based approach for learning specialist (i.e., part-specific) policies and generalist (i.e., unified) assembly policies, 3) demonstrations of specialist policies that individually solve 80 assemblies with ≈80%+ success rates in simulation, as well as a generalist policy that jointly solves 20 assemblies with an 80%+ success rate, and 4) zero-shot sim-to-real transfer that achieves similar (or better) performance than simulation, including on perception-initialized assembly. The key methodological takeaway is that a union of diverse algorithms from manufacturing engineering, character animation, and time-series analysis provides a generic and robust solution for a diverse range of robotic assembly problems.To our knowledge, AutoMate provides the first simulation-based framework for learning specialist and generalist policies over a wide range of assemblies, as well as the first system demonstrating zero-shot sim-to-real transfer over such a range.
2025-07-19 13:59:26,341 - paper_downloader - INFO - 下载完成: AutoMate Specialist and Generalist Assembly Policies over Diverse GeometriesBingjie Tang, Iretiayo A.pdf
2025-07-19 13:59:26,341 - __main__ - INFO - 成功下载: AutoMate: Specialist and Generalist Assembly Policies over Diverse GeometriesBingjie Tang, Iretiayo Akinola, Jie Xu, Bowen Wen, Ankur Handa, Karl Van Wyk, Dieter Fox, Gaurav S. Sukhatme, Fabio Ramos, Yashraj NarangPaper ID 64Session 9. Locomotion and manipulationPoster Session day 2 (Wednesday, July 17)Abstract:Robotic assembly for high-mixture settings requires adaptivity to diverse parts and poses, which is an open challenge. Meanwhile, in other areas of robotics, large models and sim-to-real have led to tremendous progress. Inspired by such work, we present AutoMate, a learning framework and system that consists of 4 parts: 1) a dataset of 100 assemblies compatible with simulation and the real world, along with parallelized simulation environments for policy learning, 2) a novel simulation-based approach for learning specialist (i.e., part-specific) policies and generalist (i.e., unified) assembly policies, 3) demonstrations of specialist policies that individually solve 80 assemblies with ≈80%+ success rates in simulation, as well as a generalist policy that jointly solves 20 assemblies with an 80%+ success rate, and 4) zero-shot sim-to-real transfer that achieves similar (or better) performance than simulation, including on perception-initialized assembly. The key methodological takeaway is that a union of diverse algorithms from manufacturing engineering, character animation, and time-series analysis provides a generic and robust solution for a diverse range of robotic assembly problems.To our knowledge, AutoMate provides the first simulation-based framework for learning specialist and generalist policies over a wide range of assemblies, as well as the first system demonstrating zero-shot sim-to-real transfer over such a range.
2025-07-19 13:59:26,341 - paper_downloader - INFO - 开始下载: An abstract theory of sensor eventificationYulin Zhang, Dylan ShellPaper ID 65Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Unlike traditional cameras, event cameras measure changes in light intensity and report differences. This paper examines the conditions necessary for other traditional sensors to admit eventified versions that provide adequate information despite outputting only changes. The requirements depend upon the regularity of the signal space, which we show may depend on several factors including structure arising from the interplay of the robot and its environment, the input–output computation needed to achieve its task, as well as the specific mode of access (synchronous, asynchronous, polled, triggered). Also, there are further notions of stability (or non-oscillatory behavior) as desiderata. This paper contributes theory and algorithms (plus a hardness result) that addresses these considerations while developing several elementary robot examples along the way.
2025-07-19 13:59:30,471 - paper_downloader - INFO - 下载完成: An abstract theory of sensor eventificationYulin Zhang, Dylan ShellPaper ID 65Session 10. Perception.pdf
2025-07-19 13:59:30,471 - __main__ - INFO - 成功下载: An abstract theory of sensor eventificationYulin Zhang, Dylan ShellPaper ID 65Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Unlike traditional cameras, event cameras measure changes in light intensity and report differences. This paper examines the conditions necessary for other traditional sensors to admit eventified versions that provide adequate information despite outputting only changes. The requirements depend upon the regularity of the signal space, which we show may depend on several factors including structure arising from the interplay of the robot and its environment, the input–output computation needed to achieve its task, as well as the specific mode of access (synchronous, asynchronous, polled, triggered). Also, there are further notions of stability (or non-oscillatory behavior) as desiderata. This paper contributes theory and algorithms (plus a hardness result) that addresses these considerations while developing several elementary robot examples along the way.
2025-07-19 13:59:30,471 - paper_downloader - INFO - 开始下载: Octopi: Object Property Reasoning with Large Tactile-Language ModelsSamson Yu, Lin Kelvin, Anxing Xiao, Jiafei Duan, Harold SohPaper ID 66Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Physical reasoning is important for effective robot manipulation. Recent work has investigated both vision and language modalities for physical reasoning; vision can reveal information about objects in the environment and language serves as an abstraction and communication medium for additional context. Although these works have demonstrated success on a variety of physical reasoning tasks, they are limited to physical properties that can be inferred from visual or language inputs. In this work, we investigate combining tactile perception with language, which enables embodied systems to obtain physical properties through interaction and apply commonsense reasoning. We contribute a new dataset PhysiCLeAR, which comprises both physical/property reasoning tasks and annotated tactile videos obtained using a GelSight tactile sensor. We then introduce Octopi, a system that leverages both tactile representation learning and large vision-language models to predict and reason about tactile inputs with minimal language fine-tuning. Our evaluations on PhysiCLeAR show that Octopi is able to effectively use intermediate physical property predictions to improve its performance on various tactile-related tasks. PhysiCLeAR and Octopi are available at https://github.com/clear-nus/octopi.
2025-07-19 13:59:31,563 - paper_downloader - INFO - 下载完成: Octopi Object Property Reasoning with Large Tactile-Language ModelsSamson Yu, Lin Kelvin, Anxing Xia.pdf
2025-07-19 13:59:31,563 - __main__ - INFO - 成功下载: Octopi: Object Property Reasoning with Large Tactile-Language ModelsSamson Yu, Lin Kelvin, Anxing Xiao, Jiafei Duan, Harold SohPaper ID 66Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Physical reasoning is important for effective robot manipulation. Recent work has investigated both vision and language modalities for physical reasoning; vision can reveal information about objects in the environment and language serves as an abstraction and communication medium for additional context. Although these works have demonstrated success on a variety of physical reasoning tasks, they are limited to physical properties that can be inferred from visual or language inputs. In this work, we investigate combining tactile perception with language, which enables embodied systems to obtain physical properties through interaction and apply commonsense reasoning. We contribute a new dataset PhysiCLeAR, which comprises both physical/property reasoning tasks and annotated tactile videos obtained using a GelSight tactile sensor. We then introduce Octopi, a system that leverages both tactile representation learning and large vision-language models to predict and reason about tactile inputs with minimal language fine-tuning. Our evaluations on PhysiCLeAR show that Octopi is able to effectively use intermediate physical property predictions to improve its performance on various tactile-related tasks. PhysiCLeAR and Octopi are available at https://github.com/clear-nus/octopi.
2025-07-19 13:59:31,563 - paper_downloader - INFO - 开始下载: 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D RepresentationsYanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe XuPaper ID 67Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Code and videos are available on https://3d-diffusion-policy.github.io .
2025-07-19 13:59:34,872 - paper_downloader - INFO - 下载完成: 3D Diffusion Policy Generalizable Visuomotor Policy Learning via Simple 3D RepresentationsYanjie Ze,.pdf
2025-07-19 13:59:34,872 - __main__ - INFO - 成功下载: 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D RepresentationsYanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe XuPaper ID 67Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Code and videos are available on https://3d-diffusion-policy.github.io .
2025-07-19 13:59:34,873 - paper_downloader - INFO - 开始下载: HRP: Human affordances for Robotic Pre-trainingMohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, Abhinav GuptaPaper ID 68Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:In order togeneralizeto various tasks in the wild, robotic agents will need a suitable representation (i.e., vision network) that enables the robot to predict optimal actions given high dimensional vision inputs. However, learning such a representation requires an extreme amount of diverse training data, which is prohibitively expensive to collect on a real robot. How can we overcome this problem? Instead of collecting more robot data, this paper proposes using internet-scale, human videos to extract “affordances,” both at the environment and agent level, and distill them into a pre-trained representation. We present a simple framework for pre-training representations on hand, object, and contact “affordance labels” that highlight relevant objects in images and how to interact with them. These affordances are automatically extracted from human video data (with the help of off-the-shelf computer vision modules) and used to fine-tune existing representations. Our approach can efficiently fine-tuneanyexisting representation, and results in models with stronger downstream robotic performance across the board. We experimentally demonstrate (using 3000+ robot trials) that this affordance pre-training scheme boosts performance by a minimum of 15% on 5 real-world tasks, which consider three diverse robot morphologies (including a dexterous hand). Unlike prior works in the space, these representations improve performance across 3 different camera views. Quantitatively, we find that our approach leads to higher levels of generalization in out-of-distribution settings. Videos of our final policies and all code/weights/data can be found on our website: https://www.cs.cmu.edu/~data4robotics/hrp/
2025-07-19 13:59:35,931 - paper_downloader - INFO - 下载完成: HRP Human affordances for Robotic Pre-trainingMohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, Abhi.pdf
2025-07-19 13:59:35,931 - __main__ - INFO - 成功下载: HRP: Human affordances for Robotic Pre-trainingMohan Kumar Srirama, Sudeep Dasari, Shikhar Bahl, Abhinav GuptaPaper ID 68Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:In order togeneralizeto various tasks in the wild, robotic agents will need a suitable representation (i.e., vision network) that enables the robot to predict optimal actions given high dimensional vision inputs. However, learning such a representation requires an extreme amount of diverse training data, which is prohibitively expensive to collect on a real robot. How can we overcome this problem? Instead of collecting more robot data, this paper proposes using internet-scale, human videos to extract “affordances,” both at the environment and agent level, and distill them into a pre-trained representation. We present a simple framework for pre-training representations on hand, object, and contact “affordance labels” that highlight relevant objects in images and how to interact with them. These affordances are automatically extracted from human video data (with the help of off-the-shelf computer vision modules) and used to fine-tune existing representations. Our approach can efficiently fine-tuneanyexisting representation, and results in models with stronger downstream robotic performance across the board. We experimentally demonstrate (using 3000+ robot trials) that this affordance pre-training scheme boosts performance by a minimum of 15% on 5 real-world tasks, which consider three diverse robot morphologies (including a dexterous hand). Unlike prior works in the space, these representations improve performance across 3 different camera views. Quantitatively, we find that our approach leads to higher levels of generalization in out-of-distribution settings. Videos of our final policies and all code/weights/data can be found on our website: https://www.cs.cmu.edu/~data4robotics/hrp/
2025-07-19 13:59:35,931 - paper_downloader - INFO - 开始下载: MIRAGE: Cross-Embodiment Zero-Shot Policy Transfer with Cross-PaintingLawrence Yunliang Chen, Karthik Dharmarajan, Kush Hari, Chenfeng Xu, Quan Vuong, Ken GoldbergPaper ID 69Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer. Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses “cross-painting”—masking out the unseen target robot and inpainting the seen source robot—during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Mirage applies to both first-person and third-person camera views and policies that take in both states and images as inputs or only images as inputs. Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy.
2025-07-19 13:59:39,533 - paper_downloader - INFO - 下载完成: MIRAGE Cross-Embodiment Zero-Shot Policy Transfer with Cross-PaintingLawrence Yunliang Chen, Karthik.pdf
2025-07-19 13:59:39,533 - __main__ - INFO - 成功下载: MIRAGE: Cross-Embodiment Zero-Shot Policy Transfer with Cross-PaintingLawrence Yunliang Chen, Karthik Dharmarajan, Kush Hari, Chenfeng Xu, Quan Vuong, Ken GoldbergPaper ID 69Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer. Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses “cross-painting”—masking out the unseen target robot and inpainting the seen source robot—during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Mirage applies to both first-person and third-person camera views and policies that take in both states and images as inputs or only images as inputs. Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy.
2025-07-19 13:59:39,533 - paper_downloader - INFO - 开始下载: Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in CluttersYitong Li, Ruihai Wu, Haoran Lu, Chuanruo Ning, Yan Shen, Guanqi Zhan, Hao DongPaper ID 70Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink. Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step. However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between. In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method.
2025-07-19 13:59:40,336 - paper_downloader - INFO - 下载完成: Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in CluttersYiton.pdf
2025-07-19 13:59:40,336 - __main__ - INFO - 成功下载: Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in CluttersYitong Li, Ruihai Wu, Haoran Lu, Chuanruo Ning, Yan Shen, Guanqi Zhan, Hao DongPaper ID 70Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:In our daily life, cluttered objects are everywhere, from scattered stationery and books cluttering the table to bowls and plates filling the kitchen sink. Retrieving a target object from clutters is an essential while challenging skill for robots, for the difficulty of safely manipulating an object without disturbing others, which requires the robot to plan a manipulation sequence and first move away a few other objects supported by the target object step by step. However, due to the diversity of object configurations (e.g., categories, geometries, locations and poses) and their combinations in clutters, it is difficult for a robot to accurately infer the support relations between objects faraway with various objects in between. In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. Experiments in both simulation and the real world demonstrate the efficiency and effectiveness of our method.
2025-07-19 13:59:40,336 - paper_downloader - INFO - 开始下载: Consistency Policy: Accelerated Visuomotor Policies via Consistency DistillationAaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, Jeannette BohgPaper ID 71Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy’s learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as three real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy’s quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.
2025-07-19 13:59:41,440 - paper_downloader - INFO - 下载完成: Consistency Policy Accelerated Visuomotor Policies via Consistency DistillationAaditya Prasad, Kevin.pdf
2025-07-19 13:59:41,440 - __main__ - INFO - 成功下载: Consistency Policy: Accelerated Visuomotor Policies via Consistency DistillationAaditya Prasad, Kevin Lin, Jimmy Wu, Linqi Zhou, Jeannette BohgPaper ID 71Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy’s learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as three real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy’s quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.
2025-07-19 13:59:41,440 - paper_downloader - INFO - 开始下载: CLOSURE: Fast Quantification of Pose Uncertainty SetsYihuai Gao, Yukai Tang, Han Qi, Heng YangPaper ID 72Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:We investigate uncertainty quantification of 6D pose estimation from learned noisy measurements (e.g., keypoints and pose hypotheses). Assuming unknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D poses compatible with the measurements. Despite being simple to formulate and its ability to embed uncertainty, the PURSE is difficult to manipulate and interpret due to the many abstract nonconvex polynomial constraints defining it. An appealing simplification of PURSE–motivated by the bounded state estimation error assumption in robust control– is to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound. We contribute (i) a geometric interpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner approximate the MEGB. Particularly, we show the PURSE corresponds to the feasible set of a constrained dynamical system or the intersection of multiple geodesic balls, and this perspective allows us to design an algorithm to densely sample the boundary of the PURSE through strategic random walks that are efficiently parallelizable on a GPU. We then use the miniball algorithm by Gärtner (1999) to compute the MEGB of PURSE samples, leading to an inner approximation of the true MEGB. Our algorithm is named CLOSURE (enClosing baLl frOm purSe boUndaRy samplEs) and it enables computing a certificate of approximation tightness by calculating the relative ratio between the size of the inner approximation and the size of the outer approximation GRCC from Tang, Lasserre, and Yang (2023). Running on a single RTX 3090 GPU, CLOSURE achieves the relative ratio of 92.8% on the LM-O object pose estimation dataset, 91.4% on the 3DMatch point cloud registration dataset and 96.6% on the LM object pose estimation dataset with an average runtime below 0.3 seconds. Obtaining comparable worst-case error bound but 398×, 833× and 23.6× faster than the outer approximation GRCC, CLOSURE enables uncertainty quantification of 6D pose estimation to be implemented in real-time robot perception applications.
2025-07-19 13:59:42,255 - paper_downloader - INFO - 下载完成: CLOSURE Fast Quantification of Pose Uncertainty SetsYihuai Gao, Yukai Tang, Han Qi, Heng YangPaper I.pdf
2025-07-19 13:59:42,255 - __main__ - INFO - 成功下载: CLOSURE: Fast Quantification of Pose Uncertainty SetsYihuai Gao, Yukai Tang, Han Qi, Heng YangPaper ID 72Session 10. PerceptionPoster Session day 2 (Wednesday, July 17)Abstract:We investigate uncertainty quantification of 6D pose estimation from learned noisy measurements (e.g., keypoints and pose hypotheses). Assuming unknown-but-bounded measurement noises, a pose uncertainty set (PURSE) is a subset of SE(3) that contains all possible 6D poses compatible with the measurements. Despite being simple to formulate and its ability to embed uncertainty, the PURSE is difficult to manipulate and interpret due to the many abstract nonconvex polynomial constraints defining it. An appealing simplification of PURSE–motivated by the bounded state estimation error assumption in robust control– is to find its minimum enclosing geodesic ball (MEGB), i.e., a point pose estimation with minimum worst-case error bound. We contribute (i) a geometric interpretation of the nonconvex PURSE, and (ii) a fast algorithm to inner approximate the MEGB. Particularly, we show the PURSE corresponds to the feasible set of a constrained dynamical system or the intersection of multiple geodesic balls, and this perspective allows us to design an algorithm to densely sample the boundary of the PURSE through strategic random walks that are efficiently parallelizable on a GPU. We then use the miniball algorithm by Gärtner (1999) to compute the MEGB of PURSE samples, leading to an inner approximation of the true MEGB. Our algorithm is named CLOSURE (enClosing baLl frOm purSe boUndaRy samplEs) and it enables computing a certificate of approximation tightness by calculating the relative ratio between the size of the inner approximation and the size of the outer approximation GRCC from Tang, Lasserre, and Yang (2023). Running on a single RTX 3090 GPU, CLOSURE achieves the relative ratio of 92.8% on the LM-O object pose estimation dataset, 91.4% on the 3DMatch point cloud registration dataset and 96.6% on the LM object pose estimation dataset with an average runtime below 0.3 seconds. Obtaining comparable worst-case error bound but 398×, 833× and 23.6× faster than the outer approximation GRCC, CLOSURE enables uncertainty quantification of 6D pose estimation to be implemented in real-time robot perception applications.
2025-07-19 13:59:42,255 - paper_downloader - INFO - 开始下载: GOAT: GO to Any ThingMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, Devendra Singh ChaplotPaper ID 73Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:In deployment scenarios such as homes and warehouses, mobile robots are expected to autonomously navigate for extended periods, seamlessly executing tasks articulated in terms that are intuitively understandable by human operators.
 We present GO To Any Thing (GOAT), a universal navigation system capable of tackling these requirements with three key features: a) Multimodal: it can tackle goals specified via category labels, target images, and language descriptions, b) Lifelong: it benefits from its past experience in the same environment, and c) Platform Agnostic: it can be quickly deployed on robots with different embodiments. 
 GOAT is made possible through a modular system design and a continually augmented instance-aware semantic memory that keeps track of the appearance of objects from different viewpoints in addition to category-level semantics.
 This enables GOAT to distinguish between different instances of the same category to enable navigation to targets specified by images and language descriptions.
 In experimental comparisons spanning over 90 hours in 9 different homes consisting of 675 goals selected across 200+ different object instances, we find GOAT achieves an overall success rate of 83%, surpassing previous methods and ablations by 32% (absolute improvement). 
 GOAT improves with experience in the environment, from a 60% success rate at the first goal to a 90% success after exploration. 
 In addition, we demonstrate that GOAT can readily be applied to downstream tasks such as pick and place and social navigation.
2025-07-19 13:59:44,349 - paper_downloader - INFO - 下载完成: GOAT GO to Any ThingMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So.pdf
2025-07-19 13:59:44,350 - __main__ - INFO - 成功下载: GOAT: GO to Any ThingMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, Devendra Singh ChaplotPaper ID 73Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:In deployment scenarios such as homes and warehouses, mobile robots are expected to autonomously navigate for extended periods, seamlessly executing tasks articulated in terms that are intuitively understandable by human operators.
 We present GO To Any Thing (GOAT), a universal navigation system capable of tackling these requirements with three key features: a) Multimodal: it can tackle goals specified via category labels, target images, and language descriptions, b) Lifelong: it benefits from its past experience in the same environment, and c) Platform Agnostic: it can be quickly deployed on robots with different embodiments. 
 GOAT is made possible through a modular system design and a continually augmented instance-aware semantic memory that keeps track of the appearance of objects from different viewpoints in addition to category-level semantics.
 This enables GOAT to distinguish between different instances of the same category to enable navigation to targets specified by images and language descriptions.
 In experimental comparisons spanning over 90 hours in 9 different homes consisting of 675 goals selected across 200+ different object instances, we find GOAT achieves an overall success rate of 83%, surpassing previous methods and ablations by 32% (absolute improvement). 
 GOAT improves with experience in the environment, from a 60% success rate at the first goal to a 90% success after exploration. 
 In addition, we demonstrate that GOAT can readily be applied to downstream tasks such as pick and place and social navigation.
2025-07-19 13:59:44,350 - paper_downloader - INFO - 开始下载: Demonstrating Arena 3.0: Advancing Social Navigation in Collaborative and Highly Dynamic EnvironmentsLinh Kästner, Volodymyir Shcherbyna, Huajian Zeng, Tuan Anh Le, Maximilian Ho-Kyung Schreff, Halid Osmaev, Nam Truong Tran, Diego Diaz, Jan Golebiowski, Harold Soh, Jens LambrechtPaper ID 74Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Building upon our previous contributions, this
 paper introduces Arena 3.0, an extension of Arena-Bench,
 Arena 1.0, and Arena 2.0 focusing on the development,
 simulation, and benchmarking of social navigation approaches
 in collaborative environments. We significantly enhance the re-
 alism of human behavior simulation by incorporating a diverse
 array of new social force models and interaction patterns,
 encompassing both human-human and human-robot dynam-
 ics. The platform provides a comprehensive set of new task
 modes, designed for extensive benchmarking and testing and is
 capable of generating realistic and human-centric environments
 dynamically, catering to a broad spectrum of social navigation
 scenarios. In addition, the platform’s functionalities have been
 abstracted across three widely used simulators, each tailored
 for specific training and testing purposes. The platform’s
 efficacy has been validated through an extensive benchmark
 and user evaluations of the platform by a global community of
 researchers and students, which noted the substantial improve-
 ment compared to previous versions and expressed interests to
 utilize the platform for future research and development. Arena
 3.0 is openly available at https://github.com/Arena-Rosnav.
2025-07-19 13:59:45,900 - paper_downloader - INFO - 下载完成: Demonstrating Arena 3.0 Advancing Social Navigation in Collaborative and Highly Dynamic Environments.pdf
2025-07-19 13:59:45,900 - __main__ - INFO - 成功下载: Demonstrating Arena 3.0: Advancing Social Navigation in Collaborative and Highly Dynamic EnvironmentsLinh Kästner, Volodymyir Shcherbyna, Huajian Zeng, Tuan Anh Le, Maximilian Ho-Kyung Schreff, Halid Osmaev, Nam Truong Tran, Diego Diaz, Jan Golebiowski, Harold Soh, Jens LambrechtPaper ID 74Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Building upon our previous contributions, this
 paper introduces Arena 3.0, an extension of Arena-Bench,
 Arena 1.0, and Arena 2.0 focusing on the development,
 simulation, and benchmarking of social navigation approaches
 in collaborative environments. We significantly enhance the re-
 alism of human behavior simulation by incorporating a diverse
 array of new social force models and interaction patterns,
 encompassing both human-human and human-robot dynam-
 ics. The platform provides a comprehensive set of new task
 modes, designed for extensive benchmarking and testing and is
 capable of generating realistic and human-centric environments
 dynamically, catering to a broad spectrum of social navigation
 scenarios. In addition, the platform’s functionalities have been
 abstracted across three widely used simulators, each tailored
 for specific training and testing purposes. The platform’s
 efficacy has been validated through an extensive benchmark
 and user evaluations of the platform by a global community of
 researchers and students, which noted the substantial improve-
 ment compared to previous versions and expressed interests to
 utilize the platform for future research and development. Arena
 3.0 is openly available at https://github.com/Arena-Rosnav.
2025-07-19 13:59:45,900 - paper_downloader - INFO - 开始下载: RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Multi-Modal Large Language Model LearningJianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew GaddPaper ID 75Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving.
 By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.
2025-07-19 13:59:49,286 - paper_downloader - INFO - 下载完成: RAG-Driver Generalisable Driving Explanations with Retrieval-Augmented In-Context Multi-Modal Large .pdf
2025-07-19 13:59:49,286 - __main__ - INFO - 成功下载: RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Multi-Modal Large Language Model LearningJianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew GaddPaper ID 75Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving.
 By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.
2025-07-19 13:59:49,287 - paper_downloader - INFO - 开始下载: Dynamic Adversarial Attacks on Autonomous Driving SystemsAmirhosein Chahe, Chenan Wang, Abhishek Jeyapratap, Kaidi Xu, Lifeng ZhouPaper ID 76Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper introduces an attacking mechanism to challenge the resilience of autonomous driving systems. Specifically, we manipulate the decision-making processes of an autonomous vehicle by dynamically displaying adversarial patches on a screen mounted on another moving vehicle. These patches are optimized to deceive the object detection models into misclassifying targeted objects, e.g., traffic signs. Such manipulation has significant implications for critical multi-vehicle interactions such as intersection crossing, which are vital for safe and efficient autonomous driving systems. 
 Particularly, we make four major contributions. First, we introduce a novel adversarial attack approach where the patch is not co-located with its target, enabling more versatile and stealthy attacks. Moreover, our method utilizes dynamic patches displayed on a screen, allowing for adaptive changes and movements, enhancing the flexibility and performance of the attack. To do so, we design a Screen Image Transformation Network (SIT-Net), which simulates environmental effects on the displayed images, narrowing the gap between simulated and real-world scenarios. Further, we integrate a positional loss term into the adversarial training process to increase the success rate of the dynamic attack. Finally, we shift the focus from merely attacking perceptual systems to influencing the decision-making algorithms of self-driving systems. Our experiments demonstrate the first successful implementation of such dynamic adversarial attacks in real-world autonomous driving scenarios, paving the way for advancements in the field of robust and secure autonomous driving.
2025-07-19 13:59:51,151 - paper_downloader - INFO - 下载完成: Dynamic Adversarial Attacks on Autonomous Driving SystemsAmirhosein Chahe, Chenan Wang, Abhishek Jey.pdf
2025-07-19 13:59:51,151 - __main__ - INFO - 成功下载: Dynamic Adversarial Attacks on Autonomous Driving SystemsAmirhosein Chahe, Chenan Wang, Abhishek Jeyapratap, Kaidi Xu, Lifeng ZhouPaper ID 76Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper introduces an attacking mechanism to challenge the resilience of autonomous driving systems. Specifically, we manipulate the decision-making processes of an autonomous vehicle by dynamically displaying adversarial patches on a screen mounted on another moving vehicle. These patches are optimized to deceive the object detection models into misclassifying targeted objects, e.g., traffic signs. Such manipulation has significant implications for critical multi-vehicle interactions such as intersection crossing, which are vital for safe and efficient autonomous driving systems. 
 Particularly, we make four major contributions. First, we introduce a novel adversarial attack approach where the patch is not co-located with its target, enabling more versatile and stealthy attacks. Moreover, our method utilizes dynamic patches displayed on a screen, allowing for adaptive changes and movements, enhancing the flexibility and performance of the attack. To do so, we design a Screen Image Transformation Network (SIT-Net), which simulates environmental effects on the displayed images, narrowing the gap between simulated and real-world scenarios. Further, we integrate a positional loss term into the adversarial training process to increase the success rate of the dynamic attack. Finally, we shift the focus from merely attacking perceptual systems to influencing the decision-making algorithms of self-driving systems. Our experiments demonstrate the first successful implementation of such dynamic adversarial attacks in real-world autonomous driving scenarios, paving the way for advancements in the field of robust and secure autonomous driving.
2025-07-19 13:59:51,151 - paper_downloader - INFO - 开始下载: Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot NavigationAbdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram BurgardPaper ID 77Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded indoor robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-story environments. We provide code and trial video data at: https://hovsg.github.io.
2025-07-19 13:59:53,981 - paper_downloader - INFO - 下载完成: Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot NavigationAbdelrhman Werby,.pdf
2025-07-19 13:59:53,981 - __main__ - INFO - 成功下载: Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot NavigationAbdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram BurgardPaper ID 77Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded indoor robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-story environments. We provide code and trial video data at: https://hovsg.github.io.
2025-07-19 13:59:53,981 - paper_downloader - INFO - 开始下载: ScrewMimic: Bimanual Imitation from Human Videos with Screw Space ProjectionArpit Bahety, Priyanka Mandikal, Ben Abbatematteo, Roberto Martín-MartínPaper ID 78Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage — as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce SCREWMIMIC, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that SCREWMIMIC is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.
2025-07-19 14:00:05,956 - paper_downloader - INFO - 下载完成: ScrewMimic Bimanual Imitation from Human Videos with Screw Space ProjectionArpit Bahety, Priyanka Ma.pdf
2025-07-19 14:00:05,956 - __main__ - INFO - 成功下载: ScrewMimic: Bimanual Imitation from Human Videos with Screw Space ProjectionArpit Bahety, Priyanka Mandikal, Ben Abbatematteo, Roberto Martín-MartínPaper ID 78Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Bimanual manipulation is a longstanding challenge in robotics due to the large number of degrees of freedom and the strict spatial and temporal synchronization required to generate meaningful behavior. Humans learn bimanual manipulation skills by watching other humans and by refining their abilities through play. In this work, we aim to enable robots to learn bimanual manipulation behaviors from human video demonstrations and fine-tune them through interaction. Inspired by seminal work in psychology and biomechanics, we propose modeling the interaction between two hands as a serial kinematic linkage — as a screw motion, in particular, that we use to define a new action space for bimanual manipulation: screw actions. We introduce SCREWMIMIC, a framework that leverages this novel action representation to facilitate learning from human demonstration and self-supervised policy fine-tuning. Our experiments demonstrate that SCREWMIMIC is able to learn several complex bimanual behaviors from a single human video demonstration, and that it outperforms baselines that interpret demonstrations and fine-tune directly in the original space of motion of both arms.
2025-07-19 14:00:05,956 - paper_downloader - INFO - 开始下载: NaVid: Video-based VLM Plans the Next Step for Vision-and-Language NavigationJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He WangPaper ID 79Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.
2025-07-19 14:00:11,090 - paper_downloader - INFO - 下载完成: NaVid Video-based VLM Plans the Next Step for Vision-and-Language NavigationJiazhao Zhang, Kunyu Wan.pdf
2025-07-19 14:00:11,090 - __main__ - INFO - 成功下载: NaVid: Video-based VLM Plans the Next Step for Vision-and-Language NavigationJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, He WangPaper ID 79Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.
2025-07-19 14:00:11,090 - paper_downloader - INFO - 开始下载: RACER: Epistemic Risk-Sensitive RL Enables Fast Driving with Fewer CrashesKyle Stachowicz, Sergey LevinePaper ID 80Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction. However, this requires addressing real-world constraints, including avoidance of catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy. In many robotics settings, this amounts to avoiding certain “unsafe” states. The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of “safe” states, and therefore places a particular burden on the method to avoid frequent failures.To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum. We propose a reinforcement learning objective that uses a risk-sensitive metric to jointly train a policy and iteratively expand action bounds during training, starting with a low-speed policy and slowly increasing the speed over time. Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty.We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task. We show that our method greatly reduces the number of safety violations during the training process, and actually leads to better final high-speed driving policies at the end of training.
2025-07-19 14:00:12,424 - paper_downloader - INFO - 下载完成: RACER Epistemic Risk-Sensitive RL Enables Fast Driving with Fewer CrashesKyle Stachowicz, Sergey Lev.pdf
2025-07-19 14:00:12,424 - __main__ - INFO - 成功下载: RACER: Epistemic Risk-Sensitive RL Enables Fast Driving with Fewer CrashesKyle Stachowicz, Sergey LevinePaper ID 80Session 11. NavigationPoster Session day 2 (Wednesday, July 17)Abstract:Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction. However, this requires addressing real-world constraints, including avoidance of catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy. In many robotics settings, this amounts to avoiding certain “unsafe” states. The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of “safe” states, and therefore places a particular burden on the method to avoid frequent failures.To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum. We propose a reinforcement learning objective that uses a risk-sensitive metric to jointly train a policy and iteratively expand action bounds during training, starting with a low-speed policy and slowly increasing the speed over time. Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty.We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task. We show that our method greatly reduces the number of safety violations during the training process, and actually leads to better final high-speed driving policies at the end of training.
2025-07-19 14:00:12,425 - paper_downloader - INFO - 开始下载: Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic EnvironmentsLukas Schmid, Marcus Abate, Yun Chang, Luca CarlonePaper ID 81Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy. While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment. A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term reasoning, such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics. To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently. We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor graph formulation. We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to 
 construct a dense spatio-temporal map in real-time. We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics. We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments.
2025-07-19 14:00:15,491 - paper_downloader - INFO - 下载完成: Khronos A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic EnvironmentsLukas Sch.pdf
2025-07-19 14:00:15,491 - __main__ - INFO - 成功下载: Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic EnvironmentsLukas Schmid, Marcus Abate, Yun Chang, Luca CarlonePaper ID 81Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy. While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment. A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term reasoning, such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics. To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently. We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor graph formulation. We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to 
 construct a dense spatio-temporal map in real-time. We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics. We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments.
2025-07-19 14:00:15,491 - paper_downloader - INFO - 开始下载: Demonstrating Agile Flight from Pixels without State EstimationIsmail Geles, Leonard Bauersfeld, Angel Romero, Jiaxu Xing, Davide ScaramuzzaPaper ID 82Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Quadrotors are among the most agile flying robots. Despite recent advances in learning-based control and computer vision, autonomous drones still rely on explicit state estimation. On the other hand, human pilots only rely on a first-person-view video stream from the drone onboard camera to push the platform to its limits and fly robustly in unseen environments. To the best of our knowledge, we present the first vision-based quadrotor system that autonomously navigates through a sequence of gates at high speeds while directly mapping pixels to control commands. Like professional drone-racing pilots, our system does not use explicit state estimation and leverages the same control commands humans use (collective thrust and body rates). We demonstrate agile flight at speeds up to 40km/h with accelerations up to 2g. This is achieved by training vision-based policies with reinforcement learning (RL). The training is facilitated using an asymmetric actor-critic with access to privileged information. To overcome the computational complexity during image-based RL training, we use the inner edges of the gates as a sensor abstraction. This simple yet robust, task-relevant representation can be simulated during training without rendering images. During deployment, a Swin-transformer-based gate detector is used.
 Our approach enables autonomous agile flight with standard, off-the-shelf hardware. 
 Although our demonstration focuses on drone racing, we believe that our method has an impact beyond drone racing and can serve as a foundation for future research into real-world applications in structured environments.
2025-07-19 14:00:18,121 - paper_downloader - INFO - 下载完成: Demonstrating Agile Flight from Pixels without State EstimationIsmail Geles, Leonard Bauersfeld, Ang.pdf
2025-07-19 14:00:18,121 - __main__ - INFO - 成功下载: Demonstrating Agile Flight from Pixels without State EstimationIsmail Geles, Leonard Bauersfeld, Angel Romero, Jiaxu Xing, Davide ScaramuzzaPaper ID 82Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Quadrotors are among the most agile flying robots. Despite recent advances in learning-based control and computer vision, autonomous drones still rely on explicit state estimation. On the other hand, human pilots only rely on a first-person-view video stream from the drone onboard camera to push the platform to its limits and fly robustly in unseen environments. To the best of our knowledge, we present the first vision-based quadrotor system that autonomously navigates through a sequence of gates at high speeds while directly mapping pixels to control commands. Like professional drone-racing pilots, our system does not use explicit state estimation and leverages the same control commands humans use (collective thrust and body rates). We demonstrate agile flight at speeds up to 40km/h with accelerations up to 2g. This is achieved by training vision-based policies with reinforcement learning (RL). The training is facilitated using an asymmetric actor-critic with access to privileged information. To overcome the computational complexity during image-based RL training, we use the inner edges of the gates as a sensor abstraction. This simple yet robust, task-relevant representation can be simulated during training without rendering images. During deployment, a Swin-transformer-based gate detector is used.
 Our approach enables autonomous agile flight with standard, off-the-shelf hardware. 
 Although our demonstration focuses on drone racing, we believe that our method has an impact beyond drone racing and can serve as a foundation for future research into real-world applications in structured environments.
2025-07-19 14:00:18,122 - paper_downloader - INFO - 开始下载: You’ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property PredictionParker Ewen, Hao Chen, Yuzhen Chen, Anran Li, Anup Bagali, Gitesh Gunjal, Ram VasudevanPaper ID 83Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical proper- ties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies as well as the open-source C++ and ROS interface can be found at https://roahmlab.github.io/multimodal_mapping/.
2025-07-19 14:00:20,175 - paper_downloader - INFO - 下载完成: You’ve Got to Feel It To Believe It Multi-Modal Bayesian Inference for Semantic and Property Predict.pdf
2025-07-19 14:00:20,175 - __main__ - INFO - 成功下载: You’ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property PredictionParker Ewen, Hao Chen, Yuzhen Chen, Anran Li, Anup Bagali, Gitesh Gunjal, Ram VasudevanPaper ID 83Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical proper- ties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies as well as the open-source C++ and ROS interface can be found at https://roahmlab.github.io/multimodal_mapping/.
2025-07-19 14:00:20,175 - paper_downloader - INFO - 开始下载: AnyFeature-VSLAM: Automating the Usage of Any Feature into Visual SLAMAlejandro Fontan, Javier Civera, Michael MilfordPaper ID 84Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Feature-based SLAM heavily relies on the specific type of visual features employed. The most effective feature in some conditions may perform worse or not be suitable for other ones, leading to significant performance variability. Seamlessly switching to the most effective visual feature is a desirable quality for SLAM, but, currently, this involves a cumbersome manual task that demands substantial parameter tuning efforts and expert knowledge.In this paper, we present AnyFeature-VSLAM, an automated visual SLAM pipeline capable of switching to a chosen type of feature effortlessly and without manual intervention. The tuning of parameters associated with visual features is performed automatically to achieve the best performance. We built AnyFeature-VSLAM on top of ORB-SLAM2, one of the most popular and widely used feature-based visual SLAM implementations. Through extensive experiments across various benchmark datasets, we demonstrate that AnyFeature-VSLAM consistently delivers good results irrespective of the chosen visual feature, outperforming baseline implementations. Specifically, our paper includes a quantitative assessment of trajectory estimation involving seven different keypoint and descriptor combinations across thirty sequences spanning four distinct publicly available datasets. Furthermore, we showcase the enhanced flexibility of our system by subjecting it to four additional challenging datasets. Code publicly available at: https://github.com/alejandrofontan/AnyFeature-VSLAM.
2025-07-19 14:00:24,557 - paper_downloader - INFO - 下载完成: AnyFeature-VSLAM Automating the Usage of Any Feature into Visual SLAMAlejandro Fontan, Javier Civera.pdf
2025-07-19 14:00:24,557 - __main__ - INFO - 成功下载: AnyFeature-VSLAM: Automating the Usage of Any Feature into Visual SLAMAlejandro Fontan, Javier Civera, Michael MilfordPaper ID 84Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Feature-based SLAM heavily relies on the specific type of visual features employed. The most effective feature in some conditions may perform worse or not be suitable for other ones, leading to significant performance variability. Seamlessly switching to the most effective visual feature is a desirable quality for SLAM, but, currently, this involves a cumbersome manual task that demands substantial parameter tuning efforts and expert knowledge.In this paper, we present AnyFeature-VSLAM, an automated visual SLAM pipeline capable of switching to a chosen type of feature effortlessly and without manual intervention. The tuning of parameters associated with visual features is performed automatically to achieve the best performance. We built AnyFeature-VSLAM on top of ORB-SLAM2, one of the most popular and widely used feature-based visual SLAM implementations. Through extensive experiments across various benchmark datasets, we demonstrate that AnyFeature-VSLAM consistently delivers good results irrespective of the chosen visual feature, outperforming baseline implementations. Specifically, our paper includes a quantitative assessment of trajectory estimation involving seven different keypoint and descriptor combinations across thirty sequences spanning four distinct publicly available datasets. Furthermore, we showcase the enhanced flexibility of our system by subjecting it to four additional challenging datasets. Code publicly available at: https://github.com/alejandrofontan/AnyFeature-VSLAM.
2025-07-19 14:00:24,557 - paper_downloader - INFO - 开始下载: iMESA: Incremental Distributed Optimization for Collaborative Simultaneous Localization and MappingDaniel McGann, Michael KaessPaper ID 85Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM). For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication. Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals. To address this capability gap we present Incremental on Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots. Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends.
2025-07-19 14:00:25,086 - paper_downloader - INFO - 下载完成: iMESA Incremental Distributed Optimization for Collaborative Simultaneous Localization and MappingDa.pdf
2025-07-19 14:00:25,086 - __main__ - INFO - 成功下载: iMESA: Incremental Distributed Optimization for Collaborative Simultaneous Localization and MappingDaniel McGann, Michael KaessPaper ID 85Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM). For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication. Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals. To address this capability gap we present Incremental on Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots. Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends.
2025-07-19 14:00:25,086 - paper_downloader - INFO - 开始下载: Scalable Distance-based Multi-Agent Relative State Estimation via Block Multiconvex OptimizationTianyue Wu, Fei GaoPaper ID 86Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper explores the distance-based relative state estimation problem in large-scale systems, which is hard to solve effectively due to its high-dimensionality and non-convexity. In this paper, we alleviate this inherent hardness to simultaneously achieve scalability and robustness of inference on this problem. Our idea is launched from a universal geometric formulation, called generalized graph realization, for the distance-based relative state estimation problem. Based on this formulation, we introduce two collaborative optimization models, one of which is convex and thus globally solvable, and the other enables fast searching on non-convex landscapes to refine the solution offered by the convex one. Importantly, both models enjoy multiconvex and decomposable structures, allowing efficient and safe solutions using block coordinate descent that enjoys scalability and a distributed nature. The proposed algorithms collaborate to demonstrate superior or comparable solution precision to the current centralized convex relaxation-based methods, which are known for their high optimality. Distinctly, the proposed methods demonstrate scalability and unique computational efficiency beyond the reach of previous convex relaxation-based methods. We also demonstrate that the combination of the two proposed algorithms achieves a more robust pipeline than deploying the local search method alone in a continuous-time scenario.
2025-07-19 14:00:27,634 - paper_downloader - INFO - 下载完成: Scalable Distance-based Multi-Agent Relative State Estimation via Block Multiconvex OptimizationTian.pdf
2025-07-19 14:00:27,634 - __main__ - INFO - 成功下载: Scalable Distance-based Multi-Agent Relative State Estimation via Block Multiconvex OptimizationTianyue Wu, Fei GaoPaper ID 86Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:This paper explores the distance-based relative state estimation problem in large-scale systems, which is hard to solve effectively due to its high-dimensionality and non-convexity. In this paper, we alleviate this inherent hardness to simultaneously achieve scalability and robustness of inference on this problem. Our idea is launched from a universal geometric formulation, called generalized graph realization, for the distance-based relative state estimation problem. Based on this formulation, we introduce two collaborative optimization models, one of which is convex and thus globally solvable, and the other enables fast searching on non-convex landscapes to refine the solution offered by the convex one. Importantly, both models enjoy multiconvex and decomposable structures, allowing efficient and safe solutions using block coordinate descent that enjoys scalability and a distributed nature. The proposed algorithms collaborate to demonstrate superior or comparable solution precision to the current centralized convex relaxation-based methods, which are known for their high optimality. Distinctly, the proposed methods demonstrate scalability and unique computational efficiency beyond the reach of previous convex relaxation-based methods. We also demonstrate that the combination of the two proposed algorithms achieves a more robust pipeline than deploying the local search method alone in a continuous-time scenario.
2025-07-19 14:00:27,634 - paper_downloader - INFO - 开始下载: Experience-based multi-agent path finding with narrow corridorsRachel A Moan, Courtney McBeth, Marco Morales, Nancy Amato, Kris HauserPaper ID 87Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Multi-agent path finding is a computationally challenging problem that is relevant to many areas in robotics. Experience-based planning methods have been shown to significantly reduce the planning time of this problem, but the type of problem in which experience can be used has so far been limited to warehouse-like environments with ample open space. We present an experience-based multi-agent path finding algorithm that specifically addresses narrow corridors of width 1 (also known as doorways). This expands the domain of experience-based problems to include environments such as most houses, office spaces, retail spaces, and hospitals. We also present novel techniques for conflict resolution strategies that result in up to a $94\%$ decrease in waiting steps per robot and final paths closer to the optimal decoupled path by up to $71\%$ than the strategies used in current experience-based methods. We demonstrate our planner solving problems with hundreds of robots in congested environments in seconds, finding solutions in an allotted time more often than existing state of the art optimal methods.
2025-07-19 14:00:28,445 - paper_downloader - INFO - 下载完成: Experience-based multi-agent path finding with narrow corridorsRachel A Moan, Courtney McBeth, Marco.pdf
2025-07-19 14:00:28,446 - __main__ - INFO - 成功下载: Experience-based multi-agent path finding with narrow corridorsRachel A Moan, Courtney McBeth, Marco Morales, Nancy Amato, Kris HauserPaper ID 87Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Multi-agent path finding is a computationally challenging problem that is relevant to many areas in robotics. Experience-based planning methods have been shown to significantly reduce the planning time of this problem, but the type of problem in which experience can be used has so far been limited to warehouse-like environments with ample open space. We present an experience-based multi-agent path finding algorithm that specifically addresses narrow corridors of width 1 (also known as doorways). This expands the domain of experience-based problems to include environments such as most houses, office spaces, retail spaces, and hospitals. We also present novel techniques for conflict resolution strategies that result in up to a $94\%$ decrease in waiting steps per robot and final paths closer to the optimal decoupled path by up to $71\%$ than the strategies used in current experience-based methods. We demonstrate our planner solving problems with hundreds of robots in congested environments in seconds, finding solutions in an allotted time more often than existing state of the art optimal methods.
2025-07-19 14:00:28,446 - paper_downloader - INFO - 开始下载: Event-based Visual Inertial VelometerXiuyuan LU, Yi Zhou, Junkai Niu, sheng zhong, Shaojie ShenPaper ID 88Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution.
 Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion.
 However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time.
 One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment.
 This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data.
 Therefore, we propose a map-free design for event-based visual-inertial state estimation in this paper.
 Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras.
 The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit.
 Experiments on both synthetic and real data demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.
2025-07-19 14:00:30,476 - paper_downloader - INFO - 下载完成: Event-based Visual Inertial VelometerXiuyuan LU, Yi Zhou, Junkai Niu, sheng zhong, Shaojie ShenPaper.pdf
2025-07-19 14:00:30,476 - __main__ - INFO - 成功下载: Event-based Visual Inertial VelometerXiuyuan LU, Yi Zhou, Junkai Niu, sheng zhong, Shaojie ShenPaper ID 88Session 8. Perception and navigationPoster Session day 2 (Wednesday, July 17)Abstract:Neuromorphic event-based cameras are bio-inspired visual sensors with asynchronous pixels and extremely high temporal resolution.
 Such favorable properties make them an excellent choice for solving state estimation tasks under aggressive ego motion.
 However, failures of camera pose tracking are frequently witnessed in state-of-the-art event-based visual odometry systems when the local map cannot be updated in time.
 One of the biggest roadblocks for this specific field is the absence of efficient and robust methods for data association without imposing any assumption on the environment.
 This problem seems, however, unlikely to be addressed as in standard vision due to the motion-dependent observability of event data.
 Therefore, we propose a map-free design for event-based visual-inertial state estimation in this paper.
 Instead of estimating the position of the event camera, we find that recovering the instantaneous linear velocity is more consistent with the differential working principle of event cameras.
 The proposed event-based visual-inertial velometer leverages a continuous-time formulation that incrementally fuses the heterogeneous measurements from a stereo event camera and an inertial measurement unit.
 Experiments on both synthetic and real data demonstrate that the proposed method can recover instantaneous linear velocity in metric scale with low latency.
2025-07-19 14:00:30,477 - paper_downloader - INFO - 开始下载: Explore until Confident: Efficient Exploration for Embodied Question AnsweringAllen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa SadighPaper ID 89Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. 
 In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM — leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM’s question answering confidence, allowing the robot to know when to stop exploration — leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence.
2025-07-19 14:00:33,336 - paper_downloader - INFO - 下载完成: Explore until Confident Efficient Exploration for Embodied Question AnsweringAllen Z. Ren, Jaden Cla.pdf
2025-07-19 14:00:33,336 - __main__ - INFO - 成功下载: Explore until Confident: Efficient Exploration for Embodied Question AnsweringAllen Z. Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa SadighPaper ID 89Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. 
 In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM — leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM’s question answering confidence, allowing the robot to know when to stop exploration — leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence.
2025-07-19 14:00:33,337 - paper_downloader - INFO - 开始下载: Octo: An Open-Source Generalist Robot PolicyDibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Quan Vuong, Ted Xiao, Pannag R Sanketi, Dorsa Sadigh, Chelsea Finn, Sergey LevinePaper ID 90Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.
2025-07-19 14:00:34,621 - paper_downloader - INFO - 下载完成: Octo An Open-Source Generalist Robot PolicyDibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black,.pdf
2025-07-19 14:00:34,621 - __main__ - INFO - 成功下载: Octo: An Open-Source Generalist Robot PolicyDibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Quan Vuong, Ted Xiao, Pannag R Sanketi, Dorsa Sadigh, Chelsea Finn, Sergey LevinePaper ID 90Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.
2025-07-19 14:00:34,621 - paper_downloader - INFO - 开始下载: Demonstrating OK-Robot: What Really Matters in Integrating Open-Knowledge Models for RoboticsPeiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel PintoPaper ID 91Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot’s performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules.
2025-07-19 14:00:37,216 - paper_downloader - INFO - 下载完成: Demonstrating OK-Robot What Really Matters in Integrating Open-Knowledge Models for RoboticsPeiqi Li.pdf
2025-07-19 14:00:37,216 - __main__ - INFO - 成功下载: Demonstrating OK-Robot: What Really Matters in Integrating Open-Knowledge Models for RoboticsPeiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel PintoPaper ID 91Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot’s performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules.
2025-07-19 14:00:37,216 - paper_downloader - INFO - 开始下载: Any-point Trajectory Modeling for Policy LearningChuan Wen, Xingyu Lin, John Ian Reyes So, Kai Chen, Qi Dou, Yang Gao, Pieter AbbeelPaper ID 92Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, \textbf{A}ny-point \textbf{T}rajectory \textbf{M}odeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across the \textbf{130} language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80$\%$ on average. Furthermore, we show effective transfer learning of manipulation skills from human videos.
2025-07-19 14:00:38,524 - paper_downloader - INFO - 下载完成: Any-point Trajectory Modeling for Policy LearningChuan Wen, Xingyu Lin, John Ian Reyes So, Kai Chen,.pdf
2025-07-19 14:00:38,524 - __main__ - INFO - 成功下载: Any-point Trajectory Modeling for Policy LearningChuan Wen, Xingyu Lin, John Ian Reyes So, Kai Chen, Qi Dou, Yang Gao, Pieter AbbeelPaper ID 92Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, \textbf{A}ny-point \textbf{T}rajectory \textbf{M}odeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across the \textbf{130} language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80$\%$ on average. Furthermore, we show effective transfer learning of manipulation skills from human videos.
2025-07-19 14:00:38,524 - paper_downloader - INFO - 开始下载: Pushing the Limits of Cross-Embodiment Learning for Manipulation and NavigationJonathan Heewon Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey LevinePaper ID 93Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains such as robotic navigation and manipulation can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling a robotic arm, quadcopter, quadruped, and mobile base. We then investigate the extent to which transfer can occur across navigation and manipulation by framing them as a single goal-reaching task. In particular, we find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a similar but novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across a wide variety of embodiments.
2025-07-19 14:00:45,739 - paper_downloader - INFO - 下载完成: Pushing the Limits of Cross-Embodiment Learning for Manipulation and NavigationJonathan Heewon Yang,.pdf
2025-07-19 14:00:45,739 - __main__ - INFO - 成功下载: Pushing the Limits of Cross-Embodiment Learning for Manipulation and NavigationJonathan Heewon Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey LevinePaper ID 93Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains such as robotic navigation and manipulation can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling a robotic arm, quadcopter, quadruped, and mobile base. We then investigate the extent to which transfer can occur across navigation and manipulation by framing them as a single goal-reaching task. In particular, we find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a similar but novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across a wide variety of embodiments.
2025-07-19 14:00:45,739 - paper_downloader - INFO - 开始下载: DrEureka: Language Model Guided Sim-To-Real TransferYecheng Jason Ma, William Liang, Hung-Ju Wang, Yuke Zhu, Linxi Fan, Osbert Bastani, Dinesh JayaramanPaper ID 94Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.
2025-07-19 14:01:03,285 - paper_downloader - INFO - 下载完成: DrEureka Language Model Guided Sim-To-Real TransferYecheng Jason Ma, William Liang, Hung-Ju Wang, Yu.pdf
2025-07-19 14:01:03,285 - __main__ - INFO - 成功下载: DrEureka: Language Model Guided Sim-To-Real TransferYecheng Jason Ma, William Liang, Hung-Ju Wang, Yuke Zhu, Linxi Fan, Osbert Bastani, Dinesh JayaramanPaper ID 94Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:Transferring policies learned in simulation to the real world is a promising strategy for acquiring robot skills at scale. However, sim-to-real approaches typically rely on manual design and tuning of the task reward function as well as the simulation physics parameters, rendering the process slow and human-labor intensive. In this paper, we investigate using Large Language Models (LLMs) to automate and accelerate sim-to-real design. Our LLM-guided sim-to-real approach, DrEureka, requires only the physics simulation for the target task and automatically constructs suitable reward functions and domain randomization distributions to support real-world transfer. We first demonstrate that our approach can discover sim-to-real configurations that are competitive with existing human-designed ones on quadruped locomotion and dexterous manipulation tasks. Then, we showcase that our approach is capable of solving novel robot tasks, such as quadruped balancing and walking atop a yoga ball, without iterative manual design.
2025-07-19 14:01:03,285 - paper_downloader - INFO - 开始下载: Set It Up!: Functional Object Arrangement with Compositional Generative ModelsYiqing Xu, Jiayuan Mao, Yilun Du, Tomás Lozano-Pérez, Leslie Pack Kaelbling, David HsuPaper ID 95Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as “set up a dining table for two”; previous arrangement approaches have focused on much more explicit instructions, such as “put object A on the table.” We introduce a framework,SetItUp, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation ofabstract spatial relationshipsamong objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models. \footnote{Project page: https://setitup-rss.github.io/}
2025-07-19 14:01:08,864 - paper_downloader - INFO - 下载完成: Set It Up! Functional Object Arrangement with Compositional Generative ModelsYiqing Xu, Jiayuan Mao,.pdf
2025-07-19 14:01:08,865 - __main__ - INFO - 成功下载: Set It Up!: Functional Object Arrangement with Compositional Generative ModelsYiqing Xu, Jiayuan Mao, Yilun Du, Tomás Lozano-Pérez, Leslie Pack Kaelbling, David HsuPaper ID 95Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:This paper studies the challenge of developing robots capable of understanding under-specified instructions for creating functional object arrangements, such as “set up a dining table for two”; previous arrangement approaches have focused on much more explicit instructions, such as “put object A on the table.” We introduce a framework,SetItUp, for learning to interpret under-specified instructions. SetItUp takes a small number of training examples and a human-crafted program sketch to uncover arrangement rules for specific scene types. By leveraging an intermediate graph-like representation ofabstract spatial relationshipsamong objects, SetItUp decomposes the arrangement problem into two subproblems: i) learning the arrangement patterns from limited data and ii) grounding these abstract relationships into object poses. SetItUp leverages large language models (LLMs) to propose the abstract spatial relationships among objects in novel scenes as the constraints to be satisfied; then, it composes a library of diffusion models associated with these abstract relationships to find object poses that satisfy the constraints. We validate our framework on a dataset comprising study desks, dining tables, and coffee tables, with the results showing superior performance in generating physically plausible, functional, and aesthetically pleasing object arrangements compared to existing models. \footnote{Project page: https://setitup-rss.github.io/}
2025-07-19 14:01:08,865 - paper_downloader - INFO - 开始下载: Keypoint Action Tokens Enable In-Context Imitation Learning in RoboticsNorman Di Palo, Edward JohnsPaper ID 96Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator’s behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transfermers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos can be found on our website https://www.robot-learning.uk/keypoint-action-tokens.
2025-07-19 14:01:15,278 - paper_downloader - INFO - 下载完成: Keypoint Action Tokens Enable In-Context Imitation Learning in RoboticsNorman Di Palo, Edward JohnsP.pdf
2025-07-19 14:01:15,278 - __main__ - INFO - 成功下载: Keypoint Action Tokens Enable In-Context Imitation Learning in RoboticsNorman Di Palo, Edward JohnsPaper ID 96Session 12. Robot learning foundation modelsPoster Session day 3 (Thursday, July 18)Abstract:We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator’s behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transfermers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos can be found on our website https://www.robot-learning.uk/keypoint-action-tokens.
2025-07-19 14:01:15,279 - paper_downloader - INFO - 开始下载: ConTac: Continuum-Emulated Soft Skinned Arm with Vision-based Shape Sensing and Contact-aware ManipulationTuan Tai Nguyen, Quan Khanh Luu, Dinh Quang Nguyen, Van HoPaper ID 97Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Robotic systems employing continuum bodies offer a high degree of dexterity, which provides advantages in terms of accuracy and safety when operating in cluttered environments. However, current methods of describing posture or detecting contact for such continuum structures are focusing on bespoke designs or are limited to a single sensing modality, which could hinder their possibility for scalability and generalization. This study proposes a novel vision-based tactile sensing system, named ConTac, that provides both proprioception and tactile detection for a continuum-emulated arm with soft skin. To realize the mentioned functions, we employ two corresponding deep-learning models trained using simulation data. The models are zero-shot applied to real-world data without fine-tuning. The experimental results show that the system could predict the posture of a skinned robot arm with a mean tip position error of 8.83 mm, while the mean error for touch location was 28.86 mm. We then compared the model performance on two different robot modules, proving the justification of the system. An admittance control strategy is then developed using the shape and contact information, allowing the robot arm to react properly to collisions. The proposed method shows potential in adapting to hyper-redundant or continuum robots, enhancing their perception capabilities and control paradigms.
2025-07-19 14:01:20,151 - paper_downloader - INFO - 下载完成: ConTac Continuum-Emulated Soft Skinned Arm with Vision-based Shape Sensing and Contact-aware Manipul.pdf
2025-07-19 14:01:20,151 - __main__ - INFO - 成功下载: ConTac: Continuum-Emulated Soft Skinned Arm with Vision-based Shape Sensing and Contact-aware ManipulationTuan Tai Nguyen, Quan Khanh Luu, Dinh Quang Nguyen, Van HoPaper ID 97Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Robotic systems employing continuum bodies offer a high degree of dexterity, which provides advantages in terms of accuracy and safety when operating in cluttered environments. However, current methods of describing posture or detecting contact for such continuum structures are focusing on bespoke designs or are limited to a single sensing modality, which could hinder their possibility for scalability and generalization. This study proposes a novel vision-based tactile sensing system, named ConTac, that provides both proprioception and tactile detection for a continuum-emulated arm with soft skin. To realize the mentioned functions, we employ two corresponding deep-learning models trained using simulation data. The models are zero-shot applied to real-world data without fine-tuning. The experimental results show that the system could predict the posture of a skinned robot arm with a mean tip position error of 8.83 mm, while the mean error for touch location was 28.86 mm. We then compared the model performance on two different robot modules, proving the justification of the system. An admittance control strategy is then developed using the shape and contact information, allowing the robot arm to react properly to collisions. The proposed method shows potential in adapting to hyper-redundant or continuum robots, enhancing their perception capabilities and control paradigms.
2025-07-19 14:01:20,152 - paper_downloader - INFO - 开始下载: Function Based Sim-to-Real Learning for Shape Control of Deformable Free-form SurfacesYingjun Tian, Guoxin Fang, Renbo Su, Weiming Wang, Simeon Gill, Andrew Weightman, Charlie C. L. WangPaper ID 98Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:For the shape control of deformable free-form surfaces, simulation plays a crucial role in establishing the mapping between the actuation parameters and the deformed shapes. The differentiation of this forward kinematic mapping is usually employed to solve the inverse kinematic problem for determining the actuation parameters that can realize a target shape. However, the free-form surfaces obtained from simulators are always different from the physically deformed shapes due to the errors introduced by hardware and the simplification adopted in physical simulation. To fill the gap, we propose a novel deformation function based sim-to-real learning method that can map the geometric shape of a simulated model into its corresponding shape of the physical model. Unlike the existing sim-to-real learning methods that rely on completely acquired dense markers, our method accommodates sparsely distributed markers and can resiliently use all captured frames – even for those in the presence of missing markers. To demonstrate its effectiveness, our sim-to-real method has been integrated into a neural network-based computational pipeline designed to tackle the inverse kinematic problem on a pneumatically actuated deformable mannequin.
2025-07-19 14:01:23,220 - paper_downloader - INFO - 下载完成: Function Based Sim-to-Real Learning for Shape Control of Deformable Free-form SurfacesYingjun Tian, .pdf
2025-07-19 14:01:23,220 - __main__ - INFO - 成功下载: Function Based Sim-to-Real Learning for Shape Control of Deformable Free-form SurfacesYingjun Tian, Guoxin Fang, Renbo Su, Weiming Wang, Simeon Gill, Andrew Weightman, Charlie C. L. WangPaper ID 98Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:For the shape control of deformable free-form surfaces, simulation plays a crucial role in establishing the mapping between the actuation parameters and the deformed shapes. The differentiation of this forward kinematic mapping is usually employed to solve the inverse kinematic problem for determining the actuation parameters that can realize a target shape. However, the free-form surfaces obtained from simulators are always different from the physically deformed shapes due to the errors introduced by hardware and the simplification adopted in physical simulation. To fill the gap, we propose a novel deformation function based sim-to-real learning method that can map the geometric shape of a simulated model into its corresponding shape of the physical model. Unlike the existing sim-to-real learning methods that rely on completely acquired dense markers, our method accommodates sparsely distributed markers and can resiliently use all captured frames – even for those in the presence of missing markers. To demonstrate its effectiveness, our sim-to-real method has been integrated into a neural network-based computational pipeline designed to tackle the inverse kinematic problem on a pneumatically actuated deformable mannequin.
2025-07-19 14:01:23,220 - paper_downloader - INFO - 开始下载: Safe & Accurate at Speed with Tendons: A Robot Arm for Exploring Dynamic MotionSimon Guist, Jan Schneider, Hao Ma, Le Chen, Vincent Berenz, Julian Martus, Heiko Ott, Felix Grüninger, Michael Muehlebach, Jonathan Fiene, Bernhard Schölkopf, Dieter BüchlerPaper ID 99Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Operating robots precisely and at high speeds has been a long-standing goal of robotics research. Balancing these competing demands is key to enabling the seamless collaboration of robots and humans and increasing task performance. However, traditional motor-driven systems often fall short in this balancing act. Due to their rigid and often heavy design exacerbated by positioning the motors into the joints, faster motions of such robots transfer high forces at impact. To enable precise and safe dynamic motions, we introduce a four degree-of-freedom tendon-driven robot arm. Tendons allow placing the actuation at the base to reduce the robot’s inertia, which we show significantly reduces peak collision forces compared to conventional motor-driven systems. Pairing our robot with pneumatic muscles allows generating high forces and highly accelerated motions, while benefiting from impact resilience through passive compliance. Since tendons are subject to additional friction and hence prone to tear, we validate the reliability of our robotic arm on various experiments, including long-term dynamic motions. 
 We also demonstrate its ease of control by quantifying the nonlinearities of the system and the performance on a challenging dynamic table tennis task learned from scratch using reinforcement learning. We open-source the entire hardware design, which can be largely 3D printed, the control software, and a proprioceptive dataset of 25 days of diverse robot motions.
2025-07-19 14:01:25,278 - paper_downloader - INFO - 下载完成: Safe & Accurate at Speed with Tendons A Robot Arm for Exploring Dynamic MotionSimon Guist, Jan Schne.pdf
2025-07-19 14:01:25,278 - __main__ - INFO - 成功下载: Safe & Accurate at Speed with Tendons: A Robot Arm for Exploring Dynamic MotionSimon Guist, Jan Schneider, Hao Ma, Le Chen, Vincent Berenz, Julian Martus, Heiko Ott, Felix Grüninger, Michael Muehlebach, Jonathan Fiene, Bernhard Schölkopf, Dieter BüchlerPaper ID 99Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Operating robots precisely and at high speeds has been a long-standing goal of robotics research. Balancing these competing demands is key to enabling the seamless collaboration of robots and humans and increasing task performance. However, traditional motor-driven systems often fall short in this balancing act. Due to their rigid and often heavy design exacerbated by positioning the motors into the joints, faster motions of such robots transfer high forces at impact. To enable precise and safe dynamic motions, we introduce a four degree-of-freedom tendon-driven robot arm. Tendons allow placing the actuation at the base to reduce the robot’s inertia, which we show significantly reduces peak collision forces compared to conventional motor-driven systems. Pairing our robot with pneumatic muscles allows generating high forces and highly accelerated motions, while benefiting from impact resilience through passive compliance. Since tendons are subject to additional friction and hence prone to tear, we validate the reliability of our robotic arm on various experiments, including long-term dynamic motions. 
 We also demonstrate its ease of control by quantifying the nonlinearities of the system and the performance on a challenging dynamic table tennis task learned from scratch using reinforcement learning. We open-source the entire hardware design, which can be largely 3D printed, the control software, and a proprioceptive dataset of 25 days of diverse robot motions.
2025-07-19 14:01:25,278 - paper_downloader - INFO - 开始下载: Evolution and learning in differentiable robotsLuke Strgar, David Matthews, Tyler Hummer, Sam KriegmanPaper ID 100Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines. Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior. Non-differentiable changes to the mechanical structure of each robot in the population—mutations that rearrange, combine, add, or remove body parts—were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent. This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies. We found that evolution reliably produces ``increasingly differentiable’’ robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors. Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior. This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot’s physical structure can influence the ability to train policies for it. Videos and code at https://sites.google.com/view/eldir.
2025-07-19 14:01:28,580 - paper_downloader - INFO - 下载完成: Evolution and learning in differentiable robotsLuke Strgar, David Matthews, Tyler Hummer, Sam Kriegm.pdf
2025-07-19 14:01:28,580 - __main__ - INFO - 成功下载: Evolution and learning in differentiable robotsLuke Strgar, David Matthews, Tyler Hummer, Sam KriegmanPaper ID 100Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines. Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior. Non-differentiable changes to the mechanical structure of each robot in the population—mutations that rearrange, combine, add, or remove body parts—were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent. This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies. We found that evolution reliably produces ``increasingly differentiable’’ robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors. Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior. This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot’s physical structure can influence the ability to train policies for it. Videos and code at https://sites.google.com/view/eldir.
2025-07-19 14:01:28,580 - paper_downloader - INFO - 开始下载: Construction of a Multiple-DOF Underactuated Gripper with Force-Sensing via Deep LearningJihao Li, Keqi Zhu, Guodong Lu, I-Ming Chen, HUIXU DONGPaper ID 101Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Under-actuated robotic grippers, regarded as critical components of robotic grasping, have attracted considerable attention. However, existing under-actuated grippers emerge with several primary issues, including low payload, insufficient force sensing, small grasping force, weak grasping stability as well as high cost, hindering widespread applications. Some of these grippers can only implement a
 single grasping mode, thereby imposing restrictions on dimensional ranges of objects. To well relieve all relevant research gaps, we present a novel under-actuated gripper with two 3-joint fingers, which realizes force feedback control by the deep learning technique- Long Short-Term Memory (LSTM) model, without any force sensor. First, a five-linkage mechanism stacked by double four-linkages is designed as a finger to automatically achieve the transformation between parallel and enveloping grasping modes. This enables the creation of a low-cost under-actuated gripper comprising a single actuator and two 3-phalange fingers. Second, we devise theoretical models of kinematics and power transmission based on the proposed gripper, accurately obtaining fingertip positions and contact forces. Through coupling and decoupling of five-linkage mechanisms, the proposed gripper offers the expected capabilities of grasping payload/force/stability and objects with large dimension ranges. Third, to realize the force control, an LSTM model is proposed to determine the grasping mode for synthesizing force-feedback control policies that exploit contact sensing after outlining the uncertainty of
 currents using a statistical method. Finally, a series of experiments are implemented to measure quantitative indicators, such as the payload, grasping force, force sensing, grasping stability and the dimension ranges of objects to be grasped. Additionally, the grasping performance of the proposed gripper is verified experimentally to guarantee the high versatility and robustness of the proposed gripper. A very promising strategy combining mechanism design and artificial intelligence (AI) technology will be highly impactful on the
 construction of robotic grippers. A uploaded video in YouTube: https://youtu.be/TDyCUtxnePQ.
2025-07-19 14:01:31,929 - paper_downloader - INFO - 下载完成: Construction of a Multiple-DOF Underactuated Gripper with Force-Sensing via Deep LearningJihao Li, K.pdf
2025-07-19 14:01:31,929 - __main__ - INFO - 成功下载: Construction of a Multiple-DOF Underactuated Gripper with Force-Sensing via Deep LearningJihao Li, Keqi Zhu, Guodong Lu, I-Ming Chen, HUIXU DONGPaper ID 101Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Under-actuated robotic grippers, regarded as critical components of robotic grasping, have attracted considerable attention. However, existing under-actuated grippers emerge with several primary issues, including low payload, insufficient force sensing, small grasping force, weak grasping stability as well as high cost, hindering widespread applications. Some of these grippers can only implement a
 single grasping mode, thereby imposing restrictions on dimensional ranges of objects. To well relieve all relevant research gaps, we present a novel under-actuated gripper with two 3-joint fingers, which realizes force feedback control by the deep learning technique- Long Short-Term Memory (LSTM) model, without any force sensor. First, a five-linkage mechanism stacked by double four-linkages is designed as a finger to automatically achieve the transformation between parallel and enveloping grasping modes. This enables the creation of a low-cost under-actuated gripper comprising a single actuator and two 3-phalange fingers. Second, we devise theoretical models of kinematics and power transmission based on the proposed gripper, accurately obtaining fingertip positions and contact forces. Through coupling and decoupling of five-linkage mechanisms, the proposed gripper offers the expected capabilities of grasping payload/force/stability and objects with large dimension ranges. Third, to realize the force control, an LSTM model is proposed to determine the grasping mode for synthesizing force-feedback control policies that exploit contact sensing after outlining the uncertainty of
 currents using a statistical method. Finally, a series of experiments are implemented to measure quantitative indicators, such as the payload, grasping force, force sensing, grasping stability and the dimension ranges of objects to be grasped. Additionally, the grasping performance of the proposed gripper is verified experimentally to guarantee the high versatility and robustness of the proposed gripper. A very promising strategy combining mechanism design and artificial intelligence (AI) technology will be highly impactful on the
 construction of robotic grippers. A uploaded video in YouTube: https://youtu.be/TDyCUtxnePQ.
2025-07-19 14:01:31,929 - paper_downloader - INFO - 开始下载: A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication and Sensing MechanismJingxian Wang, Andrew G. Curtis, Mark Yim, Michael RubensteinPaper ID 102Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively. 
 However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots.
 Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time.
 Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction.
 We present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably, and maneuver in the environment using peer-to-peer communication and sensing.
2025-07-19 14:01:35,284 - paper_downloader - INFO - 下载完成: A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication and Sensing MechanismJingxi.pdf
2025-07-19 14:01:35,284 - __main__ - INFO - 成功下载: A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication and Sensing MechanismJingxian Wang, Andrew G. Curtis, Mark Yim, Michael RubensteinPaper ID 102Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively. 
 However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots.
 Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time.
 Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction.
 We present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably, and maneuver in the environment using peer-to-peer communication and sensing.
2025-07-19 14:01:35,285 - paper_downloader - INFO - 开始下载: Design and Control of a Bipedal Robotic CharacterRuben Grandia, Espen Knoop, Michael A. Hopkins, Georg Wiedebach, Jared Bishop, Steven Pickles, David Müller, Moritz BächerPaper ID 103Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Legged robots have achieved impressive feats in dynamic locomotion in challenging unstructured terrain. However, in entertainment applications, the design and control of these robots face additional challenges in appealing to human audiences. This work aims to unify expressive, artist-directed motions and robust dynamic mobility for legged robots. To this end, we introduce a new bipedal robot, designed with a focus on character-driven mechanical features. We present a reinforcement learning-based control architecture to robustly execute artistic motions conditioned on command signals. During runtime, these command signals are generated by an animation engine which composes and blends between multiple animation sources. Finally, an intuitive operator interface enables real-time show performances with the robot.
2025-07-19 14:01:41,170 - paper_downloader - INFO - 下载完成: Design and Control of a Bipedal Robotic CharacterRuben Grandia, Espen Knoop, Michael A. Hopkins, Geo.pdf
2025-07-19 14:01:41,170 - __main__ - INFO - 成功下载: Design and Control of a Bipedal Robotic CharacterRuben Grandia, Espen Knoop, Michael A. Hopkins, Georg Wiedebach, Jared Bishop, Steven Pickles, David Müller, Moritz BächerPaper ID 103Session 13. Robot designPoster Session day 3 (Thursday, July 18)Abstract:Legged robots have achieved impressive feats in dynamic locomotion in challenging unstructured terrain. However, in entertainment applications, the design and control of these robots face additional challenges in appealing to human audiences. This work aims to unify expressive, artist-directed motions and robust dynamic mobility for legged robots. To this end, we introduce a new bipedal robot, designed with a focus on character-driven mechanical features. We present a reinforcement learning-based control architecture to robustly execute artistic motions conditioned on command signals. During runtime, these command signals are generated by an animation engine which composes and blends between multiple animation sources. Finally, an intuitive operator interface enables real-time show performances with the robot.
2025-07-19 14:01:41,170 - paper_downloader - INFO - 开始下载: POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard ConstraintsJean-Baptiste Bouvier, Kartik Nagpal, Negar MehrPaper ID 104Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:In this paper, we seek to learn a robot policy guaranteed to satisfy state constraints. To encourage constraint satisfaction, existing RL algorithms typically rely on Constrained Markov Decision Processes and discourage constraint violations through reward shaping. However, such soft constraints cannot offer safety guarantees. To address this gap, we propose POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard constraints in closed-loop with a black-box environment. Our key insight is to make the learned policy be affine around the unsafe set and to use this affine region as a repulsive buffer to prevent trajectories from violating the constraint. We prove that such policies exist and guarantee constraint satisfaction. Our proposed framework is applicable to both systems with continuous and discrete state and action spaces and is agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of POLICEd RL to enforce hard constraints in robotic tasks while significantly outperforming existing methods. Code available at https://iconlab.negarmehr.com/POLICEd-RL/
2025-07-19 14:01:41,937 - paper_downloader - INFO - 下载完成: POLICEd RL Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraint.pdf
2025-07-19 14:01:41,937 - __main__ - INFO - 成功下载: POLICEd RL: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard ConstraintsJean-Baptiste Bouvier, Kartik Nagpal, Negar MehrPaper ID 104Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:In this paper, we seek to learn a robot policy guaranteed to satisfy state constraints. To encourage constraint satisfaction, existing RL algorithms typically rely on Constrained Markov Decision Processes and discourage constraint violations through reward shaping. However, such soft constraints cannot offer safety guarantees. To address this gap, we propose POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard constraints in closed-loop with a black-box environment. Our key insight is to make the learned policy be affine around the unsafe set and to use this affine region as a repulsive buffer to prevent trajectories from violating the constraint. We prove that such policies exist and guarantee constraint satisfaction. Our proposed framework is applicable to both systems with continuous and discrete state and action spaces and is agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of POLICEd RL to enforce hard constraints in robotic tasks while significantly outperforming existing methods. Code available at https://iconlab.negarmehr.com/POLICEd-RL/
2025-07-19 14:01:41,937 - paper_downloader - INFO - 开始下载: Demonstrating Language-Grounded Motion ControllerRavi Tejwani, Chengyuan Ma, Paco Gomez-Paz, Paolo Bonato, Haruhiko AsadaPaper ID 105Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Recent advancements have enabled human-robot collaboration through physical assistance and verbal guidance. However, limitations persist in coordinating robots’ physical motions and speech in response to real-time changes in human behavior during collaborative contact tasks.
 We first derive principles from analyzing physical therapists’ movements and speech during patient exercises. These principles are translated into control objectives to: 1) guide users through trajectories, 2) control motion and speech pace to align completion times with varying user cooperation, and 3) dynamically paraphrase speech along the trajectory.
 We then propose a Language Controller that synchronizes motion and speech, modulating both based on user cooperation. 
 Experiments with 12 users show the Language Controller successfully aligns motion and speech compared to baselines. This provides a framework for fluent human-robot collaboration.
2025-07-19 14:01:43,221 - paper_downloader - INFO - 下载完成: Demonstrating Language-Grounded Motion ControllerRavi Tejwani, Chengyuan Ma, Paco Gomez-Paz, Paolo B.pdf
2025-07-19 14:01:43,221 - __main__ - INFO - 成功下载: Demonstrating Language-Grounded Motion ControllerRavi Tejwani, Chengyuan Ma, Paco Gomez-Paz, Paolo Bonato, Haruhiko AsadaPaper ID 105Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Recent advancements have enabled human-robot collaboration through physical assistance and verbal guidance. However, limitations persist in coordinating robots’ physical motions and speech in response to real-time changes in human behavior during collaborative contact tasks.
 We first derive principles from analyzing physical therapists’ movements and speech during patient exercises. These principles are translated into control objectives to: 1) guide users through trajectories, 2) control motion and speech pace to align completion times with varying user cooperation, and 3) dynamically paraphrase speech along the trajectory.
 We then propose a Language Controller that synchronizes motion and speech, modulating both based on user cooperation. 
 Experiments with 12 users show the Language Controller successfully aligns motion and speech compared to baselines. This provides a framework for fluent human-robot collaboration.
2025-07-19 14:01:43,221 - paper_downloader - INFO - 开始下载: VLMPC: Vision-Language Model Predictive Control for Robotic ManipulationWentao Zhao, Jiaming Chen, Ziyu Meng, DonghuiMao, Ran Song, Wei ZhangPaper ID 106Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. We shall release the code and data if the paper is accepted.
2025-07-19 14:01:44,265 - paper_downloader - INFO - 下载完成: VLMPC Vision-Language Model Predictive Control for Robotic ManipulationWentao Zhao, Jiaming Chen, Zi.pdf
2025-07-19 14:01:44,265 - __main__ - INFO - 成功下载: VLMPC: Vision-Language Model Predictive Control for Robotic ManipulationWentao Zhao, Jiaming Chen, Ziyu Meng, DonghuiMao, Ran Song, Wei ZhangPaper ID 106Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. We shall release the code and data if the paper is accepted.
2025-07-19 14:01:44,265 - paper_downloader - INFO - 开始下载: Expressive Whole-Body Control for Humanoid RobotsXuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong WangPaper ID 107Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method $\textbf{Ex}$pressive Whole-$\textbf{Body}$ Control ($\textbf{ExBody}$) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.
2025-07-19 14:01:58,352 - paper_downloader - INFO - 下载完成: Expressive Whole-Body Control for Humanoid RobotsXuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang,.pdf
2025-07-19 14:01:58,352 - __main__ - INFO - 成功下载: Expressive Whole-Body Control for Humanoid RobotsXuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong WangPaper ID 107Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method $\textbf{Ex}$pressive Whole-$\textbf{Body}$ Control ($\textbf{ExBody}$) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.
2025-07-19 14:01:58,352 - paper_downloader - INFO - 开始下载: From Compliant to Rigid Contact Simulation: a Unified and Efficient ApproachJustin Carpentier, Quentin Le Lidec, Louis MontautPaper ID 108Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Overall, the proposed approach is shown to be competitive against classic methods for simple contact problems and outperforms existing solutions on more complex scenarios, involving tens of contacts and poor conditioning.
2025-07-19 14:01:59,395 - paper_downloader - INFO - 下载完成: From Compliant to Rigid Contact Simulation a Unified and Efficient ApproachJustin Carpentier, Quenti.pdf
2025-07-19 14:01:59,395 - __main__ - INFO - 成功下载: From Compliant to Rigid Contact Simulation: a Unified and Efficient ApproachJustin Carpentier, Quentin Le Lidec, Louis MontautPaper ID 108Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Overall, the proposed approach is shown to be competitive against classic methods for simple contact problems and outperforms existing solutions on more complex scenarios, involving tens of contacts and poor conditioning.
2025-07-19 14:01:59,395 - paper_downloader - INFO - 开始下载: MPCC++: Model Predictive Contouring Control for Time-Optimal Flight with Safety ConstraintsMaria Krinner, Angel Romero, Leonard Bauersfeld, Melanie Zeilinger, Andrea Carron, Davide ScaramuzzaPaper ID 109Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a promising model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the state-of-the-art MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a track constraint and terminal set. The track constraint is designed as a spatial constraint which
 prevents gate collisions while allowing for time optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real-world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state-of-the-art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPCC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best-performing RL policy and outperforms the best model-based controller while satisfying constraints. In both simulation and real world, our approach consistently prevents gate crashes with 100% success rate, while pushing the quadrotor to its physical limits reaching speeds of more than 80km/h.
2025-07-19 14:02:01,497 - paper_downloader - INFO - 下载完成: MPCC++ Model Predictive Contouring Control for Time-Optimal Flight with Safety ConstraintsMaria Krin.pdf
2025-07-19 14:02:01,497 - __main__ - INFO - 成功下载: MPCC++: Model Predictive Contouring Control for Time-Optimal Flight with Safety ConstraintsMaria Krinner, Angel Romero, Leonard Bauersfeld, Melanie Zeilinger, Andrea Carron, Davide ScaramuzzaPaper ID 109Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:Quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a promising model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the state-of-the-art MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a track constraint and terminal set. The track constraint is designed as a spatial constraint which
 prevents gate collisions while allowing for time optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real-world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state-of-the-art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPCC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best-performing RL policy and outperforms the best model-based controller while satisfying constraints. In both simulation and real world, our approach consistently prevents gate crashes with 100% success rate, while pushing the quadrotor to its physical limits reaching speeds of more than 80km/h.
2025-07-19 14:02:01,498 - paper_downloader - INFO - 开始下载: Linear-time Differential Inverse Kinematics: an Augmented Lagrangian PerspectiveBruce Wingo, Ajay Suresha Sathya, Stéphane Caron, Seth Hutchinson, Justin CarpentierPaper ID 110Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:For decades, inverse kinematics (IK) was an intense and active research area in robotics.Beyond analytical solutions limited to a restricted range of robotic systems and applications, differential inverse kinematics has emerged as a generic class of methods, able to cope with a wider variety of robots and scenarios, with quadratic programming-based approaches as the main paradigm.In this paper, we propose to revisit differential inverse kinematics from the perspective of augmented Lagrangian methods (AL) and the well-known related alternating direction method of multipliers (ADMM).Notably, by leveraging AL techniques and in the spirit of Featherstone algorithms, we introduce a rigid-body dynamics algorithm that solves equality-constrained IK problems with linear complexity in the number of robot joints and number of constraints. Combined with the ADMM strategy developed in the OSQP solver, we provide a new solution for the same class of problems as QP-based differential IK, yet with linear complexity in problem dimensions.We propose an open-source C++ implementation of this approach, which we validate on a large set of problems including manipulation and humanoid locomotion tasks. Our benchmark measures computation times 2–3 $\times$ shorter than the QP-based state of the art.
2025-07-19 14:02:01,792 - paper_downloader - INFO - 下载完成: Linear-time Differential Inverse Kinematics an Augmented Lagrangian PerspectiveBruce Wingo, Ajay Sur.pdf
2025-07-19 14:02:01,793 - __main__ - INFO - 成功下载: Linear-time Differential Inverse Kinematics: an Augmented Lagrangian PerspectiveBruce Wingo, Ajay Suresha Sathya, Stéphane Caron, Seth Hutchinson, Justin CarpentierPaper ID 110Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:For decades, inverse kinematics (IK) was an intense and active research area in robotics.Beyond analytical solutions limited to a restricted range of robotic systems and applications, differential inverse kinematics has emerged as a generic class of methods, able to cope with a wider variety of robots and scenarios, with quadratic programming-based approaches as the main paradigm.In this paper, we propose to revisit differential inverse kinematics from the perspective of augmented Lagrangian methods (AL) and the well-known related alternating direction method of multipliers (ADMM).Notably, by leveraging AL techniques and in the spirit of Featherstone algorithms, we introduce a rigid-body dynamics algorithm that solves equality-constrained IK problems with linear complexity in the number of robot joints and number of constraints. Combined with the ADMM strategy developed in the OSQP solver, we provide a new solution for the same class of problems as QP-based differential IK, yet with linear complexity in problem dimensions.We propose an open-source C++ implementation of this approach, which we validate on a large set of problems including manipulation and humanoid locomotion tasks. Our benchmark measures computation times 2–3 $\times$ shorter than the QP-based state of the art.
2025-07-19 14:02:01,793 - paper_downloader - INFO - 开始下载: A Trajectory Tracking Algorithm for the LSMS Family of Cable-Driven CranesJavier Puig-Navarro, Dominic R Bisio, John E Pye, Yotam Granov, Joshua N Moser, Jessica S. Friz, Walter J Waltz, Julia E Cline, B. Danette AllenPaper ID 111Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:The Lightweight Surface Manipulation System, or
 LSMS, is a family of scalable long-reach cable-actuated manipulators.
 The design of the LSMS has a high payload ratio for
 efficient operations on planetary surfaces like the Moon or Mars.
 The LSMS has nonlinear, coupled, and hybrid dynamics. The
 engineering decisions that led to these challenging dynamics make
 this structure light and efficient. This paper proposes a novel
 trajectory tracking algorithm for these cranes that facilitates
 precise autonomous and teleoperated operations. This algorithm
 enables these robots to follow complex trajectories that avoid
 obstacles and pickup regolith in a construction site.
2025-07-19 14:02:05,407 - paper_downloader - INFO - 下载完成: A Trajectory Tracking Algorithm for the LSMS Family of Cable-Driven CranesJavier Puig-Navarro, Domin.pdf
2025-07-19 14:02:05,407 - __main__ - INFO - 成功下载: A Trajectory Tracking Algorithm for the LSMS Family of Cable-Driven CranesJavier Puig-Navarro, Dominic R Bisio, John E Pye, Yotam Granov, Joshua N Moser, Jessica S. Friz, Walter J Waltz, Julia E Cline, B. Danette AllenPaper ID 111Session 14. ControlPoster Session day 3 (Thursday, July 18)Abstract:The Lightweight Surface Manipulation System, or
 LSMS, is a family of scalable long-reach cable-actuated manipulators.
 The design of the LSMS has a high payload ratio for
 efficient operations on planetary surfaces like the Moon or Mars.
 The LSMS has nonlinear, coupled, and hybrid dynamics. The
 engineering decisions that led to these challenging dynamics make
 this structure light and efficient. This paper proposes a novel
 trajectory tracking algorithm for these cranes that facilitates
 precise autonomous and teleoperated operations. This algorithm
 enables these robots to follow complex trajectories that avoid
 obstacles and pickup regolith in a construction site.
2025-07-19 14:02:05,407 - paper_downloader - INFO - 开始下载: AutoGPT+P: Affordance-based Task Planning using Large Language ModelsTimo Birr, Christoph Pohl, Abdelrahman Younes, Tamim AsfourPaper ID 112Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances are the action possibilities of an agent on the environment and the objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, such as tasks with missing objects, by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an Object Affordance Mapping that is automatically generated using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors leading to a success rate of 98% on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79%. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/sw/autogpt-p.
2025-07-19 14:02:06,461 - paper_downloader - INFO - 下载完成: AutoGPT+P Affordance-based Task Planning using Large Language ModelsTimo Birr, Christoph Pohl, Abdel.pdf
2025-07-19 14:02:06,461 - __main__ - INFO - 成功下载: AutoGPT+P: Affordance-based Task Planning using Large Language ModelsTimo Birr, Christoph Pohl, Abdelrahman Younes, Tamim AsfourPaper ID 112Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances are the action possibilities of an agent on the environment and the objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, such as tasks with missing objects, by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an Object Affordance Mapping that is automatically generated using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors leading to a success rate of 98% on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79%. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/sw/autogpt-p.
2025-07-19 14:02:06,461 - paper_downloader - INFO - 开始下载: Implicit Graph Search for Planning on Graphs of Convex SetsRamkumar Natarajan, Chaoqi Liu, Howie Choset, Maxim LikhachevPaper ID 113Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Smooth, collision-free motion planning is a fundamental challenge in robotics with a wide range of applications such as automated manufacturing, search \& rescue, underwater exploration, etc. Graphs of Convex Sets (GCS) is a recent method for synthesizing smooth trajectories by decomposing the planning space into convex sets, forming a graph to encode the adjacency relationships within the decomposition, and then simultaneously searching this graph and optimizing parts of the trajectory to obtain the final trajectory. To do this, one must solve a Mixed Integer Convex Program (MICP) and to mitigate computational time, GCS proposes a convex relaxation that is empirically very tight. Despite this tight relaxation, motion planning with GCS for real-world robotics problems translates to solving the simultaneous batch optimization problem that may contain millions of constraints and therefore can be slow. This is further exacerbated by the fact that the size of the GCS problem is invariant to the planning query. Motivated by the observation that the trajectory solution lies only on a fraction of the set of convex sets, we present two implicit graph search methods for planning on the graph of convex sets called INSATxGCS (IxG) and IxG*. INterleaved Search And Trajectory optimization (INSAT) is a previously developed algorithm that alternates between searching on a graph and optimizing partial paths to find a smooth trajectory. By using an implicit graph search method INSAT on the graph of convex sets, we achieve faster planning while ensuring stronger guarantees on completeness and optimality. Moveover, introducing a search-based technique to plan on the graph of convex sets enables us to easily leverage well-established techniques such as search parallelization, lazy planning, anytime planning, and replanning as future work. Numerical comparisons against GCS demonstrate the superiority of IxG across several applications, including planning for an 18-degree-of-freedom multi-arm assembly scenario.
2025-07-19 14:02:08,800 - paper_downloader - INFO - 下载完成: Implicit Graph Search for Planning on Graphs of Convex SetsRamkumar Natarajan, Chaoqi Liu, Howie Cho.pdf
2025-07-19 14:02:08,800 - __main__ - INFO - 成功下载: Implicit Graph Search for Planning on Graphs of Convex SetsRamkumar Natarajan, Chaoqi Liu, Howie Choset, Maxim LikhachevPaper ID 113Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Smooth, collision-free motion planning is a fundamental challenge in robotics with a wide range of applications such as automated manufacturing, search \& rescue, underwater exploration, etc. Graphs of Convex Sets (GCS) is a recent method for synthesizing smooth trajectories by decomposing the planning space into convex sets, forming a graph to encode the adjacency relationships within the decomposition, and then simultaneously searching this graph and optimizing parts of the trajectory to obtain the final trajectory. To do this, one must solve a Mixed Integer Convex Program (MICP) and to mitigate computational time, GCS proposes a convex relaxation that is empirically very tight. Despite this tight relaxation, motion planning with GCS for real-world robotics problems translates to solving the simultaneous batch optimization problem that may contain millions of constraints and therefore can be slow. This is further exacerbated by the fact that the size of the GCS problem is invariant to the planning query. Motivated by the observation that the trajectory solution lies only on a fraction of the set of convex sets, we present two implicit graph search methods for planning on the graph of convex sets called INSATxGCS (IxG) and IxG*. INterleaved Search And Trajectory optimization (INSAT) is a previously developed algorithm that alternates between searching on a graph and optimizing partial paths to find a smooth trajectory. By using an implicit graph search method INSAT on the graph of convex sets, we achieve faster planning while ensuring stronger guarantees on completeness and optimality. Moveover, introducing a search-based technique to plan on the graph of convex sets enables us to easily leverage well-established techniques such as search parallelization, lazy planning, anytime planning, and replanning as future work. Numerical comparisons against GCS demonstrate the superiority of IxG across several applications, including planning for an 18-degree-of-freedom multi-arm assembly scenario.
2025-07-19 14:02:08,801 - paper_downloader - INFO - 开始下载: Real-Time Anomaly Detection and Reactive Planning with Large Language ModelsRohan Sinha, Amine Elhafsi, Christopher Agia, Matt Foutter, Edward Schmerling, Marco PavonePaper ID 114Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner’s latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on our project page: https://sites.google.com/view/aesop-llm.
2025-07-19 14:02:10,117 - paper_downloader - INFO - 下载完成: Real-Time Anomaly Detection and Reactive Planning with Large Language ModelsRohan Sinha, Amine Elhaf.pdf
2025-07-19 14:02:10,117 - __main__ - INFO - 成功下载: Real-Time Anomaly Detection and Reactive Planning with Large Language ModelsRohan Sinha, Amine Elhafsi, Christopher Agia, Matt Foutter, Edward Schmerling, Marco PavonePaper ID 114Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner’s latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on our project page: https://sites.google.com/view/aesop-llm.
2025-07-19 14:02:10,117 - paper_downloader - INFO - 开始下载: iHERO: Interactive Human-oriented Exploration and Supervision Under Scarce CommunicationZhuoli Tian, Yuyang Zhang, Jinsheng Wei, Meng GuoPaper ID 115Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Exploration of unknown scenes before human entry is essential for safety and efficiency in numerous scenarios, e.g., subterranean exploration, reconnaissance, search and rescue missions. Fleets of autonomous robots are particularly suitable for this task, via concurrent exploration, multi-sensory perception and autonomous navigation. Communication however among the robots can be severely restricted to only close-range exchange via ad-hoc networks. Although some recent works have addressed the problem of collaborative exploration under restricted communication, the crucial role of the human operator has been mostly neglected. Indeed, the operator may: (i) require timely update regarding the exploration progress and fleet status; (ii) prioritize certain regions; and (iii) dynamically move within the explored area; To facilitate these requests, this work proposes an interactive human-oriented online coordination framework for collaborative exploration and supervision under scarce communication (iHERO). The robots switch smoothly and optimally among fast exploration, intermittent exchange of map and sensory data, and return to the operator for status update. It is ensured that these requests are fulfilled online interactively with a pre-specified latency. Extensive large-scale human-in-the-loop simulations and hardware experiments are performed over numerous challenging scenes, which signify its performance such as explored area and efficiency, and validate its potential applicability to real-world scenarios. The videos are available on https://zl-tian.github.io/iHERO/.
2025-07-19 14:02:13,153 - paper_downloader - INFO - 下载完成: iHERO Interactive Human-oriented Exploration and Supervision Under Scarce CommunicationZhuoli Tian, .pdf
2025-07-19 14:02:13,153 - __main__ - INFO - 成功下载: iHERO: Interactive Human-oriented Exploration and Supervision Under Scarce CommunicationZhuoli Tian, Yuyang Zhang, Jinsheng Wei, Meng GuoPaper ID 115Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Exploration of unknown scenes before human entry is essential for safety and efficiency in numerous scenarios, e.g., subterranean exploration, reconnaissance, search and rescue missions. Fleets of autonomous robots are particularly suitable for this task, via concurrent exploration, multi-sensory perception and autonomous navigation. Communication however among the robots can be severely restricted to only close-range exchange via ad-hoc networks. Although some recent works have addressed the problem of collaborative exploration under restricted communication, the crucial role of the human operator has been mostly neglected. Indeed, the operator may: (i) require timely update regarding the exploration progress and fleet status; (ii) prioritize certain regions; and (iii) dynamically move within the explored area; To facilitate these requests, this work proposes an interactive human-oriented online coordination framework for collaborative exploration and supervision under scarce communication (iHERO). The robots switch smoothly and optimally among fast exploration, intermittent exchange of map and sensory data, and return to the operator for status update. It is ensured that these requests are fulfilled online interactively with a pre-specified latency. Extensive large-scale human-in-the-loop simulations and hardware experiments are performed over numerous challenging scenes, which signify its performance such as explored area and efficiency, and validate its potential applicability to real-world scenarios. The videos are available on https://zl-tian.github.io/iHERO/.
2025-07-19 14:02:13,153 - paper_downloader - INFO - 开始下载: Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many RobotsHaimin Hu, Gabriele Dragotto, Zixu Zhang, Kaiqu Liang, Bartolomeo Stellato, Jaime Fernández FisacPaper ID 116Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play’s permutations. To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium.
2025-07-19 14:02:20,321 - paper_downloader - INFO - 下载完成: Who Plays First Optimizing the Order of Play in Stackelberg Games with Many RobotsHaimin Hu, Gabriel.pdf
2025-07-19 14:02:20,321 - __main__ - INFO - 成功下载: Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many RobotsHaimin Hu, Gabriele Dragotto, Zixu Zhang, Kaiqu Liang, Bartolomeo Stellato, Jaime Fernández FisacPaper ID 116Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play’s permutations. To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium.
2025-07-19 14:02:20,321 - paper_downloader - INFO - 开始下载: Goal-Reaching Trajectory Design Near Danger with Piecewise Affine Reach-avoid ComputationLong Kiu Chung, Wonsuhk Jung, Chuizheng Kong, Shreyas KousikPaper ID 117Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Autonomous mobile robots must maintain safety, but should not sacrifice performance, leading to the classical reach-avoid problem: find a trajectory that is guaranteed to reach a goal and avoid obstacles. This paper addresses the near danger case, also known as a narrow gap, where the agent starts near the goal, but must navigate through tight obstacles that block its path. The proposed method builds off the common approach of using a simplified planning model to generate plans, which are then tracked using a high-fidelity tracking model and controller. Existing approaches use reachability analysis to overapproximate the error between these models and ensure safety, but doing so introduces numerical approximation error conservativeness that prevents goal-reaching. The present work instead proposes a Piecewise Affine Reach-avoid Computation (PARC) method to tightly approximate the reachable set of the planning model. PARC significantly reduces conservativeness through a careful choice of the planning model and set representation, along with an effective approach to handling time-varying tracking errors. The utility of this method is demonstrated through extensive numerical experiments in which PARC outperforms state-of-the-art reach avoid methods in near-danger goal reaching. Furthermore, in a simulated demonstration, PARC enables the generation of provably-safe extreme vehicle dynamics drift parking maneuvers. A preliminary hardware demo on a TurtleBot3 also validates the method.
2025-07-19 14:02:29,222 - paper_downloader - INFO - 下载完成: Goal-Reaching Trajectory Design Near Danger with Piecewise Affine Reach-avoid ComputationLong Kiu Ch.pdf
2025-07-19 14:02:29,222 - __main__ - INFO - 成功下载: Goal-Reaching Trajectory Design Near Danger with Piecewise Affine Reach-avoid ComputationLong Kiu Chung, Wonsuhk Jung, Chuizheng Kong, Shreyas KousikPaper ID 117Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Autonomous mobile robots must maintain safety, but should not sacrifice performance, leading to the classical reach-avoid problem: find a trajectory that is guaranteed to reach a goal and avoid obstacles. This paper addresses the near danger case, also known as a narrow gap, where the agent starts near the goal, but must navigate through tight obstacles that block its path. The proposed method builds off the common approach of using a simplified planning model to generate plans, which are then tracked using a high-fidelity tracking model and controller. Existing approaches use reachability analysis to overapproximate the error between these models and ensure safety, but doing so introduces numerical approximation error conservativeness that prevents goal-reaching. The present work instead proposes a Piecewise Affine Reach-avoid Computation (PARC) method to tightly approximate the reachable set of the planning model. PARC significantly reduces conservativeness through a careful choice of the planning model and set representation, along with an effective approach to handling time-varying tracking errors. The utility of this method is demonstrated through extensive numerical experiments in which PARC outperforms state-of-the-art reach avoid methods in near-danger goal reaching. Furthermore, in a simulated demonstration, PARC enables the generation of provably-safe extreme vehicle dynamics drift parking maneuvers. A preliminary hardware demo on a TurtleBot3 also validates the method.
2025-07-19 14:02:29,223 - paper_downloader - INFO - 开始下载: Partially Observable Task and Motion Planning with Uncertainty and Risk AwarenessAidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack KaelblingPaper ID 118Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution. We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that reasoning under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and reinforcement learning strategies. Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception.
2025-07-19 14:02:33,779 - paper_downloader - INFO - 下载完成: Partially Observable Task and Motion Planning with Uncertainty and Risk AwarenessAidan Curtis, Georg.pdf
2025-07-19 14:02:33,779 - __main__ - INFO - 成功下载: Partially Observable Task and Motion Planning with Uncertainty and Risk AwarenessAidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack KaelblingPaper ID 118Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, we learn a high-level abstraction that can be solved efficiently and then refined to continuous actions for execution. We demonstrate our approach on several robotics problems where uncertainty is a crucial factor and show that reasoning under uncertainty in these problems outperforms previously proposed determinized planning, direct search, and reinforcement learning strategies. Lastly, we demonstrate our planner on two real-world robotics problems using recent advancements in probabilistic perception.
2025-07-19 14:02:33,779 - paper_downloader - INFO - 开始下载: Logic-Skill Programming: An Optimization-based Approach to Sequential Skill PlanningTeng Xue, Amirreza Razmjoo, Suhan Shetty, Sylvain CalinonPaper ID 119Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks. Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal. To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks. We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions. To solve such programs, we leverage the use of tensor train factorization to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence. Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art reinforcement learning methods. Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives. The results demonstrate its capability to identify the optimal solution over the full logic and geometric path. The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world.
2025-07-19 14:02:36,857 - paper_downloader - INFO - 下载完成: Logic-Skill Programming An Optimization-based Approach to Sequential Skill PlanningTeng Xue, Amirrez.pdf
2025-07-19 14:02:36,857 - __main__ - INFO - 成功下载: Logic-Skill Programming: An Optimization-based Approach to Sequential Skill PlanningTeng Xue, Amirreza Razmjoo, Suhan Shetty, Sylvain CalinonPaper ID 119Session 15. PlanningPoster Session day 3 (Thursday, July 18)Abstract:Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks. Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal. To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks. We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions. To solve such programs, we leverage the use of tensor train factorization to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence. Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art reinforcement learning methods. Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives. The results demonstrate its capability to identify the optimal solution over the full logic and geometric path. The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world.
2025-07-19 14:02:36,858 - paper_downloader - INFO - 开始下载: DROID: A Large-Scale In-The-Wild Robot Manipulation DatasetAlexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail O'Neill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, Chelsea FinnPaper ID 120Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 65k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance, greater robustness, and improved generalization ability. We open source the full dataset, pre-trained model checkpoints, and a detailed guide for reproducing our robot hardware setup.
2025-07-19 14:02:37,915 - paper_downloader - INFO - 下载完成: DROID A Large-Scale In-The-Wild Robot Manipulation DatasetAlexander Khazatsky, Karl Pertsch, Suraj N.pdf
2025-07-19 14:02:37,915 - __main__ - INFO - 成功下载: DROID: A Large-Scale In-The-Wild Robot Manipulation DatasetAlexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail O'Neill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, Chelsea FinnPaper ID 120Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 65k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance, greater robustness, and improved generalization ability. We open source the full dataset, pre-trained model checkpoints, and a detailed guide for reproducing our robot hardware setup.
2025-07-19 14:02:37,915 - paper_downloader - INFO - 开始下载: Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal GoalsMoritz Reuss, Ömer Erdinç Yağmurlu, Fabian Wenzel, Rudolf LioutikovPaper ID 121Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:This work introduces the Multimodal Diffusion Transformer (MDT), a novel diffusion policy framework, that excels at learning versatile behavior from multimodal goal specifications with few language annotations.
 MDT leverages a diffusion based multimodal transformer backbone and two self-supervised auxiliary objectives to master long-horizon manipulation tasks based on multimodal goals.
 The vast majority of imitation learning methods only learn from individual goal modalities, e.g. either language or goal images.
 However, existing large-scale imitation learning datasets are only partially labeled with language annotations, which prohibits current methods from learning language conditioned behavior from these datasets.
 MDT addresses this challenge by introducing a latent goal-conditioned state representation, that is simultaneously trained on multimodal goal instructions. 
 This state representation aligns image and language based goal embeddings and encodes sufficient information to predict future states.
 The representation is trained via two self-supervised auxiliary objectives that enhance the performance of the presented transformer backbone.
 MDT shows exceptional performance on 164 tasks provided by the challenging CALVIN and LIBERO benchmarks, including a LIBERO version that contains less than $2\%$ language annotations.
 Further, MDT establishes a new record on the CALVIN manipulation challenge, demonstrating an absolute performance improvement of $15\%$ over prior state-of-the-art methods, that require large-scale pretraining and contain $10\times$ more learnable parameters. 
 MDT demonstrated its ability to solve long-horizon manipulation from sparsely annotated data in both simulated and real-world environments. Demonstrations and Code are available at https://intuitive-robots.github.io/mdt_policy/.
2025-07-19 14:02:45,876 - paper_downloader - INFO - 下载完成: Multimodal Diffusion Transformer Learning Versatile Behavior from Multimodal GoalsMoritz Reuss, Ömer.pdf
2025-07-19 14:02:45,877 - __main__ - INFO - 成功下载: Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal GoalsMoritz Reuss, Ömer Erdinç Yağmurlu, Fabian Wenzel, Rudolf LioutikovPaper ID 121Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:This work introduces the Multimodal Diffusion Transformer (MDT), a novel diffusion policy framework, that excels at learning versatile behavior from multimodal goal specifications with few language annotations.
 MDT leverages a diffusion based multimodal transformer backbone and two self-supervised auxiliary objectives to master long-horizon manipulation tasks based on multimodal goals.
 The vast majority of imitation learning methods only learn from individual goal modalities, e.g. either language or goal images.
 However, existing large-scale imitation learning datasets are only partially labeled with language annotations, which prohibits current methods from learning language conditioned behavior from these datasets.
 MDT addresses this challenge by introducing a latent goal-conditioned state representation, that is simultaneously trained on multimodal goal instructions. 
 This state representation aligns image and language based goal embeddings and encodes sufficient information to predict future states.
 The representation is trained via two self-supervised auxiliary objectives that enhance the performance of the presented transformer backbone.
 MDT shows exceptional performance on 164 tasks provided by the challenging CALVIN and LIBERO benchmarks, including a LIBERO version that contains less than $2\%$ language annotations.
 Further, MDT establishes a new record on the CALVIN manipulation challenge, demonstrating an absolute performance improvement of $15\%$ over prior state-of-the-art methods, that require large-scale pretraining and contain $10\times$ more learnable parameters. 
 MDT demonstrated its ability to solve long-horizon manipulation from sparsely annotated data in both simulated and real-world environments. Demonstrations and Code are available at https://intuitive-robots.github.io/mdt_policy/.
2025-07-19 14:02:45,877 - paper_downloader - INFO - 开始下载: Don't Start From Scratch: Behavioral Refinement via Interpolant-based Policy DiffusionKaiqi Chen, Eugene Lim, Lin Kelvin, Yiyang Chen, Harold SohPaper ID 122Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to learn is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGeR, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGeR outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGeR. Code for BRIDGeR is available at https://github.com/clear-nus/bridger.
2025-07-19 14:02:51,020 - paper_downloader - INFO - 下载完成: Don't Start From Scratch Behavioral Refinement via Interpolant-based Policy DiffusionKaiqi Chen, Eug.pdf
2025-07-19 14:02:51,020 - __main__ - INFO - 成功下载: Don't Start From Scratch: Behavioral Refinement via Interpolant-based Policy DiffusionKaiqi Chen, Eugene Lim, Lin Kelvin, Yiyang Chen, Harold SohPaper ID 122Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to learn is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGeR, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGeR outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGeR. Code for BRIDGeR is available at https://github.com/clear-nus/bridger.
2025-07-19 14:02:51,020 - paper_downloader - INFO - 开始下载: Learning Manipulation by Predicting InteractionJia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, Heming Cui, Bin Zhao, Xuelong Li, Yu Qiao, Hongyang LiPaper ID 123Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Representation learning approaches for robotic manipulation have boomed in recent years. Due to the scarcity of in-domain robot data, prevailing methodologies tend to leverage large-scale human video datasets to extract generalizable features for visuomotor policy learning. Despite the progress achieved, prior endeavors disregard the interactive dynamics that capture behavior patterns and physical interaction during the manipulation process, resulting in an inadequate understanding of the relationship between objects and the environment. To this end, we propose a general pre-training pipeline that learns Manipulation by Predicting the Interaction (MPI) and enhances the visual representation. Given a pair of keyframes representing the initial and final states, along with language instructions, our algorithm predicts the transition frame and detects the interaction object, respectively. These two learning objectives achieve superior comprehension towards “how-to-interact” and “where-to-interact”. We conduct a comprehensive evaluation of four challenging robotic tasks. The experimental results demonstrate that MPI exhibits remarkable improvement by 10% to 64% compared with previous state-of-the-art in real-world robot platforms as well as simulation environments. Code and checkpoints are 
 publicly shared at https://github.com/OpenDriveLab/MPI.
2025-07-19 14:02:53,374 - paper_downloader - INFO - 下载完成: Learning Manipulation by Predicting InteractionJia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Che.pdf
2025-07-19 14:02:53,374 - __main__ - INFO - 成功下载: Learning Manipulation by Predicting InteractionJia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, Heming Cui, Bin Zhao, Xuelong Li, Yu Qiao, Hongyang LiPaper ID 123Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Representation learning approaches for robotic manipulation have boomed in recent years. Due to the scarcity of in-domain robot data, prevailing methodologies tend to leverage large-scale human video datasets to extract generalizable features for visuomotor policy learning. Despite the progress achieved, prior endeavors disregard the interactive dynamics that capture behavior patterns and physical interaction during the manipulation process, resulting in an inadequate understanding of the relationship between objects and the environment. To this end, we propose a general pre-training pipeline that learns Manipulation by Predicting the Interaction (MPI) and enhances the visual representation. Given a pair of keyframes representing the initial and final states, along with language instructions, our algorithm predicts the transition frame and detects the interaction object, respectively. These two learning objectives achieve superior comprehension towards “how-to-interact” and “where-to-interact”. We conduct a comprehensive evaluation of four challenging robotic tasks. The experimental results demonstrate that MPI exhibits remarkable improvement by 10% to 64% compared with previous state-of-the-art in real-world robot platforms as well as simulation environments. Code and checkpoints are 
 publicly shared at https://github.com/OpenDriveLab/MPI.
2025-07-19 14:02:53,374 - paper_downloader - INFO - 开始下载: URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World ImagesQiuyu Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Dieter Fox, Abhishek GuptaPaper ID 124Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with ``natural” kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a data generation pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.
2025-07-19 14:02:59,351 - paper_downloader - INFO - 下载完成: URDFormer A Pipeline for Constructing Articulated Simulation Environments from Real-World ImagesQiuy.pdf
2025-07-19 14:02:59,351 - __main__ - INFO - 成功下载: URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World ImagesQiuyu Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Dieter Fox, Abhishek GuptaPaper ID 124Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with ``natural” kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a data generation pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.
2025-07-19 14:02:59,352 - paper_downloader - INFO - 开始下载: Natural Language Can Help Bridge the Sim2Real GapAlbert Yu, Adeline Foote, Ray Mooney, Roberto Martín-MartínPaper ID 126Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control. Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data. However, when learning in the real world, data is expensive. Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task. However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar. To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics. Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images. We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation. We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations. Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40\%. See additional videos and materials at https://robin-lab.cs.utexas.edu/lang4sim2real/.
2025-07-19 14:03:00,620 - paper_downloader - INFO - 下载完成: Natural Language Can Help Bridge the Sim2Real GapAlbert Yu, Adeline Foote, Ray Mooney, Roberto Martí.pdf
2025-07-19 14:03:00,620 - __main__ - INFO - 成功下载: Natural Language Can Help Bridge the Sim2Real GapAlbert Yu, Adeline Foote, Ray Mooney, Roberto Martín-MartínPaper ID 126Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control. Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data. However, when learning in the real world, data is expensive. Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task. However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar. To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics. Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images. We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation. We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations. Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40\%. See additional videos and materials at https://robin-lab.cs.utexas.edu/lang4sim2real/.
2025-07-19 14:03:00,620 - paper_downloader - INFO - 开始下载: PoCo: Policy Composition from and for Heterogeneous Robot LearningLirui Wang, Jialiang Zhao, Yilun Du, Edward Adelson, Russ TedrakePaper ID 127Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.
2025-07-19 14:03:02,937 - paper_downloader - INFO - 下载完成: PoCo Policy Composition from and for Heterogeneous Robot LearningLirui Wang, Jialiang Zhao, Yilun Du.pdf
2025-07-19 14:03:02,937 - __main__ - INFO - 成功下载: PoCo: Policy Composition from and for Heterogeneous Robot LearningLirui Wang, Jialiang Zhao, Yilun Du, Edward Adelson, Russ TedrakePaper ID 127Session 17. Imitation learningPoster Session day 4 (Friday, July 19)Abstract:Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.
2025-07-19 14:03:02,937 - paper_downloader - INFO - 开始下载: Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHandZilin Si, Kevin Lee Zhang, Zeynep Temel, Oliver KroemerPaper ID 128Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90\% success rate.
2025-07-19 14:03:09,354 - paper_downloader - INFO - 下载完成: Tilde Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHandZilin Si, Kevin Lee .pdf
2025-07-19 14:03:09,354 - __main__ - INFO - 成功下载: Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHandZilin Si, Kevin Lee Zhang, Zeynep Temel, Oliver KroemerPaper ID 128Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Dexterous robotic manipulation remains a challenging domain due to its strict demands for precision and robustness on both hardware and software. While dexterous robotic hands have demonstrated remarkable capabilities in complex tasks, efficiently learning adaptive control policies for hands still presents a significant hurdle given the high dimensionalities of hands and tasks. To bridge this gap, we propose Tilde, an imitation learning-based in-hand manipulation system on a dexterous DeltaHand. It leverages 1) a low-cost, configurable, simple-to-control, soft dexterous robotic hand, DeltaHand, 2) a user-friendly, precise, real-time teleoperation interface, TeleHand, and 3) an efficient and generalizable imitation learning approach with diffusion policies. Our proposed TeleHand has a kinematic twin design to the DeltaHand that enables precise one-to-one joint control of the DeltaHand during teleoperation. This facilitates efficient high-quality data collection of human demonstrations in the real world. To evaluate the effectiveness of our system, we demonstrate the fully autonomous closed-loop deployment of diffusion policies learned from demonstrations across seven dexterous manipulation tasks with an average 90\% success rate.
2025-07-19 14:03:09,354 - paper_downloader - INFO - 开始下载: HACMan++: Spatially-Grounded Motion Primitives for ManipulationBowen Jiang, Yilin Wu, Wenxuan Zhou, Chris Paxton, David HeldPaper ID 129Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Although end-to-end robot learning has shown some success for robot manipulation, the learned policies are often not sufficiently robust to variations in object pose or geometry. To improve the policy generalization, we introduce spatially-grounded parameterized motion primitives in our method HACMan++. Specifically, we propose an action representation consisting of three components: “what” primitive type (such as grasp or push) to execute, “where” the primitive will be grounded (e.g. where the gripper will make contact with the world), and “how” the primitive motion is executed, such as parameters specifying the push direction or grasp orientation. These three components define a novel discrete-continuous action space for reinforcement learning. Our framework enables robot agents to learn to chain diverse motion primitives together and select appropriate primitive parameters to complete long-horizon manipulation tasks. By grounding the primitives on a spatial location in the environment, our method is able to effectively generalize across object shape and pose variations. Our approach significantly outperforms existing methods, particularly in complex scenarios demanding both high-level sequential reasoning and object generalization. With zero-shot sim-to-real transfer, our policy succeeds in challenging real-world manipulation tasks, with generalization to unseen objects. Videos can be found on the project website: https://sgmp-rss2024.github.io.
2025-07-19 14:03:15,333 - paper_downloader - INFO - 下载完成: HACMan++ Spatially-Grounded Motion Primitives for ManipulationBowen Jiang, Yilin Wu, Wenxuan Zhou, C.pdf
2025-07-19 14:03:15,333 - __main__ - INFO - 成功下载: HACMan++: Spatially-Grounded Motion Primitives for ManipulationBowen Jiang, Yilin Wu, Wenxuan Zhou, Chris Paxton, David HeldPaper ID 129Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Although end-to-end robot learning has shown some success for robot manipulation, the learned policies are often not sufficiently robust to variations in object pose or geometry. To improve the policy generalization, we introduce spatially-grounded parameterized motion primitives in our method HACMan++. Specifically, we propose an action representation consisting of three components: “what” primitive type (such as grasp or push) to execute, “where” the primitive will be grounded (e.g. where the gripper will make contact with the world), and “how” the primitive motion is executed, such as parameters specifying the push direction or grasp orientation. These three components define a novel discrete-continuous action space for reinforcement learning. Our framework enables robot agents to learn to chain diverse motion primitives together and select appropriate primitive parameters to complete long-horizon manipulation tasks. By grounding the primitives on a spatial location in the environment, our method is able to effectively generalize across object shape and pose variations. Our approach significantly outperforms existing methods, particularly in complex scenarios demanding both high-level sequential reasoning and object generalization. With zero-shot sim-to-real transfer, our policy succeeds in challenging real-world manipulation tasks, with generalization to unseen objects. Videos can be found on the project website: https://sgmp-rss2024.github.io.
2025-07-19 14:03:15,333 - paper_downloader - INFO - 开始下载: RoboPack: Learning Tactile-Informed Dynamics Models for Dense PackingBo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston Tan, Yunzhu Li, Jiajun WuPaper ID 130Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.
2025-07-19 14:03:18,957 - paper_downloader - INFO - 下载完成: RoboPack Learning Tactile-Informed Dynamics Models for Dense PackingBo Ai, Stephen Tian, Haochen Shi.pdf
2025-07-19 14:03:18,957 - __main__ - INFO - 成功下载: RoboPack: Learning Tactile-Informed Dynamics Models for Dense PackingBo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston Tan, Yunzhu Li, Jiajun WuPaper ID 130Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.
2025-07-19 14:03:18,957 - paper_downloader - INFO - 开始下载: Configuration Space Distance Fields for Manipulation PlanningYiming Li, Xuemin Chi, Amirreza Razmjoo, Sylvain CalinonPaper ID 131Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:The signed distance field (SDF) is a popular implicit shape representation in robotics, providing geometric information about objects and obstacles in a form that can easily be combined with control, optimization and learning techniques. Most often, SDFs are used to represent distances in task space, which corresponds to the familiar notion of distances that we perceive in our 3D world. However, SDFs can mathematically be used in other spaces, including robot configuration spaces. For a robot manipulator, this configuration space typically corresponds to the joint angles for each articulation of the robot. While it is customary in robot planning to express which portions of the configuration space are free from collision with obstacles, it is less common to think of this information as a distance field in the configuration space. In this paper, we demonstrate the potential of considering SDFs in the robot configuration space for optimization, which we call configuration space distance field (or CDField for short). Similarly to the use of SDF in task space, CDField provides an efficient joint angle distance query and direct access to the derivatives (joint angle velocity). Most approaches split the overall computation with one part in task space followed by one part in configuration space (evaluating distances in task space and then computing actions with inverse kinematics). Instead, CDField allows the implicit structure to be leveraged by control, optimization, and learning problems in a unified manner. In particular, we propose an efficient algorithm to compute and fuse CDFields that can be generalized to arbitrary scenes. A corresponding neural CDField representation using multilayer perceptrons (MLPs) is also presented to obtain a compact and continuous representation while improving computation efficiency. We demonstrate the effectiveness of CDField with planar obstacle avoidance examples and with a 7-axis Franka Emika robot in inverse kinematics and manipulation planning tasks.
2025-07-19 14:03:20,040 - paper_downloader - INFO - 下载完成: Configuration Space Distance Fields for Manipulation PlanningYiming Li, Xuemin Chi, Amirreza Razmjoo.pdf
2025-07-19 14:03:20,040 - __main__ - INFO - 成功下载: Configuration Space Distance Fields for Manipulation PlanningYiming Li, Xuemin Chi, Amirreza Razmjoo, Sylvain CalinonPaper ID 131Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:The signed distance field (SDF) is a popular implicit shape representation in robotics, providing geometric information about objects and obstacles in a form that can easily be combined with control, optimization and learning techniques. Most often, SDFs are used to represent distances in task space, which corresponds to the familiar notion of distances that we perceive in our 3D world. However, SDFs can mathematically be used in other spaces, including robot configuration spaces. For a robot manipulator, this configuration space typically corresponds to the joint angles for each articulation of the robot. While it is customary in robot planning to express which portions of the configuration space are free from collision with obstacles, it is less common to think of this information as a distance field in the configuration space. In this paper, we demonstrate the potential of considering SDFs in the robot configuration space for optimization, which we call configuration space distance field (or CDField for short). Similarly to the use of SDF in task space, CDField provides an efficient joint angle distance query and direct access to the derivatives (joint angle velocity). Most approaches split the overall computation with one part in task space followed by one part in configuration space (evaluating distances in task space and then computing actions with inverse kinematics). Instead, CDField allows the implicit structure to be leveraged by control, optimization, and learning problems in a unified manner. In particular, we propose an efficient algorithm to compute and fuse CDFields that can be generalized to arbitrary scenes. A corresponding neural CDField representation using multilayer perceptrons (MLPs) is also presented to obtain a compact and continuous representation while improving computation efficiency. We demonstrate the effectiveness of CDField with planar obstacle avoidance examples and with a 7-axis Franka Emika robot in inverse kinematics and manipulation planning tasks.
2025-07-19 14:03:20,040 - paper_downloader - INFO - 开始下载: Towards Tight Convex Relaxations for Contact-Rich ManipulationBernhard Paus Graesdal, Shao Yuan Chew Chia, Tobia Marcucci, Savva Morozov, Alexandre Amice, Pablo Parrilo, Russ TedrakePaper ID 132Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:We present a novel method for global motion planning of robotic systems that interact with the environment through contacts. Our method directly handles the hybrid nature of such tasks using tools from convex optimization. We formulate the motion-planning problem as a shortest-path problem in a graph of convex sets, where a path in the graph corresponds to a contact sequence and a convex set models the quasi-static dynamics within a fixed contact mode. For each contact mode, we use semidefinite programming to relax the nonconvex dynamics that results from the simultaneous optimization of the object’s pose, contact locations, and contact forces. The result is a tight convex relaxation of the overall planning problem, that can be efficiently solved and quickly rounded to find a feasible contact-rich trajectory. As an initial application for evaluating our method, we apply it on the task of planar pushing. Exhaustive experiments show that our convex-optimization method generates plans that are consistently within a small percentage of the global optimum, without relying on an initial guess, and that our method succeeds in finding trajectories where a state-of-the-art baseline for contact-rich planning usually fails. We demonstrate the quality of these plans on a real robotic system.
2025-07-19 14:03:21,897 - paper_downloader - INFO - 下载完成: Towards Tight Convex Relaxations for Contact-Rich ManipulationBernhard Paus Graesdal, Shao Yuan Chew.pdf
2025-07-19 14:03:21,897 - __main__ - INFO - 成功下载: Towards Tight Convex Relaxations for Contact-Rich ManipulationBernhard Paus Graesdal, Shao Yuan Chew Chia, Tobia Marcucci, Savva Morozov, Alexandre Amice, Pablo Parrilo, Russ TedrakePaper ID 132Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:We present a novel method for global motion planning of robotic systems that interact with the environment through contacts. Our method directly handles the hybrid nature of such tasks using tools from convex optimization. We formulate the motion-planning problem as a shortest-path problem in a graph of convex sets, where a path in the graph corresponds to a contact sequence and a convex set models the quasi-static dynamics within a fixed contact mode. For each contact mode, we use semidefinite programming to relax the nonconvex dynamics that results from the simultaneous optimization of the object’s pose, contact locations, and contact forces. The result is a tight convex relaxation of the overall planning problem, that can be efficiently solved and quickly rounded to find a feasible contact-rich trajectory. As an initial application for evaluating our method, we apply it on the task of planar pushing. Exhaustive experiments show that our convex-optimization method generates plans that are consistently within a small percentage of the global optimum, without relying on an initial guess, and that our method succeeds in finding trajectories where a state-of-the-art baseline for contact-rich planning usually fails. We demonstrate the quality of these plans on a real robotic system.
2025-07-19 14:03:21,897 - paper_downloader - INFO - 开始下载: THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic ManipulationWilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, Dieter FoxPaper ID 133Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present
 THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 14 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, physical properties perturbations and camera pose. Using THE COLOSSEUM, we compare 5 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades ≥75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated ( ̄R2 = 0.614) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation.
2025-07-19 14:03:25,229 - paper_downloader - INFO - 下载完成: THE COLOSSEUM A Benchmark for Evaluating Generalization for Robotic ManipulationWilbert Pumacay, Ish.pdf
2025-07-19 14:03:25,229 - __main__ - INFO - 成功下载: THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic ManipulationWilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, Dieter FoxPaper ID 133Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present
 THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 14 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, physical properties perturbations and camera pose. Using THE COLOSSEUM, we compare 5 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades ≥75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated ( ̄R2 = 0.614) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation.
2025-07-19 14:03:25,229 - paper_downloader - INFO - 开始下载: One-Shot Imitation Learning with Invariance Matching for Robotic ManipulationXinyu Zhang, Abdeslam BoulariasPaper ID 134Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Learning a single universal policy that can perform
 a diverse set of manipulation tasks is a promising new direction
 in robotics. However, existing techniques are limited to learning
 policies that can only perform tasks that are encountered during
 training, and require a large number of demonstrations to learn
 new tasks. Humans, on the other hand, often can learn a new
 task from a single unannotated demonstration. In this work,
 we propose the Invariance-Matching One-shot Policy Learning
 (IMOP) algorithm. In contrast to the standard practice of learning
 the end-effector’s pose directly, IMOP first learns invariant regions
 of the state space for a given task, and then computes the end-
 effector’s pose through matching the invariant regions between
 demonstrations and test scenes. Trained on the 18 RLBench
 tasks, IMOP achieves a success rate that outperforms the state-
 of-the-art consistently, by 4.5% on average over the 18 tasks.
 More importantly, IMOP can learn a novel task from a single
 unannotated demonstration, and without any fine-tuning, and
 achieves an average success rate improvement of 11.5% over the
 state-of-the-art on 22 novel tasks selected across nine categories.
 IMOP can also generalize to new shapes and learn to manipulate
 objects that are different from those in the demonstration. Further,
 IMOP can perform one-shot sim-to-real transfer using a single
 real-robot demonstration.
2025-07-19 14:03:27,359 - paper_downloader - INFO - 下载完成: One-Shot Imitation Learning with Invariance Matching for Robotic ManipulationXinyu Zhang, Abdeslam B.pdf
2025-07-19 14:03:27,359 - __main__ - INFO - 成功下载: One-Shot Imitation Learning with Invariance Matching for Robotic ManipulationXinyu Zhang, Abdeslam BoulariasPaper ID 134Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:Learning a single universal policy that can perform
 a diverse set of manipulation tasks is a promising new direction
 in robotics. However, existing techniques are limited to learning
 policies that can only perform tasks that are encountered during
 training, and require a large number of demonstrations to learn
 new tasks. Humans, on the other hand, often can learn a new
 task from a single unannotated demonstration. In this work,
 we propose the Invariance-Matching One-shot Policy Learning
 (IMOP) algorithm. In contrast to the standard practice of learning
 the end-effector’s pose directly, IMOP first learns invariant regions
 of the state space for a given task, and then computes the end-
 effector’s pose through matching the invariant regions between
 demonstrations and test scenes. Trained on the 18 RLBench
 tasks, IMOP achieves a success rate that outperforms the state-
 of-the-art consistently, by 4.5% on average over the 18 tasks.
 More importantly, IMOP can learn a novel task from a single
 unannotated demonstration, and without any fine-tuning, and
 achieves an average success rate improvement of 11.5% over the
 state-of-the-art on 22 novel tasks selected across nine categories.
 IMOP can also generalize to new shapes and learn to manipulate
 objects that are different from those in the demonstration. Further,
 IMOP can perform one-shot sim-to-real transfer using a single
 real-robot demonstration.
2025-07-19 14:03:27,359 - paper_downloader - INFO - 开始下载: Tactile-Driven Non-Prehensile Object Manipulation via Extrinsic Contact Mode ControlMiquel Oller, Dmitry Berenson, Nima FazeliPaper ID 135Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:In this paper, we consider the problem of non-prehensile manipulation using grasped objects. This problem is a superset of many common manipulation skills including instances of tool-use (e.g., grasped spatula flipping a burger) and assembly (e.g., screwdriver tightening a screw). Here, we present an algorithmic approach for non-prehensile manipulation leveraging a gripper with highly compliant and high-resolution tactile sensors. Our approach solves for robot actions that drive object poses and forces to desired values while obeying the complex dynamics induced by the sensors as well as the constraints imposed by static equilibrium, object kinematics, and frictional contact. Our method is able to produce a variety of manipulation skills and is amenable to gradient-based optimization by exploiting differentiability within contact modes (e.g., specifications of sticking or sliding contacts). We evaluate 4 variants of controllers that attempt to realize these plans and demonstrate a number of complex skills including non-prehensile planar sliding and pivoting on a variety of object geometries. The perception and controls capabilities that drive these skills are the building blocks towards dexterous and reactive autonomy in unstructured environments.
2025-07-19 14:03:28,143 - paper_downloader - INFO - 下载完成: Tactile-Driven Non-Prehensile Object Manipulation via Extrinsic Contact Mode ControlMiquel Oller, Dm.pdf
2025-07-19 14:03:28,143 - __main__ - INFO - 成功下载: Tactile-Driven Non-Prehensile Object Manipulation via Extrinsic Contact Mode ControlMiquel Oller, Dmitry Berenson, Nima FazeliPaper ID 135Session 16. ManipulationPoster Session day 4 (Friday, July 19)Abstract:In this paper, we consider the problem of non-prehensile manipulation using grasped objects. This problem is a superset of many common manipulation skills including instances of tool-use (e.g., grasped spatula flipping a burger) and assembly (e.g., screwdriver tightening a screw). Here, we present an algorithmic approach for non-prehensile manipulation leveraging a gripper with highly compliant and high-resolution tactile sensors. Our approach solves for robot actions that drive object poses and forces to desired values while obeying the complex dynamics induced by the sensors as well as the constraints imposed by static equilibrium, object kinematics, and frictional contact. Our method is able to produce a variety of manipulation skills and is amenable to gradient-based optimization by exploiting differentiability within contact modes (e.g., specifications of sticking or sliding contacts). We evaluate 4 variants of controllers that attempt to realize these plans and demonstrate a number of complex skills including non-prehensile planar sliding and pivoting on a variety of object geometries. The perception and controls capabilities that drive these skills are the building blocks towards dexterous and reactive autonomy in unstructured environments.
2025-07-19 14:03:28,143 - __main__ - INFO - 下载完成，共下载 131 个PDF文件
2025-07-19 14:03:28,143 - __main__ - INFO - 下载完成，文件保存在: /home/yx_xx/myProject/PaperRead/Data/PC_Data/downloads
2025-07-19 14:03:28,143 - __main__ - INFO - PaperCrawler运行完成！
