=== PDF文件: IMLE Policy Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Est.pdf ===
=== 时间: 2025-07-22 15:50:12.294359 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：IMLE Policy:
Fast and Sample Efficient Visuomotor Policy Learning via
Implicit Maximum Likelihood Estimation
Krishan Rana1, Robert Lee, David Pershouse1 and Niko Sunderhauf1
Equal Contribution
1QUT Centre for Robotics
IMLE Policy
Diffusion Policy
Flow Matching Policy (1 Step)
Real Data
Generated Data (ao)
Capturing Multi-Modality from Limited Demonstrations. We present IMLE Policy, a novel behaviour cloning
approach that is capable of learning complex multi-modal action distributions from less data than prior approaches, while only
requiring a single network forward pass. Top: When trained on 20 samples for 4000 epochs, IMLE Policy learns to accurately
model the underlying multi-modal function, while Diffusion Policy with 100 de-noising steps struggles. Flow Matching with a
single step collapses modes. Bottom: When trained on only 17 demonstrations for a shoe racking task, IMLE Policy maintains
the other methods predict poor trajectories.
AbstractRecent advances in imitation learning, particularly
using generative modelling techniques like diffusion, have enabled
policies to capture complex multi-modal action distributions.
inference steps for action generation, posing challenges in robotics
where the cost for data collection is high and computation
resources are limited. To address this, we introduce IMLE Policy,
a novel behaviour cloning approach based on Implicit Maximum
Likelihood Estimation (IMLE). IMLE Policy excels in low-data
requiring 38 less data on average to match the performance
of baseline methods in learning complex multi-modal behaviours.
Its simple generator-based architecture enables single-step action
Diffusion Policy, while outperforming single-step Flow Matching.
We validate our approach across diverse manipulation tasks in
simulated and real-world environments, showcasing its ability to
capture complex behaviours under data constraints. Videos and
code are provided on our project page: imle-policy.github.io.
I. INTRODUCTION
Learning policies from demonstrations is becoming a key
approach for enabling robots to perform complex tasks. At its
a way that captures the nuanced and often multi-modal nature
of human behaviour. However, leveraging behaviour cloning
for real-world robotics applications presents unique challenges
including the ability to learn these highly multi-modal action
distributions from limited demonstration data which can be ex-
pensive to collect, and the need for computational efficiency to
enable real-time operation. Recent advancements in generative
[20, 4], have demonstrated promise in capturing complex
multi-modal action distributions for behaviour cloning. How-
ing inference, as they require iterative sampling processes
to generate actions. Additionally, they often demand large
datasets to train performant models effectively. Addressing
these challenges is crucial for enhancing the practicality and
adoption of behaviour cloning in real-world tasks.
Generative Adversarial Networks (GANs) are an alternative
approach to generative modelling with efficient single-step
stability and mode collapse, where parts of the data distri-
bution are ignored, limiting their ability to effectively model
was introduced as an alternative training objective for
single-step generative models, based on a simple objective that
ensures every datapoint is well-represented by at least one
generated sample. In essence, IMLE minimises the distance
between each data point and its nearest generated sample,
guaranteeing comprehensive mode coverage and maintaining
stable training, even with smaller datasets. This makes IMLE
a compelling alternative to diffusion models for developing
efficient and faster behaviour cloning methods suitable for
real-world robotics.
In this work, we propose IMLE Policy, a novel extension
of Implicit Maximum Likelihood Estimation tailored for con-
ditional behaviour cloning. We leverage a particular instanti-
ation of IMLE that utilises rejection sampling in the training
effectively enabling the model to sample all possible modes
during inference. IMLE Policy offers several key advantages
that make it particularly promising for real-world robotic
Expressive multi-modal action distributions: By op-
timising the policy to generate samples close to every
possible data point, IMLE Policy by nature does not drop
generations for a given state (Figure 1 (Bottom)).
Sample efficiency: IMLE Policy can accurately model
complex multi-modal distributions using 38 less data
on average than Diffusion Policy and single-step Flow-
Matching to achieve similar performance (Figure 1
Fast inference: IMLE Policy can generate multi-modal
actions using only a single forward pass  a 97.3
increase in inference speed when compared to vanilla Dif-
fusion Policy without collapsing modes when compared
to single step Flow Matching.
We evaluate IMLE Policy across diverse simulation bench-
marks and multi-modal real-world tasks, demonstrating its
effectiveness as an alternative to existing generative model-
based behaviour cloning approaches. IMLE Policy meets the
critical desiderata for real-world robotics: sample efficiency,
computational speed, and expressiveness. (1) IMLE Policy
outperforms state-of-the-art baselines in low-data regimes,
while matching performance when more data is available,
learning performant real-world policies from as few as 17
demonstrations. (2) IMLE Policy captures multi-modal action
distributions in a single step, generating diverse trajectories
across varying conditions, avoiding mode collapse seen in
other single-step methods. (3) IMLE Policy enables fast real-
time inference, reducing latency by 97.3 compared to it-
erative diffusion models while maintaining competitive task
success rates. We further analyse sample efficiency, multi-
modal expressivity, and key design choices to understand
IMLE Policys strengths. We provide code and videos of our
system operating on real-world tasks on our project page:
II. RELATED WORK
We organise our discussion of prior work based on three key
behaviour cloning desiderata for real-world robotics: captur-
ing multi-modal behaviour, achieving sample efficiency, and
enabling fast inference. While existing works address each of
these aspects individually, relatively few approaches attempt
to satisfy all three simultaneously.
A. Multi-Modality in Behaviour Cloning
Behaviour cloning has been widely explored as a means
of enabling robots to learn from human demonstrations,
achieving success across various manipulation tasks [42, 12,
multi-modal nature of human demonstrations is a fundamental
challenge in behaviour cloning, and generative models have
been extensively applied to address this issue. Methods such
as Conditional Variational Autoencoders (CVAEs) [24, 43],
Energy-Based Models (EBMs) , and Vector Quantization
have been used to model multi-modal action distributions,
though each comes with trade-offs such as mode collapse or
complex multi-stage training.
a dominant approach due to their stable training dynamics
and strong performance in multi-modal imitation learning
[6, 8, 7, 10, 21]. However, these models require an iterative
IMLE Policy
Diffusion Policy
Flow Matching (1 Step)
Varying End Effector Start Location
Fig. 2: Qualitative Analysis of Mode Capturing Performance. Illustrating the open-loop trajectories generated by different
methods (rows) for varying initial end-effector positions (columns) in the Push-T task, where multiple trajectories were generated
for each condition using different random seeds. The goal is to push the T-shaped block from its initial position to align with
the target (green). The dataset exhibits multi-modality, with human demonstrations performing the task through various modes
of behaviour, for example going either left or right to get to the opposite side. IMLE Policy successfully captures all modes
without collapsing or biasing, maintaining trajectory diversity even in underrepresented modes. In contrast, Diffusion Policy
tends to bias towards one mode when close to an edge, showing sensitivity to initial conditions, while Flow Matching collapses
sampling process during inference, making them computation-
ally expensive.
An alternative class of generative models, IMLE (Implicit
Maximum Likelihood Estimation), has remained largely unex-
plored in robotics despite its effectiveness in learning expres-
sive distributions. In this paper, we introduce an IMLE-based
approach for behaviour cloning, demonstrating its ability to
efficiently model complex multi-modal action distributions in
a single inference step, offering a simpler and more efficient
alternative to existing diffusion-based methods.
B. Sample Efficiency of Behaviour Cloning
Despite the widespread success of diffusion models in
effective training , which is a significant limitation in im-
itation learning, where expert demonstrations are expensive to
collect. Improving the sample efficiency of behaviour cloning
remains an active research area.
Current research directions for improving sample efficiency
involve optimising input representations. For example, 3D
Diffusion Policy utilises point clouds to improve generalisation
in 3D space , while approaches leveraging SO(2) and
SIM(3) equivariances improve learning efficiency in tasks with
structured transformations [37, 38]. Affordance-centric repre-
sentations have also been explored to improve generalisation
from limited demonstrations .
Our approach is orthogonal to these representation-based
rithm itself to enhance sample efficiency. This allows IMLE
Policy to be complementary to existing strategies, meaning
it can be combined with improved input representations for
even greater efficiency gains. Additionally, we conduct a
comprehensive study on sample efficiency, explicitly exam-
ining the relationship between dataset size and performance.
While dataset size is often an overlooked factor in behaviour
cloning research, we highlight its direct impact on policy
cloning methods scale with available data.
C. Inference Speed and Behaviour Cloning
A key limitation of most state-of-the-art behaviour cloning
approaches is their multi-step inference process, which re-
quires iterative de-noising or auto-regressive steps to generate
actions. This significantly increases computational cost during
approaches aim to improve upon this. Consistency Models
distill multi-step diffusion policies into single-step policies
while maintaining performance [29, 35]. Streaming Diffusion
modifies the denoising process to allow earlier actions to re-
quire fewer de-noising steps, speeding up inference . Flow
Matching provides an alternative continuous-time generative
modelling framework with more efficient probability paths,
reducing inference steps [20, 22], though in practice, it still
requires multiple steps to prevent mode collapse when applied
to behaviour cloning [15, 4, 41].
While these methods aim to speed up inference, they
either require additional distillation steps or do not fully
eliminate iterative sampling. In contrast, our approach natively
enables single-step inference while still capturing complex,
multi-modal action distributions. This allows IMLE Policy to
perform fast, real-time inference without requiring multi-stage
training or distillation, making it a promising alternative for
computationally constrained settings.
III. METHOD
A. Background
In the context of generative modelling, the aim is to learn the
probability distribution of samples p(x), which would allow
us to then synthesise new samples via the trained model.
We can represent the generator as a function T : Z X
that transforms samples from the latent space Z to the space
of target samples X, implemented as a neural network with
parameters . Such models have been historically trained
via a generative adversarial objective (as in GANs), but this
approach is prone to mode collapse, where only some modes
of the target distribution are modelled.
A more recent approach, Implicit Maximum Likelihood Es-
timation (IMLE) has been introduced, providing an alternative
training objective for the generator that avoids mode collapse
. The IMLE objective ensures that each training sample
is well represented by the generator with samples generated
The IMLE training objective  is written as:
IMLE  argmin
minj[m] d (xi, T(zj))
where d(, ) is a distance metric, n is the number of data
During training, m number of samples zj are drawn from
the latent prior distribution, a standard Gaussian distribution.
These are transformed by the generator T into synthesised
samples. For each training sample, the nearest synthetic sam-
ple is selected according to the distance metric d(, ). While
Algorithm 1: IMLE Policy Training
of latents m, rejection threshold , distance
metric d(, ), policy T(z, y).
1 foreach (Oi, Ai) D do
Sample m latent vectors {zj}m
Generate m trajectories {Aj}m
Aj  T(zj, Oi);
Compute distances dj  d(Ai, Aj) for j [m];
Filter valid trajectories V  {j [m] : dj };
Select nearest trajectory j argminjV dj;
Update  to minimise d(Ai, Aj);
9 return T
Algorithm 2: Inference with Temporal Consistency
in IMLE Policy
trajectory Aprev[Ta:Tp].
1 Sample m latent vectors {zj}m
2 Batch generate m trajectories {Aj}m
Aj  T(zj, O);
3 Compute overlaps oj  d(Aprev[Ta:Tp], Aj
[0:Ta]) for
4 Select trajectory j argminj[m] oj;
5 return Aj
this objective is effective for avoiding mode collapse, the
selection procedure results in certain latent samples being
rarely selected, even if they have a high likelihood under the
latent prior distribution.
Rejection Sampling IMLE (RS-IMLE)  alleviates this
issue by rejecting samples from the selection process if
d(xi, T(z)) < , meaning they are too close to the training
data sample. The remaining samples are then used in the
IMLE training objective as before. Intuitively, this prevents
the selection process from repeatedly selecting similar samples
after they have already converged to fitting the data sample
with some accuracy, defined by the parameter .
B. Conditional RS-IMLE
We extend the RS-IMLE framework , initially devel-
oped for unconditional image generation, to the conditional
conditioning variable y, and instead of the m samples shared
between all data points, we now select m samples to synthesise
for each data point, each sharing the conditioning value of that
Diffusion
Diffusion
IMLE Policy Single-Step Inference
Observations
Baseline Multi-Step Inference
IMLE Policy Single-Step Inference with Consistency
Policy Inference
Rejection Sampling
Modelling the Underlying Distribution
Executed Actions
Unexecuted Actions
Trajectory
Predictions
Previous
Unexecuted Actions
Nearest Neighbour in overlapping
timesteps selected for consistency
Fig. 3: IMLE Policy Overview a) Training: The policy takes in a sequence of past observations O and m sampled latents z
for which the policy generates m sequences of predicted actions A. Generated trajectories that lie within the rejection sampling
threshold  are rejected. From the remaining trajectories, the nearest-neighbour to the ground truth trajectory is selected for
training. We minimise the the distance between this trajectory and the ground truth trajectory to optimise the policy. As the loss
focuses on each data sample, it ensures that all modes are captured even from small datasets. b) When compared to baselines
with similar multi-modal capturing capabilities, IMLE can generate actions with a single inference step as opposed to multi-step
de-noising processes. c) For highly multi-modal tasks, we enhance the performance of IMLE Policy by introducing a simple
inference procedure to induce consistency upon mode choice based on a nearest-neighbour search over batch-generated action
proposals with the previously executed action sequence.
vectors {zj}m
j1 from the standard Gaussian prior. These latent
vectors are then transformed and filtered based on the rejection
sampling criterion, resulting in a set of valid latent vectors Vi:
Vi  {zj N(0, I) : d(xi, T(zj, yi)) , j  1, ..., m}
The valid latent vectors in Vi are transformed by the
conditional generator T(z, yi) to produce candidates, all con-
ditioned on yi. The nearest neighbour selection is then per-
formed within this set of candidates specific to xi, according
ensures that all candidates generated for a given training
example share the same conditional context yi, allowing the
model to learn the conditional relationships present in the
training data.
The overall objective for the Conditional RS-IMLE is:
C-RS-IMLE  argmin
zVi d (xi, T(z, yi))
where C-RS-IMLE represents the optimal parameters of the
model T under the Conditional RS-IMLE objective. These
parameters minimize the sum of expected minimum distances
between each real data point xi and the generated outputs
T(z, yi) over the valid latent vectors z in the corresponding
C. IMLE Policy
With our conditional variant of RS-IMLE, we can now apply
this training objective (Eq. 3) to behaviour cloning, where we
formulate our policy as a generator T(z, o) 7a, where our
conditioning variable o takes the form of image and robot
state information and a represents the generated actions. For
the distance function d(, ) we use Euclidean distance. We
utilise the same action generation procedure as , generating
a sequence of actions which we can utilise for closed-loop
receding-horizon control. At each time step t, the policy
processes the latest To steps of observation data, Ot, as input
and predicts Tp steps of future actions. Out of these, Ta steps
are executed on the robot before re-planning occurs. Here,
To is referred to as the observation horizon, Tp as the action
prediction horizon, and Ta as the action execution horizon.
Although we use the same UNet architecture as Diffusion
in this work, the approach is generally applicable to other
architectures such as transformers. We summarise the IMLE
Policy algorithm in Algorithm 1 and illustrate a full overview
in Figure 3.
D. Temporal Consistency
IMLE Policy effectively models multi-modal action distri-
butions; however, since each inference step is independent, the
policy may exhibit mode-switching at decision points where
multiple valid behaviour exist. This can lead to oscillatory
and consistent execution is necessary. To mitigate this, we
introduce a batched trajectory selection mechanism that en-
forces temporal consistency while maintaining the policys
multi-modal expressivity during inference.
At each time step t, we generate a batch of m latent-
conditioned trajectories,
Aj  T(zj, Ot),
for j [m],
where zj N(0, I) are sampled latent vectors, and Aj repre-
sents a candidate trajectory. To maintain temporal consistency,
we select the trajectory that minimises deviation from the
previously generated trajectory over the overlapping horizon:
j argmin
d(Aprev[Ta:Tp], Aj
where d(, ) is a distance metric, Aprev[Ta:Tp] represents the
unexecuted segment of the previously generated trajectory and
[0:Ta] represents the executable Ta actions from the generated
action sequence. As in training, we use Euclidean distance here
for d(, ). The first Ta steps of the selected trajectory Ajare
then executed at timestep t.
To prevent the policy from overcommitting to a suboptimal
j Uniform([m]).
This ensures adaptability while preventing long-term commit-
ment to potentially poor behaviours.
model that accurately captures multi-modal behaviour, as the
policy must balance between expressivity and trajectory con-
sistency. Prior works have explored alternative strategies such
as bidirectional decoding and streaming diffusion [23, 16].
our batch-based selection strategy is straightforward and com-
putationally efficient and can run at > 100 Hz. We summarise
inference using IMLE policy with consistency in Algorithm
IV. EVALUATION
We evaluate IMLE Policy to understand how effective it is
in modelling behaviours across different datasets and environ-
ments when compared to prior state-of-the-art methods. We
focus our evaluation on the core requirements for real-world
robotics applications particularly: sample efficiency, multi-
modal expressivity and inference speed. Concretely, we seek
to answer the following questions through our experiments:
1) How well does IMLE Policy perform on the respective
benchmarks for behaviour cloning as a single-step gen-
erative model?
2) How well does IMLE Policy perform when under data
constraints?
3) How well does IMLE capture multiple modes seen in
demonstrations?
4) Does IMLE Policy scale beyond simulation environ-
5) What hyper-parameters of IMLE Policy make the most
impact on its performance?
We systematically evaluate IMLE Policy to answer the
above questions through a series of experiments across both
simulated and real-world environments. For all experiments,
we evaluate the following policies:
IMLE Policy:
Our proposed algorithm given in Al-
gorithm 2 which leverages temporal consistency during
inference.
IMLE Policy (wout consistency):
Our proposed algo-
rithm without temporal consistency during inference.
Diffusion Policy:
Vanilla Diffusion Policy proposed by
Chi et al.  trained using DDPM and 100 denoising
Flow Matching (1-step): We modify the Diffusion Policy
implementation from Chi et al.  to utilise the Flow
Matching objective and evaluate against the 1-step setting
as a comparative baseline to the 1-step performance of
our method.
A. Simulation Environments
IMLE Policy is evaluated across 8 different simulation tasks
(Figure 4) from 4 benchmark environments [9, 27, 17, 14].
We categorise our sets of tasks based on the level of multi-
modality exhibited by the dataset with the high multi-modality
tasks exhibiting the highest variance across the demonstra-
tions. A high-level description of each task is provided below
and specific details are summarised in Table I.
1) Push-T : We utilise the variant of this task provided
by Chi et al.  which involves pushing a T-shaped block
(gray) to a designated target position (red) using a circular end-
effector (blue). Variation is introduced through randomized ini-
tial positions of the T block and the end-effector. Successfully
completing the task requires leveraging complex, contact-rich
object dynamics to precisely push the T block using point
contacts to sufficiently overlap with the target. Note: we report
Simulation Benchmark
Transport
ToolHang
UR3 Block Push
Realworld Benchmark
Shoe Rack
TABLE I: Tasks Summary
number of objects, ActD: action dimension, Demo: number of
whether the task has a high precision requirement.
Tool Hang
UR3 Block Push
Transport
Fig. 4: Simulation Environments. Visualisation of the environments used to evaluate IMLE Policy. The first 5 tasks are part
of the Robomimic benchmark  and the other environments include Push-T , UR3 BlockPush  and Franka Kitchen
High Multimodality
Low Multimodality
UR3 Block Push
Tool Hang
Transport
100 Dataset
IMLE Policy
IMLE Policy (wout consistency)
Diffusion Policy
Flow Matching Policy
20 Demos
IMLE Policy
IMLE Policy (wout consistency)
Diffusion Policy
Flow Matching Policy
TABLE II: Behaviour Cloning Benchmark We present success rates in the format of (max success rate)  (average of last 3
checkpoints), with each averaged across 3 training seeds and 50 different environment initial conditions.
success rate which corresponds to the T-block covering at least
0.95 of the target .
2) Robomimic : A large-scale robotic manipulation bench-
mark  designed for studying imitation learning and offline
reinforcement learning (RL). It includes five tasks and we
utilise the Proficient Human (PH) teleoperated demonstration
dataset for all evaluations. Successful completion of these
tasks occurs when each of the objects is placed in the target
locations.
3) UR3 Block Push : In this task, a UR3 robot pushes two
blocks to goal circles on the opposite side of the table [17, 18].
The dataset exhibits multimodal behaviour, as either block can
be moved first. Success is evaluated based on whether each
block reaches the goal.
4) Franka Kitchen : This environment, based on the Franka
Kitchen manipulation task , uses a Franka Panda robotic
arm with a 7-dimensional action space and 18 successful
human-collected demonstrations. It features seven possible
sequences making it highly multimodal. Successful completion
in this environment is considered when a series of 4 unique
tasks are completed sequentially. We focus our evaluation on
multimodal long-horizon demonstrations, therefore training on
the complete-v2 subset from the D4RL repository  and
evaluating success rate of the entire task being completed in a
specific order. Prior works in contrast have used a different
dataset (partial demos) and performance metric (unordered
sub-task completion).
B. Real World Tasks
We additionally evaluate IMLE Policy on two real robot
manipulation tasks to demonstrate its capability of learning
manipulation tasks in the real world. To investigate real-world
on 35 and 17 demonstration subsets.
1) Real World Push-T: This task is a real world variant of
the simulated Push-T task, with a 3D printed T block and a
cylindrical robot end-effector being controlled on a 2D plane.
We use 2 camera views, one on the wrist of the robot, and
the other a side view scene camera. We record a successful
trial when the robot has successfully placed the T-Block to
sufficiently overlap with the target and then return to an end
state within a 2 minute time frame, as shown in Figure 8.
2) Real World Shoe Racking Task: We evaluate our method
on an additional multi-modal real-world shoe racking task,
where the robot must pick and place two shoes and place them
side-by-side on the shoe rack. Either shoe can be selected
with demonstrations including a variety of these behaviours,
making this task highly multi-modal. Wrist and scene cameras
are used to provide full view of the task scene. A successful
trial is recorded when both shoes are placed correctly side-by-
side (i.e. left shoe on the left of the right shoe) on the rack and
the robot has returned to its home position within a 1 minute
time frame, as shown in Figure 9
C. Experiments
1) Dataset Size Study: We conduct a study to evaluate how
well IMLE can learn under data constrains when compared
to the baselines. We use the Push-T simulated task for this,
Fig. 5: Dataset Size Study. We evaluate how each of the
methods performs when trained with different size subsets
of the full dataset used in the Push-T benchmark task. The
dashed line indicates that to achieve the same reward of
0.5 on the task, IMLE Policy requires less than 29 of the
Flow Matching Policy requires over 80 of the full dataset.
Success Rate ()
Epsilon Sweep
Number of Samples Per Condition
Success Rate ()
Number of Samples Per Condition Sweep
Fig. 6: Robustness to Hyper-parameters. We evaluate the
performance of IMLE Policy across varying different values
for two key hyper-parameters used by the IMLE objective:
rejection sampling radius  and the number of samples used
per condition. Note how the algorithm remain relatively stable
across the entire range of values for both tasks.
dividing the full dataset provided by Chi et al.  in 10
each method on these datasets for 1000 epochs across 3 seeds
and report the average evaluation performance for each method
in Figure 5. This experiment gives us a detailed view of how
performance improves as data increases.
2) Benchmark Evaluation: We evaluate all methods across
the full datasets provided by each respective benchmarks
indicated as 100 Dataset in Table II. To understand how
our policy operates in the low data regime, we additionally
conduct an evaluation when all methods are trained on 20
randomly sampled demonstrations from the full dataset. These
20 demonstrations are kept consistent across all methods.
While 20 demonstrations might not be enough data to learn
to successfully complete each task, it allows us to gauge the
relative sample efficiency of all methods. For each setting,
we report the best success rate achieved by the method
across 1000 training epochs. The success rates are reported
every 50 epochs and are calculated as the average success
across 50 different initialisations within that environment. We
additionally report the average of the last 3 success rates
from the run. All results are averaged across 3 seeds and are
summarised in Table II.
3) Real World Validation: We evaluate all methods on
the real-world Push-T and shoe racking tasks described in
IV-B. Our evaluation protocol tests both sample efficiency and
overall performance by training on 17 and 35 demonstrations
for each task. For both tasks, we conduct 20 evaluation
trials per method and report the success rates based on the
completion criteria outlined in the environment descriptions.
This evaluation framework allows us to assess how effectively
each method can learn from limited real-world demonstrations
while handling both precise manipulation requirements in
Push-T and the multi-modal decision space in the shoe racking
task. Results are summarised in Figure 7
4) Mode Capturing Ablation: In this qualitative ablation
study we demonstrate how IMLE Policy can capture multiple
modes even in states where certain modes appear less fre-
quently in the dataset. We conduct this study in the Push-T
simulation benchmark where we gradually sweep the location
of the robots end effector from one side of the T blocks top
edge to the other. The idea here is to capture points where
the demonstration data exhibits a high level of multi-modality
(at the centre) and less multi-modality (towards the corners)
where majority of the demonstrations will bias towards one
side of the T block when in one of these particular states. The
key results are illustrated in Figure 2.
5) Real World Inference Speed Evaluation: We evaluate
the computational efficiency of our approach by measuring
inference speed across different policy architectures for the
shoe racking task. Using a standard Dell Precision 3680 i7
workstation equipped with an NVIDIA GeForce RTX 3090
sequence of actions. For each method, we conduct 30 separate
generations and report the average inference speed in Hertz.
We summarise the results in Table III.
6) Hyperparameter Ablation Study: Finally we evaluate
how robust our algorithm is to the two key hyperparame-
ters used by the IMLE Policy objective. The first parameter
pertains to the rejection sampling radius,  which determines
how far candidate generations have to be away from a data
point to be considered in the loss computation. The second
parameter pertains to the number of samples we use per
condition in order to conduct our nearest-neighbour search in
the objective. For each hyper-parameter we sweep over a set of
values for two different tasks from our simulation benchmark
(Push-T and Square) and report the maximum success rate
achieved by each when trained over 1000 epochs. The results
are summarised in Figure 6.
Fig. 7: Real World Task Results. Across both tasks we train
each method on both 17 (left) and 35 (right) demonstrations
and report the average success rate achieved across 20 trials.
We find that IMLE Policy consistently outperforms baselines
in the low data regime, while maintaining competitive in the
higher data regime.
IMLE Policy
(wout consistency)
Diffusion
Flow Matching
Policy (1-step)
Inference Speed (Hz)
TABLE III: Real World Inference Speed. Using the two
images from shoe racking task as input, we average the
inference speed required to generate a sequence of actions
across 30 generations.
D. Key Findings
IMLE Policy can learn with significantly less data when
compared to baselines. The dataset size study shown in
Figure 5 demonstrates how IMLE Policy consistently achieves
higher max rewards across all dataset percentages and a
significant advantage in the lower data settings. Notably, IMLE
Policy achieves a reward of 0.5 with less than 30 of the
data to reach the same performance. This underscores the
ability of IMLE Policy to generalise effectively with minimal
data. Furthermore, as the dataset size increases, IMLE Policy
continues to maintain its advantage, achieving near-optimal
performance faster, while converging to comparable perfor-
mance to Diffusion Policy when the full dataset is available.
IMLE Policy can maximise the utility of only a few
in Table II, across all benchmark environment tasks, IMLE
Policy demonstrates a clear advantage, particularly in the
challenging setting of learning from only 20 randomly selected
demonstrations. Under this constrained scenario, IMLE Policy
consistently outperforms all baseline methods, achieving at
least 8 more successful evaluation runs on average compared
to the baselines. On the full dataset benchmark, IMLE Policy
Fig. 8: Real World Push-T Task Setup. Hardware setup and
illustration of the task. The robot needs to precisely push
the T-Shaped block into the target region, and move the
end-effector to the end-zone.
continues to demonstrate competitive or superior performance,
outperforming the baselines in at least 5 of the 8 tasks.
IMLE Policy is highly multi-modal. Figure 2 shows that
IMLE Policy maintains multi-modality across varying initial
conditions of the end effector. Unlike Diffusion Policy, which
biases towards majority modes, and Flow Matching Policy
(one-step), which collapses modes and produces averaged
reflect the full distribution of the demonstration data. This is
particularly important in low-data regimes, where capturing
all modes without overfitting to dominant behaviours enables
efficient learning. Figure 1 further supports this, showing that
IMLE Policy preserves multi-modality and accurately captures
all modes, even with sparse or imbalanced data.
IMLE Policy is robust to hyperparameter variations.
Figure 6 demonstrates that IMLE Policy consistently performs
well across a wide range of hyperparameter settings, with
stable success rates even as the rejection sampling threshold
() and the number of samples per condition are varied. This
robustness ensures that IMLE Policy adapts effectively without
significant degradation in performance, making it practical
for real-world applications where hyperparameter searches or
fine-tuning may be infeasible. Notably, for the simulation
benchmark evaluations, hyperparameters were not optimized
per task but held constant throughout, yet IMLE Policy still
outperformed baseline methods.
IMLE Policy is well suited for real world robotics. IMLE
Policy demonstrates impressive traits that make it suitable
for real-world robotics by achieving the best performance
across both real-world visuomotor tasks as shown in Figure 7.
Even with as few as 17 demonstrations, IMLE Policy learns
performant policies, outperforming both baseline methods by
a significant margin. The robustness of IMLE Policy in low-
data regimes makes it particularly valuable in real-world
scenarios where collecting large datasets is often impractical.
single-step inference, achieving up to 97.3 faster inference
speeds compared to vanilla Diffusion Policy, while either
outperforming it or maintaining competitive performance. This
efficiency enables real-time control and is well suited for
robotics applications where computational resources can be
limited. While its inference speed is similar to that of single-
step Flow Matching we note that IMLE Policy outperforms
Flow Matching, particularly in the low data regime and
consistently across the simulation benchmarks. We provide
videos demonstrating how our policy perform across both
tasks when compared to the baselines and additional videos
to demonstrate its robustness in the real world on our project
High multi-modality calls for temporal consistency.
While IMLE Policy demonstrates impressive multi-modal ex-
pressivity even from a limited number of demonstrations, a
key empirical insight we identified was the tendency for the
policy to occasionally switch b
