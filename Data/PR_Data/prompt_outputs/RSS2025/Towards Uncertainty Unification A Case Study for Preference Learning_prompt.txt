=== PDF文件: Towards Uncertainty Unification A Case Study for Preference Learning.pdf ===
=== 时间: 2025-07-22 09:42:49.762742 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Towards Uncertainty Unification:
A Case Study for Preference Learning
Shaoting Peng, Haonan Chen, Katherine Driggs-Campbell
University of Illinois Urbana-Champaign
AbstractLearning human preferences is essential for human-
robot interaction, as it enables robots to adapt their behaviors
to align with human expectations and goals. However, the
inherent uncertainties in both human behavior and robotic
systems make preference learning a challenging task. While
probabilistic robotics algorithms offer uncertainty quantifica-
underexplored. To bridge this gap, we introduce uncertainty
unification and propose a novel framework, uncertainty-unified
preference learning (UUPL), which enhances Gaussian Process
(GP)-based preference learning by unifying human and robot
uncertainties. Specifically, UUPL includes a human preference
uncertainty model that improves GP posterior mean estimation,
and an uncertainty-weighted Gaussian Mixture Model (GMM)
that enhances GP predictive variance accuracy. Additionally, we
design a user-specific calibration process to align uncertainty
representations across users, ensuring consistency and reliability
in the model performance. Comprehensive experiments and user
studies demonstrate that UUPL achieves state-of-the-art perfor-
mance in both prediction accuracy and user rating. An ablation
study further validates the effectiveness of human uncertainty
model and uncertainty-weighted GMM of UUPL. Video and code
are available at
I. INTRODUCTION
Uncertainty is pervasive in human-robot interaction (HRI),
stemming from both robotic and human sources. On the robot
imperfect control models, unpredictable human behavior, and
many other aspects . Properly modeling these uncertainties
enhances HRI efficiency and safety, while neglecting them
can lead to significant risks . Established probabilistic
Markov decision processes (POMDP), as well as modern
methods leveraging large language models for uncertainty esti-
mation [3, 4], enable effective quantification and management
of robot uncertainty, improving system performance.
In contrast, human uncertainty is inherently more chal-
lenging to quantify due to the stochastic nature of human
decision-making. Psychological and cognitive science theo-
frameworks for modeling this uncertainty. However, when
humans interact with autonomous agents, their uncertainty
becomes even more nuanced, shaped by subjective factors
like trust, familiarity with the technology, and individual risk
aversion . Additionally, dynamic interactions with robots
during HRI tasks introduce more variability, as users adapt
their behaviors based on perceived system performance .
Despite these complexities, accounting for human uncertainty
Hmm I think
be slightly better, but
Im not so sure
uncertainty and integrate
into my estimation!
Uncertainty-unied Model
Do you prefer
Uncertainty-averse Model
preference data
Learned trajectory w
unied uncertainty
trajectory
wo unied
uncertainty
Intuition on uncertainty unification for preference learning.
Imagine a robot inferring Alices (a human user) ideal trajectory for passing
a cup of coffee above a table using preference learning. In one sample pair,
trajectory x(1) poses a risk of spilling coffee on the keyboard, while trajectory
x(2) risks spilling it on the headphones. The keyboard and headphones
are both valuable to Alice, so she responds that she weakly prefers x(2)
with hesitation, reflecting her uncertainty in the decision. An uncertainty-
averse model ignores this nuance, potentially learning a suboptimal and
undesirable trajectory (e.g., still passing above the headphones). In contrast,
an uncertainty-unified model incorporates Alices expressed uncertainty into
its uncertainty-aware framework, enabling it to learn an ideal trajectory that
aligns with her true preferences.
is crucial for optimizing both user experience and system
To date, research on robot and human uncertainties has
largely progressed in isolation. This misalignment between
two uncertainties can lead to inefficiencies, increased cog-
nitive load, and even failures in critical decision-making
scenarios like physical assistance and collaborations [8, 9].
Integrating these perspectives is vital for improving system
As this challenge is both technically complex and essential
for advancing trustworthy human-robot partnerships, we argue
a unified framework is necessary to tackle the misalignment.
human uncertainty into robotic uncertainty-aware algorithms,
enabling outcomes informed by both human and robot uncer-
tainties. To explore the potential of uncertainty unification, we
focus on preference learning, a key HRI domain where robots
learn from human feedback through comparative judgments.
As suggested by Laidlaw and Russell , this domain is
well-suited for our study due to its rich uncertainty dynamics.
Human uncertainty can be captured via self-reported confi-
dence levels, while Gaussian Processes (GPs) are used to
model robots estimation of human preferences, with robot
uncertainty quantified by the GP variances. Compared to
neural network-based uncertainty estimations, GP variances
offer advantages in interpretability, reliability, and sample
efficiency [11, 12]. Fig. 1 offers an intuitive example on the
motivation of uncertainty unification in preference learning.
To unify human uncertainty with robotic Gaussian un-
(UUPL), which focuses on leveraging human confidence lev-
els to refine the GP mean and covariance estimations. To
this end, we design a human preference uncertainty model
that captures varying levels of users uncertainty, which is
then integrated into the Laplace approximation algorithm to
enhance GP mean estimation. Recognizing the limitations of
the current GP predictive variance in preference learning (as
further elaborated in Fig. 5), we propose a human uncertainty-
weighted Gaussian Mixture Model (GMM) to provide a more
interpretable and adaptive GP variance estimation. This re-
fined predictive variance not only achieves synergy with the
acquisition function, but also has practical benefits in real-
world applications. Additionally, we introduce an uncertainty
calibration process to align uncertainty representations across
diverse users, ensuring consistency and reliability in the model
performance. Comprehensive simulations and real-world user
further highlights the contributions of each uncertainty-unified
component to overall system performance.
To sum up, the main contributions of our paper include:
uncertainty-unified
preference
learning
(UUPL) framework, which includes human preference
uncertainty modeling and the integration into Laplace
posterior mean estimation, uncertainty-weighted Gaus-
sian Mixture Model for GP predictive variance scaling,
and user-specific uncertainty calibration.
We conduct comprehensive evaluations against three
baselines across three simulation tasks, demonstrating
significantly higher prediction accuracy. Additionally, we
conduct three user studies, providing insights into the
calibration process and illustrating practical applications
of unified uncertainty. An ablation study further analyzes
the contributions of individual components in UUPL.
II. RELATED WORK
This section first describes uncertainty quantification, fol-
lowed by past work on modeling robot and human uncertain-
ties in HRI, and finally discusses preference learning and its
connection to uncertainty.
A. Uncertainty Quantification and Modeling
Uncertainty can broadly be categorized into two types:
Aleatoric Uncertainty (AU) and Epistemic Uncertainty (EU)
. AU, often referred to as stochastic or statistical uncertainty,
represents inherent randomness in the system and cannot be
mitigated through additional data or experiments. In contrast,
about the system or environment and can potentially be
reduced through further data collection. From the robots
probabilistic methods like Monte Carlo simulations, and EU
is typically estimated in Bayesian approaches. In Fig. 1, for
every possible trajectory. The uncertainties of trajectory x(1)
or x(2)s reward values are dominated by AU because they
have been observed, while the uncertainties of other unseen
trajectories reward values are dominated by EU. For human
uncertainty in HRI, various methods have been explored
to model AU. Laidlaw and Russell  employed inverse
decision theory (IDT) to estimate human uncertainty, while
Fisac et al.  utilized a noisy-rationality model to predict
human motion under uncertainty, recursively updating model
confidence. Additionally, Xu et al.  incorporated human
uncertainty into the Conditional Value-at-Risk (CVaR) frame-
work to design an uncertainty-aware policy, and Holladay
et al.  proposed the Comparison Learning Algorithm for
Uncertain Situations (CLAUS) based on the Luce-Shepard
Choice Rule . While significant progress has been made
in AU modeling for both robots and humans, EU modeling
for humans remains underexplored due to its inherently un-
measurable nature. Nevertheless, there is an emerging interest
in how robot behaviors might provide humans with additional
information about the environment, thereby indirectly reducing
human EU. Importantly, most existing studies focus on either
robot or human uncertainty in isolation, leaving a notable gap
in achieving an integrated approach to uncertainty modeling
in HRI. Although some work [13, 17, 18] attempt to consider
both human and robot uncertainties, they exclusively focus
on the AU part, achieving only partial unification. Our work
extends this by incorporating both aleatoric and epistemic
certainty representation framework.
B. Preference Learning
Preference learning has become increasingly popular in
lowing Furnkranz and Hullermeier , we define preference
learning as the task of learning a reward function from a
set of comparison pairs with known preference relationships,
formally defined in Section III-A1. In the context of reinforce-
ment learning (RL), preference learning can be seen as a gener-
alization of in
