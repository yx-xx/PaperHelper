=== PDF文件: Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos.pdf ===
=== 时间: 2025-07-21 13:48:38.244237 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Particle-Grid Neural Dynamics for Learning
Deformable Object Models from RGB-D Videos
Kaifeng Zhang1
Baoyu Li2
Kris Hauser2
Yunzhu Li1
1Columbia University
2University of Illinois Urbana-Champaign
Learning-Based Deformable Models with Appearance, Geometry, and Dynamics
Real-World Interaction Data with Dense Tracking
Training
Inference
Particle-Grid Neural Dynamics
Dual-Arm Cloth Lifting
Rope Manipulation
Plush Toy Relocating
Dual-Arm Box Closing
Dense Particles
Fig. 1: Modeling deformable objects from RGB-D videos presents a significant challenge due to occlusions and complex physical interactions.
Our Particle-Grid Neural Dynamics framework learns the behavior of deformable objects directly from real-world observations. To train the
predicts the motion of dense particles under robot-object interactions. We demonstrate the ability of Particle-Grid Neural Dynamics to model
complex interactions across a diverse set of objects, including ropes, cloth, plush toy, box, and bread.
AbstractModeling the dynamics of deformable objects is
challenging due to their diverse physical properties and the
difficulty of estimating states from limited visual information.
We address these challenges with a neural dynamics framework
that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and
motion information while predicting dense particle movements,
enabling the modeling of objects with varied shapes and materials.
Particles represent object shapes, while the spatial grid discretizes
the 3D space to ensure spatial continuity and enhance learning
efficiency. Coupled with Gaussian Splattings for visual rendering,
our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos.
Through experiments, we demonstrate that our model learns
the dynamics of diverse objectssuch as ropes, cloths, stuffed
robot-object interactions, while also generalizing at the category
level to unseen instances. Our approach outperforms state-of-the-
art learning-based and physics-based simulators, particularly in
scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The
project page is available at
I. INTRODUCTION
Learning predictive models is crucial for a wide range of
robotic tasks. In deformable object manipulation, an accu-
rate predictive object dynamics model enables model-based
that are both accurate and generalizable remains a significant
challenge. For example, physics-based simulators [12, 31] often
struggle to generalize to the real world due to the inherent sim-
to-real gap and the difficulties of system identification and state
estimation. Meanwhile, video-based predictive models [9, 56]
are computationally expensive, lack 3D spatial understanding,
and are highly sensitive to viewpoint and appearance changes.
Recent advancements in deformable object modeling have
focused on learning dynamics models for particles that encode
3D geometric information directly obtained from RGB-D
cameras. The current state-of-the-art methods in this area
utilize Graph Neural Networks (GNNs), where particle sets
are modeled as spatial adjacency graphs, and message passing
is performed to aggregate information and generate motion
predictions [48, 58]. However, these methods face significant
sensitive to the spatial distribution and connectivity of the
graph nodes, making them vulnerable to partial observations.
capture global information, while excessive steps can lead to
overly smoothed predictions. As a result, these approaches are
often limited to relatively simple simulated environments or
real-world objects with easily perceivable geometries, where
graph construction is straightforward with nearest neighbors.
To address these limitations, we introduce a novel class of
dynamic models called particle-grid neural dynamics. This
model leverages a hybrid representation that combines object
particles with fixed spatial grids. It takes the kinematic states
of the particles as input and predicts a spatial velocity field at
fixed grid points. A global point cloud encoder is employed
to capture comprehensive information from the particle set,
effectively overcoming the connectivity challenges commonly
faced by GNNs and enhancing the models robustness to
incomplete observations. Additionally, this encoder allows
for the processing of denser particle sets compared to sparse
graph nodes, thereby enriching the model with finer geometric
details. The grid representation further acts as a regularizer,
ensuring spatial continuity in velocity predictions while also
reducing computational costs when handling large numbers
of particles. By combining object particles with spatial grids,
our framework parameterizes dynamics in both Lagrangian
and Eulerian coordinates, drawing an analogy to physics-based
deformable object simulation methods [44, 12]. By utilizing
neural networks as message integrators to bridge particle and
grid representations, our model handles a diverse range of
materials without requiring material-specific physical laws, and
can operate effectively with only partial observations as input.
robots interacting with objects through diverse and non-task-
specific behaviors. To extract object particles from raw record-
This framework utilizes foundational vision models [18, 39, 15]
to estimate segmentation masks and pixel tracks, which are
then fused in 3D to generate persistent, dense 3D tracks that
serve as training data for the neural dynamics model.
In our experiments, we demonstrate that the proposed
particle-grid neural dynamics model can effectively simulate
a diverse range of challenging deformable objects, including
compare our model with existing state-of-the-art approaches
and show that it consistently outperforms the baselines in pre-
diction accuracy. Additionally, we highlight that our dynamics
model seamlessly integrates with 3D appearance reconstruc-
tion methods such as 3D Gaussian Splatting (3DGS) ,
enabling the fusion of both techniques to generate highly
realistic renderings of predictions. This improved accuracy
in dynamics prediction directly translates to higher-fidelity
renderings compared to existing methods. To further validate
its robustness, we conduct experiments that demonstrate the
models effectiveness under sparse-view conditions. Finally,
we showcase the models applicability to deformable object
manipulation by incorporating it into a Model Predictive
Control (MPC) framework, underscoring its potential for real-
world robotic applications.
II. RELATED WORK
A. Physics-Based Deformable Modeling.
The simulation of deformable objects is essential for
advancing both the modeling and robotic manipulation of
analytical physics-based approaches, including, but not limited
(FEM) [10, 33, 7], the Discrete Elastic Rods (DER) [5, 6], the
Position-Based Dynamics (PBD) [31, 21, 23], and the Material
Point Method (MPM) [44, 12, 13, 29]. However, real-world
analytical deformable modeling remains challenging due to
the difficulty in property identification and state estimation.
While our method uses a hybrid particle-grid representation
similar to MPM, we leverage neural networks as message
integrators and reduce dependence on full-state information
as input. This enhances robustness to partial observations and
enables the handling of a wide variety of deformable objects
without requiring predefined material-specific constitutive laws.
[16, 28, 8] has enabled the fusion of Gaussian Splatting
reconstructions with physics-based models to simulate the
dynamics of deformable objects [53, 37, 60, 59, 1]. In our
particle-grid neural dynamics model, the particle motion
predictions can also be integrated with Gaussian Splatting
rendering of deformable objects, purely learned from real data.
B. Learning-Based Deformable Modeling.
Learning-based dynamics models, which use deep neural
networks to model the future evolution of dynamical sys-
tasks [11, 55, 17, 61, 54, 4]. Among these learning-based
great promise, as they explicitly model spatial relational
biases within complex physical systems [19, 34, 40, 3, 48].
Previous research has investigated the use of GBND across
a range of material types, such as rigid bodies [14, 25, 2],
plasticine [42, 43], fabrics [22, 26, 35, 27], ropes ,
and granular piles [47, 41]. Beyond simulation and single-
material scenarios, GBND has also demonstrated flexibility
and generalization in modeling diverse materials using a unified
framework [57, 58]. However, these approaches often operate
on spatially sparse graph vertices, rely on expert knowledge
to determine graph connectivity, and do not consider partial
observations. For learning dense particle dynamics, Whitney
et al.  propose transformer-based backbones for higher
computational efficiency. However, their work mainly focuses
on modeling rigid objects for grasping and pushing tasks. In
using a hybrid particle-grid neural dynamics framework,
achieving dense particle prediction while remaining robust
Particle Features
Grid Velocity
Iterative Rollout
Grid Positions
Particle Velocity
Velocity
RGB-D Observation
Particles
Interpolation
Initial Frame
Action Sequence
(a) Particle-Grid Neural Dynamics Model
(b) 3D Action-Conditioned Video Prediction
(c) Model-Based Planning
Fig. 2: Overview of proposed framework: Particle-Grid Neural Dynamics. (a) A diagram of our dynamics model. Given particle positions
Xt and velocities Vt fused from multi-view depth images as input, our model predicts dense per-particle motion by first using a point
encoder to extract particle features and predict the velocity field, which is then transformed into a grid representation to estimate the velocity
distribution in 3D space. The model updates particle positions Xtt with the predicted velocities Vtt to perform iterative rollouts.
(b) Our framework enables 3D action-conditioned video prediction by reconstructing objects with 3D Gaussian Splatting and interpolating the
6DoF transformation of Gaussian kernels using the predicted particle motions. (c) The model can be integrated into model-based planning
frameworks to generate plausible motions for manipulating deformable objects.
to incomplete observations and flexible for diverse types of
deformable objects with distinct physical properties.
III. METHODS
Our Particle-Grid Neural Dynamics framework models the
dynamics of objects represented by a set of particles. The
core of this framework is a dynamics function that predicts the
future motion of each particle based on its current and historical
A detailed description of the model is provided in Section
III-A and III-B. We introduce the data collection pipeline and
the models training method in Section III-C. Additionally, we
explore the integration of Particle-Grid Neural Dynamics with
3D Gaussian Splatting for 3D video rendering, as discussed in
Section III-D. The application of this model within a Model
Predictive Control (MPC) framework is covered in Section
III-E. An overview of our method is also provided in Fig. 2.
A. Particle-Grid Neural Dynamics
1) State Representation: We intend to learn a particle-
based dynamics model, which represents the target object as
a collection of particles Xt R3n, where n is the number
of particles, and t is the time. The velocity of the particles,
Vt R3n is defined as the time derivative of X at time t.
2) Action Representation: The action At represents the
external effects caused to the object by the robot at time t. We
define At as
At  (y, Tt, Tt, ot),
where y is the action type label, Tt is the end-effector pose,
Tt its time derivative, and ot the gripper open distance. In our
experiments y is a binary label indicating whether the action
is a grasped or nonprehensile interaction; the implementation
details of these two types are given in Sec. III-B4.
3) Dynamics Function: We consider the change of state
caused by the state itself (e.g., objects falling due to gravity)
and the robots actions (e.g., robots grasping an object thus
making it move) in the objects dynamic functions:
Vtt  f(Xt, Vt, At),
where f is the dynamics function predicting the state evolution.
Since the particle representation typically cannot capture the
full state information (e.g., internal stress or contact mode with
other objects), a common practice is to incorporate historical
states as additional inputs for making predictions:
Vtt  f(Xtht:t, Vtht:t, At),
where h is the history window size. In our neural dynamics
parameterized by  and derive the next particle positions by
applying forward Euler time integration:
Xtt  Xt  t  f(Xtht:t, Vtht:t, At).
4) The Particle-Grid Model: Learning the neural dynamics
parameters  with an end-to-end neural network usually leads to
unstable predictions due to accumulative error. Motivated by the
use of hybrid Lagrangian-Eulerian representations in MPM ,
our model also utilizes a hybrid particle-grid representation
to inject inductive bias related to spatial continuity and local
information integration. Specifically, we define a uniformly
distributed grid in the space as
The parameters lx, ly, lz,  control the spatial limits and
resolution of the grid. Empirically, we set lx, ly, lz to 100
or 50 and  to 1 cm or 2 cm to balance computational cost and
resolution. To enforce translational invariance during prediction,
we always translate the positions of the particles and the robot
end effector to the volume defined by Glx,ly,lz,.
The particle-grid dynamics function is defined by the
following components:
f  hG2P  ggrid  f field
f feature
where f feature
is the neural network-based point encoder for
extracting feature from the input particles (Sec. III-B1), f field
the neural network-based function for parameterizing a neural
velocity field based on the extracted features (Sec. III-B2),
and ggrid is a grid velocity editing (GVE) control method
to encode collision surfaces and robot gripper movements
(Sec. III-B4), and hG2P is the grid-to-particle integration
function for calculating particle velocities from grid-based
velocity field (Sec. III-B3).
B. Model Components
1) Point Encoder: The point encoder encodes particle posi-
tions and velocities to per-particle latent features Zt Rdn,
where d is the feature dimension:
Zt  f feature
(Xtht:t, Vtht:t).
We use PointNet  as the encoder for its efficiency and
strong performance in extracting 3D point features. The encoder
captures global information from the set of all particles,
including the objects shape and the historical motion of the
properties and dynamic state. This is essential for handling
incomplete observations, where the feature encoder must extract
occlusion-robust features for subsequent velocity decoding.
2) Neural Velocity Field: In this step, we use a neural
implicit function f field
to predict a spatial velocity grid, at time
function f field
is instantiated as an MLP that takes the grid
locations xg and the corresponding locality-aware feature zg,t
as inputs, then predict per-grid velocity vector vg,t by
((xg), zg,t),
where  is the sinusoidal positional encoding, and the locality-
aware feature zg,t is defined as the average pooling of particle
features within the neighborhood of grid location:
pNr(Xt,xg) zp,t
Nr(Xt, xg)
where Nr(Xt, xg) is the set of indices of particles within Xt
whose positions are within radius r of the grid location xg.
By incorporating the radius hyperparameter r, we can control
the number of particle features a grid point attends to, thus
encouraging the network to predict velocities that are dependent
on local geometry. Empirically, we set r  0.2 m.
3) G2P: After calculating the grids velocities, we transfer
from the Eulerian grid to Lagrangian particles via spline
interpolation. Following MPM, we utilize a continuous B-spline
kernel to transfer grid velocities vg,t to particle velocities vp,t:
where the wpg,t is the value of the B-spline kernel defined on
the grid position xg and evaluated at the particle location xp,t.
It assigns larger weights to closer grid-particle pairs, achieving
smooth spatial interpolation. The predictions V R3n serves
as the final output of the dynamics function f and is used to
perform time integration in Eq. 4.
4) Controlling Deformation: We present two methods for
controlling deformations by interactions with external objects:
Grid Velocity Editing (GVE) and Robot Particles (RP). GVE
is inspired from MPM approaches and we use it for grasped
interactions and object-ground interaction. Simply put, the
operator ggrid changes the velocities on the grid to match
physical constraints. For ground contact, we project velocity
back from the contact surface and incorporate friction terms.
To define the motion of a rigidly grasped point, we calculate
the set of grid points Ggrasp,t within a distance a of the grasp
center point xgrasp,t. For each point g Ggrasp,t, we modify
the velocities as follows:
where t and xgrasp,t are the angular and linear velocities of
the gripper at time t, and xg and vg,t are the position and
velocity of grid point g, respectively.
The Robot Particles method allows us to model nonprehensile
actions for the Box example in which the object is pushed.
that carry gripper action information, and fuse this into the
object point cloud. Specifically, at each step, we augment the
point cloud by
Xt Xrobot,t,
Vt Vrobot,t,
and model the particle-grid dynamics function on the aug-
mented point cloud. This injects action information into
particle features but does not explicitly force particles to
move at a prescribed velocity, thus supporting nonprehensile
manipulation. Our implementation samples points from the
gripper shape and calculates their velocities based on the end-
effector transformation from proprioception.
C. Data Collection and Training
We collect training data through teleoperation and automatic
annotation using foundation models. Specifically, we record
multi-view RGB-D videos of random robot-object interactions.
For each camera view, we apply Segment-Anything [18, 39]
to extract persistent object masks across the video. The
segmented objects are then cropped and tracked over time
using CoTracker , providing 2D trajectories. Using depth
Particle
Particle
Particle
TABLE I: Quantitative Results on Dynamics Prediction. We compare our method with the Material Point Method (MPM) , Graph-Based
Neural Dynamics (GBND) , and a particle-based dynamics model without the grid representation. We report the mean and standard
deviation of the prediction error over a 3-second future horizon. The best results are highlighted in bold and blue.
velocities into 3D, resulting in multi-view fused point clouds
with persistent particle tracking.
With the collected tracking data, we define particle sets
and their trajectories over a look-forward time window as
actions Atht:tKt, where K is the horizon length hy-
perparameter. Empirically, we set h  2 and K  5. Model
training begins from a given point cloud at time t, followed
by iterative dynamics model rollouts for K steps. Since the
dynamics function f is fully differentiable, we optimize the
network parameters  and  using gradient descent. The loss
function is defined as the mean squared error (MSE) between
the predicted and actual particle positions:
Xtit Xtit2
where Xtit is the predicted particle positions at step i.
D. Rendering and Action-Conditioned Video Prediction
Our predictions can be integrated with 3D Gaussian Splatting
(3DGS) to achieve a realistic rendering of the results. The 3DGS
reconstruction of the object is defined as
G  {XGS, C, RGS, S, O},
where XGS, C, RGS, S, and O represent the Gaussian kernels
center location, color, rotation, scale, and opacity, respectively.
To transform Gaussians between frames, we first apply the
dynamics model to the point cloud set X, yielding the next-
frame prediction X. The points X can either be sampled from
XGS or obtained from additional point cloud observations
within the same coordinate frame with XGS. The 6-DoF
motions of the Gaussian kernels are interpolated using Linear
Blend Skinning (LBS) , which updates XGS and RGS by
treating X as control points and interpolating their predicted
motion to generate new Gaussian centers and rotations. We
assume that the color, scale, and opacity of the Gaussian
splatting remain constant.
E. Planning
Our model can be integrated with Model Predictive Control
(MPC) for model-based planning. Given multi-view RGB-
D captures, we obtain object particles through segmentation,
inverse projection into 3D space, and downsampling. The
downsampled particles serve as inputs to the dynamics model
for future prediction. With a specified cost function, the MPC
framework rolls out the dynamics model using sampled actions
and optimizes the total cost. In our experiments, we use the
Chamfer Distance between the predicted state X and the target
state Xtarget as the cost function:
J( X1:N, A1:N)
CD( Xt, Xtarget).
We apply the Model-Predictive Path Integral (MPPI)
trajectory optimization algorithm to minimize the cost and to
synthesize the robots actions. During deployment, we perform
IV. EXPERIMENTS
Our experiments are designed to address the following
How well does the particle-grid model learn the dynamics
of various types of deformable objects?
Does the model perform effectively under limited visual
observation (e.g., sparse views)?
Can we train a unified model for multiple instances within
an object category, and how well does it generalize to
unseen instances?
Can the model improve the performance of 3D action-
conditioned video prediction and model-based planning?
We evaluate our method on a diverse set of challenging
deformable objects, including cloth, rope, plush toys, bags,
previous state-of-the-art approaches in dynamics prediction
accuracy while remaining robust to incomplete camera views.
State and action
Ground Truth
Paper Bag
State and action
Ground Truth
Plush Toy
Fig. 3: Qualitative Comparisons on Dynamics Prediction. Given initial states and actions, we show the prediction results of the GBND
baseline compared to our particle-grid neural dynamics model. The red spheres indicate the position and orientation of robot grippers. We
overlay the predictions with ground truth final state images to highlight the prediction errors. Our models predictions are more aligned with
the ground truth, offering higher-density particle predictions and fewer artifacts compared to the baseline.
level training and its effectiveness in downstream applications,
such as video prediction and planning.
A. Experiment Setup
We conduct data collection and experiments using a bimanual
xArm setup, with each robot arm having seven degrees of
freedom. The objects used in the experiments include rope,
a) Rope: A single robot arm grasps one end of a rope,
while the other end remains free on the table surface. The
robot manipulates the rope in 3D space, generating various
deformation patterns such as bending and dragging.
b) Cloth: Two robot arms grasp a rectangular piece of
cloth and manipulate it in 3D space. The lower half of the
cloth remains in contact with the table, resulting in significant
deformations under lifting, moving, and folding actions.
c) Plush: A single robot arm grasps one limb of a plush
toy while the rest of the toy remains in contact with the
table. The robot manipulates the plush in 3D space, creating
deformation patterns such as limb movements and flipping.
d) Paper Bag: One robot arm grasps and stabilizes
one side of an envelope-shaped mailer bag, while the other
manipulates it in 3D space. The robot performs various actions,
including opening, closing, and rotating the bag.
e) Box: Two robot arms are used to open and close ship-
ping boxes. The grippers remain closed, and the manipulation
is performed in a nonprehensile manner, utilizing the surfaces
of the grippers to push against the movable parts of the box.
f) Bread: Two robot arms are used to tear pieces of bread.
The grippers remain closed, holding the bread in the air. One
robot arm stays still while the other pulls, creating stretching
effects and eventual breakage.
The baseline models in our comparisons are as follows:
MPM-based deformable object simulation [12, 29]: This
baseline assumes a hyperelastic material with an unknown
uniform Youngs modulus and friction coefficient with
the tabletop. Parameter identification is performed via
J -Score  IoU
Particle
Particle
Particle
TABLE II: Quantitative Results on 3D Action-Conditioned Video Prediction. We compared our method on 3D action-conditioned video
prediction quality with MPM , GBND , and particle-based baselines. The J -ScoreIoU and the F-Score measures mask similarities
and the LPIPS score measures appearance-wise similarities between predicted frames and ground truth video recordings. We report the mean
and standard deviation of the prediction error over a 3-second horizon. The best results are highlighted in bold and blue.
Fig. 4: Quantitative Comparisons on Prediction under Partial
Views. We compare our method with the GBND baseline in the cloth
and paper bag categories while varying the number of input camera
views. We report the mean and standard deviation of the dynamics
prediction error. Our method consistently achieves lower error than
the baseline, and its error increase rate as the number of camera views
decreases is also lower.
GBND-Seen
GBND-Unseen
Ours-Seen
Ours-Unseen
Fig. 5: Quantitative Comparisons on Generalization. Our method
is compared with GBND on seen and unseen instances of the rope
and cloth categories. We present the mean and standard deviation of
dynamics prediction error. Our methods prediction error is lower on
both seen and unseen instances compared to the baseline.
gradient descent.
Graph-Based Neural Dynamics (GBND) : This model
represents objects using subsampled sparse vertices, along
with the robot end-effectors, and employs a Graph Neural
Network (GNN) to predict particle motions.
Particle-based dynamics model (ours wo grid): In this
and directly query the velocity field at particle positions
to predict per-particle velocities.
For additional information on the experiment setups and
baseline implementations, please refer to Appendix B.
B. Dynamics Learning and Prediction
We evaluate accuracy using a held-out set of robot-object
interactions. The interaction videos are divided into 3-second
point cloud metrics that compute the distance between predicted
particle positions and ground truth future points. The metrics
include Mean Squared Error (MSE), Chamfer Distance (CD),
and Earth Movers Distance (EMD). All metrics are calculated
with m or m2 as the unit of measurement. For the box category,
we use the Robot Particles control method representation, and
since it is not directly compatible with MPM. Therefore, we
omit MPM from the box comparison.
Quantitative results are shown in Table I, where our method
outperforms all baselines in terms of dynamics rollout accuracy.
We visualize the particle prediction results of our method
alongside the baselines in Fig. 3. Our methods predictions
align more closely with the ground truth images and also exhibit
higher resolution. In contrast, GBND predicts inadequate
particle motions for objects like cloth, plush toy, box, bread,
and it generates artifacts for objects like rope.
C. Sparse-View Dynamics Prediction
We evaluate the performance of our method in sparse-
view scenarios by training the model on partial observation
data. Specifically, during training, we use point cloud from a
randomly sampled number of camera views as model input.
During evaluation, we test the models performance with 1 to
4 camera views.
Ground Truth
rJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw<latexit>t
Ground Truth
Predicted
Predicted
Fig. 6: Qualitative Comparisons on 3D Action-Conditioned Video Prediction. We show our method and the GBND baselines prediction
on two examples of rope and plush toy, compared with the ground truth video. The predictions are based on the 3DGS reconstructions on the
first frame (leftmost image) and the robot action sequence. Differences are highlighted with red dashed boxes. Our method aligns better with
the ground truth while the baseline method predicts visually nonrealistic deformations.
Predicted
tKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjdmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSoSCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn5GNQombTYS
w1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw
<latexit>t
Predicted
Fig. 7: Qualitative Visualizations of Simulation from Scanned Scenes. Our method gives higher-quality video prediction results with
high-resolution Gaussians reconstructed from phone scans. Given the initial reconstruction (green frame), we apply our particle-grid dynamics
model to simulate the segmented object, and visualize from different views.
The results shown in Fig. 4 demonstrate that our model
outperforms the GBND baseline in dynamics prediction accu-
performance drop when decreasing the number of views is also
less significant than the baseline. For the cloth category, the
baseline performance drops significantly when decreasing from
4 camera views to 1 camera view, while our model maintains
a low prediction error.
D. Category-Level Model
multiple instances within the same category by training on
a combined dataset of various object instances. For ropes,
the model is trained on 4 distinct ropes and evaluated on 2
unseen ropes. For cloths, it is trained on 6 cloth instances and
tested on 2 unseen cloths. The 6 rope instances are cotton
the cotton rope and utility rope included in the test set and
unseen during training. For cloths, the 8 instances include
a flannel blanket, cotton towel, microfiber cloth, cotton bed
blanket and foam sheet included in the test set. These instances
are selected to have diverse physical properties and shapes,
(a) Cloth Lifting
(d) Plush Toy Relocating
(c) Rope Manipulation
(b) Box Closing
Fig. 8: Quantitative Comparisons on Planning. For four manipulation taskscloth lifting, box closing, rope manipulation, and plush toy
relocatingwe present the error curve and the final success rate curve with respect to the error threshold for task success. The error is always
measured using the Chamfer Distance between the current and target point clouds. Our method outperforms the GBND baseline in both error
reduction rate and success rate.
(a) Cloth Lifting
Initial State
Planning
Final State
(b) Box Closing
Initial State
Planning
Final State
(c) Rope Manipulation
(d) Plush Toy Relocating
wFtKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjdmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSoSCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn
5GNQombTYSw1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1r
Tj5zAngfP4A47uNAw<latexit>t
E">AB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEFwFtKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjd
mJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSo
SCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn5GNQombTYSw1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJX
P09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq
3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw<latexit>t
Fig. 9: Qualitative Comparisons on Planning. For each of the four tasks, we visualize a representative planning sequence for both our
method and the GBND baseline. Given similar initial states and the same number of planning steps, our method achieves a lower final error,
as measured by Chamfer Distance (CD), and produces results that are visually more similar to the target.
allowing us to thoroughly evaluate the models generalization.
The results in Fig. 5 show that our model achieves lower
prediction errors than the GBND baseline across both categories
and for both seen and unseen instances. Notably, for the
cloth category, the baseline method exhibits a significantly
performance drop on unseen instances, whereas our method
keeps a relatively low error, demonstrating better generalization
to novel objects at test time.
E. Action-Conditioned Video Prediction
For action-conditioned video prediction, we use the predicted
point cloud trajectories to interpolate Gaussian kernel trans-
formations using LBS . We reconstruct Gaussians from
4 input views using Gaussian Splatting . The Gaussians
are trained with a segmentation mask loss, following previous
works [58, 28]. Videos are rendered with a fixed input camera
pose. The video prediction quality is assessed using mask-based
accuracy), and the image-based metric LPIPS.
The resulting metrics are shown in Table II. Our approach
achieves the best overall performance. For categories with
relatively large objects, for instance boxes and paper bags, we
observe that spiky Gaussian reconstructions often negatively
impact mask prediction performance, especially when objects
undergo significant deformation. The higher mask alignment
scores in GBND and Particle baselines are largely due to
inadequate particle motion predictions.
In Fig. 6, we show the action-conditioned video prediction
results by reconstructing the object using Gaussian Splatting
from 4 views and deforming the Gaussians with predictions
from our dynamics model. Our method achieves higher-quality
rendering and better alignment with the ground truth.
In Fig. 7, we further demonstrate that our method can be
used for simulation based on high-quality phone-scanned GS
reconstructions. The scenes are reconstructed using video scans
of a static workspace. Coupled with our learned particle-grid
neural dynamics, we can generate 3D action-conditioned video
predictions with even greater visual fidelity.
F. Planning
In planning experiments, we evaluate the models ability to
integrate with MPC to generate actions for manipulating objects.
We test on 4 tasks with distinct object types: cloth lifting, box
assessed using error curves and task success rates.
The quantitative results are shown in Fig. 8. Across all four
planning tasks, our method achieves a lower terminal error and
a higher error reduction rate compared to the GBND baseline.
In Fig. 9, we visualize the initial states, intermediate steps, and
final states, comparing them to the target. In all four tasks, our
method produces results that are visually closer to the target.
For example, in the box closing task, our method successfully
lifts both sides of the box, whereas the baseline struggles to
predict the correct actions and often loses contact with the box.
In rope manipulation, our method accurately bends the rope
by pressing downward, while the baseline fails to achieve this
due to lower prediction resolution.
V. LIMITATIONS
While we have demonstrated that Particle-Grid Neural
Dynamics can model diverse types of deformable objects,
the current framework has several limitations: (i) The current
formulation assumes a fixed number of particles during iterative
appearance or disappearance of particles. This limitation
could be addressed by correcting the particle sets with new
observations or modeling the per-frame visibility of particles.
(ii) The model implicitly infers an objects physical properties
from its point cloud and short-term motion history. While
this is sufficient for modeling a single object instance or
multiple instances with distinct shapes and physical properties,
a more systematic approach to modeling physical properties
is needed for interpretable identification and adaptation at
test time. This could involve learning a parameter-conditioned
neural dynamics model . (iii) Training the model and
applying it to video prediction depend on accurate predictions
from computer vision models such as Segment-Anything ,
perception and reconstruction models could negatively impact
our methods performance.
VI. CONCLUSION
In this paper, we introduce Particle-Grid Neural Dynamics,
a novel framework for learning neural dynamics models of de-
formable objects directly from sparse-view RGB-D recordings
of robot-object interactions. By leveraging a hybrid particle-
grid representation to capture object states and robot actions in
3D space, our method outperforms previous graph-based neural
dynamics models in terms of prediction accuracy and modeling
density. This advancement enables the modeling of a wide range
of challenging deformable objects. Additionally, integration
with 3D Gaussian Splatting facilitates 3D action-conditioned
video prediction, simultaneously capturing both object geometry
and appearance changes, thereby creating a learning-based
digital twin of real-world objects. We further demonstrate
that our model can be applied to various deformable object
manipulation tasks, achieving improvements in both task
execution efficiency and success rate.
ACKNOWLEDGMENT
We thank the members of the RoboPIL lab at Columbia
University for their helpful discussions. This work is partially
supported by the Toyota Research Institute (TRI), the Sony
Group Corporation, Google, and Dalus AI. This article solely
reflects the opinions and conclusions of its authors and should
not be interpreted as necessarily representing the official
REFERENCES
Jad Abou-Chakra, Krishan Rana, Feras Dayoub, and Niko
Suenderhauf. Physically embodied gaussian splatting: A
realtime correctable world model for robotics. In 8th
Annual Conference on Robot Learning, 2024. 2
Bo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston
tactile-informed dynamics models for dense packing.
Kelsey R Allen, Tatiana Lopez Guevara, Yulia Rubanova,
can learn discontinuous, rigid contact dynamics. In Karen
of The 6th Conference on Robot Learning, volume 205
of Proceedings of Machine Learning Research, pages
Dominik Bauer, Zhenjia Xu, and Shuran Song. Doughnet:
A visual predictive model for topological manipulation of
deformable objects. European Conference on Computer
Vision (ECCV), 2024. 2
Mikls Bergou, Max Wardetzky, Stephen Robinson,
Basile Audoly, and Eitan Grinspun.
Discrete elastic
rods. In ACM SIGGRAPH 2008 Papers, SIGGRAPH 08,
New York, NY, USA, 2008. Association for Computing
Machinery. ISBN 9781450301121. doi: 10.11451399504.
Yizhou Chen, Yiting Zhang, Zachary Brei, Tiancheng
Differentiable discrete elastic rods for real-time modeling
of deformable linear objects, 2024. 2
Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew
Differentiable projective dynamics. ACM Trans. Graph.,
Bardienus P Duisterhof, Zhao Mandi, Yunchao Yao, Jia-
Wei Liu, Mike Zheng Shou, Shuran Song, and Jeffrey
Ichnowski. Md-splatting: Learning metric deformation
from 4d gaussians in highly deformable scenes. arXiv
preprint arXiv:2312.00583, 2023. 2
Chelsea Finn and Sergey Levine. Deep visual foresight
for planning robot motion. In 2017 IEEE International
Conference on Robotics and Automation (ICRA), pages
Marcos Garca, Csar Mendoza, Luis Pastor, and Angel
Rodrguez. Optimized linear fem for modeling deformable
objects.
Comput. Animat. Virtual Worlds, 17(34):
Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya
Visuospatial
foresight for multi-step, multi-task fabric manipulation.
arXiv preprint arXiv:2003.09044, 2020. 2
Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin
least squares material point method with displacement
discontinuity and two-way rigid body coupling. ACM
Transactions on Graphics (TOG), 37(4):114, 2018. 1, 2,
Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun,
Nathan Carr, Jonathan Ragan-Kelley, and Frdo Durand.
Isabella Huang, Yashraj Narang, Ruzena Bajcsy, Fabio
Grasp planning on 3d fields with graph neural nets, 2023.
Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia
labelling real videos. In Proc. arXiv:2410.11831, 2024.
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics,
Thomas Kipf, Elise Van der Pol, and Max Welling.
Contrastive learning of structured world models. arXiv
preprint arXiv:1911.12247, 2019. 2
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
ment anything. arXiv preprint arXiv:2304.02643, 2023.
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenen-
for manipulating rigid bodies, deformable objects, and
fluids. In ICLR, 2019. 2, 16
Junbang Liang, Ming Lin, and Vladlen Koltun.
ferentiable cloth simulation for inverse problems.
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-
Information Processing Systems, volume 32. Curran
Xingyu Lin, Yufei Wang, Jake Olkin, and David Held.
deformable object manipulation. In Conference on Robot
Xingyu Lin, Yufei Wang, Zixuan Huang, and David
Held. Learning visible connectivity dynamics for cloth
smoothing.
In Conference on Robot Learning, pages
Fei Liu, Entong Su, Jingpei Lu, Mingen Li, and Michael C.
Yip. Robotic manipulation of deformable rope-like objects
using differentiable compliant position-based dynamics.
IEEE Robotics and Automation Letters, 8(7):39643971,
Tiantian Liu, Adam W Bargteil, James F OBrien, and
Ladislav Kavan. Fast simulation of mass-spring systems.
ACM Transactions on Graphics (TOG), 32(6):17, 2013.
Ziang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci,
Li Fei-Fei, Jiajun Wu, and Yunzhu Li.
Model-based
control with sparse neural dynamics. In Thirty-seventh
Conference on Neural Information Processing Systems,
Alberta Longhini, Marco Moletta, Alfredo Reichlin,
Michael C Welle, David Held, Zackory Erickson, and
Danica Kragic. Edo-net: Learning elastic properties of
deformable objects from graph dynamics. In 2023 IEEE
International Conference on Robotics and Automation
(ICRA), pages 38753881. IEEE, 2023. 2
Alberta Longhini, Marcel Bsching, Bardienus Pieter
estimation from RGB supervision for deformable objects.
In 8th Annual Conference on Robot Learning, 2024. 2
Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan.
Dynamic 3d gaussians: Tracking by
persistent dynamic view synthesis. In 3DV, 2024. 2, 9
Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua B
Learning neural constitutive laws from motion observa-
tions for generalizable pde dynamics. In International
Conference on Machine Learning, pages 2327923300.
Miles Macklin.
framework for gpu simulation and graphics, March 2022.
NVIDIA GPU Technology Conference (GTC). 16
Miles Macklin, Matthias Mller, Nuttapong Chentanez,
and Tae-Yong Kim. Unified particle physics for real-time
applications. ACM Transactions on Graphics (TOG), 33
Carsten Moenning and Neil A. Dodgson. Fast Marching
farthest point sampling. Technical Report UCAM-CL-
April 2003. 16
Jing-Chen Peng, Shaoxiong Yao, and Kris Hauser. 3d
force and contact estimation for a soft-bubble visuotactile
sensor using fem. In IEEE International Conference on
Robotics and Automation (ICRA), 2024. doi: 10.1109
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,
and Peter W Battaglia. Learning mesh-based simulation
with graph networks. arXiv preprint arXiv:2010.03409,
Kavya Puthuveetil, Sasha Wald, Atharva Pusalkar,
Pratyusha Karnati, and Zackory Erickson. Robust body
exposure (robe): A graph-based dynamics modeling
approach to manipulating blankets over people. IEEE
Robotics and Automation Letters, 2023. 2
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J
3d classification and segmentation. In Proceedings of
the IEEE conference on computer vision and pattern
Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong
Wang. Language-driven physics-based scene synthesis
and editing via feature splatting. In European Conference
on Computer Vision (ECCV), 2024. 2
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang
ing Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-
Yuan Wu, Ross Girshick, Piotr Dollr, and Christoph
Feichtenhofer. Sam 2: Segment anything in images and
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang
Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang,
Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam:
Assembling open-world models for diverse visual tasks,
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff,
Rex Ying, Jure Leskovec, and Peter Battaglia. Learning
to simulate complex physics with graph networks. In
International conference on machine learning, pages 8459
Keyi Shen, Jiangwei Yu, Huan Zhang, and Yunzhu Li.
bound and neural dynamics, 2024. 2
Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and
Jiajun Wu. Robocraft: Learning to see, simulate, and
shape elasto-plastic objects with graph networks. arXiv
preprint arXiv:2205.02909, 2022. 2
Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li,
and Jiajun Wu. Robocook: Long-horizon elasto-plastic
object manipulation with diverse tools. arXiv preprint
Deborah Sulsky, Shi-Jian Zhou, and Howard L Schreyer.
Application of a particle-in-cell method to solid mechan-
ics. Computer physics communications, 87(1-2):236252,
Robert W. Sumner, Johannes Schmid, and Mark Pauly.
Embedded deformation for shape manipulation. ACM
Trans. Graph., 26(3):80es, jul 2007. ISSN 0730-0301.
Changhao Wang, Yuyou Zhang, Xiang Zhang, Zheng
Tomizuka. Offline-online learning of deformation model
for cable manipulation with graph neural networks. IEEE
Robotics and Automation Letters, 7(2):55445551, 2022.
Yixuan Wang, Yunzhu Li, Katherine Driggs-Campbell,
Li Fei-Fei, and Jiajun Wu. Dynamic-Resolution Model
Learning for Object Pile Manipulation. In Proceedings
of Robotics: Science and Systems, Daegu, Republic of
William F. Whitney, Tatiana Lopez-Guevara, Tobias Pfaff,
Yulia Rubanova, Thomas Kipf, Kimberly Stachenfeld, and
Kelsey R. Allen. Learning 3d particle-based simulators
from rgb-d videos, 2023. 2
William F. Whitney, Jacob Varley, Deepali Jain, Krzysztof
eling the real world with high-density visual particle
Grady Williams, Andrew Aldrich, and Evangelos A
Theodorou. Model predictive path integral control: From
theory to parallel computation.
Journal of Guidance,
Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and
Pieter Abbeel. Gello: A general, low-cost, and intuitive
teleoperation framework for robot manipulators.
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), 2024. 15
Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu,
Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and
Hengshuang Zhao. Point transformer v3: Simpler faster
stronger. In Proceedings of the IEEECVF Conference on
Computer Vision and Pattern Recognition, pages 4840
Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao
Physics-integrated 3d gaussians for generative dynamics.
arXiv preprint arXiv:2311.12198, 2023. 2, 16
Shangjie Xue, Shuo Cheng, Pujith Kachana, and Danfei
Xu. Neural field dynamics model for granular object piles
manipulation. In Conference on Robot Learning, pages
Mengyuan Yan, Yilin Zhu, Ning Jin, and Jeannette
Bohg. Self-supervised learning of state estimation for
manipulating deformable linear objects. IEEE Robotics
and Automation Letters, 5(2):23722379, 2020.
Mengjiao
Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.
Learning interactive real-world simulators. arXiv preprint
Kaifeng Zhang, Baoyu Li, Kris Hauser, and Yunzhu
Li. Adaptigraph: Material-adaptive graph-based neural
dynamics for robotic manipulation. In Proceedings of
Mingtong Zhang, Kaifeng Zhang, and Yunzhu Li. Dy-
namic 3d gaussian tracking for graph-based neural dy-
namics modeling. arXiv preprint arXiv:2410.18912, 2024.
Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y.
William T. Freeman. PhysDreamer: Physics-based inter-
action with 3d objects via video generation. arxiv, 2024.
Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yun-
Reconstruction and simulation of elastic ob-
jects with spring-mass 3d gaussians.
arXiv preprint
Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel
Pinto. Dino-wm: World models on pre-trained visual
features enable zero-shot planning, 2024. 2
APPENDIX
Contents
Method Details
Data Collection
Video Prediction . . . . . . . . . . . . .
Model-Based Planning
Implementation Details
Experiment Setup . . . . . . . . . . . .
Baselines Details
Method Implementation . . . . . . . . .
Additional Experiments
Qualitative Comparisons on Partial Views 17
Qualitative Comparisons on Generalization 17
Qualitative Comparisons on Dynamics
Prediction
Additional Quantitative Experiments . .
Runtime Analysis . . . . . .
Ablation Studies . . . . . . .
Further Comparison with MPM 18
APPENDIX A
METHOD DETAILS
Data Collection
In this work, all training data are collected through an object
segmentation and tracking pipeline that utilizes foundational
vision models. The pipeline consists of the following steps:
Object detection and segmentation;
2D tracking and 3D velocity computation;
3D tracking extraction.
Object detection and segmentation: or each cam-
era view, we apply Grounded-SAM-2 [38, 18, 39] to extract
object masks. Using text prompts containing object descriptions,
Grounded-SAM-2 detects and segments object in the first frame
of a sequence and then propagates the mask across subsequent
frames. To apply the model, we partition the recording into
non-overlapping clips, each containing 720 frames (i.e., 24
seconds).
2D tracking and 3D velocity computing: We
use CoTracker  as the tracking model due to its stable
and robust performance in 2D tracking. Each camera views
recording is parsed into clips of 30 frames (i.e., 1 second)
with 15 overlapping frames. Within each clip, we crop objects
from the video based on the segmentation mask and fill the
background with black pixels. Using the cropped the clip as
from uniformly sampled grid locations in the first frame of the
clip. We then interpolate each pixels 3D velocity using the
predicted trajectories and the corresponding depth image, as
described by the following equation:
tt(xi,tt) proj1
where proj1
t () is the inverse projection function that maps a
pixel location to its corresponding 3D point using depth and
camera transformation. xi,t denotes the 2D location of pixel
i at time t, and xi,tt is the interpolated location of pixel i
at the next time step, t  t. The resulting vi,t represents the
3D velocity of pixel i in the world frame at time t.
To ensure that each pixel has sufficient nearby trajectories
for interpolation, we restrict velocity prediction to the first 15
frames of the clip. With the 15 overlapping frames between
adjacent clips, we ensure that each frame contains velocity
predictions. Finally, we combine the per-pixel velocities from
all camera views to compute the point cloud velocity for each
3D tracking extraction: After obtaining the 3D
point cloud with per-point velocities, we extract persistent point
tracks uusing an iterative rollout approach. Starting from frame
v. At each subsequent time step, we perform k-NN to identify
the closest points and update the particle velocities to the mean
velocity of the selected neighbors.
In summary, our segmentation and tracking pipeline does
not rely on differentiable rendering or optimization, making
it computationally more efficient. Moreover, since foundation
models typically perform well on various kinds of texture-less
deformable objects, such as cloth and bread, our tracking model
is also capable of handling these challenging objects.
Video Prediction
In this section, we detail how we interpolate the 6DoF
transformation of the Gaussian kernels based on particle
motion predictions. Given the reconstructed Gaussian G
{XGS, C, RGS, S, O}, the predictions Xtt, and the previous-
frame particle positions Xt, we first calculate the rotations
of each particles, assuming local rigidity over a small time
gap. For each particle p, we identify its krot nearest particles,
denoted as N(p). We then calculate the rotation rp of particle
rp  arg min
r(xs,ttxp,tt)(xs,txp,t)2.
the nearest particles of p to their new positions. Using
the estimated rotations, we perform Linear Blend Skinning
(LBS)  to interpolate the transformations. For a Gaussian
kernel i with i,t XGS,t and qi,t RGS,t, we denote its kLBS
nearest particles to be N(i). We calculate the interpolated
positions and quaternions, i,tt and qi,tt, using the
(a) Robot workspace
4x RealSence D455
2x XARM 7
Paper Bag
(b) Diverse objects
Fig. 10: Experiment Setup. (a) Our robot workspace includes four calibrated RGB-D cameras positioned at each corner of the table, along
with a GELLO  system for teleoperating the dual xArm 7 robotic arms equipped with parallel grippers. (b) Our experiments involved six
types of materials: (i) a paper bag, (ii) a stuffed animal, (iii) three varieties of bread, (iv) three boxes of different shapes, (v) eight cloth
pieces varying in fabric type and size, and (vi) six ropes differing in length, thickness, and stiffness.
following equations:
wiprp(i,t xp,t)  xp,t  (xp,tt xp,t),
where denotes quaternion multiplication, and wip,t is the
interpolation weight between Gaussian kernel i and particle p
at time t, calculated as:
sN(i) i,t xs,t1 .
particles that are spatially closer to the Gaussian kernel. The
weights are used to compute the weighted average of particle
rotations and translations, which are then applied to the kernel
(as shown in Eq. 19 and 20). Empirically, we set the k-NN
parameters for rotation and LBS to krot  kLBS  8.
Model-Based Planning
For the four manipulation experiments, the planning settings
are detailed as follows. The cloth lifting and box closing
tasks are bimanual, while the rope manipulation and plush
toy relocation tasks involve only a single arm. For the latter
two tasks, we do not consider gripper rotations, so the action
space consists of the 3-DoF (x, y, z) position of the end effector
in the task space. For the two bimanual tasks, we account for
3D rotations, resulting in an action space that includes the
translation and rotation of both grippers, totaling 12 degrees
of freedom. In each iteration of the MPPI  algorithm, we
sample N delta actions from a normal distribution:
eef N(0, ),
where xeef and reef represent the residual translations
and rotations of the end-effector. We then construct action
trajectories by adding the delta actions to the current optimal
{(xeef,1:T , reef,1:T )xeef,1:t  x
eef  t  x(i)
eef  t  r(i)
We then evaluate the sampled action trajectories using the
learned dynamics model and calculate the cost based on
Chamfer Distance (Eq. 16). For the cloth lifting task, to prevent
damage to the cloth, we include an additional penalty term
with an indicator function that penalizes trajectories where the
distance between the grippers exceeds L  0.6 m.
APPENDIX B
IMPLEMENTATION DETAILS
Experiment Setup
Our robot setup and experimental objects are shown in
Fig. 10. Our experiments, including real-world data collection
and manipulation tasks, are conducted in a workspace (Fig. 10a)
equipped with four calibrated RealSense D455 cameras. These
cameras are positioned at the corners of the table to capture
RGB-D images at 30Hz with a resolution of 848x480. Ad-
system with dual UFACTORY xArm 7 robotic arms, each with
7 degrees of freedom and xArms parallel grippers, enabling a
human operator to collect bi-manual random interaction data.
We consider 6 types of deformable objects in our work
(Fig. 10b): (i) a paper bag, (ii) a stuffed animal plush toy,
(iii) three types of bread, (iv) 3 boxes, (v) 8 cloth pieces
differing in fabric, texture, and size, and (vi) 6 ropes varying
in length, thickness, and stiffness. For the dynamics prediction
experiments (Table I and II), we use only one type of cloth and
rope. The remaining objects are used only in the category-level
model experiments (Fig. 5).
Baselines Details
In this section, we provide additional details on the imple-
mentation of the baseline methods, which include both physics-
based and learning-based simulators for deformable objects,
as well as an ablated version of our method that excludes the
grid representation.
implementation and parameter optimization of MPM. In the
MPM algorithm, an object is discretized into Lagrangian
particles that carry dynamic information such as position,
are described by the materials elasticity and plasticity models,
known as constitutive models. Different object types require
different constitutive models, and objects within the same type
may have varying parameters. We assume that the materials
considered in our work are hyperelastic, and thus we use
the fixed corotated elasticity model with an identity plasticity
model. The optimizable parameters in the models include
Youngs modulus E, which characterizes the stiffness or stress
in response to deformations, and the Poissons ratio , which is
kept fixed. We also optimize the friction coefficient  between
the object and surface. Since we assume uniform material
properties across the objects, the optimizable parameters E
and  are shared across all particles.
We optimize the MPM model using the same training setting
as our particle-grid neural dynamics model. Since it is infeasible
to identify per-frame internal strain in the objects, we assume
that the object is in its rest state with no strain at the first
frame of each data sequence. The MPM model is trained using
the same loss function (Eq. 14). We employ Adam as the
optimizer with gradient clipping and perform gradient descent
optimization on both parameters.
state as a graph: zt
Gt  (Vt, Et), where Vt is the vertex
set representing object particles, and Et denotes the edge set
representing interacting particle pairs at time step t. Given dense
particles of object and robot end-effector pose, we apply farthest
point sampling  to downsample object into sparse vertices,
while representing the robot gripper with a single particle. For a
vertex vi,t Vt, we incorporate the history positions xi,tht:t
to implicitly capture velocity information, along with vertex
attributes ci,t to indicate whether the particle corresponds
to the object or the robot end-effector. The final input is the
concatenated feature vector (xi,tht:t, ci,t). In practice, we set
h  2 for all materials. For end-effector particles, the action ut,
represented as delta gripper transformation, is also included in
the input feature. The edges between particles are dynamically
constructed over time by identifying the nearest neighbors
of each particle within a predefined radius. Additionally, we
impose a limit on the maximum number of edges that any
single node can have, which we empirically set to 5.
After constructing the graph, we instantiate the dynamics
model as graph neural networks (GNNs) that predict the
evolution of the graph representation zt. The graph design
follows previous works [19, 58]. To control the accumulation
of dynamics prediction errors, we supervise the models
predictions over K  4 steps. Formally, we train the model
using following the loss function:
Ltrain  1
(xi,t1 xj,t1xi,t xj,t)2 (27)
where xi,t and xi,t represent the predicted and ground truth
positions of the ith vertex, respectively. The first term in the
loss is the mean squared error, while the second term is an
edge length regularization term, which has been shown to be
effective for objects with complex shapes . Empirically,
we set   0.1.
sentation in our model and directly query the velocity field at
particle positions to predict per-particle velocities. Specifically,
we predict the particle velocities with a modified version of
((xp), zp,t),
where zp,t is the locally-averaged particle features calculated
similarly to zg,t in Eq. 9. After calculating vp,tt, we apply
a similar process as in Grid Velocity Editing to edit particle
velocities according to grasping actions and collisions with the
table. The remaining parts of the model, as well as the training
of the Particle baseline, are identical to those in our model.
By simplifying the grid representation, the velocity field
must give predictions for the entire set of particles. Compared
to querying the field at fixed grid locations, this increases
the difficulty of learning the velocity field MLP. Since the
input coordinates are processed with high-frequency sinusoidal
positional encoding , the spatial smoothness of the field is not
regularized. Empirically, we show that this negatively impacts
the dynamics prediction accuracy in the experiments results in
the main paper (Table I and II).
Method Implementation
Following previous works [53, 29], we implement the grid
velocity editing and G2P processes with NVIDIA Warp ,
which accelerates the differentiable process on GPU. Training
our model typically takes around 8 hours on a single NVIDIA
4090 GPU, using a batch size of 32.
In all experiments, we use a time step of t  0.1 s. To
effectively model smaller objects, such as bread, we introduce
a scaling factor s that scales the object before being input into
Starting Frame
Prediction
Ground Truth
Input Point Cloud
Ground Truth Frame
Input View
Side  View
Input View
Side  View
Starting Frame
Prediction
Ground Truth
Input Point Cloud
Ground Truth Frame
Fig. 11: Additional Qualitative Comparisons on Partial Observation. In this experiment, we use only point clouds from a single view as
input to the model. We visualize the starting frame and the input point clouds, followed by our prediction results compared to the ground
truth future particle positions and the future frame. Additionally, we provide a side view where the incompleteness of the input point cloud is
more apparent. Our models predictions closely align with the ground truth, demonstrating its robustness under partial observations.
the model. Specifically, we use s  3.0 and grid size   1 cm
for bread, s  0.8,   2 cm for cloth, and s  1.0,   2 cm
for all other object categories. In all experiments, the particle
velocities are set to zero at the start of an episode, assuming
that the object starts from a static state particle. Otherwise, the
velocities are computed from the difference between current
and previous observations.
The training dataset size, measured by the total duration of
recorded videos, is as follows: (i) cloth: 20.3 minutes, (ii) rope:
21.7 minutes, (iii) plush toy: 3.8 minutes, (iv) box: 10.1 minutes,
(v) paper bag: 6.7 minutes, and (vi) bread: 4.8 minutes. The
evaluation dataset for each category contains test episodes of
each 3 seconds in length, as this duration captures challenging
deformations while remaining computationally efficient. We
use 40 test episodes for cloth and rope, and 20 test episodes
for all other categories.
For the friction coefficient parameter in ground contact, we
fix it empirically to reduce complexity, as our experiments
do not involve extreme friction variations and the fixed value
yields strong performance across tasks.
APPENDIX C
ADDITIONAL EXPERIMENTS
Qualitative Comparisons on Partial Views
In Fig. 11, we provide qualitative visualizations of our
models prediction when using partial view inputs. Specifically,
we visualize the input partial view from a single depth image,
alongside our models predictions and the ground truth. The
results demonstrate that our model is capable of making
accurate predictions using despite being given only a highly
incomplete set of particle data.
Qualitative Comparisons on Generalization
In Fig. 12, we present qualitative visualizations of our
models predictions on unseen object instances. Specifically, we
show video predictions on unseen cloth and rope instances and
compare them against the ground truth. The results demonstrate
that our model generalizes well to novel objects not encountered
during training and predicts realistic rendering results.
Qualitative Comparisons on Dynamics Prediction
In Fig. 13 and 14, we provide additional qualitative compar-
isons between our method and the three baselines on dynamics
Ground Truth
Prediction
Prediction
Fig. 12: Additional Qualitative Comparisons on Generalization. In this experiment, we deploy the trained model on objects not seen
during training. We visualize the video prediction results using Gaussian Splatting, showing both the starting and predicted frames. The
predicted outcomes closely match the ground truth, demonstrating the models ability to generalize to unseen instances.
Batch size
Time (ms)
TABLE III: Runtime analysis.
Cloth GVE
Cloth RP
TABLE IV: Ablation study on deformation controlling methods.
prediction. These results extend those presented in Fig. 3 to
include the MPM and Particle baselines. From the qualitative
predict soft, drifting motions that do not align well with the
ground truth. Furthermore, the particles in the MPM model tend
to scatter or fall to the ground, likely due to the insufficient point
density in the interior volume and the effects of gravity. The
Particle-only baseline predicts dense particle motions similar to
our method. However, it predicts inadequate particle motions
and creates artifacts near the gripper, especially for objects like
plush toys, boxes, and breads.
Additional Quantitative Experiments
Runtime Analysis: We evaluate our models runtime
on the rope test set, with results shown in Table III. During
a batch size of 1, and at 11.9 ms with a batch size of 50,
enabling real-time, batched inference crucial for planning.
Ablation Studies: To validate our design choice of
using different deformation control methods for different action
show that Grid Velocity Editing (GVE) performs better for
prehensile tasks (e.g., cloth), while Robot Particles (RP) are
more effective for nonprehensile cases (e.g., boxes), supporting
the claims made in Sec. III-B4. Furthermore, to assess the
choice of PointNet  as our point encoder and the grid size
Ours-1.25cm
Ours-4cm
MLP Encoder
PTv3 Encoder
TABLE V: Ablation study on grid size and point encoder design.
SeenUnseen MSE
1-4 Views MSE
Planning CD
TABLE VI: Further comparison with MPM.
(1.25cm and 4cm vs. our default 2cm) and the encoder design
(shared MLP and PTv3  vs. PointNet). Results on the rope
1.25cm and PTv3 achieve performance comparable to ours,
they incur higher computational costs.
Further Comparison with MPM: Additional com-
parisons with the model-based simulator MPM on rope data
are provided in Table VI. Empirically, we find that despite
having explicit material parameters, MPM underperforms our
method across all tasks due to its inability to handle partial
observations from depth inputs.
State and action
Particle
Ground Truth
Paper Bag
Plush Toy
Fig. 13: Additional Qualitative Comparisons on Dynamics Prediction. Given the initial states and actions (leftmost column), we present
the prediction results of the MPM with parameter identification baseline, the GBND baseline, and the Particle baseline, compared to our
particle-grid neural dynamics model (middle columns), compared to the ground truth particles at the final frame (rightmost column). We use
a pair of red spheres to indicate the position and orientation of robot grippers. The half-transparent background in the middle columns are the
ground truth final state images, which help highlight the prediction errors. From the results, we observe that our methods predictions align
the best with the ground truth, with fewer artifacts. In contrast, the baseline methods tend to predict motions that do not match well with the
ground truth (e.g., GBND-paper bag, MPM-cloth), cause unwanted breakage (e.g., MPM-rope, GBND-plush toy), or display insufficient
particle motions (e.g. GBND-box, Particle-box, Particle-bread).
State and action
Particle
Ground Truth
Paper Bag
Plush Toy
Fig. 14: Additional Qualitative Comparisons on Dynamics Prediction. Given the initial states and actions (leftmost column), we show
the prediction results of the MPM with parameter identification baseline, the GBND baseline, and the Particle baseline, compared to our
particle-grid neural dynamics model (middle columns), compared to the ground truth particles at the final frame (rightmost column). We use
a pair of red spheres to indicate the position and orientation of robot grippers. The half-transparent background in the middle columns are the
ground truth final state images, which help highlight the prediction errors. From the results, we observe that our methods predictions align
the best with the ground truth, with fewer artifacts. In contrast, the baseline methods tend to predict motions that do not match well with the
ground truth (e.g., GBND-plush toy, MPM-cloth), cause unwanted breakage (e.g., MPM-paper bag, MPM-bread), or display insufficient
particle motions (e.g. Particle-plush toy, Particle-box).
