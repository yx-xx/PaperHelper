=== PDF文件: Learning Humanoid Standing-up Control across Diverse Postures.pdf ===
=== 时间: 2025-07-21 15:13:51.258239 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Learning Humanoid Standing-up Control across
Diverse Postures
Tao Huang2,1
Junli Ren1,3
Huayi Wang1,2
Zirui Wang1,4
Qingwei Ben1,5
Muning Wen1,2
Xiao Chen1,5
Jianan Li5
Jiangmiao Pang1
1Shanghai AI Laboratory
2Shanghai Jiao Tong University
3The University of Hong Kong
4Zhejiang University
5The Chinese University of Hong Kong
Fig. 1: Overview. (a) Our proposed framework HOST enables the humanoid robot to learn standing-up control via reinforcement learning without prior data,
where the robot can successfully stand up across diverse postures in both laboratory and outdoor environments. (b) HOST also demonstrates strong robustness
to many environmental disturbances, including external forces, stumbling blocks, 12kg payload, and challenging initial postures.
AbstractStanding-up control is crucial for humanoid robots,
with the potential for integration into current locomotion and
loco-manipulation systems, such as fall recovery. Existing ap-
proaches are either limited to simulations that overlook hard-
ware constraints or rely on predefined ground-specific motion
world scenes. To bridge this gap, we present HOST (Humanoid
Standing-up Control), a reinforcement learning framework that
learns standing-up control from scratch, enabling robust sim-
to-real transfer across diverse postures. HOST effectively learns
posture-adaptive motions by leveraging a multi-critic architecture
and curriculum-based training on diverse simulated terrains. To
ensure successful real-world deployment, we constrain the motion
with smoothness regularization and implicit motion speed bound
to alleviate oscillatory and violent motions on physical hardware,
respectively. After simulation-based training, the learned control
policies are directly deployed on the Unitree G1 humanoid robot.
Our experimental results demonstrate that the controllers achieve
range of laboratory and outdoor environments (Fig. 1). Videos
and code are available on our project page.
I. INTRODUCTION
Can humanoid robots stand up from a sofa, walk to a
recent advancements in humanoid robot hardware and control
have enabled significant progress in bipedal locomotion [38,
robots to navigate environment and interact with objects
effectively. However, the fundamental capabilitystanding-
up control [43, 17]remains underexplored. Most existing
systems assume the robots start from a pre-standing posture,
limiting their applicability to many scenes, such as transition-
ing from a seated position or recovering after a loss of balance.
We envision that unlocking this standing-up capability would
broaden the real-world applications of humanoid robots. To
this end, we investigate how humanoid robots can learn to
stand up across diverse postures in real environments.
A classical approach for this control task involves tracking
handcrafted motion trajectories through model-based motion
planning or trajectory optimization [17, 18, 22, 43]. Although
effective in generating motions, these methods require exten-
sive tuning of analytical models and often perform subopti-
mally in real-world settings with external disturbances [29,
23] or inaccurate actuator modeling . Besides, real-time
optimization on the robot makes these methods computa-
tionally intensive, prompting workarounds such as reduced
optimization precision or offload computations to external
machines [34, 8], though both are with practical limitations.
Reinforcement learning (RL) offers an alternative effective
framework for humanoid locomotion and whole-body con-
trol [36, 13, 4, 54], benefiting from minimal modeling assump-
tions. However, compared to these tasks that partially decou-
ple upper- and lower-body dynamics, RL-based standing-up
control involves a highly dynamic and synergistic maneuver
on both halves of the body. This complex maneuver features
time-varying contact points , multi-stage motor skills ,
and precise angular momentum control , making RL ex-
ploration challenging. Although predefined motion trajectories
can guide RL exploration, they are typically limited to ground-
specific postures [35, 36, 52, 12], leaving the scalability to
other postures unclear. Conversely, training RL agents from
scratch with wide explorative strategies on the ground can
lead to violent and abrupt motions that hinder real-world
and wide joint limits. In summary, learning posture-adaptive,
real-world deployable standing-up control with RL remains an
open problem (see Appendix B).
In this work, we address this problem by proposing HOST,
an RL-based framework that learns humanoid standing-up con-
trol across diverse postures from scratch. To enable posture-
adaptive motion beyond the ground, we introduce multiple
terrains for training and a vertical pull force during the initial
stages to facilitate exploration. Given the multiple stages of the
groups independently for a better reward balance. To ensure
real-world deployment, we apply smoothness regularization
and motion speed constraints to mitigate oscillatory and vio-
lent motions. Our control policies, trained in simulation
with domain randomization , can be directly deployed on
the Unitree G1 humanoid robot. The resulting motions, tested
in both laboratory and outdoor environments, demonstrate high
including forces, stumbling blocks, and heavy payloads.
We overview the real-world performance of our controllers
in Fig. 1 and summarize our core contributions as follows:
TABLE I: Comparison with existing methods on standing-up control.
wo Prior
Trajectory
Training
Peng et al.
Yang et al.
Tao et al.
Haarnoja et al.
Gaspard et al.
HOST (ours)
Real-world posture-adaptive motions are well achieved
through our proposed RL-based method, without relying on
predefined trajectories or sim-to-real adaptation techniques.
demonstrated by our learned control policies, even under
challenging external disturbances.
Evaluation protocols are elaborately designed to analyze
standing-up control comprehensively, aiming to guide fu-
ture research and development in this control task.
II. RELATED WORK
A. Learning Humanoid Standing-up Control
Classical approaches to standing-up control rely on tracking
handcrafted motion trajectories through model-based opti-
mization [17, 18, 22, 43]. While effective, these methods are
computationally intensive, sensitive to disturbances [29, 23],
and require precise actuator modeling , limiting their
real-world applicability. In contrast, RL-based methods learn
control policies with minimal modeling assumptions, either
by leveraging predefined motion trajectories to guide explo-
ration [35, 36, 52, 12] or employing exploratory strategies to
learn from scratch . However, none of these methods have
demonstrated real-world standing-up motion across diverse
postures. Our proposed RL framework addresses these limita-
tions by achieving posture adaptivity and real-world deploy-
ability without predefined motions, enabling smooth, stable,
and robust standing-up across a wide range of laboratory and
outdoor environments.
B. Reinforcement Learning for Humanoid Control
Reinforcement learning (RL) has been effectively applied
to various humanoid control tasks, showcasing its versatility
and effectiveness. For example, RL has enabled humanoid
robots to achieve robust locomotion on diverse terrains [38,
motions [35, 36, 13, 14, 4], versatile jumping , and
loco-manipulation [7, 27, 50]. Building on these advances,
we address humanoid standing-up control, a parallel problem
presenting unique challenges due to its dynamic nature and
the need for precise coordination of multi-stage motor skills
and time-varying contact points [17, 29]. We propose a novel
approach that integrates a multi-critic framework, motion
Environment
Multiple
RL Agent
Pulling Force
PD Controller
Rescaler
PD Target
(a) Training HoST in simulation
(b) Real-world deployment
PD Controller
Rescaler
PD Target
Encoders
Constraints
Fig. 2: Framework overview. (a) We train policies in simulation from scratch
with multiple critics and motion constraints operationalized by rewards,
smoothness regularization, and action bound (rescaler). (b) The trained polices
can be directly deployed in the real robot to produce standing-up motions.
C. Learning Quadrupedal Robot Standing-up Control
Standing-up control for quadrupedal robots shares similari-
ties with humanoid robots but faces distinct challenges due to
morphological differences, such as quadrupedal designs. Clas-
sical approaches for quadrupedal robots often rely on model-
based optimization and predefined motion primitives [3, 40],
which work well in controlled environments but struggle with
adaptability to diverse postures and real-world uncertainties.
Recent RL-based methods have enabled quadrupedal robots to
recover from falls and transition between poses [23, 30, 52],
using exploratory learning to manage complex dynamics and
environmental interactions. Our work draws inspiration from
these advances, extending them to humanoid robots by ad-
dressing the unique challenges of bipedal standing-up control.
By incorporating posture adaptivity, motion constraints, and a
structured training curriculum, our framework bridges the gap
between quadrupedal and humanoid robot control, enabling
robust standing-up motions across diverse environments.
III. PROBLEM FORMULATION
We formulate the problem of humanoid standing up as a
Markov decision process (MDP; ) with finite horizon,
which is defined by the tuple M  S, A, T , R, . At each
timestep t, the agent (i.e., the robot) perceives the state st S
from the environment and executes an action at A produced
by its policy (st). The agent then observes a successor
state st1 T (st, at) following the environment transition
function T and receives a reward signal rt R. To solve the
goal learn an optimal policy  that maximizes the expected
cumulative reward (return) E[PT 1
t0 trt] the agent receives
during the whole T-length episode, where  [0, 1] is the
discount factor. The expected return is estimated by a value
function (critic) V. In this paper, we adopt Proximal Policy
Optimization (PPO; ) as our RL algorithm because of its
stability and efficiency in large-scale parallel training.
1) Observation Space: We hypothesize that the propri-
oceptive states of robots provide sufficient information for
standing-up control in our target environments. We thus in-
clude the proprioceptive information read from robots In-
ertial Measurement Unit (IMU) and joint encoders into the
state st  [t, rt, qt, pt, pt, at1, ], where t is the angular
velocity of robot base, rt and qt are the roll and pitch, pt
and pt are positions and velocities of the joints, at1 is the
last action, and  (0, 1] is a scalar that scale the output
action. Given the contact-rich nature of the standing-up task,
we implicitly enhance contact detection by feeding the policy
with the previous five states .
2) Action Space: We employ a PD controller for torque-
based robot actuation. The action at represents the difference
between the current and next-step joint positions, with the PD
target computed as pd
t  pt  at, where each dimension of
at is constrained to [1, 1]. The action rescalar  restricts
the action bounds to regulate the motion speed implicitly.
This is essential to constrain the standing-up motion and will
be discussed in later sections. The torque at timestep t is
computed as:
t  Kp  (pd
t pt) Kd  pt,
where Kp and Kd represent the stiffness and damping coef-
ficients of the PD controller. The dimension of action space
A corresponds to the number of robot actuators.
IV. METHOD
This section introduces HOST (Humanoid Standing-up
Control), a reinforcement learning (RL)-based framework for
learning humanoid robots to stand up across diverse postures,
as summarized in Fig. 2. This control task is highly dynamic,
tional RL approaches. We first outline the key challenges
addressed in this work in Section IV-A, then describe the core
components of the framework in the following sections.
A. Key Challenges  Overview
1) Reward Design  Optimization (Section IV-B): The
standing-up task involves multiple motor skills: righting the
stages is challenging without explicit stage separation [25, 19].
We address this by dividing the task into three stages and
activating corresponding reward functions at each stage. The
complexity of these skills requires multiple reward functions,
which can complicate policy optimization. To mitigate this,
we employ multi-critic RL , grouping reward functions to
balance objectives effectively.
2) Exploration Challenges (Section IV-C): Despite multi-
critic RL, exploration remains difficult due to the robots high
degrees of freedom and wide joint limits. Drawing inspiration
from human infant skill development [6, 49], we facilitate
exploration by applying a curriculum-based vertical pulling
force on the base link of the robot.
3) Motion Constraints (Section IV-D): With only reward
driven by high torque limits and numerous actuators. Such
behaviors are impractical for real-world deployment. To ad-
dress this, we introduce an action rescaler  to gradually
tighten action output bounds, implicitly limiting joint torques
and motion speed. Additionally, we incorporate smoothness
regularization  to mitigate motion oscillation.
Fig. 3: Simulation terrains and real-world scenes. We design four terrains
in simulation: ground, platform, wall, and slope to create initial robot postures
that are likely to be met in real-world environments. Examples of these real-
world environments are shown at the bottom of the figure.
4) Sim-to-Real Gap (Section IV-E): A significant challenge
is the sim-to-real gap. We address this through two strategies:
(1) designing diverse terrains to better simulate real-world
starting postures, and (2) applying domain randomization
to reduce the influence of physical discrepancies between
simulation and the real world.
B. Reward Functions  Multiple Critics
Considering the multi-stage nature of the task, we divide
the task into three stages: righting the body hbase < Hstage1,
rising the body Hstage1 < hbase < Hstage2, and standing
hbase > Hstage2, indicated by the height of the robot base
hbase. Corresponding reward functions are activated at each
stage. We then classify reward functions into four groups: (1)
task reward rtask that specifies the high-level task objectives,
(2) style reward rstyle that shapes the style of standing-up
izes the motionw, and (4) post-task reward rpost that specify
the desired behaviors after successful standing up, i.e., stay
standing. The overall reward function is expressed as follows:
rt  wtask rtask
wstyle rstyle
wregu rregu
wpost rpost
where w with superscript represents the corresponding reward
weight. Each reward group contains multiple reward functions.
A comprehensive list of all reward functions and groups is
provided in Table VI.
(critic) presents significant challenges in learning effective
standing-up motions. Besides, the large number of reward
functions makes hyperparameter tuning computationally inten-
sive and difficult to balance. To address these challenges, we
implement multiple critics (MuC; [33, 51, 53]) to estimate
returns for each reward group independently, where each
reward group is regarded as a separate task with its own
assigned critic Vi. These multiple critics are then integrated
into the PPO framework for optimization as follows:
t  Vi(st) Vi(st1)2
where ri
t is the total reward and Vi is the target value function
of reward group i. Each critic independently computes its
TABLE II: Domain randomization settings for standing-up control.
Trunk Mass
U(2, 5)kg
Base CoM offset
Link mass
U(0.8, 1.2) default kg
Restitution
Torque RFI
U(0.05, 0.05) torque limit Nm
Motor Strength
Control delay
Initial joint angle offset
Initial joint angle scale
U(0.9, 1.1) default joint angle rad
advantage function Ai estimated through GAE . These
individual advantages are then aggregated into an overall
weighted advantage: A  P
, where Ai
and Ai are the batch mean and standard deviation of each
advantage. The critics are updated simultaneously with the
policy network  according to:
L()  E [min (t()At, clip(t(), 1 , 1  )At)] ,
where t() and  are the probability ratio and the clipping
C. Force Curriculum as Exploration Strategy
The primary exploration challenges emerge during the tran-
sition from falling to stable kneeling, a stage that proves
difficult to explore effectively through random action noise
alone. While human infants are likely to learn motor skills with
external supports [6, 49], it inspires us to design environmental
assistance to accelerate the exploration. Specifically, we apply
an upward force F on the robot base, which is largely set
at the start of training. This force takes effect only when the
robots trunk achieves a near-vertical orientation, indicating
a successful ground-sitting posture. The force magnitude de-
creases progressively as the robot can maintain a target height
at the end of the episode. See more details in Appendix B.
D. Motion Constraints
1) Action Bound (Rescaler): Humanoid robots often feature
many DoFs, each equipped with wide position limits and high-
power actuators. This configuration often results in violent
motions after RL training, characterized by violent ground
hitting and rapid bouncing movements. While setting low
action bounds could mitigate this behavior, it might prevent
the robot from exploring effective standing-up motions. To
this end, we introduce an action rescaler  to scale the action
on each actuator. This scale coefficient gradually decreases like
vertical force reduction. See more details in Appendix B.
2) Smoothness Regularization:
To prevent motion os-
L2C2  into our multi-critic formulation. This method
applies regularization to both the actor-network  and critics
TABLE III: Main simulation results. We present a performance comparison between HOST and baselines for the proposed metrics. The means and standard
variation are reported across 5 evaluations, each with 250 testing episodes.  indicates that the method completely failed on a certain task.
Platform
(a) Ablation on Number of Critics
HOST-wo-MuC
(b) Ablation on Exploration Strategy
HOST-wo-Force
HOST-wo-Force-RND
(c) Ablation on Motion Constraints
HOST-wo-Bound
HOST-Bound0.25
HOST-wo-L2C2
HOST-wo-rstyle
(d) Ablation on Historical States
HOST-History0
HOST-History2
HOST-History5 (ours)
HOST-History10
Platform
Fig. 4: Motion analysis in simulation. (Left) UMAP visualization of joint-space trajectories demonstrates similar but distinct motion patterns on the terrains
except for the wall. Besides, the trajectories of each terrain are overall consistent, with variation to handle the difference of starting postures. (Right) 3D
trajectory visualizations reveal stable, coordinated hand-foot motion and dynamic posture adaptability, demonstrating effective whole-body coordination and
validating the proposed framework. Point color in the plot indicates motion progression, with lighter shades for earlier and darker for later times.
Vi by introducing a bounded sampling distance between
consecutive states st and st1:
LL2C2  D((st), (st))  V
D(Vi(st), Vi(st)),
where D is a distance metric,  and V are weight coeffi-
uniform noise u U(). We combine this objective function
with ordinary PPO objectives to train our control policies.
E. Training in Simulation  Sim-to-Real Transfer
We use Isaac Gym  simulator with 4096 parallel envi-
ronments and the 23-DoF Unitree G1 robot to train standing-
up control policies with the PPO  algorithm.
1) Terrain Design: To model the diverse starting postures
in the real world, we design 4 terrains to diversify the starting
the trunk of robot, (3) wall that supports the trunk of the
the whole robot. We visualize these terrains and examples of
their corresponding scenes in the real world in Fig. 3.
2) Domain Randomization: To enhance real-world deploy-
physical gap between simulation and reality. The random-
ization parameters, detailed in Table II, include body mass,
base center of mass (CoM) offset, PD gains, torque offset,
and initial pose, following [2, 28]. Notably, the CoM offset is
CoM position noise, which may arise from insufficient torques
or discrepancies between simulated and real robot models.
F. Implementation Details
Our implementation of PPO is based on . The actor and
critic networks are structured as 3-layer and 2-layer MLPs,
respectively. Each episode has a rollout length of 500 steps.
For smoothness regularization, the weight coefficients  and
V are set to 1 and 0.1, respectively. The PD controller
operates at 200 Hz in simulation and 500 Hz on the real robot
to ensure accurate tracking of the PD targets, while the control
policies run at 50 Hz. Additional implementation details and
hardware setup are provided in Appendix A.
V. SIMULATION EXPERIMENTS
A. Experimenrt Setup
1) Evaluation Metrics.: While the design of evaluation
metrics for humanoid standing-up control remains an open
the following metrics:
Success rate Esucc: The episode is considered successful
if the robots base height, hbase, exceeds a target height
htarg and is maintained for the remainder of the episode,
indicating stable standing.
CoM Offset
Sagittal Force
Initial Pose
Random Torque Dropout
Fig. 5: Robustness analysis in simulation. Evaluation of control policies under four environmental disturbances demonstrates the robustness of our controllers.
The poor performance of HOST-History1 indicates the importance of historical information for robustness, while HOST-Bound0.25s high energy consumption
reveals limitations in motion quality under disturbance, demonstrating the effect of curriculum setup of action bound.
Fig. 6: Trade-off analysis in simulation. Trade-offs between motion speed,
smoothness relationship, indicating the importance of constrained motion
speed achieved by our method for real-world deployment.
Feet movement Efeet: The distance traveled by the robots
feet after reaching the target height htarg, indicating stabil-
ity in the standing pose.
Motion smoothness Esmth: We aggregate the movement of
all joint angles of consecutive control steps to measure the
smoothness of the motion. It indicates that the robot should
keep a smooth motion during the whole episode.
Energy Eengy: The energy consumed before reaching htarg,
indicating the avoidance of violent standing-up motion.
2) Baselines: To evaluate the effectiveness of the key
design choices in HOST, we compare it against the following
ablated versions:
Single critic: A baseline using a single critic RL to assess
the impact of multiple critics on motor skill learning.
Exploration strategy: Baselines with random noise and
curiosity-based rewards (e.g., RND ) to evaluate the
effectiveness of the force curriculum.
Motion constraints: Ablation of action bounds  and
smoothness regularization L2C2 to test their influence on
motion smoothness.
Historical states: Ablation of the number of historical
states to assess their effect on standing-up motion.
B. Main Results
HOST demonstrates good efficacy in learning standing-up
control across all terrains, as shown in Table III. The effect of
key design choices is summarized as follows:
Multiple critics are crucial for learning motor skills Using
the same reward functions, the performance of the single critic
version of HOST deteriorates significantly across all terrains,
achieving zero success rates. This highlights the importance of
multiple critics in learning and integrating motor skills while
also reducing the hyperparameter tuning burden.
Force curriculum enhances exploration efficiency. Without
the proposed force curriculum, the robot fails to stand up on
all terrains except the platform, as the other terrains require
exploration from a fully fallen state to stable kneeling. While
curiosity-based exploration partially alleviates this challenge,
performance remains unsatisfactory. In contrast, the force
curriculum greatly improves exploration efficiency.
Action bound prevents abrupt motions. While the robot can
learn to stand up without action bounds (HOST-wo-Bound),
its movements are excessively violent, as indicated by three
performance metrics. With action bounds, HOST demonstrates
smoother motions and higher success rates. Although HOST-
Bound0.25 performs well, its motions are less natural due to
restricted exploration during training.
Smoothness regularization prevents motion oscillation.
Adding smoothness constraints significantly reduces motion
oscillation and increases energy efficiency, validating the ef-
fectiveness of smooth regularization. Further discussion is
presented in Section VI.
Medium history length yields great performance. HOST
with short history length underperforms in contact-rich sce-
length improves performance, though it slightly reduces mo-
tion smoothness and increases energy consumption compared
to the default setting.
C. More Analyses
Trajectory analysis (Fig. 4). Following , we apply Uni-
form Manifold Approximation and Projection (UMAP; )
to project joint-space motion trajectories into 2D, providing a
visualization of the humanoid robots motion across diverse
Fig. 7: Snapshot of real robot motion. We directly transfer our policies from simulation to four real-world scenes that correspond to four simulation terrains.
We conclude that (1) our policies can produce smooth and successful standing-up motion in all tested scenes and (2) smooth regularization of L2C2 is
important to avoid oscillation and improve stability.
Fig. 8: Snapshot of outdoor experiments. We test our controllers in diverse outdoor environments, demonstrating smooth motion on unseen terrains such
as grassland, wooden platforms, and stone roads, as well as successful performance on stone platforms and tree-leaning postures.
TABLE IV: Main results for real robot experiments. We report the success rate and motion smoothness to quantitatively compare our methods with the
baseline. The results demonstrate the superiority of our method and the importance of adding smooth regularization into our method.
Platform
HOST-wo-L2C2
HOST (ours)
terrains. The resulting UMAP figure demonstrates distinct
motion patterns: smooth, controlled movement on flat ground,
while more complex, yet consistent, trajectories emerge on
challenging terrains such as Wall. Additionally, in the 3D
trajectory plots, the coordinated motion of the robots hands
and feet reveals significant posture adaptability, as the robot
adjusts its stance dynamically for balance and stability. These
observations highlight the harmonious whole-body coordina-
tion achieved by our controllers and validate the effectiveness
of our proposed framework.
Robustness analysis (Fig. 5). We comprehensively evaluate
the robustness of our learned control policies by simulating
various environmental disturbances. Specifically, we test four
types of external perturbations: CoM position offset in the
sagittal direction, consistent sagittal force, initial joint angle
strate that the policies exhibit remarkable robustness across
all disturbances, achieving high success rates and efficient
motion energy utilization. Notably, the poor performance of
HOST-History1 underscores the critical role of historical
maintaining robustness. Furthermore, while HOST-Bound0.25
achieves a high success rate, its elevated energy consumption
highlights its limited ability to maintain motion smoothness
under disturbance. These findings validate the robustness of
our policies while indicating the importance of historical
context and curriculum of action bound for robust standing-up.
Trade-off analysis (Fig. 6). We examine trade-offs between
motion speed, smoothness, and energy consumption across
terrains. On the left, motion speed and smoothness exhibit
an inverse relationship: longer fall-to-standing times enhance
smoothness but reduce speed, a trend consistent across all
terrains. On the right, energy consumption increases with fall-
to-standing time, with terrain-specific variations. For exam-
Fig. 9: Sim-to-real analysis. (a) We analyze the effect of each domain randomization term, showing that our randomization terms effectively mitigate the
sim-to-real gap, with the CoM position being particularly influential. (b) To further investigate the sim-to-real gap, we compare the phases of knee and hip
joints that are crucial for standing-up control. The results reveal significant discrepancies in joint velocities, suggesting a sim-to-real gap in joint torques.
Fig. 10: Emergent properties in real robot experiments. (a) our controllers show great robustness to the external force (3kg ball), blocking objects on the
fully falling down. (c) Our policies also exhibit the ability of dynamic balancing over a 15slippery slope without falling down.
TABLE V: Robustness to payload and random torque dropout.
Payload Mass
Torque Dropout Ratio
consumption rises sharply at longer fall-to-standing times
despite low motion speed, suggesting greater energy intensity.
This is likely due to the need for increased force or modified
body mechanics to push against a vertical surface, making
the motion in Wall less energy-efficient than other terrains.
motion speed and smoothness, indicating the importance of
constrained motion speed for real-world deployment and vali-
dating the necessity of our approach to achieve such motions.
VI. REAL ROBOT EXPERIMENTS
A. Main Results
We evaluate our method in both laboratory and outdoor en-
vironments corresponding to simulation terrains, using HOST-
wo-L2C2 as the baseline to examine the effect of smoothness
regularization during deployment.
Smooth regularization improves motions (Fig. 7). Motion
oscillations are observed in all scenes without smoothness reg-
our method produces smooth and stable motions, especially
on 10.5slope. Quantitative results in Table IV strengthen
Fig. 11: Standing stability. Our control policies demonstrate great stability
against external disturbances after successful standing up.
this conclusion, with our approach achieving a 100 success
rate and high motion smoothness across all scenes.1
Generalization to outdoor environments (Fig. 8). We eval-
uate our learned controllers in a variety of outdoor envi-
encountered during training. On flat ground, the controllers
produce stable, smooth motions across grassland, wooden
included in the training simulations. Additionally, our con-
trollers successfully handle more complex scenarios, including
stone platforms and tree-leaning postures, demonstrating their
adaptability to diverse real-world conditions.
B. Sim-to-real Analysis
In this analysis, we investigate the effect of various domain
randomization terms on the sim-to-real gap, as shown in
Fig. 9. Our results demonstrate that the introduction of these
randomization terms significantly reduces the sim-to-real gap,
particularly with respect to the Center of Mass (CoM) position.
1We select the successful episode to compute smoothness to reflect the
effect of L2C2 regularization better. Due to the unavailability of the height,
we compute the smoothness Esmth within two seconds after starting up.
Fig. 12: More diverse postures. HoST can learn across prone postures on
the ground. The learn policies can also handle side-lying postures.
Phase plot. To further investigate the sources of this gap, we
examine the phase plots of the knee and hip roll joints. These
joints are considered most important for standing-up motions.
We observe a notable discrepancy between simulated and real-
world joint velocities, suggesting a gap in joint torques. This
highlights the need for more accurate actuator modeling to
bridge the sim-to-real gap in humanoid standing-up tasks,
which is also suggested by previous work on quadrupedal
robots . Despite this, our controllers remain effective in
handling these discrepancies, exhibiting joint paths consistent
with the simulated ones.
C. Emergent Properties
Robustness to external disturbance (Fig. 10a). The robust-
ness of our control policies was tested through experiments
involving external disturbances, such as a 3 kg ball impact
and obstructive objects. The controllers maintained stability
even under significant disturbances, like objects disrupting the
robots center of gravity. Additionally, the controllers managed
payloads up to 12kg, twice the mass of the humanoid robots
trunk. We also quantitatively verify the great robustness of
payload and torque dropout ratio in Table V.
Fall recovery (Fig. 10b). Our controllers also exhibited strong
resilience in recovering from large external forces without
fully falling down. This capability is vital for humanoid robots
navigating unpredictable real-world scenarios with sudden
impacts or balance shifts. Testing showed that, even under
abrupt perturbations, the robots regained their upright posture,
demonstrating the effectiveness of our control strategies in
maintaining dynamic stability.
Dynamic balance (Fig. 10c). We further tested our controllers
on a 15slippery slope, simulating challenging real-world
conditions such as unstable surfaces. The controllers not only
maintained stability on the incline but also adjusted posture
and center of mass in real time to counteract the slippery con-
ditions. These results highlight the adaptability and stability of
our controllers, ensuring humanoid robots can operate safely
on diverse and unpredictable terrains.
Standing stability (Fig. 11). Our controllers demonstrate
strong standing stability, effectively resisting external distur-
bances after successful standing up. This stability is beneficial
for integrating our controllers into existing control systems.
Fig. 13: Extension to large-size robots. HoST can be easily extended to
Unitree H1 and H1-2 humanoid robots with minor hyperparameter tuning.
D. Prone and Side-lying Postures on the Ground
We demonstrate that HoST is capable of learning across
prone postures on the ground, as visualized in Fig. 12. Be-
without any tuning. However, there are significant differences
in motion patterns between prone and supine postures. This
somehow limits our method: when training from posture
prevent violent motions, making the feasibility of joint training
from prone and supine postures unclear currently.
E. Extend HoST to Large-size Humanoid Robots
We believe that standing-up control is more challenging
in larger humanoid robots than in G1 due to their increased
weight and the limited actuators. As an initial test, we extend
HoST to Unitree H1 and H1-2. Simulation and real-world
motions are visualized in Fig. 13. Compared to G1, we observe
greater reliance on (i) upper-body contact with the ground and
(ii) high hip actuation. While successful, two sim-to-real gaps
insufficient torques and, which is consistent with the observa-
tion in Section VI-B; (ii) noticeable deviations in upper-body
posture. It remains unclear whether these gaps originate from
our framework or hardware limitations. Identifying the source
of these gaps is valuable in the future.
VII. CONCLUSION
Our proposed framework, HOST, advances humanoid
standing-up control by addressing the limitations of existing
predefined motion trajectories. By leveraging reinforcement
learning from scratch, HOST enables the learning of posture-
adaptive standing-up motions across diverse terrains, ensuring
effective sim-to-real transfer. The multi-critic architecture,
along with smoothness regularization and implicit speed con-
Experimental results with the Unitree G1 humanoid robot
demonstrate smooth, stable, and robust standing-up motions
in a variety of real-world scenarios. Looking forward, this
work paves the way for integrating standing-up control into
existing humanoid systems, with the potential of expanding
their real-world applicability.
VIII. LIMITATIONS AND FUTURE DIRECTIONS
While our method demonstrates strong real-world perfor-
addressed in the near future.
Perception of the environment. Although proprioception
alone is sufficient for many postures, some failures were
observed during outdoor tests, such as standing from a seated
position and colliding with surroundings. Integrating percep-
tual capabilities will help address this issue.
More diverse postures. We observe that training with both
supine and prone postures has negatively impacted perfor-
mance due to interference between sampled rollouts. Ad-
dressing this issue could further enhance capabilities like fall
recovery and improve overall system generalization.
Integration with existing humanoid systems. Although this
paper does not demonstrate integration with existing humanoid
fectively incorporated into current humanoid frameworks to
extend their real-world applications.
ACKNOWLEDGMENTS
This work is funded in part by the National Key RD
Program of China (2022ZD0160201), and Shanghai Artificial
Intelligence Laboratory.
REFERENCES
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg
Klimov. Exploration by random network distillation. In
International Conference on Learning Representations
(ICLR), 2019.
Luigi Campanaro, Siddhant Gangapurwala, Wolfgang
Learning and deploying
robust locomotion policies with minimal dynamics ran-
domization.
In 6th Annual Learning for Dynamics
Control Conference (L4DC), 2024.
Juan Alejandro Castano, Chengxu Zhou, and Nikos
Tsagarakis. Design a fall recovery strategy for a wheel-
legged quadruped robot using stability feature space. In
International Conference on Robotics and Biomimetics
(ROBIO), 2019.
Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang,
Ge Yang, and Xiaolong Wang. Expressive whole-body
control for humanoid robots. In Robotics Science and
Systems (RSS), 2024.
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback.
arXiv preprint
Laura J Claxton, Dawn K Melzer, Joong Hyun Ryu,
and Jeffrey M Haddad.
The control of posture in
newly standing infants is task dependent.
Journal of
Experimental Child Psychology, 2012.
Jeremy Dao, Helei Duan, and Alan Fern.
real learning for humanoid box loco-manipulation.
International Conference on Robotics and Automation
(ICRA), 2024.
Farbod Farshidian, Michael Neunert, Alexander W Win-
An efficient
optimal planning and control framework for quadrupedal
locomotion. In International Conference on Robotics and
Automation (ICRA), 2017.
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein,
and Chelsea Finn. Humanplus: Humanoid shadowing and
imitation from humans. In Conference on Robot Learning
(CoRL), 2024.
Clement Gaspard, Marc Duclusaud, Gregoire Passault,
Melodie Daniel, and Olivier Ly. Frasa: An end-to-end
reinforcement learning agent for fall recovery and stand
up of humanoid robots. arXiv preprint arXiv:2410.08655,
Ambarish Goswami and Vinutha Kallem. Rate of change
of angular momentum and balance maintenance of biped
In International Conference on Robotics and
Automation (ICRA), 2004.
Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H
Roland Hafner, et al. Learning agile soccer skills for a
bipedal robot with deep reinforcement learning. Science
Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong
Guanya Shi. Omnih2o: Universal and dexterous human-
to-humanoid whole-body teleoperation and learning. In
Conference on Robot Learning (CoRL), 2024.
Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia
Xiaolong Wang, et al.
body controller for humanoid robots.
In International
Conference on Robotics and Automation (ICRA), 2025.
Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario
Learning agile and dynamic motor skills for
legged robots. Science Robotics, 2019.
Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu,
and Yuke Zhu. Harmon: Whole-body motion generation
of humanoid robots from language descriptions.
Conference on Robot Learning (CoRL), 2024.
Fumio Kanehiro, Kenji Kaneko, Kiyoshi Fujiwara, Ken-
suke Harada, Shuuji Kajita, Kazuhito Yokoi, Hirohisa
The first humanoid robot that has the same size as a
human and that can lie down and get up. In International
Conference on Robotics and Automation (ICRA), 2003.
Fumio Kanehiro, Kiyoshi Fujiwara, Hirohisa Hirukawa,
Shinichiro Nakaoka, and Mitsuharu Morisawa. Getting
up motion planning using mahalanobis distance.
International Conference on Robotics and Automation
(ICRA), 2007.
Dohyeong Kim, Hyeokjin Kwon, Junseok Kim, Gun-
min Lee, and Songhwai Oh.
Stage-wise reward
shaping for acrobatic robots: A constrained multi-
objective
reinforcement
learning
approach.
preprint arXiv:2409.15755, 2024.
Taisuke Kobayashi.
ous constraint towards stable and smooth reinforcement
learning.
In International Conference on Intelligent
Robots and Systems (IROS), 2022.
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra
Malik. Rma: Rapid motor adaptation for legged robots.
In Robotics: Science and Systems (RSS), 2021.
Yasuo Kuniyoshi, Yoshiyuki Ohmura, Koji Terada, and
Akihiko Nagakubo.
Dynamic roll-and-rise motion by
an adult-size humanoid robot. International Journal of
Humanoid Robotics, 2004.
Joonho Lee, Jemin Hwangbo, and Marco Hutter. Robust
recovery controller for a quadrupedal robot using deep re-
inforcement learning. arXiv preprint arXiv:1901.07517,
Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo
humanoid robots manipulation skills through single video
imitation.
In Conference on Robot Learning (CoRL),
Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey
versatile bipedal jumping control through reinforcement
learning. In Robotics Science and Systems (RSS), 2023.
Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey
forcement learning for versatile, dynamic, and robust
bipedal locomotion control. The International Journal
of Robotics Research (IJRR), 2024.
Fukang Liu, Zhaoyuan Gu, Yilin Cai, Ziyi Zhou, Shijie
feasible whole-body trajectories for versatile humanoid
loco-manipulation.
arXiv preprint arXiv:2409.20514,
Junfeng Long, Junli Ren, Moji Shi, Zirui Wang, Tao
Learning hu-
manoid locomotion with perceptive internal model. arXiv
preprint arXiv:2411.14386, 2024.
Dingsheng Luo, Yaoxiang Ding, Zidong Cao, and Xi-
hong Wu. A multi-stage approach for efficiently learning
humanoid robot stand-up behavior.
In International
Conference on Mechatronics and Automation, 2014.
Yuntao Ma, Farbod Farshidian, and Marco Hutter. Learn-
ing arm-assisted fall damage reduction and recovery for
legged mobile manipulators. In International Conference
on Robotics and Automation (ICRA), 2023.
Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong
performance
gpu-based
physics simulation for robot learning.
arXiv preprint
Leland McInnes, John Healy, and James Melville. Umap:
Uniform manifold approximation and projection for di-
mension reduction.
arXiv preprint arXiv:1802.03426,
Siddharth Mysore, George Cheng, Yunqi Zhao, Kate
Multi-critic actor learning:
Teaching rl policies to act with style. In International
Conference on Learning Representations (ICLR), 2022.
Michael Neunert, Farbod Farshidian, Alexander W Win-
contacts and automatic gait discovery for quadrupeds.
Robotics and Automation Letters (RA-L), 2017.
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel
Van de Panne.
inforcement learning of physics-based character skills.
Transactions On Graphics (TOG), 2018.
Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey
Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming. John Wiley  Sons,
Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell,
Jitendra Malik, and Koushil Sreenath.
Real-world hu-
manoid locomotion with reinforcement learning. Science
N. Rudin, David Hoeller, Philipp Reist, and Marco
Learning to walk in minutes using massively
parallel deep reinforcement learning. In Conference on
Robot Learning (CoRL), 2024.
Uluc Saranli, Alfred A Rizzi, and Daniel E Koditschek.
Model-based dynamic self-righting maneuvers for a
hexapedal robot. The International Journal of Robotics
Research (IJRR), 2004.
Michael I. Jordan, and P. Abbeel.
High-dimensional
continuous
generalized
advantage
estimation. arXiv preprint arXiv:1506.02438, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Jorg Stuckler, Johannes Schwenk, and Sven Behnke.
Getting back on two feet: Reliable standing-up routines
for a humanoid robot. In IAS, 2006.
Dimitrios
A survey on control of
humanoid fall over. Robotics and Autonomous Systems,
Richard S Sutton and Andrew G Barto. Reinforcement
Tianxin Tao, Matthew Wilson, Ruiyu Gou, and Michiel
Van De Panne.
Learning to get up.
In SIGGRAPH
Conference Proceedings, 2022.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez,
Yazhe Li, Diego de Las Casas, David Budden, Abbas
mind control suite.
arXiv preprint arXiv:1801.00690,
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,
Wojciech Zaremba, and Pieter Abbeel. Domain random-
ization for transferring deep neural networks from simu-
lation to the real world. In 2017 IEEERSJ International
Conference on Intelligent Robots and Systems (IROS),
Claes Von Hofsten. Eyehand coordination in the new-
born. Developmental psychology, 1982.
Francesco Ruscelli, and Nikos Tsagarakis. Hypermotion:
Learning hybrid behavior planning for autonomous loco-
manipulation. In Conference on Robot Learning (CoRL),
Pei Xu, Xiumin Shang, Victor Zordan, and Ioannis
Karamouzas.
Composite motion learning with task
control. Transactions on Graphics (TOG), 2023.
Chuanyu Yang, Can Pu, Guiyang Xin, Jie Zhang, and
Zhibin Li.
Learning complex motor skills for legged
robot fall recovery.
Robotics and Automation Letters
Fatemeh Zargarbashi, Jin Cheng, Dongho Kang, Robert
locomotion with high-level objectives via mixture of
dense and sparse rewards.
In Conference on Robot
Learning (CoRL), 2024.
Chong Zhang, Wenli Xiao, Tairan He, and Guanya Shi.
sequential contacts. In Conference on Robot Learning
(CoRL), 2024.
Ziwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid
parkour learning.
In Conference on Robot Learning
(CoRL), 2024.
APPENDIX
A. More Experimental Details
Hardware Setup. We conducted our experiments using the
Unitree G1 humanoid robot, which has a mass of 35 kg, a
height of 1.32 m, and 23 actuated degrees of freedom (6 per
a Jetson Orin NX for onboard computation and uses an IMU
and joint encoders to provide proprioceptive feedback.
Evaluation Protocol. Each policy is evaluated on each terrain
with 5 repetitions of 250 episodes each, totaling 1250 episodes.
We report the mean and standard deviation of performance.
The target standing-up height is set to 0.6m for the slope
terrain and 0.7m for all other terrains during evaluation.
Robustness Test. The CoM bias and sagittal force are set on
the x-axis direction of the robot. The initial joint angle offset
is applied to all joints of the robot. The random torque dropout
is applied to each simulation step (200Hz), where the torques
are set to zero if being dropout.
Expression of metrics: Smoothness Esmth is computed via
t0 pt2 2pt1  pt2, where pt is the joint positions.
Energy Eengy is computed via Phbase<Hstage2
t   ptT dt
pt is joint
B. More Implementation Details
Curriculum Setup. The curriculum adjustment condition is
consistent for both the vertical force and action bound: the
head height hhead must reach a target height Hhead by the
end of each episode. Initially, the vertical force F is set to
target head height, the vertical force decreases by 20N, and
the action bound decreases by 0.02. The lower bounds for the
vertical force and action bound are 0N and 0.25, respectively.
Stage Division. The first stage involves righting the body,
where we set Hstage1 to 0.45m. The second stage involves
rising the body, with Hstage2 set to 0.65m.
Reward Functions. We present the complete set of re-
ward functions and their detailed descriptions in Table VI.
Several regularization reward terms are adapted from prior
work [21, 28, 13]. Additionally, we incorporate a tolerance
is computed as a function of an input value i, which is
constrained by three parameters: bounds b, margin m, and
value v. The bounds b define the region where the reward is
1 if i lies within the bounds. Outside this region, the reward
smoothly decreases according to a Gaussian function, reaching
the value v at a distance determined by the margin m.
Ankle parallel reward is calculated as the
variance of keypoints height of the ankle, as
visualized in the right figure. These keypoints
are handcrafted without collision models.
PPO Implementation. Our PPO implementation follows the
framework outlined in . The actor network consists of
a 3-layer MLP with hidden dimensions [512, 256, 128],
while each critic network is a 2-layer MLP with hidden
dimensions [512, 256]. Each iteration includes 50 steps per
epoch. The discount factor  is set to 0.99, the clip ratio
is set to 0.2, and the entropy coefficient is 0.01. The multi-
critic architecture is based on previous work , where each
advantage function is independently calculated and normalized
within its corresponding reward group.
Baseline Implementations. HOST-wo-MuC represents a
baseline with a single value network, essentially a standard
RL implementation. HOST-wo-Force-RND removes the ver-
tical force curriculum and introduces an RND reward with a
coefficient of 0.2 . HOST-Bound0.25 uses a fixed action
bound of   0.25 without a curriculum. HOST-wp-rstyle
eliminates all style-related reward functions. Lastly, HOST-
History modifies the history length of states while keeping
other implementations unchanged.
Terrains. The heights of the platforms range from 20cm to
92cm. The slope inclination varies from approximately 1 to
14. The wall inclination spans from approximately 14 to 84.
PD Controller. In simulation, the stiffness values are set as
100 for the upper body, 40 for the ankle, 150 for the hip, and
TABLE VI: Reward functions and groups used for learning standing-up control. Reward functions within the same group are independently normalized,
whose associated advantaged functions are estimated via a distinct critic. The bold symbols represent vectors. The H with subscripts represents the threshold
height of standing-up stages defined in Section IV-B. The ftol is a gaussian-style function with a saturation bound, referring to [47, 46] for more details. G
denotes ground, and the letters in PSW denote platform, slope, and wall, respectively.
Expression
Description
(a) Task Reward
wtask  2.5
It specifies the high-level task objectives.
Head height
ftol (hhead, [1, inf], 1, 0.1)
The head of robot head hhead in the world frame.
Base orientation
The orientation of the robot base represented by projected gravity vector.
(b) Style Reward
wstyle  1
It specifies the style of standing-up motion.
Waist yaw deviation
1(qwaist > 1.4)
It penalizes the large joint angle of the waist yaw.
Hip rollyaw deviation
1(max(ql,r
hip) > 1.4)  1(min(ql,r
It penalizes the large joint angle of hip rollyaw joints.
Knee deviation
1(max(ql,r
knee) > 2.85)  1(min(ql,r
knee) < 0.06)
10(P SW )
It penalizes the large joint angle of knee joints.
Shoulder roll deviation
shoulder < 0.02)  1(qr
shoulder > 0.02)
It penalizes the large joint angle of shoulder roll joint.
Foot displacement
base qxy
foot2.clip(0.3, inf)
1(hbase > Hstage2)
It encourages robot CoM locates in support polygon, inspired by .
Ankle parallel
left ankle)  var(qz
right ankle))2 < 0.05
It encourages the ankles to be parallel to the ground via ankle keypoints.
Foot distance
feet2 > 0.9
It penalizes a far distance between feet.
Feet stumble
1(i, Fxy
i  > 3F z
25(P SW )
It penalizes a horizontal contact force with the environment.
Shank orientation
ftol(mean(l,r
shank), [0.8, inf], 1, 0.1)  1(hbase > Hstage1)
It encourages the leftright shank to be perpendicular to the ground.
Base angular velocity
exp(2  xy
base2)  1(hbase > Hstage1)
It encourages low angular velocity of the during rising up.
(c) Regularization Reward
wregu  0.1
It specifies the regulariztaion on standing-up motion.
Joint acceleration
It penalizes the high joint accelrations.
Action rate
It penalizes the high changing speed of action.
Smoothness
at 2at1  at22
It penalizes the discrepancy between consecutive actions.
It penalizes the high joint torques.
Joint power
It penalizes the high joint power
Joint velocity
It penalizes the high joint velocity.
Joint tracking error
pt ptarget
It penalizes the error between PD target (Eq. (1)) and actual joint position.
Joint position limits
i[(pi pLower
).clip(inf, 0)  (pi pHigher
).clip(0, inf)]
It penalizes the joint position that beyond limits.
Joint velocity limits
i[( pi pLimit
).clip(0, inf)]
It penalizes the joint velocity that beyond limits.
(d) Post-task Reward
wpost  1
It specifies the desired behaviors after a successful standing up.
Base angular velocity
exp(2  xy
base2)  1(hbase > Hstage2)
It encourages low angular velocity of robot base after standing up.
Base linear velocity
exp(5  vxy
base2)  1(hbase > Hstage2)
It encourages low linear velocity of robot base after standing up.
Base orientation
exp(5  xy
base2  1(hbase > Hstage2)
It encourages the robot base to be perpendicular to the ground.
Base height
exp(20  hbase htarget
base 2  1(hbase > Hstage2)
It encourages the robot base to reach a target height.
Upper-body posture
exp(0.1  pupper ptarget
upper 2)  1(hbase > Hstage2)
It encourages the robot to track a target upper body postures.
Feet parallel
exp(20  hl
feet.clip(0.02, inf))  1(hbase > Hstage2)
In encourages the feet to be parallel to each other.
200 for the knee. The damping values are set to 4 for the
upper body, 2 for the ankle, 4 for the hip, and 6 for the knee.
High stiffness values for the hip and knee are used due to the
high torque demands during the standing-up process. During
real-world deployment, we observe a significant torque gap
between simulation and reality (see Fig. 9). Thus, the stiffness
of the hip and knee are adjusted to 200 and 275, respectively.
Shoulder
Observation noises are without curriculum, set as below:
Observation
Ang. Velocity
Pitch  Roll
DoF Position
DoF Velocity
Action Rescaler
Noise Scale
Learning across prone postures. We make the following
adjustment to work the algorithm: more strict constraints on
hip joint deviation rewards, weights for reward groups, and
additional thigh orientation reward functions as a replacement
for shank orientation rewards.
Extending HoST to Unitree H1 and H1-2. We make the
following adjustment to work the algorithm: scale of pulling
our implementations, H1 has 19 actuators and H1-2 has 27
actuators. During the hardware deployment, the stiffness of hip
and knee joints are amplified to 1.5 times than the simulation
repository. Please refer to there for more details.
