=== PDF文件: Is Your Imitation Learning Policy Better than Mine Policy Comparison with Near-Optimal Stopping.pdf ===
=== 时间: 2025-07-22 09:41:23.367270 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Is Your Imitation Learning Policy Better than Mine?
Policy Comparison with Near-Optimal Stopping
David Snyder1,2, Asher James Hancock2, Apurva Badithela2, Emma Dixon1, Patrick Miller1,
Rares Andrei Ambrus1, Anirudha Majumdar2, Masha Itkina1, and Haruki Nishimura1
1Toyota Research Institute (TRI), 2Princeton University
dasnyderprinceton.edu
AbstractImitation learning has enabled robots to perform
ulation settings. As new methods are developed, they must
be rigorously evaluated and compared against corresponding
baselines through repeated evaluation trials. However, policy
comparison is fundamentally constrained by a small feasible
sample size (e.g., 10 or 50) due to significant human effort and
limited inference throughput of policies. This paper proposes a
novel statistical framework for rigorously comparing two policies
in the small sample size regime. Prior work in statistical policy
comparison relies on batch testing, which requires a fixed, pre-
determined number of trials and lacks flexibility in adapting
the sample size to the observed evaluation data. Furthermore,
extending the test with additional trials risks inducing inadvertent
proposed statistical test is sequential, allowing researchers to
decide whether or not to run more trials based on intermediate
results. This adaptively tailors the number of trials to the
difficulty of the underlying comparison, saving significant time
and effort without sacrificing probabilistic correctness. Exten-
sive numerical simulation and real-world robot manipulation
experiments show that our test achieves near-optimal stopping,
letting researchers stop evaluation and make a decision in a near-
minimal number of trials. Specifically, it reduces the number
of evaluation trials by up to 32 as compared to state-of-the-
art baselines, while preserving the probabilistic correctness and
statistical power of the comparison. Moreover, our method is
strongest in the most challenging comparison instances (requiring
the most evaluation trials); in a multi-task comparison scenario,
we save the evaluator more than 160 simulation rollouts.
I. INTRODUCTION
The importance of trustworthy and efficient robot pol-
icy evaluation protocols has become paramount in imitation
learning as the scale of underlying deep learning models
and the complexity of tasks continue to increase. This need
is especially pronounced in dexterous manipulation where
environment introduce inherent randomness in outcomes.
A particularly important aspect of policy evaluation is policy
an environment to assess their relative performance. Policy
comparison forms the foundation of robot learning as an
empirical science , enabling researchers to objectively
measure the scientific progress of the field. Nevertheless, this
setting introduces an additional source of stochasticity due to
random outcomes of both the first and the second policies,
making the reliability of the comparison more challenging to
ensure than evaluation of single policies.
To motivate concretely, consider an example policy com-
parison scenario presented in Fig. 1 where the performance is
quantified based on binary successfailure metrics, a common
choice [7, 59, 41] as continuous-valued rewards are often
difficult to define. This scenario naturally arises when re-
searchers want to demonstrate the effectiveness of a particular
intervention (e.g., a new policy architecture) by comparing
the new policy 1 against a baseline 0. Alternatively, 1
and 0 could represent the same policy evaluated under
different environment distributions, providing insights on gen-
eralization. In either case, the evaluator faces two practical
challenges. First, only a small number of trials (e.g., 10
60) [35, 18, 14, 40, 27, 5] can be performed per evaluation
setting due to the large human effort needed to reset the
environment between trials and the substantial wall-clock time
imposed by limited inference throughput of large policies.
While high-fidelity simulators can alleviate the human effort
and still provide valuable insights into policy performance [32,
42], real-world evaluation remains indispensable for ensuring
reliable deployment in downstream applications. Second, the
evaluation results are revealed sequentially, possibly leading
to fluctuating observations depending on when the evaluation
is stopped. In the Fig. 1 example, the evaluator could observe
more successes for 0 after conducting additional trials, even
though 1 initially appeared superior after the first five.
Although recent work [53, 28] proposes statistical policy
comparison approaches, it follows the conventional batch test-
ing scheme, requiring complete results from a pre-determined
number of trials before the statistical test can be performed.
corresponding results; even if the test fails to determine the
relative performance due to closely matched results, the evalu-
ator cannot append more evaluation trials to the existing results
to run the test again, as doing so would constitute p-hacking
that invalidates statistical assurances . Unfortunately, this
is a common but harmful research practice outside of robotics
, which needs to be averted to ensure reproducible science.
To address these challenges, we propose a novel sequen-
tial testing framework named STEP (Sequential Testing for
Efficient Policy Comparison) for rigorously comparing per-
formance of imitation learning policies1. Unlike batch testing,
1Although this paper focuses on imitation learning, STEP is naturally
applicable to evaluating any types of policies based on binary metrics,
including reinforcement learning (RL) policies with sparse 01 reward.
Fig. 1: Robot policy comparison problem under binary successfailure metrics. Novel policy 1 is compared against baseline
0 in a sequence of trials. Within a given evaluation budget, the evaluator seeks a statistically significant comparison in as few
trials as possible. Due to the cost of hardware setup and calibration, as well as the limited inference speed of complex policies,
these results generally arrive in sequence from a single (or few) hardware setups. Allowing the evaluator to adaptively and
near-optimally tailor the number of trials based on the data observed so far  without compromising statistical assurances of
the comparison  is a central contribution of this work.
our approach allows the number of trials to be varied within
a given experimental budget. This critical feature offers two
practical advantages. First, the evaluator can stop conducting
trials early without sacrificing the probabilistic correctness of
the comparison if enough statistical evidence is accumulated
quickly in favor of 1 (or 0). Second, it reduces the epistemic
risk of overconfident (and potentially incorrect) conclusions
when 0 and 1 are closely matched, since the test will abstain
from making a decision if statistical evidence remains low
after all the planned trials have been executed. Alternatively,
the evaluator may safely append additional trials to the original
samples and re-conduct statistical analysis on all the results ob-
tained thus far without inadvertent p-hacking. We demonstrate
these advantages through simulation and real-world robot
manipulation experiments. Furthermore, extensive numerical
experiments show that STEP significantly outperforms state-
of-the-art (SOTA) sequential methods, reducing the required
number of trials by up to 32 without sacrificing probabilistic
correctness. The specific contributions of this paper are as
We propose STEP, a novel sequential statistical frame-
work for evaluating relative performance of two policies
with tunable probabilistic correctness 2.
Our sequential testing approach admits an adaptive num-
ber of evaluation trials tailored to the difficulty of the un-
derlying comparison while achieving near-minimal sam-
ple complexity.
We additionally present a straightforward extension of
our framework to (1) multi-task and (2) multi-policy
comparison settings via a reduction to a set of pair-wise
statistical comparisons.
II. PRELIMINARIES
Consider a physical robot trained to complete a task in a
variety of environment realizations. This setting is naturally
2Associated website and code can be found at:
modeled as a partially observable Markov decision process
(POMDP) , where the underlying state s represents the
environment and robot states. The observation o is determined
by the robots embodiment and sensing apparatus. The training
pipeline is designed to synthesize a policy taking actions a that
achieve a high reward r(s, a)  1[s Ssuccess] on a particular
which the episode will terminate. This reward encodes a binary
successfailure criterion of the task. In imitation learning, a
surrogate loss function is used to train a policy that matches
the behavior of expert demonstrations  instead of directly
solving the POMDP. At the end of a training process, a policy
1 will be obtained. This policy has a true success rate (i.e.,
expected total episode reward) under a distribution Ds0,o0 over
the initial state and observation:
p1  EDs0,o0,1
r(st, at)
where dependence on the state transition and observation
models are omitted for brevity. The true success rate is
unknown and must be estimated via multiple evaluation trials.
Assumption 1 (Regularity): In each evaluation trial, the ini-
tial state s0 and the observation o0 are drawn in an independent
and identically distributed (i.i.d.) fashion from the underly-
ing distribution Ds0,o0
3. We assume access to samples from
Under Assumption 1, the nth evaluation trial involves
making an i.i.d. draw of an environment from Ds0,o0 and
running the policy 1 in this environment. This yields a binary
successfailure outcome z1,n corresponding to an i.i.d. draw
from a Bernoulli random variable with mean p1, which is the
true performance (success rate) of the policy 1 on the task:
3Note that this is a standard assumption in statistical testing. A discussion of
practical methods by which to approximately satisfy this c
