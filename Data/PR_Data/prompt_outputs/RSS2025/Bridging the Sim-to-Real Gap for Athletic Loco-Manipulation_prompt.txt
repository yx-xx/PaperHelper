=== PDF文件: Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation.pdf ===
=== 时间: 2025-07-22 10:00:17.127113 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Bridging the Sim-to-Real Gap for Athletic
Loco-Manipulation
Nolan Fey, Gabriel B. Margolis, Martin Peticco, and Pulkit Agrawal
Improbable AI Lab
Massachusetts Institute of Technology, Cambridge, MA 02139
AbstractAchieving athletic loco-manipulation on robots re-
quires moving beyond traditional tracking rewardswhich sim-
ply guide the robot along a reference trajectoryto task rewards
that drive truly dynamic, goal-oriented behaviors. Commands
such as throw the ball as far as you can or lift the weight as
quickly as possible compel the robot to exhibit the agility and
power inherent in athletic performance. However, training solely
with task rewards introduces two major challenges: these rewards
are prone to exploitation (reward hacking), and the exploration
process can lack sufficient direction. To address these issues, we
propose a two-stage training pipeline. First, we introduce the Un-
supervised Actuator Net (UAN), which leverages real-world data
to bridge the sim-to-real gap for complex actuation mechanisms
without requiring access to torque sensing. UAN mitigates reward
hacking by ensuring that the learned behaviors remain robust
and transferable. Second, we use a pre-training and fine-tuning
strategy that leverages reference trajectories as initial hints to
guide exploration. With these innovations, our robot athlete
learns to lift, throw, and drag with remarkable fidelity from
simulation to reality.
I. INTRODUCTION
General whole-body control comes naturally to animals
after years of evolution, yet it remains a long-standing chal-
lenge in robotics. Fluid whole-body motion requires balancing
multiple competing tasks and constraints that depend on both
the robots morphology and its environment . Recent
work [7, 34] demonstrates that sim-to-real reinforcement learn-
ing (RL), using methods such as Proximal Policy Optimization
(PPO) , is a promising paradigm for learning these behav-
iors by leveraging parallel simulations .
For dynamic, goal-oriented loco-manipulation, it is natural
to train robots with task rewardscommands like throw the
ball as far as possible or lift the weight as quickly as
possible that drive athletic behaviors. However, these task
rewards pose two major challenges: (i) they are prone to
reward hacking, where the policy exploits imperfections in the
guidance. To circumvent these issues, many works on sim-
to-real transfer instead train whole-body controllers (WBCs)
to track dense reference motions [4, 7, 13, 23, 34]. Dense
tracking objectives provide strong regularization by constrain-
ing the policy to adhere to a reference trajectorythereby
reducing reward hackingand they offer a structured path for
1Authors are also affiliated with Computer Science and Artificial Lab-
oratory (CSAIL), the Laboratory for Information and Decision Systems
(LIDS), and the MIT-IBM Watson AI Lab at MIT. Correspondence to
nolanfeymit.edu
Ball Throw
Dumbbell Snatch
Sled Pull
Fig. 1: Sim-to-real transfer of athletic loco-manipulation.
We reduce the sim-to-real gap for a quadruped manipulator
by learning a corrective model for the simulated actua-
tor dynamics based on real-world data, formulated as an
unsupervised actuator net (UAN). Policies trained with the
corrected simulator exhibit improved sim-to-real transfer and
push the limits of the robots physical capabilities in athletic
tasks involving whole-body coordination. Videos of the robots
behaviors are available at
exploration. However, this strategy relies on defining high-
quality reference commands a priori, which in turn demands
access to high-quality reference data. For robots with non-
human morphologies like legged manipulators, obtaining such
data is particularly challenging, and the resulting reference
commands may not capture the optimal, athletic strategies that
a policy might otherwise discover.
To fully harness the benefits of task rewards, it is crucial
to ensure that the simulation faithfully replicates real-world
dynamics. Inaccurate simulation models allow policies to
exploit imperfections, leading to reward hacking, particularly
so when the reward is underspecified. Although techniques
like domain randomization [50, 51, 53] and online system
identification [15, 19, 24, 29, 31, 35] address this by sampling
over parameter distributions, they rely on a priori assumptions
that may not fully capture the complex dynamics of real hard-
ware. For instance, harmonic drive actuators exhibit non-linear
proxies like motor current unreliable for torque estimation.
A promising alternative is to enhance the simulations
physics model directly with real-world data, focusing on
accurately modeling the actuator dynamics. With this moti-
framework for learning corrective actuator models without the
need for torque sensors. UAN is trained using reinforcement
learning to predict corrective torques,   UAN(e), by
minimizing discrepancies between simulated and real-world
joint encoder measurements. In doing so, UAN effectively
bridges the sim-to-real gap even for robots with complex
transmission mechanisms and noisy or unavailable torque
measurements. We evidence the effectiveness of UAN through
sim-to-real demonstrations incorporating the Unitree Z1 Pro
Building on this enhanced simulation environment, we
address the challenge of guided exploration for athletic be-
haviors. Rather than enforcing strict adherence to a reference
In our approach, a WBC is first pre-trained on random base
velocities and end-effector pose commands to establish a
strong motion prior. Then, to learn a new athletic behavior, we
initialize the controller with a reference trajectory and fine-
tune it using a task-specific rewardallowing the policy to
depart from the reference when beneficial.
In summary, our paper presents an easy-to-use training
pipeline for whole-body athletic behaviors that reliably transfer
to reality. First, we employ the Unsupervised Actuator Net
(UAN) to calibrate actuator dynamics and mitigate reward
physics. With this improved simulation environment, we then
pre-train a whole-body controller (WBC) to establish fun-
damental motion skills and fine-tune it with task-specific
rewardsusing a reference trajectory merely as a hint to guide
exploration. This integrated approach enables our robot to
perform dynamic tasks such as throwing, lifting, and dragging
with remarkable fidelity.
II. METHOD
Our training pipeline (see Figure 2) is separated into two
training (Sections II-B and II-C). The real-to-sim calibration
phase involves collecting data on the real robot and training
a UAN to close the sim-to-real gap for non-ideal actuation
mechanisms. Similar to past work [8, 23, 33, 44], our WBC
training is split into two distinct sub-phases: pre-training (Sec-
tion II-B) and fine-tuning (Section II-C). After pre-training, the
policy can track reference trajectories if provided as a sequence
of base velocity and end effector pose commands. During
the fine-tuning phase, the policy observes a reference task
trajectory. This helps warm start exploration when learning a
new task because the policy can simply track these commands
to achieve reasonable task performance. Through training with
the task reward itself rather than a tracking reward, the policy
learns how to depart from the reference trajectory to achieve
higher task performance. Our simulation environments for the
pre-training and fine-tuning phases rely on the same strate-
gies for sim-to-real transfer, including domain randomization
(Section II-B) and a UAN (Section II-A).
Our experiments consider a Unitree B2 quadruped with
a modified Unitree Z1 Pro arm mounted on its back. The
quadruped is 65 cm tall when standing and weighs 60 kg, while
the arm is 74 cm fully extended and weighs 6.8 kg. The system
has 19 actuated joints: 3 for each leg, 6 for the arm, and 1 for
the gripper.
A. Unsupervised Actuator Net
Some actuators are challenging to model in simulation, es-
pecially when they have complex transmission mechanisms. In
such cases, standard domain randomization and online system
identification techniques may be insufficient, and instead, it
is preferable to learn to model the actuator directly from
hardware data. Previous approaches rely on output torque
to learn how to predict the motors torque. Alternatively, we
propose a method for matching the transition dynamics of the
actuator such that
fsim freal(s, ) fsim(s, ).
To influence the simulator dynamics, fsim, we learn a residual
velocity errors, e, and outputs a corrective torque, , for the
simulator such that
UAN freal
fsim (s,   UAN (e)) .
The corrective torques needed to minimize the transition error
are unlabeled, so we parameterize UAN as a neural network
and train it with RL.
1) Architecture and observation space: The network is
designed as a 2-layer MLP with layer sizes [128, 128] and
ELU activations. It is executed at every simulation time step
(5 ms). Assuming each arm joint is identical, a single UAN
is shared across all of the arms actuators, with each actuator
being processed independently by the shared network .
We constrain the observation space to include a history of the
past 20 (equivalent to 100 ms) position and velocity errors
for each relevant actuator. These design choices help prevent
overfitting to other aspects of the training data, such as inertial
coupling. Also, sharing the data across actuators improves
data efficiency. While our training data from individual joints
covers 75 of the operating region of the actuators torque
and velocity, combining data from all arm actuatorseach
Simulator
Reinforcement Learning
AB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXJHoMevGYgHlAsoTZSW8yZnZ2mZkVQsgXePGgiFcyZt4yTZgyYWNBRV3XR3BYng2rjut5Nb
W9Y3MpvF3Z29YPiodHTR2nimGDxSJW7YBqFxiw3AjsJ0opFEgsBWM7mZ6wmV5rF8MOMEYgOJA85o8ZK9YteseSW3TnIKvEyUoIMtV7
xq9uPWRqhNExQrTuemxhQpXhTOC0E01JpSN6A7lkoaofYn80On5MwqfRLGypY0ZK7npjQSOtxFNjOiJqhXvZm4n9eJzXhjThMkNS
rZYFKaCmJjMviZ9rpAZMbaEMsXtrYQNqaLM2GwKNgRveV0rwse5VypX5Vqt5mceThBE7hHDy4hircQw0awADhGV7hzXl0Xpx352PRmnOy
mWP4AfzB3UXjLo<latexit>
Real-world
Rollouts
rsimtoreal
rsmoothness
Simulator
Actuator
lMn0ph06mYSZG6GEoYbF4q49WfcTdO2iy0emDgcM693DMnSKQw6LpfTmltfWNzq7xd2dnd2zoHh51TJxqDm0ey1j3AmZACgVtFCihl2hgUSChG0xvc77CNqIWD3gLIFBxMZKhIztJLvRwnQZjBfIjDas2tuwvQv8QrSI0UaA2rn4o5mkECrlkxvQ9N8FBxjQK
LmFe8VMDCeNTNoapYpFYAbZIvOcnlRMNY26eQLtSfGxmLjJlFgZ3M5pVLxf8ophteDTKgkRVB8eShMJcWY5gXQkdDAUc4sYVwLm5XyCdOMo62pYkvwVr8l3Qu6l6j3rirDVvijrK5IScknPikSvSJHekRdqEk4Q8kRfy6qTOsPmvC9HS06xc0xwfn4BnZ5
kfs<latexit>et
Reinforcement Learning
FIYzXmu9jbp0E6yCIJQZkepGKLpxWcFeoB2HTJpQzOZITkjlqEv4MZXceNCEbfu3fk2phdBW38IPnOSTnDxLBNTjOlzU3v7C4tJxbyaurW9s2lvbNR2nirIqjUWsGgHRTHDJ
qsBsEaiGIkCwepB73JYr98xpXksb6CfMC8iHclDTgkY5Nv7ygd8jtVt1lIRBqJ7AwOfgBJ783dtwtO0RkJzxp3Ygpopvf7baMU0jJoEKonXTdRLwMqKAU8EGVaqWUJoj3R
Y01hJIqa9bLTNAB8Y0sZhrMyRgEf090RGIq37UWA6IwJdPV0bwv9qzRTCMyjMkmBSTpKEwFhgPo8FtrhgF0TeGUMXNXzHtEkUomADzJgR3euVZUzsuqVi6fqkUL6YxJFDu2
gPHSIXnaIyukIVEUPaAn9IJerUfr2Xqz3setc9ZkZgf9kfXxDXkxmzg<latexit>
rt  rtask
Reference
GZjYxCW1OwK8QBJhLUJs2RCcBdPXjbN84pbrVTvLsq16zyOIjgCxAUuOAS1MAtqIMGwOARPINX8GY9WSWuUxay1YcwhmJP19QPh6aF8<latexit>
Reference
1. Train UAN
2. Pre-train  Fine-tune WBC
3. Deploy
ZPpB06mYSZG6GEoYbF4q49WfcTdO2iy0emDgcM693DMnSKQw6LpfTmltfWNzq7xd2dnd2zoHh51TJxqxtslrHuBdRwKRvo0DJe4nmNAok7wbT29zvPnJtRKwecJbwQUTHSoSCUbS70cUJ0GY0fkQh9WaW3cXIHJV5AaFGgNq5KGZpxBUySY3pe26Cg4xqFE
zyecVPDU8om9Ix71uqaMTNIFtknpMzq4xIGv7FJKFnMjo5Exsyiwk3lGsrl4n9eP8XwepAJlaTIFVseClNJMCZ5AWQkNGcoZ5ZQpoXNStiEasrQ1lSxJXirX5LOhd1r1Fv3FWmjdFHWU4gVM4BwuoAl30I2MEjgCV7g1UmdZfNeVOlpxi5xhwfn4BnBdkf
c<latexit>at
w2m3bpZhN2vwgh1Ffx4kERrz6IN9GTZuDtg4sO8x8Hzs7QcqZAtvNmpr6xubWXtxs7u3v6BeXg0UEkmCe2ThCdyGBFORO0Dw4HaS4jg9CGY3pTwyOViXiHvKUejEeCxYxgkFLvtl0g4SHKo1VbiAs5kPvtmy2Yc1ipxKtJCFXqeWGCcliKoBwrNTIsVPw
CiyBEU5nDTdTNMVkisd0pKnAMVeMQ8s061ElpRIvURYM3V3xsFjlWZT0GCZq2SvF7xRBtGVzCRZkAFWTwUZdyCxCqbsEImKQGea4KJZDqrRSZYgK6r4YuwVn8ioZnLedTrtzd9HqXld1NExOkFnyEGXqItuUQ1EUE5ekav6M14Ml6MdNjMVozqp0mgPj8w
ezT5V4<latexit> t
>ABHicbVDNSsNAGNzUv1roj16CRbBU0lEqseiF48VbC0IWw2m3bpZhN2vwgh1Ffx4kERrz6IN9GTZuDtg4sO8x8Hzs7QcqZ
AtvNmpr6xubWXtxs7u3v6BeXg0UEkmCe2ThCdyGBFORO0Dw4HaS4jg9CGY3pTwyOViXiHvKUejEeCxYxgkFLvtl0g4SHK
o1VbiAs5kPvtmy2Yc1ipxKtJCFXqeWGCcliKoBwrNTIsVPwCiyBEU5nDTdTNMVkisd0pKnAMVeMQ8s061ElpRIvURYM3V3x
sFjlWZT0GCZq2SvF7xRBtGVzCRZkAFWTwUZdyCxCqbsEImKQGea4KJZDqrRSZYgK6r4YuwVn8ioZnLedTrtzd9HqXld1NEx
OkFnyEGXqItuUQ1EUE5ekav6M14Ml6MdNjMVozqp0mgPj8wezT5V4<latexit> t
Fig. 2: Unsupervised Actuator Network (UAN) approach for real-to-sim-to-real. Our training pipeline involves three steps:
1) Train a UAN to close the sim-to-real gap for actuators with complex transmission mechanisms by mapping a history of joint
position and velocity errors, et, to corrective torques,  t, 2) Pre-train a WBC using random motion references (base velocity
and EE pose), then and fine-tune it on an athletic task reward with the UAN in loop, and 3) Deploy. During the fine-tuning
task performance.
experiencing distinct loads (e.g., actuators near the base see
higher loads) covers 99 (see Figure 7).
2) Data collection: We collect data on real hardware to
construct a dataset of transitions {(st,  t, st1)i}N
each actuator. Our intention during data collection was to
sufficiently cover the state space to avoid overfitting. Thus,
we opted not to use policy data and instead collected data
with three types of action sequences: 1) square waves, 2) sine
while keeping the rest of the actuators at a fixed position
target. We swept 12 different combinations of amplitude and
frequency for each wave, resulting in about 50 seconds of
data for each actuator. For the Gaussian noise data, we passed
torque commands to all the robots joints simultaneously. We
sampled a new action from a Gaussian distribution every 5 to
400 ms for about 5 minutes.
3) Training Environment: We designed the training envi-
ronment in Isaac Sim  with 4096 parallel environments.
We train policies with the RSL-RL implementation  of
PPO  with default hyperparameters, minus a few modifi-
cations (see Appendix A for the full list of learning algorithm
hyperparameters). Following , we apply a separate fixed
learning rate to the critic while using an adaptive learning rate
for the actor. Additionally, we divide the data of each epoch
into four mini-batches for the actor while using the entire batch
for the critic, as we found that larger batch sizes produce more
stable gradients and result in lower value function loss.
4) Task Design: For each environment at each timestep,
we uniformly sample a real-world transition, (st,  t, st1)k,
and set the state of the simulator to match st and the initial
torque to  t. After policy inference, we modify the torque by
adding the correction,  t, and then step the simulator. We
then compute the reward as
rsimtoreal
rsmoothness
where rsimtoreal
aims to minimize the difference between
the real joint position and the simulated joint position, and
rsmoothness
biases exploration to gradual deviations. For a
complete list of reward terms, please refer to Appendix A.
Each training episode consists of a 20 s rollout executing the
torque sequence from the hardware data from  t to  t20s.
Through training on rollouts, the actuator net learns to remain
stable across many simulation time steps.
B. Whole-body Controller Pre-training
Before training on task-specific behaviors, we pre-train
the WBC to learn foundational trajectory-tracking skills. Our
training scheme builds upon the method proposed in  by
incorporating a strategy for learning to track an EE orientation
command. As in Section II-A3, we designed the training
environment in Isaac Sim with 4096 parallel environments and
trained the policies with PPO [38, 39] (using separate learning
rates and batch sizes for the actor and critic).
1) Policy Architecture:
The WBC is a control policy,
at  (otH:t), where the action at time t, at, is a vector
of position targets for each of the robots joints and otH:t is
an observation history of length H  10 timesteps (200 ms).
We parameterize  as a 3-layer multi-layer perceptron (MLP)
with layer sizes [512, 512, 512] and ELU activations. The value
function approximator network has the same architecture but
does not share weights with the policy.
2) Observation Space: The policys observation space con-
sists of proprioceptive readings from the robots onboard sen-
frame g, a base velocity command vcmd
, an end effector pose
command pcmd
, the joint positions q, the joint velocities q, the
previous actions at1, and a timing variable t  sin(2ft)
with f  2.2 Hz corresponding to the gait cycle frequency.
embedding vector zt (set to zero during pre-training).
3) Sim-to-Real Considerations: Our approach for bridging
the sim-to-real gap uses a combination of domain random-
ization (DR) and real-to-sim calibration. To learn locomotion
behaviors robust to terrain variations, we randomize terrain
in the robots URDF, we randomize the mass and center of
mass position of each of the robots links. We also randomize
the PD gains and stall torques for each actuator in the robots
observed on hardware. To encourage learning recovery behav-
and periodically perturb it with external forces and torques at
the base, hips, feet, and end-effector, following the approach
proposed in . The DR ranges used for both pre-training and
fine-tuning are provided in Appendix A.
Inspired by , we clip the commanded motor torques
such that
where  max and qmax are the maximum torques and velocities
of the actuators, respectively. This clipping strategy enforces a
physical motor constraint by ensuring that torque commands
do not demand power beyond the motors maximum output
capacity. Furthermore, we clip the arm torques a second time
to satisfy the constraint
where Pmax is the maximum total power of the arm joints,
because we found experimentally this helps prevent the arm
from entering a power protect state enforced by the robots
manufacturer.
Since typical DR strategies were insufficient for athletic
behaviors in the arm (which uses harmonic drives), we in-
corporate the UAN (Section II-A) for the arm actuators.
4) Task Specification: The pre-training task for the WBC
is to track a desired base velocity and EE pose. The velocity
, consists of a desired
forward velocity vcmd
desired yaw-rate cmd
rotated frame aligned with the robots center of mass at a fixed
height above the terrain. The choice of frame encourages the
robot to coordinate with its legs to expand its workspace. The
EE command pcmd
comprises a Cartesian
position pEE
and orientation oEE
(provided as the first two
columns of a rotation matrix).
5) Reward Function: The reward function is split as rt
, where rtrack
includes tracking terms (EE pose,
base velocity) and gait terms, while raux
includes regular-
ization and smoothing terms. The EE tracking term rewards
minimizing the distance between four key points, where one
key point is positioned at the frames origin, and the others
are positioned along each axis of the frame. Full details are
provided in Appendix A.
Fig. 3: Unitree Z1 Pro arm. This arms harmonic actuators
behave substantially differently from the quasi-direct-drive
motors common in small legged robots. This image also
shows the reinforcements we designed to ensure that the limit
on athleticism comes from actuation rather than the linkage
structural integrity.
6) Command Sampling Scheme: We adopt the approach
first proposed in  to sample commands during training.
We sample a new base velocity command and a new goal
end effector pose every 7 seconds of simulation time. Upon
seconds) from the previous command. While this sampling
scheme suffices for foundational loco-manipulation skills, it
may be too smooth for highly agile motions  this motivates
our task-specific fine-tuning (Section II-C).
C. Task-Specific Finetuning
After pre-training, the policy can track reference trajectories,
but struggles on high-acceleration tasks. To address this, we
fine-tune the policy directly with task rewards. The same WBC
base policy weights can be reused for multiple task policies,
thus avoiding repeated pre-training.
1) Initialization: The policy weights are initialized to those
learned during pre-training. To avoid policy collapse, we set
a low initial learning rate (1  105) for the actor and retain
the standard deviation from pre-training. Additionally, we set
the entropy coefficient in PPO to zero during fine-tuning to
improve training stability.
2) Reference trajectory and task embedding: During fine-
and a one-hot task embedding to inform which phase of
the task (e.g., set-up, execute, settle) is active. We hand-
designed the reference trajectories through joint interpolation
and forward kinematics, but they could also come from an
expert policy or human demonstration.
3) Fine-tuning with task reward: The environment for fine-
tuning phase extends that of pre-training (same DR ranges,
external pushes, etc.). The reward becomes ri
t is task-specific. Initially, the policy tracks the reference,
aiding exploration; later, it learns to deviate to maximize task
performance.
III. EXPERIMENTAL SETUP
We chose the Unitree B2 with Unitree Z1 Pro arm as
our hardware platform, and we consider three athletic tasks:
Structural upgrades to the arm were custom designed and
fabricated to withstand the high loads during athletic behaviors
(see Section III-A).
Difference (m)
Sim-to-Real Difference in Throw Distance (m)
Distance (m)
Real Throw Distance (m)
Actuator Net
UAN (ours)
Fig. 4: UAN improves simulator accuracy and real throwing performance. UAN (Ours) achieves lower sim-to-real
difference in throw distance as compared to standard baselines, resulting in a better real throw distance. For this comparison,
we train and test policies with a fixed-base arm, to avoid the risk of the legged base falling during performance-critical ablations.
Following our UAN training (Section II-A), we pre-trained
a WBC (Section II-B) and then fine-tuned policies for each
task (Section II-C). Ablations comparing our method with
alternatives are described in Section IV-A and Section IV-B.
A. Arm Modifications
During development, the Unitree Z1 Pro arm experienced
structural failures at links 2 and 4, with minor deformations
at link 5. The damage resulted from the highly dynamic
movements in the athletic experiments, which applied loads
to the links that exerted excessive stress and strain on the
links exceeding the materials yield strength. Modifications
were made to reinforce links 2, 3, 4, and 5 by adding supports
at the joints. This prevents loads from being transferred solely
through the motors which are cantilevered. A mass-efficient
aluminum square tube was used for link 2, which experiences
the highest stress of all the links. Idler bearings are used to
apply support at the motor outputs without restricting their
movement. In the URDF, link masses, centers of mass, and
inertias were updated based on CAD calculations and the
parallel axis theorem. Figure 3 shows the reinforced arm.
IV. EXPERIMENTAL RESULTS
In this section, we report ablations that identify the contri-
bution of key system components and present results for the
athletic tasks. Supplemental videos are provided on the project
Our experiments address the following questions:
1) Does our unsupervised actuator net reduce the sim-to-
real gap and improve transfer?
2) What are the benefits of our two-stage pre-training and
fine-tuning pipeline relative to alternatives?
3) Does our approach enable sim-to-real transfer of athletic
whole-body control tasks?
A. Comparing System Identification Approaches
We compare several methods for modeling the actuator
dynamics of the Unitree Z1 Pro arm in Isaac Sim. In particular,
we consider:
1) Default: The baseline simulator with no additional
modifications.
2) DR: The simulator augmented with domain random-
ization (randomizing PD gains, friction, and armature
parameters).
3) ROA: A domain randomization baseline enhanced with
an online system identification module via Regularized
Online Adaptation .
4) Actuator Net: A supervised actuator network fol-
lowing Hwangbo et al.  where torque labels are
estimated from the motor current. (Note that these labels
do not capture the nonlinear effects introduced by the
harmonic reducers.)
5) CEM: A method in which friction, frictional damping,
and armature parameters are optimized using the cross-
entropy method to minimize the mean-square joint po-
sition error between simulation and hardware.
6) UAN: Our proposed unsupervised actuator network that
learns corrective torques without requiring torque su-
from harmonic reduction.
We first evaluate the modeling accuracy of these approaches
by reporting the mean-square joint position error on both the
training data and on an unseen test trajectory (see Figure 6).
Our results show that the UAN method achieves the best
windows of simulator rollouts for a single arm joint
