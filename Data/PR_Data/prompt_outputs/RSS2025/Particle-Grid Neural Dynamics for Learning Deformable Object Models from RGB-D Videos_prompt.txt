=== PDF文件: Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos.pdf ===
=== 时间: 2025-07-22 16:03:18.585761 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Particle-Grid Neural Dynamics for Learning
Deformable Object Models from RGB-D Videos
Kaifeng Zhang1
Baoyu Li2
Kris Hauser2
Yunzhu Li1
1Columbia University
2University of Illinois Urbana-Champaign
Learning-Based Deformable Models with Appearance, Geometry, and Dynamics
Real-World Interaction Data with Dense Tracking
Training
Inference
Particle-Grid Neural Dynamics
Dual-Arm Cloth Lifting
Rope Manipulation
Plush Toy Relocating
Dual-Arm Box Closing
Dense Particles
Fig. 1: Modeling deformable objects from RGB-D videos presents a significant challenge due to occlusions and complex physical interactions.
Our Particle-Grid Neural Dynamics framework learns the behavior of deformable objects directly from real-world observations. To train the
predicts the motion of dense particles under robot-object interactions. We demonstrate the ability of Particle-Grid Neural Dynamics to model
complex interactions across a diverse set of objects, including ropes, cloth, plush toy, box, and bread.
AbstractModeling the dynamics of deformable objects is
challenging due to their diverse physical properties and the
difficulty of estimating states from limited visual information.
We address these challenges with a neural dynamics framework
that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and
motion information while predicting dense particle movements,
enabling the modeling of objects with varied shapes and materials.
Particles represent object shapes, while the spatial grid discretizes
the 3D space to ensure spatial continuity and enhance learning
efficiency. Coupled with Gaussian Splattings for visual rendering,
our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos.
Through experiments, we demonstrate that our model learns
the dynamics of diverse objectssuch as ropes, cloths, stuffed
robot-object interactions, while also generalizing at the category
level to unseen instances. Our approach outperforms state-of-the-
art learning-based and physics-based simulators, particularly in
scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The
project page is available at
I. INTRODUCTION
Learning predictive models is crucial for a wide range of
robotic tasks. In deformable object manipulation, an accu-
rate predictive object dynamics model enables model-based
that are both accurate and generalizable remains a significant
challenge. For example, physics-based simulators [12, 31] often
struggle to generalize to the real world due to the inherent sim-
to-real gap and the difficulties of system identification and state
estimation. Meanwhile, video-based predictive models [9, 56]
are computationally expensive, lack 3D spatial understanding,
and are highly sensitive to viewpoint and appearance changes.
Recent advancements in deformable object modeling have
focused on learning dynamics models for particles that encode
3D geometric information directly obtained from RGB-D
cameras. The current state-of-the-art methods in this area
utilize Graph Neural Networks (GNNs), where particle sets
are modeled as spatial adjacency graphs, and message passing
is performed to aggregate information and generate motion
predictions [48, 58]. However, these methods face significant
sensitive to the spatial distribution and connectivity of the
graph nodes, making them vulnerable to partial observations.
capture global information, while excessive steps can lead to
overly smoothed predictions. As a result, these approaches are
often limited to relatively simple simulated environments or
real-world objects with easily perceivable geometries, where
graph construction is straightforward with nearest neighbors.
To address these limitations, we introduce a novel class of
dynamic models called particle-grid neural dynamics. This
model leverages a hybrid representation that combines object
particles with fixed spatial grids. It takes the kinematic states
of the particles as input and predicts a spatial velocity field at
fixed grid points. A global point cloud encoder is employed
to capture comprehensive information from the particle set,
effectively overcoming the connectivity challenges commonly
faced by GNNs and enhancing the models robustness to
incomplete observations. Additionally, this encoder allows
for the processing of denser particle sets compared to sparse
graph nodes, thereby enriching the model with finer geometric
details. The grid representation further acts as a regularizer,
ensuring spatial continuity in velocity predictions while also
reducing computational costs when handling large numbers
of particles. By combining object particles with spatial grids,
our framework parameterizes dynamics in both Lagrangian
and Eulerian coordinates, drawing an analogy to physics-based
deformable object simulation methods [44, 12]. By utilizing
neural networks as message integrators to bridge particle and
grid representations, our model handles a diverse range of
materials without requiring material-specific physical laws, and
can operate effectively with only partial observations as input.
robots interacting with objects through diverse and non-task-
specific behaviors. To extract object particles from raw record-
This framework utilizes foundational vision models [18, 39, 15]
to estimate segmentation masks and pixel tracks, which are
then fused in 3D to generate persistent, dense 3D tracks that
serve as training data for the neural dynamics model.
In our experiments, we demonstrate that the proposed
particle-grid neural dynamics model can effectively simulate
a diverse range of challenging deformable objects, including
compare our model with existing state-of-the-art approaches
and show that it consistently outperforms the baselines in pre-
diction accuracy. Additionally, we highlight that our dynamics
model seamlessly integrates with 3D appearance reconstruc-
tion methods such as 3D Gaussian Splatting (3DGS) ,
enabling the fusion of both techniques to generate highly
realistic renderings of predictions. This improved accuracy
in dynamics prediction directly translates to higher-fidelity
renderings compared to existing methods. To further validate
its robustness, we conduct experiments that demonstrate the
models effectiveness under sparse-view conditions. Finally,
we showcase the models applicability to deformable object
manipulation by incorporating it into a Model Predictive
Control (MPC) framework, underscoring its potential for real-
world robotic applications.
II. RELATED WORK
A. Physics-Based Deformable Modeling.
The simulation of deformable objects is essential for
advancing both the modeling and robotic manipulation of
analytical physics-based approaches, including, but not limited
(FEM) [10, 33, 7], the Discrete Elastic Rods (DER) [5, 6], the
Position-Based Dynamics (PBD) [31, 21, 23], and the Material
Point Method (MPM) [44, 12, 13, 29]. However, real-world
analytical deformable modeling remains challenging due to
the difficulty in property identification and state estimation.
While our method uses a hybrid particle-grid representation
similar to MPM, we leverage neural networks as message
integrators and reduce dependence on full-state information
as input. This enhances robustness to partial observations and
enables the handling of a wide variety of deformable objects
without requiring predefined material-specific constitutive laws.
[16, 28, 8] has enabled the fusion of Gaussian Splatting
reconstructions with physics-based models to simulate the
dynamics of deformable objects [53, 37, 60, 59, 1]. In our
particle-grid neural dynamics model, the particle motion
predictions can also be integrated with Gaussian Splatting
rendering of deformable objects, purely learned from real data.
B. Learning-Based Deformable Modeling.
Learning-based dynamics models, which use deep neural
networks to model the future evolution of dynamical sys-
tasks [11, 55, 17, 61, 54, 4]. Among these learning-based
great promise, as they explicitly model spatial relational
biases within complex physical systems [19, 34, 40, 3, 48].
Previous research has investigated the use of GBND across
a range of material types, such as rigid bodies [14, 25, 2],
plasticine [42, 43], fabrics [22, 26, 35, 27], ropes ,
and granular piles [47, 41]. Beyond simulation and single-
material scenarios, GBND has also demonstrated flexibility
and generalization in modeling diverse materials using a unified
framework [57, 58]. However, these approaches often operate
on spatially sparse graph vertices, rely on expert knowledge
to determine graph connectivity, and do not consider partial
observations. For learning dense particle dynamics, Whitney
et al.  propose transformer-based backbones for higher
computational efficiency. However, their work mainly focuses
on modeling rigid objects for grasping and pushing tasks. In
using a hybrid particle-grid neural dynamics framework,
achieving dense particle prediction while remaining robust
Particle Features
Grid Velocity
Iterative Rollout
Grid Positions
Particle Velocity
Velocity
RGB-D Observation
Particles
Interpolation
Initial Frame
Action Sequence
(a) Particle-Grid Neural Dynamics Model
(b) 3D Action-Conditioned Video Prediction
(c) Model-Based Planning
Fig. 2: Overview of proposed framework: Particle-Grid Neural Dynamics. (a) A diagram of our dynamics model. Given particle positions
Xt and velocities Vt fused from multi-view depth images as input, our model predicts dense per-particle motion by first using a point
encoder to extract particle features and predict the velocity field, which is then transformed into a grid representation to estimate the velocity
distribution in 3D space. The model updates particle positions Xtt with the predicted velocities Vtt to perform iterative rollouts.
(b) Our framework enables 3D action-conditioned video prediction by reconstructing objects with 3D Gaussian Splatting and interpolating the
6DoF transformation of Gaussian kernels using the predicted particle motions. (c) The model can be integrated into model-based planning
frameworks to generate plausible motions for manipulating deformable objects.
to incomplete observations and flexible for diverse types of
deformable objects with distinct physical properties.
III. METHODS
Our Particle-Grid Neural Dynamics framework models the
dynamics of objects represented by a set of particles. The
core of this framework is a dynamics function that predicts the
future motion of each particle based on its current and historical
A detailed description of the model is provided in Section
III-A and III-B. We introduce the data collection pipeline and
the models training method in Section III-C. Additionally, we
explore the integration of Particle-Grid Neural Dynamics with
3D Gaussian Splatting for 3D video rendering, as discussed in
Section III-D. The application of this model within a Model
Predictive Control (MPC) framework is covered in Section
III-E. An overview of our method is also provided in Fig. 2.
A. Particle-Grid Neural Dynamics
1) State Representation: We intend to learn a particle-
based dynamics model, which represents the target object as
a collection of particles Xt R3n, where n is the number
of particles, and t is the time. The velocity of the particles,
Vt R3n is defined as the time derivative of X at time t.
2) Action Representation: The action At represents the
external effects caused to the object by the robot at time t. We
define At as
At  (y, Tt, Tt, ot),
where y is the action type label, Tt is the end-effector pose,
Tt its time derivative, and ot the gripper open distance. In our
experiments y is a binary label indicating whether the action
is a grasped or nonprehensile interaction; the implementation
details of these two types are given in Sec. III-B4.
3) Dynamics Function: We consider the change of state
caused by the state itself (e.g., objects falling due to gravity)
and the robots actions (e.g., robots grasping an object thus
making it move) in the objects dynamic functions:
Vtt  f(Xt, Vt, At),
where f is the dynamics function predicting the state evolution.
Since the particle representation typically cannot capture the
full state information (e.g., internal stress or contact mode with
other objects), a common practice is to incorporate historical
states as additional inputs for making predictions:
Vtt  f(Xtht:t, Vtht:t, At),
where h is the history window size. In our neural dynamics
parameterized by  and derive the next particle positions by
applying forward Euler time integration:
Xtt  Xt  t  f(Xtht:t, Vtht:t, At).
4) The Particle-Grid Model: Learning the neural dynamics
parameters  with an end-to-end neural network usually leads to
unstable predictions due to accumulative error. Motivated by the
use of hybrid Lagrangian-Eulerian representations in MPM ,
our model also utilizes a hybrid particle-grid representation
to inject inductive bias related to spatial continuity and local
information integration. Specifically, we define a uniformly
distributed grid in the space as
The parameters lx, ly, lz,  control the spatial limits and
resolution of the grid. Empirically, we set lx, ly, lz to 100
or 50 and  to 1 cm or 2 cm to balance computational cost and
resolution. To enforce translational invariance during prediction,
we always translate the positions of the particles and the robot
end effector to the volume defined by Glx,ly,lz,.
The particle-grid dynamics function is defined by the
following components:
f  hG2P  ggrid  f field
f feature
where f feature
is the neural network-based point encoder for
extracting feature from the input particles (Sec. III-B1), f field
the neural network-based function for parameterizing a neural
velocity field based on the extracted features (Sec. III-B2),
and ggrid is a grid velocity editing (GVE) control method
to encode collision surfaces and robot gripper movements
(Sec. III-B4), and hG2P is the grid-to-particle integration
function for calculating particle velocities from grid-based
velocity field (Sec. III-B3).
B. Model Components
1) Point Encoder: The point encoder encodes particle posi-
tions and velocities to per-particle latent features Zt Rdn,
where d is the feature dimension:
Zt  f feature
(Xtht:t, Vtht:t).
We use PointNet  as the encoder for its efficiency and
strong performance in extracting 3D point features. The encoder
captures global information from the set of all particles,
including the objects shape and the historical motion of the
properties and dynamic state. This is essential for handling
incomplete observations, where the feature encoder must extract
occlusion-robust features for subsequent velocity decoding.
2) Neural Velocity Field: In this step, we use a neural
implicit function f field
to predict a spatial velocity grid, at time
function f field
is instantiated as an MLP that takes the grid
locations xg and the corresponding locality-aware feature zg,t
as inputs, then predict per-grid velocity vector vg,t by
((xg), zg,t),
where  is the sinusoidal positional encoding, and the locality-
aware feature zg,t is defined as the average pooling of particle
features within the neighborhood of grid location:
pNr(Xt,xg) zp,t
Nr(Xt, xg)
where Nr(Xt, xg) is the set of indices of particles within Xt
whose positions are within radius r of the grid location xg.
By incorporating the radius hyperparameter r, we can control
the number of particle features a grid point attends to, thus
encouraging the network to predict velocities that are dependent
on local geometry. Empirically, we set r  0.2 m.
3) G2P: After calculating the grids velocities, we transfer
from the Eulerian grid to Lagrangian particles via spline
interpolation. Following MPM, we utilize a continuous B-spline
kernel to transfer grid velocities vg,t to particle velocities vp,t:
where the wpg,t is the value of the B-spline kernel defined on
the grid position xg and evaluated at the particle location xp,t.
It assigns larger weights to closer grid-particle pairs, achieving
smooth spatial interpolation. The predictions V R3n serves
as the final output of the dynamics function f and is used to
perform time integration in Eq. 4.
4) Controlling Deformation: We present two methods for
controlling deformations by interactions with external objects:
Grid Velocity Editing (GVE) and Robot Particles (RP). GVE
is inspired from MPM approaches and we use it for grasped
interactions and object-ground interaction. Simply put, the
operator ggrid changes the velocities on the grid to match
physical constraints. For ground contact, we project velocity
back from the contact surface and incorporate friction terms.
To define the motion of a rigidly grasped point, we calculate
the set of grid points Ggrasp,t within a distance a of the grasp
center point xgrasp,t. For each point g Ggrasp,t, we modify
the velocities as follows:
where t and xgrasp,t are the angular and linear velocities of
the gripper at time t, and xg and vg,t are the position and
velocity of grid point g, respectively.
The Robot Particles method allows us to model nonprehensile
actions for the Box example in which the object is pushed.
that carry gripper action information, and fuse this into the
object point cloud. Specifically, at each step, we augment the
point cloud by
Xt Xrobot,t,
Vt Vrobot,t,
and model the particle-grid dynamics function on the aug-
mented point cloud. This injects action information into
particle features but does not explicitly force particles to
move at a prescribed velocity, thus supporting nonprehensile
manipulation. Our implementation samples points from the
gripper shape and calculates their velocities based on the end-
effector transformation from proprioception.
C. Data Collection and Training
We collect training data through teleoperation and automatic
annotation using foundation models. Specifically, we record
multi-view RGB-D videos of random robot-object interactions.
For each camera view, we apply Segment-Anything [18, 39]
to extract persistent object masks across the video. The
segmented objects are then cropped and tracked over time
using CoTracker , providing 2D trajectories. Using depth
Particle
Particle
Particle
TABLE I: Quantitative Results on Dynamics Prediction. We compare our method with the Material Point Method (MPM) , Graph-Based
Neural Dynamics (GBND) , and a particle-based dynamics model without the grid representation. We report the mean and standard
deviation of the prediction error over a 3-second future horizon. The best results are highlighted in bold and blue.
velocities into 3D, resulting in multi-view fused point clouds
with persistent particle tracking.
With the collected tracking data, we define particle sets
and their trajectories over a look-forward time window as
actions Atht:tKt, where K is the horizon length hy-
perparameter. Empirically, we set h  2 and K  5. Model
training begins from a given point cloud at time t, followed
by iterative dynamics model rollouts for K steps. Since the
dynamics function f is fully differentiable, we optimize the
network parameters  and  using gradient descent. The loss
function is defined as the mean squared error (MSE) between
the predicted and actual particle positions:
Xtit Xtit2
where Xtit is the predicted particle positions at step i.
D. Rendering and Action-Conditioned Video Prediction
Our predictions can be integrated with 3D Gaussian Splatting
(3DGS) to achieve a realistic rendering of the results. The 3DGS
reconstruction of the object is defined as
G  {XGS, C, RGS, S, O},
where XGS, C, RGS, S, and O represent the Gaussian kernels
center location, color, rotation, scale, and opacity, respectively.
To transform Gaussians between frames, we first apply the
dynamics model to the point cloud set X, yielding the next-
frame prediction X. The points X can either be sampled from
XGS or obtained from additional point cloud observations
within the same coordinate frame with XGS. The 6-DoF
motions of the Gaussian kernels are interpolated using Linear
Blend Skinning (LBS) , which updates XGS and RGS by
treating X as control points and interpolating their predicted
motion to generate new Gaussian centers and rotations. We
assume that the color, scale, and opacity of the Gaussian
splatting remain constant.
E. Planning
Our model can be integrated with Model Predictive Control
(MPC) for model-based planning. Given multi-view RGB-
D captures, we obtain object particles through segmentation,
inverse projection into 3D space, and downsampling. The
downsampled particles serve as inputs to the dynamics model
for future prediction. With a specified cost function, the MPC
framework rolls out the dynamics model using sampled actions
and optimizes the total cost. In our experiments, we use the
Chamfer Distance between the predicted state X and the target
state Xtarget as the cost function:
J( X1:N, A1:N)
CD( Xt, Xtarget).
We apply the Model-Predictive Path Integral (MPPI)
trajectory optimization algorithm to minimize the cost and to
synthesize the robots actions. During deployment, we perform
IV. EXPERIMENTS
Our experiments are designed to address the following
How well does the particle-grid model learn the dynamics
of various types of deformable objects?
Does the model perform effectively under limited visual
observation (e.g., sparse views)?
Can we train a unified model for multiple instances within
an object category, and how well does it generalize to
unseen instances?
Can the model improve the performance of 3D action-
conditioned video prediction and model-based planning?
We evaluate our method on a diverse set of challenging
deformable objects, including cloth, rope, plush toys, bags,
previous state-of-the-art approaches in dynamics prediction
accuracy while remaining robust to incomplete camera views.
State and action
Ground Truth
Paper Bag
State and action
Ground Truth
Plush Toy
Fig. 3: Qualitative Comparisons on Dynamics Prediction. Given initial states and actions, we show the prediction results of the GBND
baseline compared to our particle-grid neural dynamics model. The red spheres indicate the position and orientation of robot grippers. We
overlay the predictions with ground truth final state images to highlight the prediction errors. Our models predictions are more aligned with
the ground truth, offering higher-density particle predictions and fewer artifacts compared to the baseline.
level training and its effectiveness in downstream applications,
such as video prediction and planning.
A. Experiment Setup
We conduct data collection and experiments using a bimanual
xArm setup, with each robot arm having seven degrees of
freedom. The objects used in the experiments include rope,
a) Rope: A single robot arm grasps one end of a rope,
while the other end remains free on the table surface. The
robot manipulates the rope in 3D space, generating various
deformation patterns such as bending and dragging.
b) Cloth: Two robot arms grasp a rectangular piece of
cloth and manipulate it in 3D space. The lower half of the
cloth remains in contact with the table, resulting in significant
deformations under lifting, moving, and folding actions.
c) Plush: A single robot arm grasps one limb of a plush
toy while the rest of the toy remains in contact with the
table. The robot manipulates the plush in 3D space, creating
deformation patterns such as limb movements and flipping.
d) Paper Bag: One robot arm grasps and stabilizes
one side of an envelope-shaped mailer bag, while the other
manipulates it in 3D space. The robot performs various actions,
including opening, closing, and rotating the bag.
e) Box: Two robot arms are used to open and close ship-
ping boxes. The grippers remain closed, and the manipulation
is performed in a nonprehensile manner, utilizing the surfaces
of the grippers to push against the movable parts of the box.
f) Bread: Two robot arms are used to tear pieces of bread.
The grippers remain closed, holding the bread in the air. One
robot arm stays still while the other pulls, creating stretching
effects and eventual breakage.
The baseline models in our comparisons are as follows:
MPM-based deformable object simulation [12, 29]: This
baseline assumes a hyperelastic material with an unknown
uniform Youngs modulus and friction coefficient with
the tabletop. Parameter identification is performed via
J -Score  IoU
Particle
Particle
Particle
TABLE II: Quantitative Results on 3D Action-Conditioned Video Prediction. We compared our method on 3D action-conditioned video
prediction quality with MPM , GBND , and particle-based baselines. The J -ScoreIoU and the F-Score measures mask similarities
and the LPIPS score measures appearance-wise similarities between predicted frames and ground truth video recordings. We report the mean
and standard deviation of the prediction error over a 3-second horizon. The best results are highlighted in bold and blue.
Fig. 4: Quantitative Comparisons on Prediction under Partial
Views. We compare our method with the GBND baseline in the cloth
and paper bag categories while varying the number of input camera
views. We report the mean and standard deviation of the dynamics
prediction error. Our method consistently achieves lower error than
the baseline, and its error increase rate as the number of camera views
decreases is also lower.
GBND-Seen
GBND-Unseen
Ours-Seen
Ours-Unseen
Fig. 5: Quantitative Comparisons on Generalization. Our method
is compared with GBND on seen and unseen instances of the rope
and cloth categories. We present the mean and standard deviation of
dynamics prediction error. Our methods prediction error is lower on
both seen and unseen instances compared to the baseline.
gradient descent.
Graph-Based Neural Dynamics (GBND) : This model
represents objects using subsampled sparse vertices, along
with the robot end-effectors, and employs a Graph Neural
Network (GNN) to predict particle motions.
Particle-based dynamics model (ours wo grid): In this
and directly query the velocity field at particle positions
to predict per-particle velocities.
For additional information on the experiment setups and
baseline implementations, please refer to Appendix B.
B. Dynamics Learning and Prediction
We evaluate accuracy using a held-out set of robot-object
interactions. The interaction videos are divided into 3-second
point cloud metrics that compute the distance between predicted
particle positions and ground truth future points. The metrics
include Mean Squared Error (MSE), Chamfer Distance (CD),
and Earth Movers Distance (EMD). All metrics are calculated
with m or m2 as the unit of measurement. For the box category,
we use the Robot Particles control method representation, and
since it is not directly compatible with MPM. Therefore, we
omit MPM from the box comparison.
Quantitative results are shown in Table I, where our method
outperforms all baselines in terms of dynamics rollout accuracy.
We visualize the particle prediction results of our method
alongside the baselines in Fig. 3. Our methods predictions
align more closely with the ground truth images and also exhibit
higher resolution. In contrast, GBND predicts inadequate
particle motions for objects like cloth, plush toy, box, bread,
and it generates artifacts for objects like rope.
C. Sparse-View Dynamics Prediction
We evaluate the performance of our method in sparse-
view scenarios by training the model on partial observation
data. Specifically, during training, we use point cloud from a
randomly sampled number of camera views as model input.
During evaluation, we test the models performance with 1 to
4 camera views.
Ground Truth
rJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw<latexit>t
Ground Truth
Predicted
Predicted
Fig. 6: Qualitative Comparisons on 3D Action-Conditioned Video Prediction. We show our method and the GBND baselines prediction
on two examples of rope and plush toy, compared with the ground truth video. The predictions are based on the 3DGS reconstructions on the
first frame (leftmost image) and the robot action sequence. Differences are highlighted with red dashed boxes. Our method aligns better with
the ground truth while the baseline method predicts visually nonrealistic deformations.
Predicted
tKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjdmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSoSCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn5GNQombTYS
w1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw
<latexit>t
Predicted
Fig. 7: Qualitative Visualizations of Simulation from Scanned Scenes. Our method gives higher-quality video prediction results with
high-resolution Gaussians reconstructed from phone scans. Given the initial reconstruction (green frame), we apply our particle-grid dynamics
model to simulate the segmented object, and visualize from different views.
The results shown in Fig. 4 demonstrate that our model
outperforms the GBND baseline in dynamics prediction accu-
performance drop when decreasing the number of views is also
less significant than the baseline. For the cloth category, the
baseline performance drops significantly when decreasing from
4 camera views to 1 camera view, while our model maintains
a low prediction error.
D. Category-Level Model
multiple instances within the same category by training on
a combined dataset of various object instances. For ropes,
the model is trained on 4 distinct ropes and evaluated on 2
unseen ropes. For cloths, it is trained on 6 cloth instances and
tested on 2 unseen cloths. The 6 rope instances are cotton
the cotton rope and utility rope included in the test set and
unseen during training. For cloths, the 8 instances include
a flannel blanket, cotton towel, microfiber cloth, cotton bed
blanket and foam sheet included in the test set. These instances
are selected to have diverse physical properties and shapes,
(a) Cloth Lifting
(d) Plush Toy Relocating
(c) Rope Manipulation
(b) Box Closing
Fig. 8: Quantitative Comparisons on Planning. For four manipulation taskscloth lifting, box closing, rope manipulation, and plush toy
relocatingwe present the error curve and the final success rate curve with respect to the error threshold for task success. The error is always
measured using the Chamfer Distance between the current and target point clouds. Our method outperforms the GBND baseline in both error
reduction rate and success rate.
(a) Cloth Lifting
Initial State
Planning
Final State
(b) Box Closing
Initial State
Planning
Final State
(c) Rope Manipulation
(d) Plush Toy Relocating
wFtKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjdmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSoSCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn
5GNQombTYSw1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1r
Tj5zAngfP4A47uNAw<latexit>t
E">AB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEFwFtKJvtpl272YTdiVBCf4EXD4p49Sd5894bXPQ1gcDjd
mJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfju5nfuLaiFg94CThfkSHSo
SCUbRSAulsltx5yCrxMtJGXLU6Wv3iBmacQVMkmN6Xpugn5GNQombTYSw1PKBvTIe9aqmjEjZND52Sc6sMSBhrWwrJX
P09kdHImEkU2M6I4sgsezPxP6bYnjZ0IlKXLFovCVBKMyexrMhCaM5QTSyjTwt5K2IhqytBmU7QheMsvr5LWZcWrVqNq
3LtNojAKdwBhfgwTXU4B7q0AQGHJ7hFd6cRfFeXcFq1rTj5zAngfP4A47uNAw<latexit>t
Fig. 9: Qualitative Comparisons on Planning. For each of the four tasks, we visualize a representative planning sequence for both our
method and the GBND baseline. Given similar initial states and the same number of planning steps, our method achieves a lower final error,
as measured by Chamfer Distance (CD), and produces results that are visually more similar to the target.
allowing us to thoroughly evaluate the models generalization.
The results in Fig. 5 show that our model achieves lower
prediction errors than the GBND baseline across both categories
and for both seen and unseen instances. Notably, for the
cloth category, the baseline method exhibits a significantly
performance drop on unseen instances, whereas our method
keeps a relatively low error, demonstrating better generalization
to novel objects at test time.
E. Action-Conditioned Video Prediction
For action-conditioned video prediction, we use the predicted
point cloud trajectories to interpolate Gaussian kernel trans-
formations using LBS . We reconstruct Gaussians from
4 input views using Gaussian Splatting . The Gaussians
are trained with a segmentation mask loss, following previous
works [58, 28]. Videos are rendered with a fixed input camera
pose. The video prediction quality is assessed using mask-based
accuracy), and the image-based metric LPIPS.
The resulting metrics are shown in Table II. Our approach
achieves the best overall performance. For categories with
relatively large objects, for instance boxes and paper bags, we
observe that spiky Gaussian reconstructions often negatively
impact mask prediction performance, especially when objects
undergo significant deformation. The higher mask alignment
scores in GBND and Particle baselines are largely due to
inadequate particle motion predictions.
In Fig. 6, we show the action-conditioned video prediction
results by reconstructing the object using Gaussian Splatting
from 4 views and deforming the Gaussians with predictions
from our dynamics model. Our method achieves higher-quality
rendering and better alignment with the ground truth.
In Fig. 7, we further demonstrate that our method can be
used for simulation based on high-quality phone-scanned GS
reconstructions. The scenes are reconstructed using video scans
of a static workspace. Coupled with our learned particle-grid
neural dynamics, we can generate 3D action-conditioned video
predictions with even greater visual fidelity.
F. Planning
In planning experiments, we evaluate the models ability to
integrate with MPC to generate actions for manipulating objects.
We test on 4 tasks with distinct object types: cloth lifting, box
assessed using error curves and task success rates.
The quantitative results are shown in Fig. 8. Across all four
planning tasks, our method achieves a lower terminal error and
a higher error reduction rate compared to the GBND baseline.
In Fig. 9, we visualize the initial states, intermediate steps, and
final states, comparing them to the target. In all four tasks, our
method produces results that are visually closer to the target.
For example, in the box closing task, our method successfully
lifts both sides of the box, whereas the baseline struggles to
predict the correct actions and often loses contact with the box.
In rope manipulation, our method accurately bends the rope
by pressing downward, while the baseline fails to achieve this
due to lower prediction resolution.
V. LIMITATIONS
While we have demonstrated that Particle-Grid Neural
Dynamics can model diverse types of deformable objects,
the current framework has several limitations: (i) The current
formulation assumes a fixed number of particles during iterative
appearance or disappearance of particles. This limitation
could be addressed by correcting the particle sets with new
observations or modeling the per-frame visibility of particles.
(ii) The model implicitly infers an objects physical properties
from its point cloud and short-term motion history. While
this is sufficient for modeling a single object instance or
multiple instances with distinct shapes and physical properties,
a more systematic approach to modeling physical properties
is needed for interpretable identification and adaptation at
test time. This could involve learning a parameter-conditioned
neural dynamics model . (iii) Training the model and
applying it to video prediction depend on accurate predictions
from computer vision models such as Segment-Anything ,
perception and reconstruction models could negatively impact
our methods performance.
VI. CONCLUSION
In this paper, we introduce Particle-Grid Neural Dynamics,
a novel framework for learning neural dynamics models of de-
formable objects directly from sparse-view RGB-D recordings
of robot-object interactions. By leveraging a hybrid particle-
grid representation to capture object states and robot actions in
3D space, our method outperforms previous graph-based neural
dynamics models in terms of prediction accuracy and modeling
density. This advancement enables the modeling of a wide range
of challenging deformable objects. Additionally, integration
with 3D Gaussian Splatting facilitates 3D action-conditioned
video prediction, simultaneously capturing both object geometry
and appearance changes, thereby creating a learning-based
digital twin of real-world objects. We further demonstrate
that our model can be applied to various deformable object
manipulation tasks, achieving improvements in both task
execution efficiency and success rate.
ACKNOWLEDGMENT
We thank the members of the RoboPIL lab at Columbia
University for their helpful discussions. This work is partially
supported by the Toyota Research Institute (TRI), the Sony
Group Corporation, Google, and Dalus AI. This article solely
reflects the opinions and conclusions of its authors and should
not be interpreted as necessarily representing the official
REFERENCES
Jad Abou-Chakra, Krishan Rana, Feras Dayoub, and Niko
Suenderhauf. Physically embodied gaussian splatting: A
realtime correctable world model for robotics. In 8th
Annual Conference on Robot Learning, 2024. 2
Bo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston
tactile-informed dynamics models for dense packing.
Kelsey R Allen, Tatiana Lopez Guevara, Yulia Rubanova,
can learn discontinuous, rigid contact dynamics. In Karen
of The 6th Conference on Robot Learning, volume 205
of Proceedings of Machine Learning Research, pages
Dominik Bauer, Zhenjia Xu, and Shuran Song. Doughnet:
A visual predictive model for topological manipulation of
deformable objects. European Conference on Computer
Vision (ECCV), 2024. 2
Mikls Bergou, Max Wardetzky, Stephen Robinson,
Basile Audoly, and Eitan Grinspun.
Discrete elastic
rods. In ACM SIGGRAPH 2008 Papers, SIGGRAPH 08,
New York, NY, USA, 2008. Association for Computing
Machinery. ISBN 9781450301121. doi: 10.11451399504.
Yizhou Chen, Yiting Zhang, Zachary Brei, Tiancheng
Differentiable discrete elastic rods for real-time modeling
of deformable linear objects, 2024. 2
Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew
Differentiable projective dynamics. ACM Trans. Graph.,
Bardienus P Duisterhof, Zhao Mandi, Yunchao Yao, Jia-
Wei Liu, Mike Zheng Shou, Shuran Song, and Jeffrey
Ichnowski. Md-splatting: Learning metric deformation
from 4d gaussians in highly deformable scenes. arXiv
preprint arXiv:2312.00583, 2023. 2
Chelsea Finn and Sergey Levine. Deep visual foresight
for planning robot motion. In 2017 IEEE International
Conference on Robotics and Automation (ICRA), pages
Marcos Garca, Csar Mendoza, Luis Pastor, and Angel
Rodrguez. Optimized linear fem for modeling deformable
objects.
Comput. Animat. Virtual Worlds, 17(34):
Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya
Visuospatial
foresight for multi-step, multi-task fabric manipulation.
arXiv preprint arXiv:2003.09044, 2020. 2
Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin
least squares material point method with displacement
discontinuity and two-way rigid body coupling. ACM
Transactions on Graphics (TOG), 37(4):114, 2018. 1, 2,
Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun,
Nathan Carr, Jonathan Ragan-Kelley, and Frdo Durand.
Isabella Huang, Yashraj Narang, Ruzena Bajcsy, Fabio
Grasp planning on 3d fields with graph neural nets, 2023.
Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia
labelling real videos. In Proc. arXiv:2410.11831, 2024.
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics,
Thomas Kipf, Elise Van der Pol, and Max Welling.
Contrastive learning of structured world models. arXiv
preprint arXiv:1911.12247, 2019. 2
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
ment anything. arXiv preprint arXiv:2304.02643, 2023.
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenen-
for manipulating rigid bodies, deformable objects, and
fluids. In ICLR, 2019. 2, 16
Junbang Liang, Ming Lin, and Vladlen Koltun.
ferentiable cloth simulation for inverse problems.
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-
Information Processing Systems, volume 32. Curran
Xingyu Lin, Yufei Wang, Jake Olkin, and David Held.
deformable object manipulation. In Conference on Robot
Xingyu Lin, Yufei Wang, Zixuan Huang, and David
Held. Learning visible connectivity dynamics for cloth
smoothing.
In Conference on Robot Learning, pages
Fei Liu, Entong Su, Jingpei Lu, Mingen Li, and Michael C.
Yip. Robotic manipulation of deformable rope-like objects
using differentiable compliant position-based dynamics.
IEEE Robotics and Automation Letters, 8(7):39643971,
Tiantian Liu, Adam W Bargteil, James F OBrien, and
Ladislav Kavan. Fast simulation of mass-spring systems.
ACM Transactions on Graphics (TOG), 32(6):17, 2013.
Ziang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci,
Li Fei-Fei, Jiajun Wu, and Yunzhu Li.
Model-based
control with sparse neural dynamics. In Thirty-seventh
Conference on Neural Information Processing Systems,
Alberta Longhini, Marco Moletta, Alfredo Reichlin,
Michael C Welle, David Held, Zackory Erickson, and
Danica Kragic. Edo-net: Learning elastic properties of
deformable objects from graph dynamics. In 2023 IEEE
International Conference on Robotics and Automation
(ICRA), pages 38753881. IEEE, 2023. 2
Alberta Longhini, Marcel Bsching, Bardienus Pieter
estimation from RGB supervision for deformable objects.
In 8th Annual Conference on Robot Learning, 2024. 2
Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Deva Ramanan.
Dynamic 3d gaussians: Tracking by
persistent dynamic view synthesis. In 3DV, 2024. 2, 9
Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua B
Learning neural constitutive laws from motion observa-
tions for generalizable pde dynamics. In International
Conference on Machine Learning, pages 2327923300.
Miles Macklin.
framework for gpu simulation and graphics, March 2022.
NVIDIA GPU Technology Conference (GTC). 16
Miles Macklin, Matthias Mller, Nuttapong Chentanez,
and Tae-Yong Kim. Unified particle physics for real-time
applications. ACM Transactions on Graphics (TOG), 33
Carsten Moenning and Neil A. Dodgson. Fast Marching
farthest point sampling. Technical Report UCAM-CL-
April 2003. 16
Jing-Chen Peng, Shaoxiong Yao, and Kris Hauser. 3d
force and contact estimation for a soft-bubble visuotactile
sensor using fem. In IEEE International Conference on
Robotics and Automation (ICRA), 2024. doi: 10.1109
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,
and Peter W Battaglia. Learning mesh-based simulation
with graph networks. arXiv preprint arXiv:2010.03409,
Kavya Puthuveetil, Sasha Wald, Atharva Pusalkar,
Pratyusha Karnati, and Zackory Erickson. Robust body
exposure (robe): A graph-based dynamics modeling
approach to manipulating blankets over people. IEEE
Robotics and Automation Letters, 2023. 2
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J
3d classification and segmentation. In Proceedings of
the IEEE conference on computer vision and pattern
Ri-Zhao Qiu, Ge Yang, Weijia Zeng, and Xiaolong
Wang. Language-driven physics-based scene synthesis
and editing via feature splatting. In European Conference
on Computer Vision (ECCV), 2024. 2
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang
ing Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-
Yuan Wu, Ross Girshick, Piotr Dollr, and Christoph
Feichtenhofer. Sam 2: Segment anything in images and
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang
Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang,
Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam:
Assembling open-world models for diverse visual tasks,
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff,
Rex Ying, Jure Leskovec, and Peter Battaglia. Learning
to simulate complex physics with graph networks. In
International conference on machine learning, pages 8459
Keyi Shen, Jiangwei Yu, Huan Zhang, and Yunzhu Li.
bound and neural dynamics, 2024. 2
Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and
Jiajun Wu. Robocraft: Learning to see, simulate, and
shape elasto-plastic objects with graph networks. arXiv
preprint arXiv:2205.02909, 2022. 2
Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li,
and Jiajun Wu. Robocook: Long-horizon elasto-plastic
object manipulation with diverse tools. arXiv preprint
Deborah Sulsky, Shi-Jian Zhou, and Howard L Schreyer.
Application of a particle-in-cell method to solid mechan-
ics. Computer physics communications, 87(1-2):236252,
Robert W. Sumner, Johannes Schmid, and Mark Pauly.
Embedded deformation for shape manipulation. ACM
Trans. Graph., 26(3):80es, jul 2007. ISSN 0730-0301.
Changhao Wang, Yuyou Zhang, Xiang Zhang, Zheng
Tomizuka. Offline-online learning of deformation model
for cable manipulation with graph neural networks. IEEE
Robotics and Automation Letters, 7(2):55445551, 2022.
Yixuan Wang, Yunzhu Li, Katherine Driggs-Campbell,
Li Fei-Fei, and Jiajun Wu. Dynamic-Resolution Model
Learning for Object Pile Manipulation. In Proceedings
of Robotics: Science and Systems, Daegu, Republic of
William F. Whitney, Tatiana Lopez-Guevara, Tobias Pfaff,
Yulia Rubanova, Thomas Kipf, Kimberly Stachenfeld, and
Kelsey R. Allen. Learning 3d particle-based simulators
from rgb-d videos, 2023. 2
William F. Whitney, Jacob Varley, Deepali Jain, Krzysztof
eling the real world with high-density visual particle
Grady Williams, Andrew Aldrich, and Evangelos A
Theodorou. Model predictive path integral control: From
theory to parallel computation.
Journal of Guidance,
Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and
Pieter Abbeel. Gello: A general, low-cost, and intuitive
teleoperation framework for robot manipulators.
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), 2024. 15
Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu,
Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and
Hengshuang Zhao. Point transformer v3: Simpler faster
stronger. In Proceedings of the IEEECVF Conference on
Computer Vision and Pattern Recognition, pages 4840
Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao
Physics-integrated 3d gaussians for generative dynamics.
arXiv preprint arXiv:2311.12198, 2023. 2, 16
Shangjie Xue, Shuo Cheng, Pujith Kachana, and Danfei
Xu. Neural field dynamics model for granular object piles
manipulation. In Conference on Robot Learning, pages
Mengyuan Yan, Yilin Zhu, Ning Jin, and Jeannette
Bohg. Self-supervised learning of state estimation for
manipulating deformable linear objects. IEEE Robotics
and Automation Letters, 5(2):23722379, 2020.
Mengjiao
Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel.
Learning interactive real-world simulators. arXiv preprint
Kaifeng Zhang, Baoyu Li, Kris Hauser, and Yunzhu
Li. Adaptigraph: Material-adaptive graph-based neural
dynamics for robotic manipulation. In Proceedings of
Mingtong Zhang, Kaifeng Zhang, and Yunzhu Li. Dy-
namic 3d gaussian tracking for graph-based neural dy-
namics modeling. arXiv preprint arXiv:2410.18912, 2024.
Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y.
William T. Freeman. PhysDreamer: Physics-based inter-
action with 3d objects via video generation. arxiv, 2024.
Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yun-
Reconstruction and simulation of elastic ob-
jects with spring-mass 3d gaussians.
arXiv preprint
Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel
Pinto. Dino-
