=== PDF文件: Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL.pdf ===
=== 时间: 2025-07-22 15:47:30.375774 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Solving Multi-Agent Safe Optimal Control with
Distributed Epigraph Form MARL
Songyuan Zhang, Oswin So, Mitchell Black, Zachary Serlin and Chuchu Fan
Department of Aeronautics and Astronautics, MIT, Cambridge, Massachusetts, USA
MIT Lincoln Laboratory, Lexington, Massachusetts, USA
Fig. 1: Two agents using Def-MARL to safely and collaboratively inspect a moving target. We propose a novel safe
MARL algorithm, Def-MARL, that solves the multi-agent safe optimal control problem. Def-MARL translates the original
problem to its epigraph form to avoid unstable training and extends the epigraph form to the CTDE paradigm for distributed
execution. (a): Long exposure photo of the trajectories of the drones. The trajectory of the target is shown in green and that
of the agents is shown in blue. (b)-(i): Snapshots of the agents policy. Using Def-MARL, the agents learn to collaborate to
maintain visual contact with the target at all times, with each agent being responsible only when the target is on their side.
AbstractTasks for multi-robot systems often require the
robots to collaborate and complete a team goal while maintaining
safety. This problem is usually formalized as a constrained
Markov decision process (CMDP), which targets minimizing
a global cost and bringing the mean of constraint violation
below a user-defined threshold. Inspired by real-world robotic
many safe multi-agent reinforcement learning (MARL) algo-
rithms have been proposed to solve CMDPs, these algorithms
suffer from unstable training in this setting. To tackle this, we
use the epigraph form for constrained optimization to improve
training stability and prove that the centralized epigraph form
problem can be solved in a distributed fashion by each agent.
This results in a novel centralized training distributed execution
MARL algorithm named Def-MARL. Simulation experiments
on 8 different tasks across 2 different simulators show that
Def-MARL achieves the best overall performance, satisfies safety
experiments on Crazyflie quadcopters demonstrate the ability
of Def-MARL to safely coordinate agents to complete complex
collaborative tasks compared to other methods.1
I. INTRODUCTION
Multi-agent systems (MAS) play an integral role in our
aspirations for a more convenient future with examples such
Equal contribution.
1Project website:
DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.
as autonomous warehouse operations , large-scale au-
tonomous package delivery , and traffic routing .
These tasks often require designing distributed policies for
agents to complete a team goal collaboratively while main-
taining safety. To construct distributed policies, multi-agent
reinforcement learning (MARL) under the centralized train-
ing distributed execution (CTDE) paradigm [84, 25] has
emerged as an attractive method. To incorporate the safety
design their objective function to incorporate soft constraints
[69, 58, 79, 74, 54, 57], or model the problem using the
constrained Markov decision process (CMDP) , which
asks for the mean constraint violation to stay below a user-
defined threshold [32, 38, 18, 41, 26, 90]. However, real-world
robotic applications always require zero constraint violation.
While this can be addressed by setting the constraint violation
threshold to zero in the CMDP, in this setting the popular
Lagrangian methods experience training instabilities which
result in sharp drops in performance during training, and non-
convergence or convergence to poor policies [66, 35, 24, 36].
These concerns have been identified recently, resulting in a
series of works that enforce hard constraints [82, 89, 66, 24,
86] using techniques inspired by Hamilton-Jacobi reachability
[71, 44, 49, 46, 5] in deep reinforcement learning (RL) for
the single-agent case and have been shown to improve safety
compared to other safe RL approaches significantly. However,
to the best of our knowledge, theories and algorithms for safe
RL are still lacking for the multi-agent scenario, especially
when policies are executed in a distributed manner. While
single-agent RL methods can be directly applied to the MARL
setting by treating the MAS as a centralized single agent, the
joint action space grows exponentially with the number of
with a large number of agents [33, 69, 23].
To tackle the problem of zero constraint violation in multi-
agent scenarios with distributed policies2 while achieving high
collaborative performance, we propose Distributed epigraph
form MARL (Def-MARL) (Fig. 1). Instead of considering
the CMDP setting, Def-MARL directly tackles the multi-agent
safe optimal control problem (MASOCP), whose solution
satisfies zero constraint violation. To solve the MASOCP,
Def-MARL uses the epigraph form technique , which
has previously been shown to yield better policies compared
to Lagrangian methods in the single-agent setting . To
adapt to the multi-agent setting we consider in this work, we
prove that the centralized epigraph form of MASOCP can be
solved in a distributed fashion by each agent. Using this result,
Def-MARL falls under the CTDE paradigm.
We validate Def-MARL using 8 different tasks from 2
different simulators, multi-particle environments (MPE)
and Safe Multi-agent MuJoCo , with varying numbers
of agents, and compare its performance with existing safe
MARL algorithms using the penalty and Lagrangian methods.
The results suggest that Def-MARL achieves the best perfor-
mance while satisfying safety: it is as safe as conservative
baselines that achieve high safety but sacrifice performance,
while matching the performance of unsafe baselines that
sacrifice safety for high performance. In addition, while the
baseline methods require different choices of hyperparam-
eters to perform well in different environments and suffer
from unstable training because of zero constraint violation
hyperparameters across all environments, indicative of the
algorithms robustness to environmental changes.
We also perform real-world hardware experiments using
the Crazyflie (CF) drones  on two complex collaborative
tasks and compare Def-MARL with both centralized and
decentralized model predictive control (MPC) methods .
The results indicate that Def-MARL finishes the tasks with
100 safety rates and success rates, while the MPC methods
get stuck in local minima or have unsafe behaviors.
To summarize, our contributions are presented below:
Drawing on prior work that addresses the training in-
stability of Lagrangian methods in the zero-constraint
violation setting, we extend the epigraph form method
from single-agent RL to MARL, improving upon the
training instability of existing MARL algorithms.
2In this paper, the policies are distributed if each agent makes decisions
using local informationsensor data and information received via message
passing with other agents , although this setting is sometimes called
decentralized in MARL .
We present theoretical results showing that the outer prob-
lem of the epigraph form can be decomposed and solved
in a distributed manner during online execution. This
allows Def-MARL to fall under the CTDE paradigm.
We illustrate through extensive simulations that, without
any hyperparameter tuning, Def-MARL achieves stable
training and is as safe as the most conservative baseline
while simultaneously being as performant as the most
aggressive baseline across all environments.
We demonstrate on Crazyflie drones in hardware that
Def-MARL can safely coordinate agents to complete
complex collaborative tasks. Def-MARL performs the
task better than centralizeddecentralized MPC methods
and does not get stuck in suboptimal local minima or
exhibit unsafe behaviors.
II. RELATED WORK
Unconstrained MARL. Early works that approach the
problem of safety for MARL focus on navigation problems
and collision avoidance [14, 13, 21, 64], where safety is
achieved by a sparse collision penalty , or a shaped
reward penalizing getting close to obstacles and neighboring
agents [14, 13, 21, 64]. However, adding a penalty to the
reward function changes the original objective function, so the
resulting policy may not be optimal for the original constraint
optimization problem. In addition, the satisfaction of collision
avoidance constraints is not necessarily guaranteed by even
the optimal policy [47, 21, 39].
Shielding for Safe MARL. One popular method that
provides safety to learning-based methods is using shielding
or a safety filter . Here, an unconstrained learning method
is paired with a shield or safety filter using techniques such
as predictive safety filters [88, 50], control barrier functions
[11, 55], or automata [19, 77, 48, 7]. Such shields are often
constructed before the learning begins and are used to modify
either the feasible actions or the output of the learned policy
to maintain safety. One benefit is that safety can be guar-
anteed during both training and deployment since the shield
is constructed before training. However, they require domain
expertise to build a valid shield, which can be challenging in
the single-agent setting and even more difficult for MAS .
Other methods can automatically synthesize shields but face
scalability challenges [48, 20]. Another drawback is that the
policy after shielding might not consider the same objective
as the original policy and may result in noncollaborative
behaviors or deadlocks [56, 85, 87].
Constrained MARL. In contrast to unconstrained MARL
to an unconstrained problem, constrained MARL methods
explicitly solve the CMDP problem. For the single-agent
[8, 70, 35, 36], and trust-region-based approaches [1, 35].
These methods provide guarantees either in the form of asymp-
totic convergence guarantees to the optimal (safe) solution
[8, 70] using stochastic approximation theory [59, 9], or
recursive feasibility of intermediate policies [1, 60] using
ideas from trust region optimization . The survey
provides an overview of the different methods of solving
safety-constrained single-agent RL. In multi-agent cases, how-
stationary behavior of other agents, and similar approaches
have been presented only recently [32, 38, 18, 41, 26, 90, 15].
for them to handle hard constraints, and results in poor
performance with zero constraint violation threshold .
Model predictive control.
Distributed MPC methods
have been proposed to handle MAS, incorporating multi-agent
path planning, machine learning, and distributed optimization
[75, 72, 92, 22, 42, 16, 52]. However, the solution quality of
nonlinear optimizers used to solve MPC when the objective
function and constraints are nonlinear highly depends on
the initial guess [73, 29]. Moreover, the real-time nonlinear
optimizers typically require access to (accurate) first and
second-order derivatives [53, 29], which present challenges
when trying to solve tasks that have non-differentiable or
discontinuous cost functions and constraints such as the ones
we consider in this work.
III. PROBLEM SETTING AND PRELIMINARIES
A. Multi-agent safe optimal control problem
We consider the multi-agent safe optimal control problem
(MASOCP) as defined below. Consider a homogeneous MAS
with N agents. At time step k, the global state and control
input are given by xk X Rn and uk U Rm,
respectively. The global control vector is defined by concate-
nation uk : [uk
N], where uk
i Ui is the control input
of agent i. We consider the general nonlinear discrete-time
dynamics for the MAS:
xk1  f(xk, uk),
where f : X  U X is the global dynamics function. We
consider the partially observable setting, where each agent has
a limited communication radius R > 0 and can only commu-
nicate with other agents or observe the environment within its
communication region. Denote ok
i  Oi(xk) O Rno as
the vector of the information observed by agent i at the time
step k, where Oi : X O is an encoding function of the
information shared from neighbors of agent i and the observed
data of the environment. We allow multi-hop communication
between agents, so an agent may communicate with another
agent outside its communication region if a communication
path exists between them.
Let the avoidunsafe set of agent i be Ai : {oi O :
hi(oi) > 0}, for some function hi : O R. The global
avoid set is then defined as A : {x X : h(x) > 0},
where h(x)  maxi hi(oi)  maxi hi(Oi(x)). In other words,
X U R describing the task for the agents to accomplish3,
3The cost function l is not the cost in CMDP. Rather, it corresponds to the
negation of the reward in CMDP.
we aim to find distributed control policies i : O Ui such
that starting from any given initial states x0 A, the policies
keep the agents outside the avoid set A and minimize the
infinite horizon cost. In other words, denoting  : X U
as the joint policy such that (x)  [1(o1); . . . ; N(oN)]
[1(O1(x)); . . . ; N(ON(x))], we aim to solve the following
infinite-horizon MASOCP for a given initial state x0:
l(xk, (xk))
hi(Oi(xk)) 0,
xk1  f(xk, (xk)),
Note that the safety constraint (2b) differs from the average
constraints considered in CMDPs . Consequently, instead
of allowing safety violations to occur as long as the mean
constraint violation is below a threshold, this formulation
disallows any constraint violation. From hereon, we omit the
dynamics constraint (2c) for conciseness.
B. Epigraph form
Existing methods are unable to solve (2) well. This has been
observed previously in the single-agent setting [82, 89, 66, 24].
We show later that the poor performance of methods that
tackle the CMDP setting to the constrained problem (2)
also translates to the multi-agent setting, as we observe a
similar phenomenon in our experiments (Section V). Namely,
although unconstrained MARL can be used to solve (2)
using the penalty method , this does not perform well
in practice, where a small penalty results in policies that
violate constraints, and a large penalty results in higher total
costs. The Lagrangian method  can solve the problem
poor performance in practice when the constraint violation
threshold is zero [66, 24]. In this section, we introduce a new
method of solving (2) that can mitigate the above problems
by extending prior work  to the multi-agent setting.
Given a constrained optimization problem with objective
function J (e.g., J  P
k0 l(xk, (xk)) as in (2a)), and
constraints h (e.g., (2b)):
its epigraph form  is given as
where z R is an auxiliary variable. In other words, we add
a constraint to enforce z as an upper bound of the cost J(),
then minimize z. The solution to (4) is identical to the original
problem (3) . Furthermore, (4) is equivalent  to
Jz(, z) : max{h(), J() z} 0
As a result, the original constrained problem (3) is decom-
posed into the following two subproblems:
1) An unconstrained inner problem (5b), where, given an
arbitrary desired cost upper bound z, we find  such that
Jz(, z) is minimized, i.e., best satisfies the constraints
h 0 and J z.
2) A 1-dimensional constrained outer problem (5a) over z,
which finds the smallest cost upper bound z such that z
is indeed a cost upper bound (J z) and the constraints
of the original problem h() 0 holds.
Comparison with the Lagrangian method. Another pop-
ular way to solve MASOCP (2) is the Lagrangian method .
the zero constraint violation [66, 35] setting. More specifically,
this refers to the case with constraints P
k0 c(xk) 0 for
convert our problem setting (3) to the zero constraint violation
setting by taking c(x) : max{0, h(x)}. Then, (3) reads as
max{0, h(xk)} 0.
The Lagrangian form of (6) is then
max{h(xk), 0},
where  is the Lagrangian multiplier and is updated with gradi-
ent ascent. However,
k0 max{h(xk), 0}
J(, ) scales linearly in  when h(xk) > 0 for some k,
a large value of  causes a large gradient w.r.t x, and makes
the training unstable. Note that for the epigraph form, since z
does not multiply with the cost function J but is added to J
in (5b), the gradient
Jz(, z) does not scale with the value
of z resulting in more stable training. We validate this in our
experiments (Section V).
IV. DISTRIBUTED EPIGRAPH FORM MULTI-AGENT
REINFORCEMENT LEARNING
In this section, we propose the Distributed epigraph form
MARL (Def-MARL) algorithm to solve MASOCP (2) using
MARL. First, we transfer MASOCP (2) to its epigraph form
with an auxiliary variable z to model the desired cost upper
bound. The epigraph form includes an inner problem and
an outer problem. For distributed execution, we provide a
theoretical result that the outer problem can be solved distribu-
tively by each agent. This allows Def-MARL to fit the CTDE
are trained together given the desired cost upper bound z,
and in distributed execution, the agents distributively find the
smallest cost upper bound z that ensures safety.
A. Epigraph form for MASOCP
To rewrite MASOCP (2) into its epigraph form (5), we first
define the cost-value function V l for a joint policy  using
the standard optimal control notation :
V l(x; ) :
l(xk, (xk)).
We also define the constraint-value function V h as the maxi-
mum constraint violation:
V h(x; ) : max
k h(xk)  max
i )  max
functions V h
i ; )  maxk hi(ok
i ). Each V h
uses only
the agents local observation and thus is distributed. We now
introduce the auxiliary variable z for the desired upper bound
of V l, allowing us to restate (2) concisely as
V l(x0; )
The epigraph form (5) of (10) then takes the form
:V (x0,z;)
By interpreting the left-hand side of (11b) as a new policy
optimization problem, we define the total value function V as
the objective function to (11b). This can be simplified as
V (x, z; )  max{max
Vi(x, z; ),
i ; ), V l(x; ) z} as the per-agent total value
function. Using this to rewrite (11) then yields
This decomposes the original problem (2) into an uncon-
strained inner problem (13b) over policy  and a constrained
outer problem (13a) over z. During offline training, we solve
the inner problem (13b): for parameter z, find the optimal
policy (, z) to minimize V (x0, z; ). Note that the optimal
policy of the inner problem depends on z. During execution,
we solve the outer problem (13a) online to get the mini-
mal z that satisfies constraint (13b). Using this z in the z-
conditioned policy (, z) found in the inner problem gives
us the optimal policy for the overall epigraph form MASOCP
(EF-MASOCP).
To solve the inner problem (13b), the total value function V
must be amenable to dynamic programming, which we show
in the following proposition.
Proposition 1: Dynamic programming can be applied to
EF-MASOCP (13), resulting in
V (xk, zk; )  max{h(xk), V (xk1, zk1; )},
zk1  zk l(xk, (xk)).
Centralized cost-value function
Centralized training
Distributed constraint-value
functions ()
( 0 Safe)
Total value function
PPO policy loss
Data collection
Distributed execution
Fig. 2: Def-MARL algorithm. Randomly sampled initial states and z0 are used to collect trajectories in x and z using the
current policy . In the centralized training (orange blocks), distributed constraint-value functions V h
and policies i and a
centralized cost-value function V l are jointly trained. During distributed execution (green blocks), the distributed V h
i are used
to solve the outer problem (15b) to compute the optimal zi, which is used in each agents z-conditioned policy.
The proof of Proposition 1 is provided in Appendix A follow-
ing the proof of the single-agent version . In other words,
for a given cost upper bound zk, the value function V at the
current state xk can be computed using the value function
at the next state xk1 but with a different cost upper bound
zk1  zkl(xk, (xk)) which itself is a function of zk. This
can be interpreted as a dynamics for the cost upper bound z.
a cost l(xk, (xk)), then the upper bound at the next time step
should be smaller by l(xk, (xk)) so that the total cost from
xk remains upper bounded by zk. Additional discussion on
Proposition 1 is provided in Appendix C.
Remark 1 (Effect of z on the learned policy): From
for a fixed x and , observe that for z large enough (i.e.,
V l(x; )z is small enough), we have V (x, z; )  V h(x; ).
taking a gradient step on V h(x; ), which reduces the con-
straint violation. Otherwise, V (x, z; )  V l(x; )z. Taking
gradient steps on V (x, z; ) equals taking gradient steps on
V l(x; ), which reduces the total cost.
B. Solving the inner problem using MARL
Following So and Fan , we solve the inner problem
using centralized training with proximal policy optimization
(PPO) . We use a graph neural network (GNN) backbone
for the z-conditioned policy (oi, z), cost-value function
(x, z), and the constraint-value function V h
(oi, z) with
network (NN) structures can be used as well. The implemen-
tation details are introduced in Appendix E.
Policy and value function updates.
During centralized
(13b), i.e., for a randomly sampled z, find policy (, z) that
minimizes the total value function V (x0, z; ). We follow
MAPPO  to train the NNs. Specifically, when calculating
the advantage with the generated advantage estimation (GAE)
for the i-th agent, Ai , instead of using the cost func-
tion V l , we apply the decomposed total value function
(oi, z), V l
(x, z) z}. We perform trajectory rollouts
following the dynamics for x (1) and z (14) using the learned
collecting the trajectories, we train the cost-value function V l
and the constraint-value function V h
via regression and use
the PPO policy loss to update the z-conditioned policy .
C. Solving the outer problem during distributed execution
During execution, we solve the outer problem of EF-
MASOCP (13) online. However, the outer problem is still cen-
tralized because the constraint (13b) requires the centralized
cost-value function V l. To achieve a distributed policy during
Theorem 1: Assume no two unique values of z achieves the
same unique cost. Then, the outer problem of EF-MASOCP
(5a) is equivalent to the following:
The proof is provided in Appendix B. Theorem 1 enables
computing z without the use of the centralized V l during
execution. Specifically, each agent i solves the local problem
(15b) for zi, which is a 1-dimensional optimization problem
and can be efficiently solved using root-finding methods (e.g.,
) as in , then communicates zi among the other
agents to obtain the maximum (15a). One challenge is that
this maximum may not be computable if the agents are not
connected. However, in our problem setting, if one agent is not
connected agents. Therefore, it would not contribute to the
Goallandmark
Obstacle
Target positions
Agent-agentobstacle
Agent-goal
FORMATION
CORRIDOR
CONNECTSPREAD
SAFE HALFCHEETAH(2X3)
SAFE COUPLED HALFCHEETAH(4X3)
Fig. 3: Simulation Environments. Visualization of the (top) modified MPE  and (bottom) Safe Multi-agent MuJoCo
environments we consider.
constraint-value function V h of other agents. As a result, it is
sufficient for only the connected agents to communicate their
zi. Furthermore, we observe experimentally that the agents
can achieve low cost while maintaining safety even if zi is
not communicated (see Section V-C). Thus, we do not include
zi communication for our method. The overall framework of
Def-MARL is provided in Fig. 2.
Dealing with estimation errors. Since there may be errors
estimating V h using NN, we can reduce the resulting safety
violation by modifying h to add a buffer region. Specifically,
for a constant  > 0, we modify h such that h  when
the constraints are violated and h  otherwise. We then
modify (15b) to V h
(oi, zi) , where  [0, ] is a
hyperparameter (where we want   to emphasize more
on safety). This makes z more robust to estimation errors of
V h. We study the importance of  in Section V-C.
V. SIMULATION EXPERIMENTS
In this section, we design simulation experiments to answer
the following research questions:
(Q1): Does Def-MARL satisfy the safety constraints and
achieve low cost with constant hyperparameters across
all environments?
(Q2): Can Def-MARL achieve the global optimum of the
original constrained optimization problem?
(Q3): How stable is the training of Def-MARL?
(Q4): How well does Def-MARL scale to larger MAS?
(Q5): Does the learned policy from Def-MARL generalize
to larger MAS?
Details for the implementation, environments, and hyperpa-
rameters are provided in Appendix E.
A. Setup
Environments.
We evaluate Def-MARL in two sets of
simulation environments: modified Multi-agent Particle Envi-
ronments (MPE) , and Safe Multi-agent MuJoCo envi-
ronments  (see Fig. 3). In MPE, the agents are assumed
to have double integrator dynamics with bounded continuous
action spaces [1, 1]2. We provide the full details of all tasks
in Appendix E. To increase the difficulty of the tasks, we add
3 static obstacles to these environments. For Safe Multi-agent
MuJoCo environments, we consider SAFE HALFCHEETAH
2X3 and SAFE COUPLED HALFCHEETAH 4X3. The agents
must collaborate to make the cheetah run as fast as possible
without colliding with a moving wall in front. To design the
constraint function h, we let   0.5 in all our experiments
and   0.4 when solving the outer problem.
Baselines.
We compare our algorithm with the state-of-
the-art (SOTA) MARL algorithm InforMARL  with a
constraint-penalized cost l(x, u)  l(x, u)   max{h(x), 0},
where  {0.02, 0.1, 0.5} is a penalty parameter, and denote
this baseline as Penalty(). We also consider the SOTA
safe MARL algorithm MAPPO-Lagrangian [30, 32]4. In ad-
is tiny (107) in the official implementation of MAPPO-
determined by the initial value 0 of . We thus consider
two 0 {1, 5}. Moreover, to compare the training stability,
we consider increasing the learning rate of  in MAPPO-
Lagrangian to 3  103.5 For a fair comparison, we reimple-
ment MAPPO-Lagrangian using the same GNN backbone as
4We omit the comparison with MACPO [30, 32] as it was shown to perform
similarly to MAPPO-Lagrangian but have significantly worse time complexity
and wall clock time for training.
5This is the smallest learning rate for  that does not make MAPPO-
Lagrangian ignore the safety constraint. We set 0  0.78 following .
Safety rate
Safety rate
Safety rate
Formation
Safety rate
Safe HalfCheetah 2x3
Safety rate
Safety rate
Corridor
Safety rate
ConnectSpread
Safety rate
Safe Coupled HalfCheetah 4x3
Def-MARL (ours)
Penalty(0.02)
Penalty(0.1)
Penalty(0.5)
Lagr(lr)
Fig. 4: Comparison on modified MPE (N  3) and Safe Multi-agent MuJoCo. Def-MARL is consistently closest to the
top-left corner in all environments, achieving low cost with near 100 safety rate. The dots show the mean values and the
error bars show one standard deviation.
used in Def-MARL and InforMARL, denoted as Lagr(0)
and Lagr(lr) for the increased learning rate one. We run
each method for the same number of update steps, which is
large enough for all the methods to converge.
Evaluation criteria. Following the objective of MASOCP,
we use the cost and safety rate as the evaluation criteria for the
performance of all algorithms. The cost is the cumulative cost
over the trajectory PT
k0 l(xk, uk). The safety rate is defined
as the ratio of agents that remain safe over the entire trajectory,
i.e., hi(ok
i ) 0, k, over all agents. Unlike the CMDP setting,
we do not report the mean of constraint violations over time
but the violation of the hard safety constraints.
B. Results
We train all algorithms with 3 different random seeds and
test the converged policies on 32 different initial conditions.
As discussed in Section IV-C, we disable the communication
of zi between agents (investigated in Section V-C). We draw
the following conclusions.
(Q1): Def-MARL achieves the best performance with
constant hyperparameters across all environments. First,
we plot the safety rate (y-axis) and cumulative cost (x-axis) for
each algorithm in Fig. 4. Thus, the closer an algorithm is to the
top-left corner, the better it performs. In both MPE and Safe
Multi-agent MuJoCo environments, Def-MARL is always
closest to the top-left corner, maintaining a low cost while
having near 100 safety rate. For the baselines Penalty
and Lagr, their performance and safety are highly sensitive
to their hyperparameters. While Penalty with   0.02
and Lagr with 0  1 generally have low costs, they
also have frequent constraint violations. With   0.5 or
0  5, they prioritize safety but at the cost of high cumulative
costs. Def-MARL, however, maintains a safety rate similar
to the most conservative baselines (Penalty(0.5) and
Lagr(5)) but has much lower costs. We point out that no
single baseline method behaves considerably better on all the
wildly between environments, demonstrating the sensitivity of
these algorithms to the choice of hyperparameters. On the
a single set of constant hyperparameters, which demonstrates
its insensitivity to the choice of hyperparameters.
(Q2): Def-MARL is able to reach the global optimum of
the original problem.
An important observation is that for
Penalty and Lagr with a non-optimal , the cost function
optimized in their training process is different from the original
cost function. Consequently, they can have different optimal
solutions compared to the original problem. Therefore, even
if their training converges, they may not reach the optimal
solution to the original problem. In Fig. 5, the converged states
of Def-MARL and four baselines are shown. Def-MARL
reaches the original problems global optimum and covers all
three goals. On the contrary, the optima of Penalty(0.02)
and Lagr(1) are changed by the penalty term, so they
choose to leave one agent behind to have a lower safety
penalty. With an even more significant penalty, the optima
of Penalty(0.5) and Lagr(5) are changed dramatically,
and they forget the goal entirely and only focus on safety.
Goallandmark
Obstacle
Target positions
Agent-agentobstacle
Agent-goal
Def-MARL (ours)
Penalty(0.02)
Penalty(0.5)
Fig. 5: Converged states in CORRIDOR.
Def-MARL achieves the global minimum, while other baselines converge to a
different optimum (partly) due to training using a different cost function.
Safety rate
Safety rate
Def-MARL (ours)
Lagr(lr)
Fig. 6: Training Curves in TARGET and SPREAD.
Def-MARL has a smoother, more stable training curve compared to
Lagr(lr). We plot the mean and shade the 1 standard deviation.
TABLE I: Policy Generalization. Testing Def-MARL on
TARGET with more agents after training with N  8 agents.
Safety rate
(Q3): Training of Def-MARL is more stable.
pare the training stability of Def-MARL and the Lagrangian
method Lagr(lr), we plot their cost and safety rate during
training in Fig. 6. Def-MARL has a smoother curve compared
to Lagr(lr), supporting our theoretical analysis in Sec-
tion III-B. Due to space limits, the plots for other environments
and other baseline methods are provided in Appendix E-D.
(Q4): Def-MARL can scale to more agents while main-
taining high performance and safety, but is limited by
GPU memory due to centralized training. We test the limits
of Def-MARL on the number of agents during training by
comparing all methods on N  5, 7 with FORMATION and
unable to increase N further due to GPU memory limitations
due to the use of centralized training. For this experiment, we
omit Lagr(lr) as it has the worst performance in MPE with
N  3. Def-MARL is closest to the upper left corner in all
increasing number of agents.
(Q5): The trained policy from Def-MARL generalizes to
much larger MAS. To test the generalization capabilities of
larger MASs with up to N  512 on TARGET (Table I) with
the same agent density to avoid distribution shift. Def-MARL
maintains a high safety rate and low costs despite being
applied on a MAS with 64 times more agents.
C. Ablation studies
Here we do ablation studies on the communication of zi,
and study the hyperparameter sensitivity of Def-MARL.
Is communicating zi necessary?
As introduced in Sec-
tion IV-C, theoretically, all connected agents should commu-
nicate and reach a consensus on z  maxi zi. However, we
observe in Section V-B that the agents can perform well even
if agents take z zi without communicating to compute
the maximum. We perform experiments on MPE (N  3)
to understand the impact of this approximation in Table II
and see that using the approximation does not result in much
performance difference compared to communicating zi and
using the maximum.
Varying  in the outer problem.
To robustify our
approach against estimation errors in V h, we solve for a
zi that is slightly more conservative by modifying (15b) to
(oi, zi)  (Section IV-C). We now perform experiments
Safety rate
Formation (N  5)
Safety rate
Formation (N  7)
Safety rate
Line (N  5)
Safety rate
Line (N  7)
Safety rate
Target (N  8)
Safety rate
Target (N  12)
Safety rate
Target (N  16)
Def-MARL (ours)
Penalty(0.02)
Penalty(0.1)
Penalty(0.5)
Fig. 7: Comparison on larger-scale modified MPE. Def-MARL remains in the top-left corner even when the number of
agents increases. The dots show the mean and the error bars show one standard deviation.
TABLE II: Effect of zi communication (Section IV-C) in different environments.
Environment
No communication (z zi)
Communication (z  maxi zi)
Safety rate
Safety rate
FORMATION
CORRIDOR
CONNECTSPREAD
TABLE III: Effect of varying
(Section IV-C) for LINE (N
3) with fixed   0.5.
Safety rate
to study the effect of different choices of  (Table III) on
LINE (N  3). The results show that higher values of
result in higher safety rates and slightly higher costs, while the
reverse is true for smaller . This matches our intuition that
modifying (15b) can help improve constraint satisfaction when
the learned V h has estimation errors. We thus recommend
choosing  close to . We also provide the sensitivity analysis
on more hyperparameters in Appendix E.
VI. HARDWARE EXPERIMENTS
Crazyflie (CF) drones  to demonstrate Def-MARLs ability
to safely coordinate agents to complete complex collaborative
tasks in the real world. We consider the following two tasks.
CORRIDOR. A swarm of drones collaboratively gets
through a narrow corridor and reaches a set of goals
without explicitly assigning drones to goals.
INSPECT. Two drones collaborate to maintain direct vi-
sual contact with a target drone that follows a path shaped
like an eight while staying out of the target drones avoid
zone. Visual contact only occurs when the line of sight
to the target drone is not blocked by obstacles.
For both tasks, all drones have collision constraints with other
drones and static obstacles. We visualize the tasks in Fig. 8.
Baselines.
Def-MARL has demonstrated superior perfor-
mance among RL methods in simulation experiments. Con-
the hardware experiments. Instead, we present a comparative
analysis between Def-MARL and model predictive control
(MPC), a widely employed technique in practical robotic ap-
plications. Notably, MPC necessitates model dynamics knowl-
against the following two MPC baselines.
Decentralized. We consider a decentralized MPC method
(DMPC), where each drone tries to individually minimize
the total cost and prevents collision with other controlled
drones by assuming a constant velocity motion model
using the current measurement of their velocity.
Centralized. We also test against a centralized MPC
(CMPC) method to better disentangle phenomena related
Obstacle
(a) CORRIDOR
Obstacle
Avoid region
Observation range
Observation range
of CF1 (blocked by
the obstacle)
(b) INSPECT
Fig. 8: Hardware tasks. We perform hardware experiments using a swarm of CF drones on the CORRIDOR and INSPECT
tasks. In CORRIDOR, the team must cross a narrow corridor and cover a set of goals collectively without prior assignment. In
Def-MARL
Fig. 9: Hardware Results on CORRIDOR (N  3). Left to right: key frames of the trajectories generated by different
algorithms. Arrows with different colors indicate the positions of different drones. Def-MARL (top) finishes the task with
100 success rate because the drones learn to cross the corridor one by one. CMPC and CMPCwA (middle) sometimes get
stuck in local minima and cannot finish the task because of the highly nonconvex cost function. DMPC (bottom) has unsafe
cases where the agents cross the unsafe radius so they cannot reach the goals because the MPC problem becomes infeasible.
to numerical nonlinear optimization and performing de-
centralized control. This method uses the same cost
function used by Def-MARL.
Both MPC methods are implemented in CasADi  with the
SNOPT nonlinear optimizer . Details for the hardware
setup and experiment videos are provided in Appendix F.
A. CORRIDOR
We run each algorithm from 16 random initial conditions
and use the task success rate to measure their performance.
The task is defined to be successful if all the goals are covered
by the agents and all agents stay safe during the whole task.
successful tasks divided by 16. In our tests, the success rates of
analyze this result, we visualize the trajectory of Def-MARL
and some failure cases of the baselines in Fig. 9.
CMPC is prone to local minima.
We first compare
Def-MARL with CMPC. Since we sum the distance from each
goal to the closest drone, the cost function in this task is very
nonconvex. Consequently, CMPC results in very suboptimal
to reach the goals, with the remaining drones left on the other
side. To alleviate this issue, although the original task does
Fig. 10: Hardware Results of Def-MARL on CORRIDOR (N  7). Even in this crowded environment, Def-MARL maintains
a success rate of 100.
Def-MARL
Fig. 11: Hardware Results on INSPECT. The CF drone overlayed with the yellowgreen sphere is the target. The sphere turns
green when the target is observed and yellow otherwise. The CF drones overlayed with blue spheres are agents, which turn
red if the agents become unsafe. The red spheres around the target show the avoid zone that agents cannot enter. Def-MARL
finishes the task with safe and collaborative behaviors. For example, they learn to wait on two sides of the obstacles and take
turns to observe the target. CMPC gets stuck in local minima and only moves the closest drone to the target, leaving the other
drone stationary. DMPC makes both agents chase after the target without collaboration, and even has unsafe cases.
not have an explicit goal allocation, we provide a handicap to
the CMPC methods and rerun the experiments with explicitly
assigned goals for each drone. This simplifies the optimization
problem by removing the discrete nature of goal assignment.
We name this baseline CMPCwA (CMPC with assignment).
sometimes one of the drones in the team gets stuck behind
the corridor, resulting in a success rate of 87.5. In contrast,
Def-MARL does not succumb to this local minimum and
completes the task with a success rate of 100.
DMPC has unsafe cases. As DMPC without goal assignment
will also suffer from similar issues as CMPC, we choose
to assign goals for each agent in this baseline. This results
in a simpler problem, as explained above. However, unlike
actions of the other agents and can only make predictions
based on their observations because of the decentralized nature
of DMPC. Therefore, collisions may occur if the other agents
behave significantly differently from the predictions. In the
DMPC row of Fig. 9, the agents collide6 in the middle of
the tasks, causing the MPC optimization problem to become
6For our safety, the safety radius of the drones is larger than their actual
radius. Here, we mean they have entered each others safety radius, although
they have not collided in reality.
infeasible and preventing the agents from reaching their goals.7
Def-MARL can scale up to 7 agents. We also test the
scalability of Def-MARL with 7 CF drones in the same
environment. Notably, the size of the environment remains un-
thus more challenging. We test Def-MARL with 9 different
random initial conditions, and it maintains a success rate of
100. We visualize one of the trajectories in Fig. 10. Note that
we were limited to only 7 drones here due to only having 7
drones available. However, given the simulation results, we are
hopeful that Def-MARL can scale to larger hardware swarms.
B. INSPECT
We also run each algorithm from 16 different random initial
conditions. Note that the agents may not be able to observe
the target at their initial positions at the first step. In this
duration where the goal is observed by at least one drone.
Measuring the task performance by the number of timesteps
where the target is not visible, we obtain that the performance
of Def-MARL, CMPC, and DMPC are 85.542.9, 20653.2,
and 251  59.1, respectively. We also report the safety rate,
7Some MPC-based methods can solve the CORRIDOR environment [43, 68]
but assume pre-assigned goals. Additionally, these approaches need additional
methods for collision avoidance (e.g., Buffered Voronoi Cells , on-demand
collision avoidance approaches ), which require more domain knowledge.
defined as the ratio of tasks where all agents stay safe, which
are 100, 100, and 43.75, respectively. We visualize the
trajectories of different methods in Fig. 11.
Agents using Def-MARL have better collaboration. The
INSPECT is designed such that collaboration between the
agents is necessary to observe the target without any down-
time. One agent cannot observe the target all the time on
its own because the agents observation can be blocked by
obstacles. It also cannot simply follow the target because of the
avoid region. Using Def-MARL, the agents learn collaborative
behaviors such as waiting on each side of the obstacles and
taking turns to observe the target when the target is on their
side. The MPC methods, however, do not have such global
optimal behavior but get stuck in local minima. For example,
CMPC only moves the closest drone to the target, leaving
the other drone stationary, while DMPC makes both agents
chase after the target without collaboration. Therefore, both
MPC methods have small periods of time where neither drone
has visual contact with the target. In addition, similar to
in CORRIDOR, we observe that DMPC sometimes results in
collisions due to the lack of coordination between drones.
VII. CONCLUSION
To construct safe distributed policies for real-world multi-
agent systems, this paper introduces Def-MARL for the multi-
agent safe optimal control problem, which defines safety as
zero constraint violation. Def-MARL takes advantage of the
epigraph form of the original problem to address the train-
ing instability of Lagrangian methods in the zero-constraint
violation setting. We provide a theoretical result showing that
the centralized epigraph form can be solved in a distributed
fashion by each agent, which enables distributed execution of
Def-MARL. Simulation results on MPE and the Safe Multi-
agent MuJoCo environments suggest that, unlike baseline
across all environments, and achieves a safety rate similar
to the most conservative baseline and similar performance
to the baselines that prioritize performance but violate safety
constraints. Hardware results on the Crazyflie drones demon-
strate Def-MARLs ability to solve complex collaborative
tasks safely in the real world.
VIII. LIMITATIONS
The theoretical analysis in Section IV-C suggests that the
connected agents must communicate z and reach a consen-
sus. If the communication on z is disabled, although our
experiments show that the agents still perform similarly, the
theoretical optimality guarantee may not be valid. In addition,
the framework does not consider noise, disturbances in the
as a safe RL method, although safety can be theoretically
guaranteed under the optimal value function and policy, this
does not hold under inexact minimization of the losses. We
leave tackling these issues as future work.
ACKNOWLEDGMENTS
This work was partly supported by the Under Secretary
of Defense for Research and Engineering under Air Force
Contract No. FA8702-15-D-0001. In addition, Zhang, So, and
Fan are supported by the MIT-DSTA program. Any opinions,
publication are those of the authors and dont necessarily
reflect the views of the sponsors.
2025 Massachusetts Institute of Technology.
Delivered to the U.S. Government with Unlimited Rights,
as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014).
Notwithstanding any copyright notice, U.S. Government rights
in this work are defined by DFARS 252.227-7013 or DFARS
252.227-7014 as detailed above. Use of this work other than
as specifically authorized by the U.S. Government may violate
any copyrights that exist in this work.
REFERENCES
Joshua Achiam, David Held, Aviv Tamar, and Pieter
Constrained policy optimization.
In Interna-
tional Conference on Machine Learning, pages 2231.
Akshat Agarwal, Sumit Kumar, Katia Sycara, and
Michael Lewis.
Learning transferable cooperative be-
havior in multi-agent team. In International Conference
on Autonomous Agents and Multiagent Systems (AA-
MAS2020). IFMAS, 2020.
Eitan Altman. Constrained Markov decision processes.
Joel AE Andersson, Joris Gillis, Greg Horn, James B
framework for nonlinear optimization and optimal con-
trol. Mathematical Programming Computation, 11:136,
Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J
Tomlin. Hamilton-jacobi reachability: A brief overview
and recent advances. In 2017 IEEE 56th Annual Confer-
ence on Decision and Control (CDC), pages 22422253.
Dimitri Bertsekas. Dynamic programming and optimal
Suda Bharadwaj, Roderik Bloem, Rayna Dimitrova,
Bettina Konighofer, and Ufuk Topcu.
Synthesis of
minimum-cost shields for multi-agent systems. In ACC.
Vivek S Borkar. An actor-critic algorithm for constrained
markov decision processes. Systems  Control Letters,
Vivek S Borkar. Stochastic Approximation: A Dynamical
Systems Viewpoint, volume 48. Springer, 2009.
Stephen P Boyd and Lieven Vandenberghe.
optimization. Cambridge university press, 2004.
Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and
Hao Xiong.
Safe multi-agent reinforcement learning
through decentralized multiple control barrier functions.
arXiv preprint arXiv:2103.12553, 2021.
Tirupathi
Chandrupatla.
quadraticbisection
algorithm
nonlinear
function
derivatives.
Advances
Engineering
Yu Fan Chen, Michael Everett, Miao Liu, and Jonathan P
Socially aware motion planning with deep re-
inforcement learning. In 2017 IEEERSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 13431350. IEEE, 2017.
Yu Fan Chen, Miao Liu, Michael Everett, and Jonathan P
How. Decentralized non-communicating multiagent col-
lision avoidance with deep reinforcement learning.
2017 IEEE International Conference on Robotics and
Automation (ICRA), pages 285292. IEEE, 2017.
Ziyi Chen, Yi Zhou, and Heng Huang. On the duality
gap of constrained cooperative multi-agent reinforcement
learning.
In The Twelfth International Conference on
Learning Representations, 2024.
Christian Conte, Tyler Summers, Melanie N Zeilinger,
Manfred Morari, and Colin N Jones.
Computational
aspects of distributed optimization in model predictive
control. In 2012 IEEE 51st IEEE conference on decision
and control (CDC), pages 68196824. IEEE, 2012.
Philip Dames, Pratap Tokekar, and Vijay Kumar.
moving targets using a team of mobile robots.
International Journal of Robotics Research, 36(13-14):
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran
alized lagrangian policy optimization for safe multi-agent
reinforcement learning. In Learning for Dynamics and
Control Conference, pages 315332. PMLR, 2023.
Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Am-
multi-agent reinforcement learning via shielding. arXiv
preprint arXiv:2101.11196, 2021.
Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Amato,
Rudiger Ehlers, Ufuk Topcu, and Lu Feng. Safe multi-
agent reinforcement learning via shielding. AAMAS 21,
Michael Everett, Yu Fan Chen, and Jonathan P How. Mo-
tion planning among dynamic, decision-making agents
with deep reinforcement learning. In 2018 IEEERSJ In-
ternational Conference on Intelligent Robots and Systems
(IROS), pages 30523059. IEEE, 2018.
Giuseppe Fedele and Giuseppe Franze.
A distributed
model predictive control strategy for constrained multi-
agent systems: The uncertain target capturing scenario.
IEEE Transactions on Automation Science and Engineer-
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras,
Nantas Nardelli, and Shimon Whiteson. Counterfactual
multi-agent policy gradients. In Proceedings of the AAAI
conference on artificial intelligence, volume 32, 2018.
Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert,
and Sicun Gao. Iterative reachability estimation for safe
reinforcement learning. Advances in Neural Information
Processing Systems, 36, 2024.
Kunal Garg, Songyuan Zhang, Oswin So, Charles Daw-
robot systems: Methods, verification, and open chal-
lenges. Annual Reviews in Control, 57:100948, 2024.
Nan Geng, Qinbo Bai, Chenyi Liu, Tian Lan, Vaneet
ment learning framework for vehicular network routing
under peak and average constraints. IEEE Transactions
on Vehicular Technology, 2023.
Wojciech Giernacki, Mateusz Skwierczynski, Wojciech
2.0 quadrotor as a platform for research and education
in robotics and control engineering.
In 2017 22nd
International Conference on Methods and Models in
Automation and Robotics (MMAR), pages 3742. IEEE,
Philip E Gill, Walter Murray, and Michael A Saunders.
optimization. SIAM review, 47(1):99131, 2005.
Lars Grne and Jrgen Pannek. Nonlinear model predictive
Springer Publishing
Shangding Gu, Jakub Grudzien Kuba, Munning Wen,
Ruiqing Chen, Ziyan Wang, Zheng Tian, Jun Wang,
Alois Knoll, and Yaodong Yang. Multi-agent constrained
policy optimisation.
arXiv preprint arXiv:2110.02793,
Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian
review of safe reinforcement learning: Methods, theory
and applications. arXiv preprint arXiv:2205.10330, 2022.
Shangding Gu, Jakub Grudzien Kuba, Yuanpei Chen,
Yali Du, Long Yang, Alois Knoll, and Yaodong Yang.
Safe multi-agent reinforcement learning for multi-robot
control. Artificial Intelligence, 319:103905, 2023.
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr.
Coordinated reinforcement learning. In ICML, volume 2,
pages 227234. Citeseer, 2002.
Matthew Hausknecht and Peter Stone. Deep recurrent
q-learning for partially observable mdps. In 2015 aaai
fall symposium series, 2015.
Tairan He, Weiye Zhao, and Changliu Liu.
Evolving intrinsic cost for zero-violation reinforcement
learning.
In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 37, pages 1484714855,
Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang,
and Yaodong Yang.
learning with world models. In The Twelfth International
Conference on Learning Representations, 2024.
Ajay Kattepur, Hemant Kumar Rath, Anantha Simha, and
Arijit Mukherjee. Distributed optimization in multi-agent
robotics for industry 4.0 warehouses. In Proceedings of
the 33rd Annual ACM Symposium on Applied Computing,
Chenyi Liu, Nan Geng, Vaneet Aggarwal, Tian Lan,
Yuan Yang, and Mingwei Xu.
agent reinforcement learning with peak and average con-
straints. In Machine Learning and Knowledge Discovery
in Databases. Research Track: European Conference,
ECML PKDD 2021, Bilbao, Spain, September 1317,
Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao
Towards optimally decentralized
multi-robot collision avoidance via deep reinforcement
learning.
In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pages 62526259.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI
Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-
critic for mixed cooperative-competitive environments.
Advances in neural information processing systems, 30,
Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Basar,
and Lior Horesh. Decentralized policy gradient descent
ascent for safe multi-agent reinforcement learning.
Proceedings of the AAAI Conference on Artificial Intel-
Carlos E Luis and Angela P Schoellig.
Trajectory
generation for multiagent point-to-point transitions via
distributed model predictive control. IEEE Robotics and
Automation Letters, 4(2):375382, 2019.
Carlos E Luis, Marijan Vukosavljev, and Angela P
Schoellig. Online trajectory generation with distributed
model predictive control for multi-robot motion planning.
IEEE Robotics and Automation Letters, 5(2):604611,
John Lygeros. On reachability and minimum cost optimal
control. Automatica, 40(6):917927, 2004.
Hang Ma, Jiaoyang Li, TK Kumar, and Sven Koenig.
Lifelong multi-agent path finding for online pickup and
delivery tasks. arXiv preprint arXiv:1705.10868, 2017.
Kostas Margellos and John Lygeros.
Hamiltonjacobi
formulation for reachavoid differential games.
Transactions on automatic control, 56(8):18491861,
Pierre-Francois
Friedrich
Transactions
Automatic
Daniel Melcer, Christopher Amato, and Stavros Tripakis.
Shield decentralization for safe multi-agent reinforce-
ment learning.
In Advances in Neural Information
Processing Systems, 2022.
Ian M Mitchell, Alexandre M Bayen, and Claire J
Tomlin. A time-dependent hamilton-jacobi formulation
of reachable sets for continuous dynamic games. IEEE
Transactions on automatic control, 50(7):947957, 2005.
Simon Muntwiler, Kim P Wabersich, Andrea Carron,
and Melanie N Zeilinger. Distributed model predictive
safety certification for learning-based control.
Siddharth Nayak, Kenneth Choi, Wenqi Ding, Sydney
nan. Scalable multi-agent reinforcement learning through
intelligent information aggregation.
In International
Conference on Machine Learning, pages 2581725833.
Angelia Nedic and Ji Liu.
Distributed optimization
for control.
Annual Review of Control, Robotics, and
Autonomous Systems, 1:77103, 2018.
Jorge Nocedal and Stephen J Wright. Numerical opti-
mization. Springer, 1999.
Christian
Schroeder
agent centralised policy gradients. Advances in Neural
Information Processing Systems, 34:1220812221, 2021.
Marcus A Pereira, Augustinos D Saravanos, Oswin So,
and Evangelos A Theodorou. Decentralized safe multi-
agent stochastic optimal control using deep fbsdes and
admm. arXiv preprint arXiv:2202.10658, 2022.
Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen,
and Chuchu Fan. Learning safe multi-agent control with
decentralized neural barrier certificates. In International
Conference on Learning Representations, 2021.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon
Whiteson. Weighted qmix: Expanding monotonic value
function factorisation for deep multi-agent reinforcement
learning.
Advances in neural information processing
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder
De Witt, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson.
Monotonic value function factorisation for
deep multi-agent reinforcement learning.
Journal of
Machine Learning Research, 21(178):151, 2020.
Herbert Robbins and Sutton Monro. A stochastic approx-
imation method. The Annals of Mathematical Statistics,
Harsh Satija, Philip Amortila, and Joelle Pineau. Con-
strained markov decision processes via backward value
functions.
In International Conference on Machine
John Schulman, Sergey Levine, Pieter Abbeel, Michael
tion. In International Conference on Machine Learning,
pages 18891897. PMLR, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael
control using generalized advantage estimation.
preprint arXiv:1506.02438, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Samaneh Hosseini Semnani, Hugh Liu, Michael Everett,
Anton De Ruiter, and Jonathan P How.
Multi-agent
motion planning for dense and dynamic environments
via deep reinforcement learning.
IEEE Robotics and
Automation Letters, 5(2):32213226, 2020.
Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui
classification. arXiv preprint arXiv:2009.03509, 2020.
Oswin So and Chuchu Fan.
Solving stabilize-avoid
optimal control via epigraph form and deep reinforce-
ment learning. In Proceedings of Robotics: Science and
Oswin So, Cheng Ge, and Chuchu Fan.
minimum-cost reach avoid using reinforcement learning.
In The Thirty-eighth Annual Conference on Neural In-
formation Processing Systems, 2024.
Enrica Soria, Fabrizio Schiano, and Dario Floreano.
Predictive control of aerial swarms in cluttered environ-
ments. Nature Machine Intelligence, 3(6):545554, 2021.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Woj-
ciech Marian Czarnecki, Vinicius Zambaldi, Max Jader-
Karl Tuyls, et al.
Value-decomposition networks
for cooperative multi-agent learning.
arXiv preprint
Chen Tessler, Daniel J. Mankowitz, and Shie Mannor.
Reward constrained policy optimization. In International
Conference on Learning Representations, 2019.
Claire J Tomlin, John Lygeros, and S Shankar Sastry. A
game theoretic approach to controller design for hybrid
systems. Proceedings of the IEEE, 88(7):949970, 2000.
Charbel Toumieh and Alain Lambert.
Decentralized
multi-agent planning using model predictive control and
time-aware safe corridors. IEEE Robotics and Automa-
tion Letters, 7(4):1111011117, 2022.
Panagiotis Tsiotras, Efstathios Bakolas, and Yiming
Zhao. Initial guess generation for aircraft landing trajec-
tory optimization. In AIAA Guidance, Navigation, and
Control Conference, page 6689, 2011.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and
Chongjie Zhang. Qplex: Duplex dueling multi-agent q-
learning. arXiv preprint arXiv:2008.01062, 2020.
Peng Wang and Baocang Ding. A synthesis approach
of distributed model predictive control for homogeneous
multi-agent system with collision avoidance.
Interna-
tional Journal of Control, 87(1):5263, 2014.
Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang,
Huawei Huang, and Dapeng Oliver Wu.
Multi-agent
deep reinforcement learning for urban traffic light control
in vehicular networks. IEEE Transactions on Vehicular
Wenli Xiao, Yiwei Lyu, and John Dolan. Model-based
dynamic shielding for safe and efficient multi-agent re-
inforcement learning. arXiv preprint arXiv:2304.06281,
Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo:
A new approach for safe reinforcement learning with
convergence guarantee. In International Conference on
Machine Learning, pages 1148011491. PMLR, 2021.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao,
Guangyong Chen, Wulong Liu, and Hongyao Tang. Qat-
inforcement learning. arXiv preprint arXiv:2002.03939,
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao,
Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative multi-agent games.
Advances in Neural Information Processing Systems, 35:
Dongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen.
Reachability constrained reinforcement learning.
International conference on machine learning, pages
Mario Zanon and Sebastien Gros.
Safe reinforcement
learning using robust mpc. IEEE Transactions on Auto-
matic Control, 66(8):36383652, 2020.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang,
and Tamer Basar.
Fully decentralized multi-agent re-
inforcement learning with networked agents. In Inter-
national conference on machine learning, pages 5872
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-
agent reinforcement learning: A selective overview of
theories and algorithms.
Handbook of reinforcement
learning and control, pages 321384, 2021.
Songyuan Zhang, Kunal Garg, and Chuchu Fan. Neu-
ral graph control barrier functions guided distributed
collision-avoidance multi-agent control. In Conference
on Robot Learning, pages 23732392. PMLR, 2023.
Songyuan Zhang, Oswin So, Mitchell Black, and Chuchu
Fan. Discrete GCBF proximal policy optimization for
multi-agent safe optimal control.
In The Thirteenth
International Conference on Learning Representations,
Songyuan Zhang, Oswin So, Kunal Garg, and Chuchu
Fan. GCBF: A neural graph control barrier function
framework for distributed safe multiagent control. IEEE
Transactions on Robotics, 41:15331552, 2025.
Wenbo Zhang, Osbert Bastani, and Vijay Kumar. Mamps:
Safe multi-agent reinforcement learning via model pre-
dictive shielding.
arXiv preprint arXiv:1910.12639,
Weiye Zhao, Tairan He, and Changliu Liu. Model-free
safe control for zero-violation reinforcement learning. In
5th Annual Conference on Rob
