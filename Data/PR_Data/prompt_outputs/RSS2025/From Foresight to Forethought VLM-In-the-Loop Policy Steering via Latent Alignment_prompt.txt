=== PDF文件: From Foresight to Forethought VLM-In-the-Loop Policy Steering via Latent Alignment.pdf ===
=== 时间: 2025-07-22 09:42:42.899852 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：From Foresight to Forethought: VLM-In-the-Loop
Policy Steering via Latent Alignment
Yilin Wu1, Ran Tian2, Gokul Swamy1, Andrea Bajcsy1
1Carnegie Mellon University 2UC Berkeley
{yilinwu, gswamy, abajcsy}andrew.cmu.edu, rantianberkeley.edu
Latent World Model
The robot fails to grasp the cup.
The robot grasps the cup via the rim.
The robot grasps the cup via the handle.
Policy (:)
Latent-Aligned VLM
Proprio.
VLM Steering
Users Task
Description
Serve the cup of water to the guest.
Action Plans
Execute the best plan :
because it is  without spilling or
contaminating the drinks.
behavior. <latent token>
guest. Please select the best action plan.
Fig. 1: We present FOREWARN, an VLM-in-the-loop policy steering algorithm for multi-modal generative robot policies.
Our key idea is to decouple the VLMs burden of predicting action outcomes from evaluation. By predicting action outcomes
with a pre-trained latent dynamics model and aligning a VLM to reason about these latent states in text, FOREWARN can
select action plans at runtime that are most appropriate for new task contexts and user needs.
AbstractWhile generative robot policies have demonstrated
significant potential in learning complex, multimodal behav-
iors from demonstrations, they still exhibit diverse failures at
deployment-time. Policy steering offers an elegant solution to
reducing the chance of failure by using an external verifier to
select from low-level actions proposed by an imperfect generative
policy. Here, one might hope to use a Vision Language Model
(VLM) as a verifier, leveraging its open-world reasoning capa-
bilities. However, off-the-shelf VLMs struggle to understand the
consequences of low-level robot actions as they are represented
fundamentally differently than the text and images the VLM
was trained on. In response, we propose FOREWARN, a novel
framework to unlock the potential of VLMs as open-vocabulary
verifiers for runtime policy steering. Our key idea is to decouple
the VLMs burden of predicting action outcomes (foresight) from
evaluation (forethought). For foresight, we leverage a latent world
model to imagine future latent states given diverse low-level
action plans. For forethought, we align the VLM with these
predicted latent states to reason about the consequences of actions
in its native representationnatural languageand effectively
filter proposed plans. We validate our framework across diverse
robotic manipulation tasks, demonstrating its ability to bridge
representational gaps and provide robust, generalizable policy
steering. Videos can be found on the project website:
wu98.github.ioforewarn.
I. INTRODUCTION
Generative imitation-based policies are an increasingly pow-
erful way to learn low-level robot behaviors from multi-
modal1 expert demonstrations [6, 14, 51]. Despite their im-
pressive ability to learn diverse behaviors directly from high-
dimensional observations, these policies still degrade in nu-
anced and unexpected ways at runtime. For example, consider
the robot in the left of Figure 1 that must pick up a mug
from the table. At training time, the generative policy learns
a distribution over useful interaction modes such as grasping
the cup by different parts (e.g., handle, lip and interior, etc.)
shown in wrist camera photo in Figure 1.
down the cup during grasping, shown in the center of Fig-
ure 1), to inappropriate behaviors that are misaligned with the
deployment context or preferences of an end-user (such as
the robot placing its gripper inside of a cup of water when
serving a guest shown in the middle of Figure 1). While a
common mitigation strategy involves re-training the policy via
more demonstrations  or interventions [34, 25], runtime
failures are not always an indication that the base policy is
fundamentally incapable of producing the desired behavior. In
mode within its distribution (e.g., grasping the cup by the
handle), but due to putting too much probability mass on an
undesired mode, the robot does not reliably choose the correct
action plan at runtime.
Runtime policy steering [27, 47] offers an elegant solution
to this mode-selection problem. By using an external verifier
to select candidate plans proposed by an imperfect genera-
tive policy, the robots behavior can be steered at runtime
without any need for re-training. Despite the initial successes
demonstrated by the policy steering paradigm, the question
still remains of how to fully unlock autonomous policy steering
in the open world when the robots environment, task context,
and base policys performance are constantly changing.
Policy steering can be approached via the stochastic model-
predictive control framework of modern control theory, which
decomposes the optimal action selection (i.e. generation) prob-
lem of runtime policy steering into (a) predicting the outcomes
of a given action plan and (b) verifying how well they align
with user intent. However, this approach is only feasible when
a physics-based, low-dimensional dynamics model is available
for outcome prediction and a well-defined reward function
can be specified for verification. In open-world environments,
both of these requirements are challenging to fulfill due to the
complexity of dynamics modeling and the difficulty of hand-
crafting rewards to evaluate nuanced task requirements .
Our core idea is to leverage world models, which are well-
suited for predicting action outcomes in open world settings,
and VLMs, which have great potential as verifiers due to
their commonsense reasoning abilities, to develop a divide-
and-conquer approach to open-world policy steering. However,
doing so naively is challenging as world models and VLMs
operate on fundamentally different representations of reality.
Filtering Options via REpresenting World-model Action
Rollouts via Narration. To predict challenging action outcomes
(e.g., interaction dynamics of a manipulator and a deformable
bag), we use state-of-the-art world models [25, 50] to pre-
dict lower-dimensional latent state representations from high-
dimensional observation-action data (shown in orange in the
center of Figure 1). To critique behavior outcomes under
nuanced task specifications (e.g., Serve the cup of water to the
guest), we leverage vision-language models (VLMs) [11, 29]
as our open-world verifiers (shown in green in the center of
Figure 1). Importantly, we demonstrate that aligning the VLM
to reason directly about the predicted latent states from the
world model enables it to understand fine-grained outcomes
that it cannot directly predict zero-shot nor understand from
image reconstructions. Ultimately, this alignment step enables
our VLM-in-the-loop policy steering approach to interpret
action plans as behavior narrations and select high-quality
plans by reasoning over those narrations even under novel task
descriptions (shown on the right of Figure 1).
We evaluate FOREWARN on a suite of robotic manipula-
tion tasks, demonstrating how it can robustly filter proposed
action plans to match user intent and task goals even when
faced with variations not seen during training. In summary,
our main contributions are:
Formalizing runtime policy steering a stochastic model-
predictive control problem, revealing the generation-
verification gap [15, 9, 43] and where state-of-the-art
models have maximal potential to shine.
A latent space alignment strategy that enables a VLM to
more reliably verify action plan outcomes predicted by a
A novel, fully-autonomous policy steering framework that
improves a base generative imitation-based policy by over
Extensive experiments in hardware showing that our
latent-aligned VLM approach outperforms (by 40)
altnerative VLM approaches that do not decouple the
prediction of outcomes from verification.
II. RELATED WORK
Generative Imitation-Based Policies. With the rise of large-
scale open-source datasets of expert demonstrations [2, 4, 5, 8,
learn low-level robot control policies from data. In particular,
recent advances in generative modeling have unlocked policy
architectures that can model diverse, multi-modal behaviors
directly from high-dimensional observations [6, 23, 33]. At the
same time, generative IL policies still exhibit nuanced, hard-to-
anticipate performance degradations during deployment time.
These degradations range from complete task failures (e.g.,
inability to grasp a cup, knocking it down, or dropping
it ) potentially due to distribution shifts or visual distrac-
tors [19, 46], to inappropriate behaviors that are misaligned
with the deployment context or an end-users preferences (e.g.,
placing the gripper inside of a cup filled with water during
grasping) . In this work, our goal is to leverage the diverse
low-level behavior distribution that the base policy has learned,
but prevent these nuanced performance degradations at runtime
via our novel policy steering method.
Failure Detection, Monitoring  Prediction. The handling
of generative policy failures can be grouped into three broad
prediction. Posthoc approaches identify and explain failures
present in offline robot datasets, and have recently leveraged
Vision Language Models (VLMs) to accelerate this process
via video captioning, highlighting critical data frames, and
providing human-interpretable summaries of failures
failures as they happen during robot deployment. To quickly
identify nuanced failures, recent methods propose a fast and
slow approach: a fast online detector flags unusual situations
(e.g., binary anomaly classifier), while a slower VLM-based
reasoner provides a deeper understanding of the event and if it
is a relevant failure [1, 39]. Although these strategies can ef-
fectively identify failures, they fundamentally require the robot
to start failing for the runtime monitor to activate. The final
they occur and unlock the potential for preemptive correction
of the base policy. Here, existing approaches [22, 24, 25]
often rely on out-of-distribution (OOD) detection in a latent
space or dense human labe
