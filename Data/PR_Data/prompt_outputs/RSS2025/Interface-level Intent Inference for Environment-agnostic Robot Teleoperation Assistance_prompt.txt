=== PDF文件: Interface-level Intent Inference for Environment-agnostic Robot Teleoperation Assistance.pdf ===
=== 时间: 2025-07-22 15:51:49.844724 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Interface-Level Intent Inference for
Environment-Agnostic Robot Teleoperation
Assistance
Larisa Y.C. Loke, Brenna D. Argall
Department of Mechanical Engineering, Northwestern University, Evanston, IL
Department of Computer Science, Northwestern University, Evanston, IL
Department of Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL
Shirley Ryan AbilityLab, Chicago, IL, USA
larisayclu.northwestern.edu, brenna.argallnorthwestern.edu
AbstractIn robot teleoperation, humans issue control signals
through interfaces that require physical actuation. This interface-
level interaction largely goes unmodeled within the field, yet the
interpretation of an interface-level command can differ from
what was intended by the user, as a result of diminished human
ability or inadequate mappings from raw interface signals to
robot control signals. Interface-aware systems aim to address
this limitation in robot teleoperation by explicitly considering the
impact of a control interface on user input quality when inter-
preting interface signals for robot control. This work presents an
interface-aware formulation for the direct inference of intended
interface-level commands given known interaction characteristics
of a control interface using data-driven modeling, allowing for
teleoperation assistance without knowledge of the humans policy.
In our specific implementation, we tailor the formulation to model
a users operation of a sippuff interface using a network of
Gated Recurrent Units, chosen for their ability to model temporal
patterns and suitability for data-scarce domains. The resulting
model is agnostic to the robot being controlled, which allows
for its use in task- and environment-agnostic robot teleoperation
assistance. We deploy this model in two variations of assisted
teleoperation frameworks using a 1-D sippuff interface to control
a 7-DoF robotic arm, and conduct a human subjects study with
spinal cord injured participants to evaluate the efficacy of our
method. Our proposed task- and environment- agnostic formu-
lation is effective in reducing collisions during teleoperation, and
is preferred by users over teleoperation baselines for ease and
intuitiveness of robot operation.
I. INTRODUCTION
In robotic systems, uncertainty modeling primarily ad-
dresses uncertainties in sensing and actuation. However, in
human-robot interaction (HRI) systems, where the human
is fully or partially in control of the robot, a third critical
source of uncertainty can presentuncertainty in the human
interaction with the control interface.
When humans operate a robotic device, they must physically
actuate an interface to issue a robot control command that
achieves their intended task-level robot actionwhether by
joystick deflection, button press, or even electrical activity of
the brain. However, once a human-issued control command
has been processed and passed on to robot control, how the
interface is operated, and how its mapping to the robot control
space affects the measured user input, often is not explicitly
taken into consideration when reasoning about the human
commandit is treated the same by the control and autonomy
The impact of this oversight is especially severe for indi-
viduals with motor impairments who benefit from operating
assistive devices in their daily livesthe control interface
options that are accessible also often are limited in dimen-
sionality and continuity, which can be mentally demanding to
use for high dimensional control and can be difficult to actuate
and measured user inputs. This can be further exacerbated
by symptoms of neuromuscular injury such as tremors or
In order to make controlling assistive devices easier, and
enable greater physical independence for individuals with
motor impairments, there is work that explores the inclusion
of robotics autonomy, often in the form of shared control.
Shared-control paradigms consider the human control com-
mand at many steps along the autonomys decision-making
pipelinesuch as inferring the humans high-level goal
or determining when autonomy should step in or adapt how
it assists . Thus, any deviations in magnitude, timing, or
direction  between the true signal intended by the human
and that received by the robotics autonomy, as measured
through the interface, can have rippling effects throughout a
shared-control system.
By modeling the variability in user inputs and understanding
the mapping from interface to robot actions, we can more
effectively interpret human control commands [12, 15]. Such
an interface-aware system models, for a given person, (a)
how interface-level control actions are mapped to robot task-
level actions, and (b) how intended interface-level control
actions are distorted when measured through the interface.
This modeling enables robotics autonomy to reason about
deficiencies in human teleoperation, and provide customized
assistance at the level of the interface action, in contrast to
typical shared-control frameworks that provide assistance at
the level of robot control commandsfor example, robot end-
effector pose or mobile base wheel speeds . A critical step
of such assistance is to infer the intended interface-level action.
Exactly how to formulate this inference, and how to model
the use of an interface by a human operator, remains however
minimally addressed.
This paper dives more deeply into the question of the
inference of interface-level commands, and how this inference
can be used in an interface-aware teleoperation framework
to provide task- and environment-agnostic robot teleoperation
assistance. This work presents the following contributions:
1) A relaxed formulation of interface-aware assistance, that
removes all assumptions about the users policy.
2) A data-driven modeling method, and accompanying
specifications for a data collection task, that more closely
mimics real-world use of a given interface control map.
3) An end-user study in which individuals with spinal cord
injury evaluate our interface-aware assistance against
teleoperation baselines.
The rest of the paper is organized as follows. Section II
provides an overview of related literature. Section III details
our proposed interface-aware formulation, as well as the
modeling and data collection methods. Section IV presents
the human subjects study experimental methodology. Results
and discussion are presented in Section V, with limitations in
Section VI and conclusions presented in Section VII.
II. RELATED WORK
In this section, we present a summary of related research on
uncertainty handling in control interfaces, teleoperation, and
shared-control for robotics. We also overview the mathemat-
ical formulation of interface-awareness, and initial efforts to
apply this framework to the control of a 7-DoF robotic arm.
A. Handling Uncertainty in Interface Signals
In clinical settings, assistive device interfaces are cus-
tomized to a user to account for physical variations in interface
actuation. These customizations often take the form of manual
adjustments and tuning of preset configurations and thresholds.
For joysticks, this may include uniform scaling along principal
(1-D) interfaces such as the sippuff, signal thresholds for in-
terface actions can be adjusted . However, these adjustments
often need to be done by clinician experts, or require additional
hardware to complete , making it difficult to adapt to the
varying needs of assistive device users.
More automated signal-processing methods, featured in
non-commercial interfaces, often focus on techniques to com-
pensate for the complex and variable properties in physiologi-
cal signals such as EEG or EMG [17, 30], or high-dimensional
body motion data . By contrast, the work we present in
this paper models the operation of a conventional (commercial)
interface by a specific user, for the purpose of discerning
discrepancies between intended and measured interface-level
commands.
B. Teleoperation
Robot teleoperation typically employs a control inter-
face with interface-level actions that are distinct from the
workspace of the robot platform, introducing a requirement
for the human operator to learn a map from interface-level
actions to robot control commands and motions.
Within the domain of assistive robots, control maps be-
come increasingly challenging to operate as there is often a
dimensionality mismatch between the control interface and
the robotic platform. To enable accessibility for users with
motor impairments, assistive device interfaces often employ
simplified or compensatory control schemes, such as limiting
fine control over device motion or speed modulation . Such
limitations can result in choppy or imprecise movements, and
an increased cognitive and physical operational effort ,
in comparison to more traditional direct or continuous input
methods (such as the ubiquitous joystick). As a result, chal-
lenges in learning and adapting to the control map, which can
be mentally and physically taxing, ultimately can impact a
users ability to effectively operate the assistive device .
These challenges are exacerbated when controlling higher-
dimensional assistive devices, such as 6-DoF assistive robotic
arms. There are no standard interfaces for simultaneous 6-D
control of end-effector pose in SE(3). A common solution
involves mapping the movements of the human operators
body to the robot in an anthropomorphic manner , or parti-
tioning the robots control space into modes [19, 23] that allow
for a single interface-level action to be mapped to multiple
robot task-level actions via mode switching. Although modal
control is widely used, it introduces added cognitive load in
learning and using the control map. One common approach
to help alleviate this burden is to share control between the
human operator and a robotics autonomy system .
C. Shared-Control Robotic Assistance
Shared-control frameworks typically strive to improve the
overall quality of robot operation (performance and safety)
while maintaining human agency, by coordinating control re-
sponsibility between the human and the robot . To achieve
operators intent; (b) arbitration: the system modulates how
control is shared; and (c) communication: the system provides
feedback to the human regarding the state of autonomy .
Intent inference for shared-autonomy generally relies on
analyzing control commands received by the robot , pro-
prioceptive sensors on the robot , force sensors on the hu-
often with the purpose of inferring task-level actions or goals.
Interface-aware robotic assistance [12, 15] instead seeks to
infer interface-action level intent based on interface activation.
D. Interface-Aware Robotic Assistance
The original formulation of interface-awareness  aims
to estimate p(att
m), in order to infer the human operators
intended task-level action at given the measured interface-level
action t
m. Using Bayes theorem, the relationship between
conditional probability distributions can be derived to be:
p(atxt)p(xt)
i is the intended interface-level command, xt is the
state of the world and robot, and  is a normalizing factor.
There are intuitive interpretations of the conditional proba-
bility distributions in Equation 1. p(t
iat) is the users internal
model of the control mapping from task-level actions to the
interface-level actions that achieve them (e.g., to move the
robot forward, deflect the joystick forward), which captures
uncertainties in the users understanding of the robot control
map. p(t
i) is the users input distortion model, which
captures biases and deficits in the users ability to issue their
intended commands through the interface. p(atxt) is the
users control policy: the task-level action at they intend to
take given the state xt.
In practice, these conditional probability distributions can
be built from user teleoperation data, which can be collected
through simulated tasks andor through interaction with the
control interface and robot hardware . Previous approaches
make use of direct statistical learninghistogram binning of
user teleoperation datato build the models p(t
iat) and
i), which enables the evaluation of p(att
m) given a
known user policy p(atxt) . Limitations of this method
include (a) its inability to model temporal patterns in tele-
variable i when collecting user teleoperation data, and (c)
the need to know or estimate the users policy p(atxt), which
is challenging in real world scenarios . In this work, we
introduce a formulation for interface-aware robotic assistance
that aims to address each of these limitations.
III. TECHNICAL APPROACH
A problematic component of Equation 1 is the distribution
p(atxt). It represents an estimate of the humans control
assumption that is unrealistic in many domains, especially if
one wants to loosen requirements on the robot agents task
and environment knowledge. Prior works address this issue
by minimizing the assumptions baked into the estimate of
p(atxt), with only the presumption of robot safety on the part
of the human policy , or by operating within a simplified
simulation environment while performing tasks with a single
unambiguous goal, so that only one goal-achieving action at
exists at each state xt .
We present an alternate formulation of interface-awareness
that (a) removes the need for assumptions on the humans
control policy and (b) additionally accounts for known char-
acteristics and patterns of user interaction with the control
interface in the modeling approach.
A. Task- and Environment-Agnostic Interface-Awareness
In this work, we augment the users input distortion noise
model (previously p(t
i)) to explicitly represent the map-
User-robot interaction via a control interface depicted as a probabilis-
tic graphical model. The typical model of teleoperation is simply at ut
(dashed edge), while an interface-aware model additionally captures the
physical interaction with the interface (green nodes) , as well as human
understanding of interface control h(). Our extension (blue node) provides
enough context to perform direct inference of t
i from t through a modeling
of f1(), without any representation of the humans policy p(atxt).
ping g() from raw interface signal t to the interface-level
action t
m (Fig. 1, blue). For many interfaces, this map
is more than just a simple linear scaling: for example, a
sippuff device uses thresholds to map to a discrete set of
i (inhalationexhalation pressure that is strongweak), and
joysticks have center deadzones within which all positions map
to a value of zero.
We then recast the inference problem from one of estimating
m) to one of estimating f 1(): that is, of directly
estimating the inverse function mapping raw signals  to
intended interface-level action t
i. Here t is the time index
of mapped interface-level commands t
index of raw interface signals , which often are sampled at
a higher frequency than t
As we will see in Section III-C, taking into account the raw
interface signal, rather than only its mapping to the interface-
level action (as was done in prior work [12, 15]), is crucial for
reconstructing an accurate and sufficiently rich representation
of a humans interface use. The primary factors that account
for discrepancies between t
m are (a) human error or
diminished ability, and (b) suboptimal mappings g() and h().
In this work we consider the latter to be fixed (by interface and
control paradigm design), and focus our efforts on the former.
A critical aspect of our model f 1() is the incorporation
of history: critical both in building its representation and for
its use for inference. In detail, we consider a window of raw
interface signals :  [ : ] as well as the prior
interface-level action t1
g(t1). Observing a window
of raw interface signals captures a representation of the noise
in that signal, while including history in the form of t1
to capture patterns in interface use when issuing consecutive
task-level commands.
With a user-specific model of f 1(), we can assist in robot
teleoperation without knowledge of the users control policy
p(atxt) (in contrast to ) or the environment state (in
contrast to ). (How to build the model f 1() is detailed
A comparison of measured interface actions t
m using thresholding (top) and ground-truth intended (bottom) interface actions (t
i) during the data
collection task (Sec. III-D). Notably, we observe instances of: unintended mode switches (wrong direction; accidental overshoot of soft threshold); the pressure
signal passing through the soft pressure value range to get to the hard range; and inconsistent soft actions (that intermittently drop into the null range).
Algorithm 1 Interface-Aware Action Assistance
if H <  then
Model uncertainty is low
Command inferred action
Command measured action
Algorithm 1 describes our task- and environment-agnostic
interface-aware assistance, where assistance takes place at the
interface-level action. Given the window of previous interface
signals up to the current signal :, and the previously
commanded interface-level action t1
, we use the trained
user-specific model to infer the intended interface-level action
at the current timestep t
i (Line 2). If model inference un-
certainty (defined in Sec. III-C) is low, we pass the inferred
interface-level action to the robot control system (lines 3-4).
If model inference uncertainty instead is high, we pass the
measured interface-level action t
m (lines 5-6).
B. Data-driven Modeling of Physical Interface Operation
Here we detail how to build the model f 1(). Our specific
implementation tackles the sippuff device, a 1-D interface
typically used to operate assistive devices such as powered
wheelchairs by persons with very limited upper body mobility.
This interface is operated via respiration, and the continuous-
valued pressure signal (, sampled at 120 Hz) is mapped via
thresholding to one of five interface-level actions: {hard-
It most commonly is used under a Latch control paradigm
for powered wheelchair operation, where a single hard-puff
(hard-sip) action is used to start (stop) linear motion, while a
sustained soft action operates rotational motion.
In this work, we map sippuff interface-level actions to the
modal control of a 7-DoF robotic arm. Under this control
(6) of the end-effector position and orientation in SE(3), and
one for gripper operation. The actions i {hard-puff, hard-
sip} activate (clockwisecounterclockwise) switches between
control modes (with a fixed cyclical order, described in Fig. 5),
and actions i {soft-puff, soft-sip} control (positivenegative)
movement within a given control dimension.
We furthermore can characterize the operation of this
interface-robot combination according to the following em-
pirical observations:
tend to be short in nature, as they issue a discrete mode-
switch action. In contrast, pressure signals corresponding
to soft (motion) actions are longer in nature.
(soft) commands with consistency, with over or under-
shoot of the pressure value range as a result.
mands being issued is dependent on the similarity in pres-
sure value between the previous and current command.
Unintended motion: To issue a hard action, the pressure
signal must pass through the corresponding soft action
pressure value range (due to it being a 1-D signal with
multiple thresholds), which may result in unintended
robot motion during an intended mode switch action.
Unintended mode switch: When accidental hard actions
are issued, an unintended mode switch occurs, necessi-
tating a corrective mode switch (hard) action, which can
be fatiguing during prolonged robot arm operation.
Examples of these characteristics are visualized in Fig. 2.
These characteristics motivate the need for (a) the incorpora-
tion of temporal characteristics of the pressure signal, via data-
driven sequence modeling of f 1(), and (b) a data collection
task specifically designed to simulate actual interface use for
device control.
C. Modeling Raw Signals Intended Interface Commands
Given the need for the effective modeling of sequences of
interface signals, in order to incorporate temporal information,
we look to machine learning methodsspecifically, Recurrent
Neural Networks (RNNs), which are designed for processing
sequential data . Given the characteristics of sippuff
interface operation, we choose to use Gated Recurrent Units
(GRUs) to learn a user-specific mapping from  to t
are chosen for their simplicity compared to other sequen-
tial modeling techniques such as Long Short-Term Memory
(LSTM), which allows for shorter training times and are shown
to outperform LSTMs on low-complexity problems with small
datasets and shorter-term dependencies .
We train a multi-layer GRU-based neural network classifier
which is made up of three GRU layers, a final linear layer, and
a softmax activation that maps the output of the linear layer to
the output classes . The input to the classifier is described in
Section III-D. We use the Adam optimizer during training. To
motivate learning that takes into account interface operation
characteristics as well as practical usability, we design a loss
function that incorporates the following metrics:
Cross-entropy loss, LCE: The difference between the
predicted probability distribution and the ground truth
Weighted accuracy, Aw: Percentage of confident and cor-
rect predictions, rewarding the model for being confi-
dently correct.
Confidently wrong penalty, P: Percentage of confident
and wrong predictions, penalizing the model for be-
ing confidently wrongnotably, this aims to mitigate
unwanted corrections to the users input during model
deployment.
Here confidence is determined from the normalized entropy H
of the models output, where H <  is designated to be confi-
dent (certain). (In our implementation,   0.1.) The additional
loss terms Aw and P explicitly encode the selected confidence
threshold into the loss function. (In contrast to cross-entropy
without a specific target.) This explicit encoding is critical
for real-time use, as the threshold governs whether measured
interface actions are passed through to the robot or replaced
with inferred actions.1 The resulting loss function is as follows:
L  LCE Aw  P
where  and  are tunable weights on the loss terms.
The classifier is implemented in PyTorch , and model
architecture and learning hyperparameters are optimized using
Ray Tune  to maximize the weighted accuracy Aw as
1Preliminary ablations with the threshold  and combinations of loss terms
showed that  can be lowered when the additional terms are included, as
compared to only cross-entropy loss, likely because the explicit encoding of
drives the model towards making confident predictions that meet the threshold.
A lower threshold  is useful in safety-critical settings, such as powered
wheelchair driving, as model predictions should be extremely confident before
replacing human commands.
FINAL MODEL ARCHITECTURE AND HYPERPARAMETERS
Descriptor
Architecture
Hidden size
Number of GRU layers
Dropout for regularization
parameters
Initial learning rate (LR)
LR scheduler
ReduceLRonPlateau
LR decay rate
Weight decay for regularization
Data collection task. Interface-level action prompts are displayed in
groups of three, each with a countdown timer. A progress indicator (blue dot)
also serves as a proxy for the control result of the interface-level command.
visual feedback of the users raw pressure signal  is provided via the position
of the red dot. Bottom: After familiarization, this feedback is removed for the
data collection phase, since it is not available when operating the robot arm.
well as the F score of the classifier. Model architecture and
hyperparameter tuning is done on sippuff datasets collected
from volunteers (lab members) with a range of experience
using a sippuff interface. The final optimized model architec-
ture and hyperparameters are detailed in Table I. These same
hyperparameters are used when training each SCI user model.
D. Data Collection Tasks
For the GRU network to model f 1() for a specific user,
we need to collect representative labeled data of the user inter-
acting with the control interface. We design a data collection
task that, critically, is responsive to known interface operation
characteristics (identified in Section III-C). Specifically, the
task aims to elicit these characteristics during data collection,
so that they are represented within the training dataset. We fur-
thermore apply data processing and augmentation techniques
to produce a dataset suitable for GRU training.
The data collection prompts consist of a sequence of three
interface-level actions presented to users on a screen (Fig. 3).
Users are asked to provide the respiration action that would
produce the prompted action. The criteria for prompt comple-
tion differs depending on the interface-level action, to reflect
the differences in usage when issuing a motion command
(continuous soft actions) versus a mode switch (short and
discrete hard actions).
For soft actions i, users are tasked with providing the
corresponding pressure command  for a set length of
the screen (through the channel) towards the next prompt.
For hard actions i, once users have successfully pro-
vided the corresponding pressure command , the prompt
is considered complete.
For both, the blue dot also serves as a sort of proxy for the
effect of the interface-level action on the robot control, where
hard actions (mode switches) result in abrupt changes in robot
control while soft actions (motion commands) result in robot
movement. Notably, this proxy aims to simulate the feedback
that users would receive during real robot teleoperation. (Sim-
ilar feedback is absent in prior work .)
In total, the task consists of 32 sets of 3 prompts. We
generate prompts by taking the Cartesian product of the set
of actions  3 times, then randomly sample the resulting set
until we get an action-prompt-balanced set of 32. With 32 sets,
data collection takes approximately 6 minutes to complete,
excluding three breaks (every 8 sets) given to users during the
task. In our study, data collection (including breaks) took 431s
on average (7min), and did not appear taxing for participants.
The recorded data timeseries is partitioned into rolling
windows of 10 timesteps of . (Thus   9, Alg. 1). The
ground truth (intended) previous interface-level action in
is concatenated to the window as a tiled 5-vector, where the
representation of the action is embedded in the range of . At
deployment time, we do not have access to the ground truth
previous action, and so we would use the executed interface-
level action t1
from the previous timestep.
The resulting dataset consists of input tensors of size 10 by
in ). Since hard action prompts are shorter, the collected
user data has fewer time samples of hard action raw interface
signals. The dataset is upsampled with noise added to  to
ensure a balanced dataset.
IV. EXPERIMENTAL METHODOLOGY
We conduct a human subjects study to evaluate the efficacy
of our proposed task- and environment-agnostic interface-
aware assistance, and compare it with unassisted teleoperation
and environment-aware collision avoidance assistance.
A. Participants
A total of 8 participants with cervical-level spinal cord
injury (SCI) (5 male, 3 female) aged 28 to 62 (mean
47.613.2) were recruited. Participants had varying levels
(C2 to C7) and types (complete and incomplete) of injury
Instruments Corp. Right: Kinova Jaco 7-DoF robotic arm.
Cyclical screen-based mode display, with the current control mode
of the robot indicated (green circle). Text to the leftright (purpleyellow)
of each mode (circle) describes the end-effector motion that corresponds
to soft-sippuff actions. When users issue a hard-sippuff to mode switch,
the green circle jumps to the next mode in the corresponding direction
(counterclockwiseclockwise), with wrapping.
to the spinal cord. Detailed participant demographics are
provided in Appendix VIII. All participants gave their in-
formed consent to participate in the experiment, which was
approved by Northwestern Universitys Institutional Review
Board (STU00211758).
B. Hardware and Materials
We use a SipPuff Breezeinterface from Origin Instru-
ments Corporation (Texas, USA) to control a 7-DoF Kinova
Jaco robotic arm (Quebec, Canada), as seen in Fig. 4. While
controlling the arm, participants also are shown the circular
mode display in Fig. 5 on a screen. The mode display is
specifically designed to incorporate visual cues for the user
regarding the sippuff-Jaco control scheme.
C. Study Procedure
1) Participant Training: Participants are trained to use
the sippuff to issue interface-level actions as defined by a
standard thresholding map. As is the standard in clinical
pressure value to a hardsoft-sippuff. A real-time visualization
of the  signal is shown to participants, and the four raw
signal value ranges corresponding to the interface-level actions
are explained. Participants are then given time to familiarize
themselves with and practice the four actions.
ADL Task. The environment setup places a cup upside-down in
the center of the upper shelf, and initializes the robot arm pose. The task
proceeds as: (1) move the end-effector into the upper shelf, (2) grasp and lift
the cup, (3) remove the cup from the upper shelf, (4) flip the cup right-side-up,
(5) position the cup inside the left-hand compartment of the lower shelf, (6)
open the gripper. A trial is considered successful if the cup is placed (gripper
opened) in the target pose and orientation (inside lower shelf, right-side-up)
within the time limit. A trial is considered unsuccessful if participants exceed
the time limit, drop the cup, or do not achieve the goal pose and orientation.
After this, participants are introduced to the data collection
task detailed in Section III-D. They are given time to practice
the task with and without the raw pressure signal feedback
(familiarization), after which the data collection phase com-
mences. When data collection is complete, the GRU network is
trained on the participants data to build a user-specific model
of interface use (5min).
Participants then are trained to operate the robot arm using
the interface, first by a verbal and visual introduction to the 7
different control modes and mode display (Fig. 5), followed
by hands-on practice for mode switching (hard actions) and
issuing motion commands (soft actions).
2) Evaluation Tasks: Participants are tasked with complet-
ing an Activities of Daily Living (ADL) task of picking and
placing a cup in a different position and orientation (Fig. 6)
within a 7 minute time limit. Training involved coaching the
participant to complete the task as subtasks, followed by two
rounds of timed practice completing the task using the standard
map with no assistance. Participants are instructed to complete
the task as quickly as possible while avoiding collisions.
This task is chosen for its complexity because it requires (a)
enteringnavigating a tight space, and (b) orientingpivoting the
teleoperation as well as robot path planners .
Participants then execute the ADL task under four different
combinations of control mappings and assistance conditions
the standard map and user-specific interface-aware map, with
and without collision avoidance assistance:
Standard Map (Std-Map): The raw 1-D signal of the
sippuff interface is mapped to interface-level actions via
linear thresholding, as is the clinical standard.
Interface-Aware Map (IA-Map): A sequence of raw 1-
D signals from the sippuff interface are mapped to
interface-level actions using a GRU-based model trained
on a specific users interface actuation data, taking into
account temporal characteristics and command history.
Standard Map with Safety Assistance (Std-MapSafety):
The Std-Map, augmented with collision avoidance assis-
Interface-Aware Map with Safety Assistance
(IA-MapSafety): The IA-Map, augmented with collision
avoidance assistance.
Collision avoidance (Safety) assistance is provided by rep-
resenting the task environment as a potential field, wherein
collision objects (i.e., tabletop, shelves, etc.) have repulsive
potentials with respect to the end-effector position. Self colli-
sions are not represented. No attractive potentials are defined,
to keep the assistance task-agnostic.
Participants are not told which teleoperation condition is ac-
tive during each trial, and the presentation order of conditions
is randomized and balanced across participants.
3) Questionnaires: After each ADL trial, participants are
asked to fill out a Raw NASA-TLX survey  to assess their
self-perceived workload for the trial, as well as a questionnaire
of their perception of the teleoperation condition:
1) An autonomous assistance system was assisting me to
perform the task.
2) The autonomous assistance system helped me to com-
plete the task faster.
3) The autonomous assistance helped me complete the task
more safely.
4) The autonomous assistance helped me to reduce un-
wanted commands.
5) The autonomous assistance helped me to reduce the
number of mode switches.
6) I preferred doing the task with assistance.
Question 1 is a TrueFalse question, and questions 2-6 are
7-point Likert scales ranging from Strongly Disagree (1) to
Strongly Agree (7). At the end of all four trials, participants
are asked to rank the teleoperation conditions in order of ease
and intuitiveness of robot control.
V. RESULTS AND DISCUSSION
In this section, we present results relating to our user-
specific interface-aware map testing, as well as ADL task
completion metrics and questionnaire responses.
A. Interface-Aware Map Testing
We first compare the offline performance of the two inter-
face maps without any collision assistance: the Standard Map
(Std-Map) and Interface-Aware Map (IA-Map).
Fig. 7 presents the mapping accuracy of each approach:
that is, the accuracy of each map outputting the interface-level
command as prompted during the data collection task (ground
truth label) given the raw measured interface-level signal. Data
fed through each map consists of the 20 hold out from the
data collection task, where for the IA-Map the remaining 80
was used for training the GRU model.
Interface-level command mapping accuracy by the standard map
compared to the user-specific interface-aware model prediction, evaluated on
the test dataset collected from the data collection task. Interface-aware map
predictions are significantly more accurate than the standard map across all
participants. Statistical significance is computed using a permutation test on
the difference in means of prediction accuracy between the two maps for all
TABLE II
MEAN PERCENTAGE OF CORRECTWRONG AND CONFIDENTUNCERTAIN
INTERFACE-AWARE MODEL PREDICTIONS ACROSS ALL PARTICIPANTS
Percentage of Predictions
Confident
Uncertain
Confident
Uncertain
Hard Puff
Soft Puff
Soft Sip
Hard Sip
Across our population of 8 participants, the interface-aware
maps have significantly (p < 0.01) superior performance
compared to the standard maps, both in the average over all
participants and for each participant individually. These results
suggest that the method of modeling employed by the IA-Map
is able to capture nuances in the interface signal, and model
interface-level intent more accurately than the standard map.
model prediction results. Table II shows the percentage of
correctwrong and confidentuncertain model predictions aver-
aged across all participants during testing, wherein confidence
is determined by the normalized entropy H as detailed in
Section III-C. Notably w
