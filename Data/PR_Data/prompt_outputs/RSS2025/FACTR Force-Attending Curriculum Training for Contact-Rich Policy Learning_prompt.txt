=== PDF文件: FACTR Force-Attending Curriculum Training for Contact-Rich Policy Learning.pdf ===
=== 时间: 2025-07-21 14:24:19.901256 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Contact-Rich Policy Learning
Jason Jingzhou Liu, Yulong Li, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak
Carnegie Mellon University
Equal contribution
Fig. 1: FACTR. We present Force-Attending Curriculum Training (FACTR)  a system that leverages robot external joint torques for both
teleoperation and improving policy generalization for complex contact-rich tasks. [Left] Our low-cost leader-follower setup employs actuated
servo motors to enable force feedback in both the leader arm and gripper, improving teleoperation success rate, completion time, and ease of
use. [Right] FACTRs behavior cloning policy utilizes robot force information to enhance performance and generalization across objects with
diverse geometries and textures in contact-rich tasks. Video results, codebases, and instructions at
AbstractMany contact-rich tasks humans perform, such as
box pickup or rolling dough, rely on force feedback for reliable
execution. However, this force information, which is readily avail-
able in most robot arms, is not commonly used in teleoperation
and policy learning. Consequently, robot behavior is often limited
to quasi-static kinematic tasks that do not require intricate force-
feedback. In this paper, we first present a low-cost, intuitive,
bilateral teleoperation setup that relays external forces of the
follower arm back to the teacher arm, facilitating data collection
for complex, contact-rich tasks. We then introduce FACTR, a
policy learning method that employs a curriculum which corrupts
the visual input with decreasing intensity throughout training.
The curriculum prevents our transformer-based policy from over-
fitting to the visual input and guides the policy to properly
attend to the force modality. We demonstrate that by fully
utilizing the force information, our method significantly improves
generalization to unseen objects by 43 compared to baseline
approaches without a curriculum. Video results, codebases, and
instructions at
I. INTRODUCTION
Contact-rich tasks are an integral part of daily life, from
lifting a box and rolling dough to cracking an egg or opening a
door. These tasks, while seemingly simple, involve a complex
interplay of forces and require precise adjustments based on
force feedback. Humans rely heavily on this force feedback
to generalize across tasks and objects, adapting seamlessly to
variations in visual appearances and geometries. However, in
robot learning, force information remains underutilized, even
though it is readily available on many modern robotic arms,
such as the Franka Panda and the KUKA LBR iiwa. Instead,
most data-driven methods, including those using Behavior
Cloning (BC), focus primarily on visual feedback for both data
collection and policy learning, overlooking the critical role of
force. This limited use of force information hinders the vision-
only policies ability to generalize to novel objects. For instance,
in tasks like lifting up a box with two arms, the primary factor
influencing the action is the objects geometry, while attributes
such as color or texture are irrelevant. In such cases, force
feedback provides a clear signal for mode switching, such
as detecting when contact is established, which can facilitate
object generalization compared to relying solely on vision.
One of the main reasons for the under-utilization of force
feedback in robot learning is the lack of an intuitive and low-
cost teleoperation system that can capture force feedback during
data collection itself. Recently, low-cost leader-follower systems
have become popular for teleoperation, offering intuitive control
of robot arms by mirroring the joint movements of the leader
arm controlled by a teleoperator to the follower arm [32, 29].
joints are not actuated) and unilateral (the leader arm does
not receive information from the follower arm). This makes
teleoperation difficult for dynamic, contact-rich tasks where
precise force adjustments are necessary . To overcome this
that provides force feedback by actuating motors in the leader
arm joints based on external joint torques transmitted from the
follower arm (Fig. 1 Left). By actuating the motors, we also
provide active gravity compensation and resolve the kinematic
redundancy due to the redundant degrees of freedom of the
arm. These enhancements improve the teleoperation experience,
leading to a 64.7 increase in the task completion rate, a 37.4
reduction in completion time, and an 83.3 improvement in
subjective ease of use across four evaluated contact-rich tasks.
The second challenge lies in effectively incorporating robot
force information into policy learning. Although recent methods
such as diffusion policy  and action chunking transformers
achieve impressive results for fine-grained manipulation,
they often fail to generalize to unseen objects with variations
in object visual appearances and geometries. Humans, on the
other hand, can disregard irrelevant visual details once contact
is established and rely solely on force feedback to perform
tasks such as lifting a box or rolling dough. Therefore, to
improve generalization, we seek to incorporate force input
into autonomous robot policies. However, making effective
use of force information in policy learning is challenging, as
policies often overfit to using visual modality , effectively
disregarding force data. This issue arises because contact force
signals are typically less discriminative, often remaining near
zero for extended periods when the arm is not in contact with
the environment during an episode. Hence, without proper care
during training, policies tend to ignore force input and rely
primarily on visual information. We empirically analyze this
effect in Sec. V-C and Fig. 9.
To mitigate this imbalance, we propose Force-Attending
Curriculum Training (FACTR), a curriculum training strategy
designed to improve the policys ability to effectively leverage
force information. FACTR systematically reduces the reliance
on visual information during training by applying operators
such as Gaussian blurring or downsampling with varying scales
to visual inputs. A scheduler gradually decreases the blurring
scale and increases the fidelity of the visual inputs. Intuitively,
this approach encourages the policy to focus more on force
input during initial training phases and gradually balances
force with visual inputs as training progresses. We ground this
intuition with a theoretical analysis on a simplified scenario
through the framework of Neural Tangent Kernels . We
explore FACTR in both the pixel space and latent space, testing
various operators and scheduling strategies. Our experiments
show that FACTR improves the success rate for unseen objects
by an average of 40.0 in four challenging contact-rich tasks
Fig. 2: Our low-cost bimanual teleoperation system with force-
feedback. The system features two actuated leader arms, two follower
arms with external joint torque sensors (such as the Franka Panda
and the KUKA LBR iiwa), a front camera and two wrist cameras.
(Fig. 1 Right)  box lifting, prehensile pivoting, fruit pick-
force-attending curriculum training.
In summary, our contributions are as follows.
Low-Cost Teleoperation with Force Feedback: We
design a low-cost bilateral leader-follower teleoperation
system with force feedback, gravity compensation, and
redundancy resolution, demonstrating a 64.7 improve-
ment in task completion rate and an 83.3 enhancement
in ease of use for teleoperation through a user study.
Force-Attending Curriculum Training: We propose
to use force feedback in policy learning and achieves better
generalization capability to object visual appearances and
geometries. Evaluated on four challenging contact-rich
policies by 40.0 compared to policies with force
information as part of input but trained without FACTR.
II. RELATED WORKS
A. Imitation Learning with Force
Imitation learning has recently experienced significant ad-
algorithms that leverage demonstrations to train robotic poli-
cies [17, 4]. Although traditional approaches primarily rely on
visual and joint position inputs, many real-world tasks require
explicit force feedback to improve stability, adaptability, and
safety [18, 9]. Recent work has applied learning methods to
train with demonstrations that incorporate gripper force or
tactile signals, resulting in policies capable of handling small,
fragile objects and performing contact-intensive tasks such as
vegetable peeling [15, 31, 14]. However, utilizing force data
from the robot arms, such as joint torques, remains under-
explored. One approach involves using an end-effector force
sensor to estimate compliance parameters or virtual position
targets through kinesthetic teaching and force tensors [10, 3].
Another method infers a 6D wrench for low-level control by
integrating torque sensing into a diffusion policy .
learning can lead to overfitting to visual information, causing
the policy to disregard force input. FoAR  explicitly predicts
contact and non-contact phases to regulate the fusion of vision
and force modalities, which requires additional data labeling.
We propose FACTR to effectively incorporate force and vision
input into policy through a curriculum, enabling policies to
leverage force for improved object generalization.
B. Low-Cost Teleoperation Systems with Force Feedback
Parallel to advances in imitation learning, significant efforts
have been made to collect low-cost and high-quality data with
hand-held grippers [27, 5] or leader-follower systems [32, 29,
25]. Hand-held grippers naturally provide force feedback to
the operator, but they do not directly record force data. Recent
work has added force sensors to hand-held grippers to address
this limitation . However, hand-held grippers are in general
limited by the kinematic differences between humans and
the robots. Although the leader-follower systems are not prone
to this limitation, they often lack force feedback, impairing
their effectiveness in contact-rich tasks. Recently, Kobayashi et
al.  implemented a bilateral leader-follower teleoperation
system where in addition to the follower following the joint
positions of the leader, the leader also gets an additional torque
if there is a difference in its joint position from that of the
follower. However, when the follower arm is in motion without
the ease of use and precision of the system . Our approach
introduces an alternative bilateral teleoperation method by
relaying only external joint torques from the follower arm back
to the leader arm, providing force feedback without impairing
operational precision.
III. FACTR LOW-COST BILATERAL TELEOPERATION
Leader-follower
teleoperation in manipulation tasks. These systems feature
kinematically equivalent leader and follower arms, allowing
intuitive control through joint space mapping, where the leaders
joint positions are mirrored as targets for the follower. This
setup lets users naturally feel the follower arms kinematic
constraints. However, most implementations lack force feed-
of the environment, which is crucial for teleoperating contact-
rich tasks . Instead, those leader arms are mostly passive,
lacking active motor torque actuation, despite being equipped
with servo motors capable of actuation. Furthermore, the
lack of active torque means the leader arms require external
structural frames and rubber bands or strings to achieve gravity
In this paper, we aim to fully leverage the servo motors in the
leader arm and gripper to achieve force-feedback enabled teleop-
eration with affordable hardware. Similarly to GELLO , our
leader arms use off-the-shelf servos and 3D-printed components,
forming a scaled-down but kinematically equivalent version of
the follower arms, as shown in Fig. 2. By actuating the servo
resolution through nullspace projection, gravity and friction
augment the teleoperation experience while still using low-cost
hardware to provide functions that are usually only available
with much more expensive teleoperation devices. Please see
Appendix VIII for a detailed Bill of Materials.
A. Force Feedback
Force feedback provides the operator with a tangible sense of
interaction with the environment, allowing more intuitive and
delicate manipulation, especially in contact-rich tasks or tasks
with limited visual feedback . We implement a control law
that relays external joint torques sensed by the follower arm
to the leader arm, allowing the operator to feel the physical
constraints experienced by the follower arm:
feedback  fKf,pext Kf,d q
where f is a scalar constant, ext is the external joint torque
sensed by the follower arm, Kf,p and Kf,d are the PD gains
for the force feedback, respectively. Here, Kf,p is calculated as
the ratio between the maximum torque of the leader and that of
the follower, and Kf,d q helps reduce oscillations in the leader
arm when the follower arm is in contact. We note that ext is
a readily available measurement in various collaborative robot
iiwa. In particular, we implement mediated force feedback by
scaling down ext with f, which has been shown to improve
the accuracy of the operation while reducing the cognitive
load of the operator . Furthermore, we highlight that our
implementation only transmits external forces from the follower
to the leader; as a result, the operator does not experience the
internal friction and inertia of the follower arm during motion,
providing a clearer perception of the environment .
In addition, we implement force feedback for the parallel-
jaw gripper. Since our servo-based gripper does not contain
an external force sensor, we utilize the present current reading
of the gripper servo to provide force feedback as follows:
where h,t is the force feedback torque sent to the gripper leader
system sets   0.1 which provides a good user experience.
B. Customizable Redundancy Resolution
For kinematic redundant manipulators, without regulating
the joint space, the manipulator tends to drift into unde-
sirable configurations under the influence of gravity during
teleoperation. Approaches like Gello  rely on mechan-
ical components, such as springs, to regularize the joint
space. However, these components introduce non-uniform,
configuration-dependent wrenches at the end-effector, resulting
in an unintuitive teleoperation experience. In addition, using
mechanical joint regularization effectively prevents the user
from setting custom joint regularization targets for redundancy
resolution. In confined-space manipulation settings, the inability
to control the joint regularization target can impair the arms
In contrast, our proposed method leverages the following
null-space projection control law to regulate joint positions ,
which stabilizes the joint-space at any user-defined desirable
posture without imposing additional end-effector wrenches,
regardless of the arms configuration:
(Kn,p (q qrest) Kn,d q)
where J is the manipulator Jacobian matrix, qrest is a user-
defined resting posture configuration, Kn,p and Kn,d are the
PD gains for the null space projection. Note that
the null-space projector.
C. Gravity Compensation
To ensure the leader arms remain stationary, allowing the
user to easily pause teleoperation, we implement gravity
compensation. This is achieved by modeling the dynamics
of the leader arm and computing the joint torques required to
counteract dynamic forces using the recursive Newton-Euler
algorithm (RNEA) for real-time inverse dynamics .
grav  M(q)q  C(q, q)q  g(q)  RNEA(q, q, q)
where M(q) is the mass (or inertia) matrix, C(q, q) is the
Coriolis and centrifugal matrix, and g(q) is the gravity vector.
D. Additional Compensation and Controls
To reduce the perceived friction in the leader arm during tele-
prevent users from exceeding joint limits of the follower arm in
order to respect the workspace of the follower arm. Finally, for
bi-manual follower arms, the system uses Riemannian Motion
Policies  for dynamic obstacle avoidance between the two
follower arms. Please refer to Appendix IX for more details.
E. Overall Control Law for the Leader Arm
In summary, the control torques are defined as follows:
feedback relays external forces from the follower arm
back to the leader arm, allowing the operator to sense the
geometric constraints of the environment.
null resolves kinematic redundancy by regulating the
joints at a user-defined rest posture in the null-space.
grav provides gravity compensation for the leader arm.
friction compensates for the leader arm joint frictions to
enable smoother teleoperation.
limit prevents the joints of the leader arm from violating
the joint position limits of the follower arm.
The resulting combined torque applied to the servo motors
of the leader arm is defined as follows:
feedback  null  grav  friction  limit
Fig. 3: Customizable Joint Regularization [Left] Without the
flexibility to define the resting joint configuration qrest, the arms
reachability is restricted, leading to collisions in confined spaces.
[Right] Our leader arm allows the user to define custom resting joint
IV. FACTR: FORCE-ATTENDING CURRICULUM TRAINING
Naively incorporating robot force data into policy learning
does not necessarily ensure policy improvement. Contact force
signals often provide limited discriminative information for
the policy, as it remains close to zero for significant periods
when the arm is not interacting with the environment during an
episode. As a result, the policy tends to disregard force input
and rely predominantly on visual information, as empirically
analyzed in Sec. V-C and Fig. 9.
To fully leverage the robot force data collected from our
teleoperation system, we introduce Force-Attending Curriculum
Training (FACTR), a training strategy designed to effectively
integrate force information into policy learning. FACTR applies
operators like Gaussian blur or downsampling to corrupt visual
throughout training. The curriculum intuitively encourages
contribution from the force modality at the start of training. We
ground this intuition with a theoretical analysis on a simplified
scenario through the framework of Neural Tangent Kernels .
In this section, we first present the base policy model used for
learning from teleoperated demonstrations, and then motivate
and describe FACTR, our curriculum training approach. Our
overall method is summarized in Algorithm 1 and Fig. 4.
A. Problem Statement and Base Model
We consider a policy (  ) that produces a chunk of
future actions of length k qt:tk (joint positions) given (i) a
visual observation It (image at time t), and (ii) an external
joint torque reading t. Our goal is to learn  via behavior
cloning (BC) from a dataset of expert trajectories D. Each
trajectory in D comprises tuples
, where qt is the
ground-truth (expert) joint position target at time t. We let
next k time steps. The loss is defined by:
where qt:tk are the experts future joint position targets and
Transformer
Vision Encoder
External
Joint Torque
Force Encoder
Pixel Space Curriculum
Latent Space Curriculum
n  Max Iter
n  Max Iter
Fig. 4: FACTR allows our policy to better integrate force information without overfitting to visual information, resulting in better generalization
to objects with unseen visual appearances and geometries. Our policy takes as inputs RGB images I and external joint torque , which are
then tokenized by a vision encoder and a force encoder before fed into an action transformer to regress joint position targets qt:tk. FACTR
applies a blurring operator of scale n in either pixel or latent space, initialized at a large value then gradually decreased through the training.
Our policy  is based on an encoder-decoder transformer
that integrates vision and force modalities. Visual observations
and force readings are converted into tokens, fed to the encoder,
then decoded into action tokens through cross attention.
A pre-trained vision transformer (ViT) [7, 6] is used to
encode an input image It into a sequence of vision tokens
t RMvd for some number of tokens Mv and embedding
dimension d. An MLP-based force encoder is applied to the
joint torque t, resulting in a single force token: zF
The tokens are concatenated to form the model input:
R(Mv1)d.
self-attention and feed-forward layers:
R(Mv1)d.
This yields the encoded vision and force tokens.
For the decoder, we introduce k action tokens, A Rkd.
A transformer decoder Dec refines these tokens through self-
attention and cross-attention to HE
During cross attention, each action token attends to both vision
and force representations. If we split HE
t into its vision (V)
and force (F) parts, the cross-attention weights for each layer
l can be decomposed as follows. For simplicity of notation,
assume these weights are already averaged over multiple heads:
For the vision part:
(A(l)WQ(l)) (HE(l)
For the force part:
(A(l)WQ(l)) (HE(l)
These (l)
V and (l)
F measure how strongly each action token
attends to vision vs. force tokens at layer l, and will be the
main source of analysis in Sec. V-C.
t to action space,
which represents joint position targets for the follower arm:
where da is the dimension of the action space. Substituting
Appendix X for the detailed policy architecture and training
hyperparameters.
B. Force-Attending Curriculum
Through experiments, as shown in Sec. V-C and Fig. 9,
we found that naively concatenating force data to the policy
observation during training often results in policies that neglect
force input, failing to leverage force input to the fullest extent.
To address this, we employ a curriculum that gradually unveils
detailed visual information, encouraging the model to learn
to utilize force first. Specifically, we define two operators:
P (I, n) for the pixel space, and L(z, n) for the latent
of a Gaussian kernel or the kernel size of a max pooling
operator) that is updated over the course of training for N
total gradient steps. During training, we apply the pixel-space
operator P to image It or L to visual latent tokens zV
close in the metric space, thus encouraging more contribution
from the force modality, particularly at the start of the training.
Consider the limit  , each visual input converges to
Algorithm 1 Force-Attending Curriculum Training (FACTR)
steps N; Pixel-space operator P (I, ); latent-space operator
L(z, ); Scheduler defining n for n  1 . . . N
chunking transformer
n Scheduler(n)
get current scale
if pixel-space curriculum then
vision tokens
if latent-space curriculum then
force token
approximately the same tensor. Hence, the model can only
learn a single global output for all visual inputs. Thus, at the
early stage of the curriculum, the gradient updates focus more
on using the force information and updating the force encoder
to maximally differentiate between inputs.
C. Curriculum Operators
We consider two types of operators: Gaussian blur and
downsampling.
For the Gaussian blur, we define the 2D kernel G as:
The operator P (I, ) applies this kernel using the 2D
convolution operator :
P (I, )  I G
For the 1D Gaussian blur, the kernel g is defined as:
The operator L(zV , ) similarily applies this kernel using 1D
L(zV , )  zV g
For downsampling, we use MaxPool followed by nearest
interpolation. In 2D, the pixel-space operator P (I) is:
P (I)  NearestInterp(MaxPool2D(I))
In 1D, the latent-space operator I(zV ) is the same except
that a MaxPool1D is used.
By gradually reducing n, the curriculum ensures that the
model focuses first on force tokens, and then incorporates visual
information in the later stage of the training. This produces a
policy  that more robustly fuses force and vision for control,
alleviating the issue of overfitting to the vision modality.
Theoretical Analysis:
We analyze the effects of Gaussian blur
as an example of a curriculum operator through the framework
of Neural Tangent Kernels (NTK) . Although we consider
a simple two-layer model here, the intuition applies to more
complex architectures like vision transformers (ViTs), which
have a more sophisticated NTK . The formal theoretical
analysis is presented in Appendix VII.
D. Curriculum Schedulers
Over the course of training (indexed by n  1, . . . , N), we
adjust n via a scheduler to control the information released
from the visual branch. Given a initial scale 0, we consider
the following schedulers:
Decay Type
Scheduler Equation
Constant
Exponential
dsteps > 1
to 0 for certain gradient steps, and adjust the decay formula
to account for this duration. The rationale behind this step is to
warm-up the randomly initialized force encoder with relatively
low visual information.
V. EVALUATION
A. Experimental Setup
We setup four contact-rich tasks, which are illustrated in
Fig. 5 along with the training and testing objects of various
shapes and visual appearances. For all tasks, we use Franka
Panda arm(s) with OpenManipulator-X gripper(s). Each task
uses either a front ZED2 camera or wrist cameras mounted
near the grippers with RGB observations. We describe the tasks
details and the success criteria below.
Box Lift: A bi-manual task where two arms lift a box and
balance in the air for at least two seconds, using the front
camera and external joint torque from the arms.
Non-Prehensile Pivot: The robot flips an item by pivoting
it against the corner of a fixture until the item is rotated
by 90and can stand stably, using the front camera and
external joint torque from the arm.
Fruit Pick and Place: The robot grasps a soft and delicate
fruit and places it in a bowl, using the wrist camera and
the external joint torque of the gripper.
Rolling Dough: The robot continuously rolls the dough to
shape it into a cylinder for at least 8 seconds, using the
front camera and the external joint torque of the arm.
B. Teleoperation Evaluation
We compare our leader-follower teleoperation system, which
includes our leader arm with force feedback, gravity compen-
Fig. 5: Tasks. We evaluate our leader-follower teleoperation system
and autonomous policies trained with FACTR on four contact-rich
tasks. These tasks are challenging as they require the robot to perceive
and respond to the force feedback as it manipulates objects with unseen
visual appearances and geometries.
follower baseline system with mechanical joint regulation,
similar to . We summarize our results in Fig. 7.
Our experiments show that our system allows users to
complete tasks with 64.7 higher task completion rate, 37.4
reduced completion time, and 83.3 improvement in the
subjective ease of use metrics.
We observe that for tasks that require continuous contact
between the arm and an object, such as non-prehensile pivoting
and bimanual box lifting, the un-actuated teleoperation system
often causes the follower arm to lose contact with the object.
This occurs because of the absence of force feedback, which
prevents the user from perceiving the environments geometric
constraints through the leader arms. As a result, maintaining
continuous contact with the object becomes challenging.
For the un-actuated system, the follower arm frequently
exceeds its joint velocity limits when moving under continuous
contact. This occurs because the operator can easily maneuver
the leader arms in ways that cause significant deviations
between the leader and follower joint positions, especially
when the follower arm is in contact with the environment.
When contact is lost, the resulting large joint-space error causes
the PID controller to generate large torques, causing abrupt
movements that exceed the velocity limits. On the other hand,
our systems force feedback renders geometric constraints of
the environment for the operator through the leader arms,
ACT (VisionForce)
ACT (Vision-Only)
Train Objects
Test Objects
Success Rate
Box Lift
Fruit Pick-Place
Rolling Dough
Box Lift
Fruit Pick-Place
Rolling Dough
Success Rate
Fig. 6: FACTR leads to better object generalization.
preventing the operator from moving the leader arms too far
away from the follower arms during environment contacts.
C. Policy Evaluation
Questions. In our real-world evaluation, we seek to address
the following research questions regarding FACTR:
How does FACTR perform compared to baseline ap-
proaches that do not use force feedback and ones that use
force feedback without FACTR?
How do different curriculum parameters affect policy
performance?
Training and Evaluation Protocol. We collected 50 demon-
strations with our teleoperation system. We trained each method
with the same hyperparameters, where details can be found in
the Appendix X. We compare the following methods:
ACT (Vision-Only) : Action Chunking Transformer
which only takes in visual observation.
ACT (VisionForce) [13, 14]: Action Chunking Trans-
former which takes in both visual and force observation,
but trained without a curriculum.
FACTR (Ours): Action Chunking Transformer trained with
Force-Attending Curriculum. For each task, we train a
latent space curriculum with the Guassian Blur operator
and linear scheduler. We discuss more detailed ablations
on the curriculum in Sec. V-D.
For each object in each task, we evaluated 5-10 trials. We
present the average success rate for training and testing objects,
respectively. Detailed evaluation results for each object can be
found in the Appendix XII.
FACTR leads to better generalization. We present our
main quantitative results in Fig. 6. All the policies perform
similarly on the train objects for most tasks, except for the
rolling dough task, where the vision-only policy smashes the
dough without any rolling actions and fails completely. Note
that the visual observations are hard to distinguish during
the oscillatory rolling motions, while the force signals form
a) Completion Rate
b) Completion Time (sec)
c) Ease of Use
Un-Actuated Leader-Follower
Box Lift
Pick-Place Rolling
Box Lift
Pick-Place Rolling
Box Lift
Pick-Place Rolling
Fig. 7: User study. FACTR teleoperation system allows users to complete tasks with significantly higher success rate, using less time, and
they subjectively found our system to be easier to use.
Fig. 8: We visualize the external joint torque norm of the Franka arm
for a collected trajectory. Blue highlights indicate pre-contact phases,
while purple and green mark torque peaks and troughs at the doughs
left and right ends, respectively. The oscillatory torque pattern helps
the policy distinguish observations despite similar visual inputs.
a corresponding oscillatory pattern, as shown in Fig. 8; this
distinctive torque pattern helps policies with force input to
complete the task.
For the test objects, the vision-only policy achieves a success
rate of 21.3 on average, which is significantly worse than
policies incorporating force. Without a curriculum, policies
naively incorporating force achieve a success rate of 61.2,
while FACTR achieves a success rate of 87.5, which shows
that FACTR leads to significantly better generalization to novel
objects. We hypothesize that the force information provides
important signals for mode switching at moments such as when
the robots get into contact with the box in the lifting task and
when the object is grasped in the fruit pickup task.
Policies with FACTR learns to identify mode switching.
To better understand the policies trained with FACTR. We
visualize the attention behavior during policy training and
inference. Specifically, we visualize the cross attention of the
action tokens to the memory tokens denoted as (1)
for the first layer of the decoder, where (1)
defined in Sec. IV.
During policy rollout, we visualize the average cross attention
of the action tokens to the force or vision tokens of the first
decoder layer as shown in Fig. 9. FACTR learns to attend to
force more during task execution. For example, in the box
lifting task, attention to force outweighs that of vision as the
arms contact the box, signaling a mode switch. While without
the curriculum, the policy does not pay enough attention to
FACTR leads to better recovery behavior. Another notable
observation is that FACTR also facilitates recovery behavior.
per object. A trial begins when the policy successfully lifts
the box for the first time; we then knock the box down and
assess the second attempt. As shown in Table I, all policies
maintain nearly 100 recovery success on training objects.
drops significantly from 31.7 on the first attempt to 13.3
on the second. In contrast, force-attending policies maintain
similar success rates across both attempts.
We observe that vision-only policies often remain static after
the box is knocked down, failing to retry. We hypothesize that
this occurs because the vision-only policy overfits to training
training distribution. In contrast, FACTR policies detect loss of
contact through external joint torque readings, which revert to
pre-lift values when the object is dropped. Since our FACTR
policies effectively attend to force input, they successfully
recover to a pre-lift state and attempt the task again.
Train Objects
Test Objects
ACT (Vision-Only)
ACT (VisionForce)
TABLE I: Evaluation of recovery behaviors for box lifting.
D. Ablations on Curriculum
To further validate the significance of a curriculum, we
trained models with fixed n across training. Moreover, to
ablate on pixel space and latent space curriculum, and different
scheduler and operator choices, we train policies on different
FACTR (Ours)
Attention
No FACTR (Baseline)
Attention
Time Steps
Time Steps
Fig. 9: Policies trained with FACTR learns to identify mode switching. We visualize the average cross attention of the action tokens to
the force or vision tokens of the first decoder layer during policy rollout. [Left] Without the curriculum, the policy does not pay enough
attention to force, and either fails to lift or balance the novel boxes. [Right] FACTR learns to attend to force more to complete the task. For
combination of these parameters. We choose the task of
the ablations. We evaluate only on the five test objects for five
trials each, since they are more indicative of policy performance
than train objects. The results are presented in TABLE II.
Pixel Space
Latent Space
Downsample
Downsample
Constant
TABLE II: Curriculum ablation.
Fixed-Scale Operator vs. Curriculum. We found that
performance with a curriculum of decaying smoothing performs
better than a fixed curriculum across all tasks. We hypothesize
that to enable better performance, the final policy needs to
take in the fully unblurred vision information. Through the
On the other hand, with fixed smoothing, even though policy
may not overfit to visual information, it cannot extract the
necessary details from unblurred vision to complete the tasks.
Comparisons with other scheduler parameters. We further
compare policies trained with either pixel space or latent space,
two operators (Gaussian blur and downsample) as defined in
Sec. IV-C, and four schedulers (linear, cosine, exponential,
and step) as defined in Sec. IV-D. However, we do not find a
uniform advantage or disadvantage for any set of parameters.
The results suggest that FACTR is relatively robust to different
sets of curriculum parameters.
VI. CONCLUSION AND LIMITATIONS
We introduced FACTR, a curriculum approach to train force-
based policies to improve performance and object general-
ization in contact-rich tasks. FACTR leverages a blurring
operator with decreasing scales on the visual information
throughout training. This encourages the policy to leverage
force input at the beginning stages of training, preventing
the problem where the policy overfits to visual input and
thus neglects force input. This approach was demonstrated
through a series of experiments on the following tasks: box
task completion rates and generalization to unseen object
appearances and geometries. Additionally, our teleoperation
feedback and gravity compensation, was shown to provide
a more intuitive user experience, as evidenced by higher task
completion rates and user satisfaction in our studies.
While FACTR demonstrates significant improvements in
force-based policy learning for contact-rich tasks, it has limita-
tions. First, the precision of the external joint torque sensors
in our follower arm is limited. This limitation can particularly
affect tasks that involve subtle force adjustments during fine-
grained manipulation since the torque readings can be too noisy
to be used effectively. Future work could explore integrating
high-resolution tactile sensors or haptic gloves to enhance
feedback precision and improve overall system performance.
torque sensors in the follower arms. Future work can explore
adapting our system for an arm mounted with an end-effector
force-torque sensor. Third, the effectiveness of our curriculum
learning approach can be influenced by several hyperparameters,
such as the choice of the blurring operator and scheduling
strategies. These parameters can be highly task-dependent,
requiring extensive tuning for different applications. Developing
adaptive or self-tuning curriculum strategies could help mitigate
this issue by dynamically adjusting hyperparameters based on
task-specific requirements. Addressing these limitations could
further enhance FACTRs applicability and robustness across
a broader range of contact-rich manipulation tasks.
ACKNOWLEDGMENTS
We thank Arthur Allshire, Andrew Wang, Mohan Kumar
thank Tiffany Tse, Ray Liu, Sri Anumakonda, Sheqi Zhang
with teleoperation. This work is supported in part by ONR
and AFOSR FA9550-23-1-0747.
REFERENCES
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R
with an infinitely wide neural net. Advances in neural
information processing systems, 32, 2019.
Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy
transformers reason with abstract symbols? International
Conference on Learning Representations, 2023.
Claire Chen, Zhongchun Yu, Hojung Choi, Mark
informed actions from kinesthetic demonstrations for
dexterous manipulation. arXiv preprint arXiv:2501.10356,
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
fusion policy: Visuomotor policy learning via action
diffusion. In Robotics: Science and Systems, 2023.
Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau,
Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and
Shuran Song. Universal manipulation interface: In-the-
wild robot teaching without in-the-wild robots. Robotics:
Science and Systems, 2024.
Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and
Abhinav Gupta. An unbiased look at datasets for visuo-
motor pre-training. In Conference on Robot Learning,
pages 11831198. PMLR, 2023.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. In Interna-
tional Conference on Learning Representations, 2020.
Zihao He, Hongjie Fang, Jingjing Chen, Hao-Shu Fang,
and Cewu Lu.
for contact-rich robotic manipulation.
arXiv preprint
Neville Hogan.
Impedance control: An approach to
manipulation. In 1984 American control conference, pages
Yifan Hou, Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen
Shuran Song.
Adaptive compliance policy: Learning
approximate compliance for diffusion guided control.
arXiv preprint arXiv:2410.09309, 2024.
Arthur Jacot, Franck Gabriel, and Clement Hongler.
Neural tangent kernel: Convergence and generalization
in neural networks.
Advances in neural information
processing systems, 31, 2018.
Oussama Khatib. A unified approach for motion and
force control of robot manipulators: The operational space
formulation. IEEE Journal on Robotics and Automation,
Masato Kobayashi, Thanpimon Buamanee, and Takumi
Kobayashi. Alpha- and bi-act are all you need: Im-
portance of position and force information control for
imitation learning of unimanual and bimanual robotic
manipulation with low-cost system.
arXiv preprint
Kelin Li, Shubham M Wagh, Nitish Sharma, Saksham
robotic manipulation via immersive vr. arXiv preprint
Wenhai Liu, Junbo Wang, Yiming Wang, Weiming Wang,
and Cewu Lu.
learning with force-motion capture system for contact-rich
manipulation. arXiv preprint arXiv:2410.07554, 2024.
Kevin M. Lynch and Frank C. Park. Modern Robotics:
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
matters in learning from offline human demonstrations
for robot manipulation. In Conference on Robot Learning,
pages 16781690. PMLR, 2022.
Matthew T Mason. Compliance and force control for
computer controlled manipulators. IEEE Transactions on
Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu,
Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh,
Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck
robot learning environments. IEEE Robotics and Automa-
tion Letters, 8(6):37403747, 2023.
G. Morel and S. Dubowsky.
The precise control of
manipulators with joint friction: a base forcetorque sensor
method. In Proceedings of IEEE International Conference
on Robotics and Automation, volume 1, pages 360365
H. Olsson, K.J. Astrom, C. Canudas de Wit, M. Gafvert,
and P. Lischinsky.
Friction models and friction com-
pensation. European Journal of Control, 4(3):176195,
Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio,
and Giovanni Russo.
Safe haptic teleoperations of
admittance controlled robots with virtualization of the
force feedback. arXiv preprint arXiv:2404.07672, 2024.
William Peebles and Saining Xie. Scalable diffusion mod-
els with transformers. In Proceedings of the IEEECVF
international conference on computer vision, pages 4195
Nathan D. Ratliff, Jan Issac, and Daniel Kappler. Rie-
mannian motion policies. CoRR, abs1801.02854, 2018.
Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar
Deepak Pathak. Bimanual dexterity for complex tasks.
In 8th Annual Conference on Robot Learning, 2024.
Bruno Siciliano, Oussama Khatib, and Torsten Kroger.
Springer handbook of robotics, volume 200. Springer,
Shuran Song, Andy Zeng, Johnny Lee, and Thomas
Funkhouser. Grasping in the wild: Learning 6dof closed-
loop grasping from low-cost demonstrations.
Robotics and Automation Letters, 5(3):49784985, 2020.
Weiyao Wang, Du Tran, and Matt Feiszli. What makes
training multi-modal classification networks hard?
Proceedings of the IEEECVF conference on computer
vision and pattern recognition, pages 1269512705, 2020.
Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and
Pieter Abbeel. Gello: A general, low-cost, and intuitive
teleoperation framework for robot manipulators. In 2024
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), pages 1215612163. IEEE, 2024.
Yansong Wu, Zongxie Chen, Fan Wu, Lingyun Chen,
Liding Zhang, Zhenshan Bing, Abdalla Swikir, Alois
diffusion policy for precise tactile manipulation. arXiv
preprint arXiv:2409.11047, 2024.
William Xie, Stefan Caldararu, and Nikolaus Correll. Just
add force for delicate robot policies. CoRL 2024 Workshop
on Mastering Robot Manipulation in a World of Abundant
Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea
Finn. Learning fine-grained bimanual manipulation with
low-cost hardware. Robotics: Science and Systems, 2023.
Qi Zhu, Jing Du, Yangming Shi, and Paul Wei. Neu-
robehavioral assessment of force feedback simulation in
industrial robotic teleoperation. Automation in Construc-
APPENDIX
VII. ANALYSIS OF FACTR FROM NEURAL TANGENT
KERNEL (NTK) PERSPECTIVE
In this section, we aim to give a sketch of a more theoretical
argument for why operators like Gaussian blur or downsampling
in FACTR could help the policy attending to force input. We
will first briefly introduce Neural Tangent Kernel (NTK), and
then use it a theoretical framework to analyze the effects of
Gaussian blur as an example curriculum operator.
A. Preliminaries on Neural Tangent Kernel (NTK)
The Neural Tangent Kernel (NTK) is a theoretical framework
used to analyze the behavior of neural networks, particularly in
the limit of infinite width [11, 1]. For a neural network f(x)
with parameters , the NTK is defined as:
k(xi, xj)  f(xi), f(xj),
where f(x) is the gradient of the networks output with
respect to its parameters , and , denotes the inner product.
In the infinite width limit, the NTK becomes deterministic and
remains constant during training. Assuming the parameters
are initialized from a Gaussian distribution, the NTK k(xi, xj)
converges to a deterministic kernel k(xi, xj) given by:
k(xi, xj)  EN(0,I) [f(xi), f(xj)] ,
where the expectation is taken over the Gaussian initialization
The NTK k(xi, xj) is a similarity function: if k(xi, xj) is
close. If we have n training points (xi, yi), k defines a positive
semi-definite (PSD) kernel matrix K Rnn
where each entry
Kij  k(xi, xj).
network with gradient flow on the squared error, we precisely
know the model output at any point in training. At time t, the
training residual is given by:
ft(x) y  eKt(f0(x) y),
where  is the learning rate, K is the kernel matrix, and
f0(x) is the initial model output. This equation shows that
the residual error decays exponentially with a rate determined
by the kernel matrix K.
B. Analyzing the Effects of Gaussian Blur with NTK
We analyze the effects of Gaussian blur as an example
of a curriculum operator. While we consider a simple two-
layer model here, the intuition applies to more complex
architectures like vision transformers (ViTs), which have a
more sophisticated NTK .
Consider a model where the input x is first convolved with a
Gaussian kernel K before being passed through the network,
where  is the standard deviation of the Gaussian kernel. The
output of the model is:
f(x)  W T (K x),
where K x is the convolution of x with a Gaussian
kernel K. Assuming that the model has infinite width and the
parameters in W are initialized from a Gaussian distribution,
the NTK for this model is:
k(x, x)  EW N(0,I) [W f(x), W f(x)]
EW N(0,I)K x, K x.
This is the dot product between the convolved inputs K x
and K x. As  increases, the Gaussian convolution K x
acts as a low-pass filter, attenuating high-frequency components
in the input x. This causes the convolved inputs K xi and
K xj to become more similar for any pair of inputs xi and
xj. In the extreme case where  , K xi K xj for
all xi, xj, and the NTK k(xi, xj) approaches a constant value.
This implies that the kernel matrix K becomes approximately
an all-ones matrix, scaled by the magnitude of the convolved
In our curriculum, we decrease , where the model is first
exposed to smoother visual data before gradually transitioning
to the original unsmoothed data. At the beginning of the
robust features from visual inputs. As  decreases to small ,
the NTK retains discriminative power, allowing the model to
learn fine-grained features from visual inputs. . This gradual
increase in complexity can help the model learn more effectively
by avoiding overfitting to the high-frequency variations in visual
inputs in the early stages of training.
More rigorously, consider the limit  , each blurred
input K xi converges to the same vector . Hence,
k(xi, xj)  ,  2,
and the NTK matrix K becomes
This matrix is rank-1, with largest eigenvalue   n2
(for n training points) and corresponding eigenvector v
Recall that in the infinite-width NTK regime, the training
residual r satisfies
r(t)  eKt r(0),
where r(0)  f0(x) y is the initial residual. Decompose
r(0) into components parallel and perpendicular to v:
r(0)  r r,
with rv and r v  0.
Since K is rank-1,
eKt r(0)  r et r.
The parallel component decays exponentially at rate , while
the perpendicular component is unchanged. This effectively
learns a single global scalar for all inputs, reducing mean-
squared error by matching the average label but losing
discriminative power. Thus, at the early state of the curriculum,
the gradient updates will focus on using the force information
and updating the force encoder to maximally differentiate
between inputs.
VIII. COST ANALYSIS OF OUR TELEOPERATION SYSTEM
WITH FORCE FEEDBACK
Please see Table III for a detailed Bill of Materials and
breakdown of the cost to create one leader arm and leader
gripper as part of our teleoperation with force feedback system.
This is accurate pricing as of the paper release.
Quantity
Dynamixel XM430-W210-T
Dynamixel XC330-T288-T
U2D2 Control PCB
12V 20A Power Supply
FPX330-S102 Servo Bracket
Polymaker PLA PRO Filament
U2D2 Power Hub Board
14AWG Cable
34 Bearing
TABLE III: We present the bill of materials of one leader arm
teleoperation device with force feedback. The total cost is around
IX. ADDITIONAL CONTROL LAWS FOR OUR
TELEOPERATION SYSTEM
Here we define the additional control laws for our teleoper-
ation system with force feedback: friction compensation and
joint limit avoidance.
A. Friction Compensation
Joint friction introduces resistive forces that impair re-
reducing the systems intuitive feel for the operator .
Friction also increases the physical effort required to back-
drive the motor, leading to operator fatigue during prolonged
use. To this end, we use a dynamic friction model to explicitly
compensate for static, Coulomb, and viscous friction .
We mitigate the effects of static friction by introducing a
which generates micro-vibrations that prevent the motors
from settling into static friction states, thereby enabling
smoother transitions from rest to motion . The static friction
compensation torque ss is as follows:
s cos( t
if q(i) < q(i)
otherwise
where s is a calibrated static friction coefficient and f is the
control loop frequency, set to 500 Hz.
We also account for kinetic friction by compensating for
Coulomb friction and viscous friction as follows :
c sgn( q(i))  (i)
where c is the Coulomb friction coefficient and v is viscous
friction coefficient.
The total friction compensation friction is the sum of the
static friction ss and kinetic friction ks terms.
B. Joint Limit Avoidance
We implement the following artificial potential-based control
law to prevent the operator from making the leader arm go
beyond the joint limits of the follower arm:
(q(i)q(i)
min )2 ,
q(i) < q(i)
maxq(i))2 ,
q(i) > q(i)
otherwise
limit  q(i)U(q(i))
where U(q(i)) is the repulsive potential function, q is the
safety margin, and  is the scaling factor.
C. Bi-manual Follower Arms Control with Dynamic Collision
Avoidance
Most existing bi-manual teleoperation systems with a leader-
follower setup command the follower arms by directly setting
the joint position targets to the current joint positions of the
leader arm. Instead, we employ a Riemannian Motion Policy
(RMP)  implemented in Isaac Lab , where the RMP
dynamically generates joint-space targets for the follower arms
that best match the current joint positions of the leader arms
while incorporating real-time collision avoidance. Our system
prevents the follower arms from colliding with one another
or with external obstacles, such as the table, regardless of the
operators actions.
X. BEHAVIOR CLONING POLICY ARCHITECTURE AND
TRAINING HYPERPARAMETERS
Our behavior cloning policy takes as input a RGB image
and current hand joint angles (proprioception). We obtain
tokens for the image observation via a ViT  and a token for
joint proprioception via a linear layer. The weights of ViT is
initialized from the Soup 1M model from . The tokens then
pass through action chunking transformer, an encoder-decoder
space is the absolute joint angles of the two arms for box lift,
the absolute angles of a single arm for non-prehensile pivot
and rolling dough, and the absolute angles of a arm and the
gripper for fruit pick and place. A key decision that greatly
improves policy generalization is to exclude current arm joints
from the proprioception. Intuitively, this may force the model
to extract object information from image observations, rather
than overfitting to predict actions close to current arm states.
Hyperparameter
Behavior Policy Training
Optimizer
Base Learning Rate
Weight Decay
Optimizer Momentum
Batch Size
Learning Rate Schedule
Cosine Decay
Total Steps
Warmup Steps
Augmentation
RandomResizeCrop
RTX4090 (24 gb)
Wall-Clock Time
2-6 hours
Visual Backbone ViT Architecture
Patch Size
MHSA Heads
Hidden Dim
Class Token
Positional Encoding
Action Chunking Transformer Architecture
Encoder Layers
Decoder Layers
MHSA Heads
Hidden Dim
Feed-Forward Dim
Positional Encoding
Action Chunk
TABLE IV: Policy Architecture and Training Hyperparameters
We list key hyperparameters for our behavior policy training
Table IV. In general, we are able to obtain well-performing
policies with 20000-50000 gradient steps and 2-6 hours of
wall-clock time training on a RTX4090.
XI. ADDITIONAL EXPERIMENTS
Adaptive Layer Norm as an Alternative to a Curriculum
As an alternative to a curriculum, we experimented with
Adaptive Normalization Layers  on the action tokens
conditioned on the force input to improve the force conditioning
(AdaNorm in Tab. V). However, we found that it creates
instability in training and leads to overfitting.
Data Augmentation as alternative to curriculum. We run
10 data augmentation experiments varying noise probabilities
and noise levels applied to vision input. The best of these
trained with lower augmentation levels perform well on train
objects but worse than FACTR on test objects, likely because
low levels of augmentations fail to facilitate proper attention to
force. Higher levels of augmentation policies do not perform
well across train and test sets, likely because they fail to
FACTR AdaNorm NoiseAug
Train ()
TABLE V: Pivot task training and testing performance.
Box Lift
Rolling Dough
ACT (Vision-Only)
FACTR (Ours)
TABLE VI: Policy evaluation on unseen objects across 3 tasks.
Fig. 10: We more than doubled the testing set size across 3 tasks.
leverage vision altogether. Our curriculum approach eliminates
such tradeoff between vision and force, allowing the policy to
attend properly to both force and vision.
More test objects. We doubled the number of testing objects
used in three of our tasks, visualized in Fig. 10 and updated
the success rate comparison figure in Tab. VI. By conducting
testing on more unseen objects and comparing with the best
performing baseline (Bi-ACT), we observe that our method
yields an improved generalization performance.
XII. DETAILED QUANTITATIVE RESULTS
We present the detailed evaluation results for each task in
TABLE VII, VIII, IX, and X.
Train Avg
Test Avg
ACT (Vision-Only)
ACT (VisionForce)
TABLE VII: Comparison of methods for Box Lift task.
Train Avg
Test Avg
ACT (Vision-Only)
ACT (VisionForce)
TABLE VIII: Comparison of methods for Non-Prehensile Pivot task.
Train Avg
Test Avg
ACT (Vision-Only)
ACT (VisionForce)
TABLE IX: Comparison of methods for Fruit Pick-Place task.
Train Avg
Test Avg
ACT (Vision-Only)
ACT (VisionForce)
TABLE X: Comparison of methods for Rolling Dough task.
