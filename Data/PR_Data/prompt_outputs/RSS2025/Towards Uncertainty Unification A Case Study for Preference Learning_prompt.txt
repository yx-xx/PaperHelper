=== PDF文件: Towards Uncertainty Unification A Case Study for Preference Learning.pdf ===
=== 时间: 2025-07-21 14:28:29.071130 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Towards Uncertainty Unification:
A Case Study for Preference Learning
Shaoting Peng, Haonan Chen, Katherine Driggs-Campbell
University of Illinois Urbana-Champaign
AbstractLearning human preferences is essential for human-
robot interaction, as it enables robots to adapt their behaviors
to align with human expectations and goals. However, the
inherent uncertainties in both human behavior and robotic
systems make preference learning a challenging task. While
probabilistic robotics algorithms offer uncertainty quantifica-
underexplored. To bridge this gap, we introduce uncertainty
unification and propose a novel framework, uncertainty-unified
preference learning (UUPL), which enhances Gaussian Process
(GP)-based preference learning by unifying human and robot
uncertainties. Specifically, UUPL includes a human preference
uncertainty model that improves GP posterior mean estimation,
and an uncertainty-weighted Gaussian Mixture Model (GMM)
that enhances GP predictive variance accuracy. Additionally, we
design a user-specific calibration process to align uncertainty
representations across users, ensuring consistency and reliability
in the model performance. Comprehensive experiments and user
studies demonstrate that UUPL achieves state-of-the-art perfor-
mance in both prediction accuracy and user rating. An ablation
study further validates the effectiveness of human uncertainty
model and uncertainty-weighted GMM of UUPL. Video and code
are available at
I. INTRODUCTION
Uncertainty is pervasive in human-robot interaction (HRI),
stemming from both robotic and human sources. On the robot
imperfect control models, unpredictable human behavior, and
many other aspects . Properly modeling these uncertainties
enhances HRI efficiency and safety, while neglecting them
can lead to significant risks . Established probabilistic
Markov decision processes (POMDP), as well as modern
methods leveraging large language models for uncertainty esti-
mation [3, 4], enable effective quantification and management
of robot uncertainty, improving system performance.
In contrast, human uncertainty is inherently more chal-
lenging to quantify due to the stochastic nature of human
decision-making. Psychological and cognitive science theo-
frameworks for modeling this uncertainty. However, when
humans interact with autonomous agents, their uncertainty
becomes even more nuanced, shaped by subjective factors
like trust, familiarity with the technology, and individual risk
aversion . Additionally, dynamic interactions with robots
during HRI tasks introduce more variability, as users adapt
their behaviors based on perceived system performance .
Despite these complexities, accounting for human uncertainty
Hmm I think
be slightly better, but
Im not so sure
uncertainty and integrate
into my estimation!
Uncertainty-unied Model
Do you prefer
Uncertainty-averse Model
preference data
Learned trajectory w
unied uncertainty
trajectory
wo unied
uncertainty
Intuition on uncertainty unification for preference learning.
Imagine a robot inferring Alices (a human user) ideal trajectory for passing
a cup of coffee above a table using preference learning. In one sample pair,
trajectory x(1) poses a risk of spilling coffee on the keyboard, while trajectory
x(2) risks spilling it on the headphones. The keyboard and headphones
are both valuable to Alice, so she responds that she weakly prefers x(2)
with hesitation, reflecting her uncertainty in the decision. An uncertainty-
averse model ignores this nuance, potentially learning a suboptimal and
undesirable trajectory (e.g., still passing above the headphones). In contrast,
an uncertainty-unified model incorporates Alices expressed uncertainty into
its uncertainty-aware framework, enabling it to learn an ideal trajectory that
aligns with her true preferences.
is crucial for optimizing both user experience and system
To date, research on robot and human uncertainties has
largely progressed in isolation. This misalignment between
two uncertainties can lead to inefficiencies, increased cog-
nitive load, and even failures in critical decision-making
scenarios like physical assistance and collaborations [8, 9].
Integrating these perspectives is vital for improving system
As this challenge is both technically complex and essential
for advancing trustworthy human-robot partnerships, we argue
a unified framework is necessary to tackle the misalignment.
human uncertainty into robotic uncertainty-aware algorithms,
enabling outcomes informed by both human and robot uncer-
tainties. To explore the potential of uncertainty unification, we
focus on preference learning, a key HRI domain where robots
learn from human feedback through comparative judgments.
As suggested by Laidlaw and Russell , this domain is
well-suited for our study due to its rich uncertainty dynamics.
Human uncertainty can be captured via self-reported confi-
dence levels, while Gaussian Processes (GPs) are used to
model robots estimation of human preferences, with robot
uncertainty quantified by the GP variances. Compared to
neural network-based uncertainty estimations, GP variances
offer advantages in interpretability, reliability, and sample
efficiency [11, 12]. Fig. 1 offers an intuitive example on the
motivation of uncertainty unification in preference learning.
To unify human uncertainty with robotic Gaussian un-
(UUPL), which focuses on leveraging human confidence lev-
els to refine the GP mean and covariance estimations. To
this end, we design a human preference uncertainty model
that captures varying levels of users uncertainty, which is
then integrated into the Laplace approximation algorithm to
enhance GP mean estimation. Recognizing the limitations of
the current GP predictive variance in preference learning (as
further elaborated in Fig. 5), we propose a human uncertainty-
weighted Gaussian Mixture Model (GMM) to provide a more
interpretable and adaptive GP variance estimation. This re-
fined predictive variance not only achieves synergy with the
acquisition function, but also has practical benefits in real-
world applications. Additionally, we introduce an uncertainty
calibration process to align uncertainty representations across
diverse users, ensuring consistency and reliability in the model
performance. Comprehensive simulations and real-world user
further highlights the contributions of each uncertainty-unified
component to overall system performance.
To sum up, the main contributions of our paper include:
uncertainty-unified
preference
learning
(UUPL) framework, which includes human preference
uncertainty modeling and the integration into Laplace
posterior mean estimation, uncertainty-weighted Gaus-
sian Mixture Model for GP predictive variance scaling,
and user-specific uncertainty calibration.
We conduct comprehensive evaluations against three
baselines across three simulation tasks, demonstrating
significantly higher prediction accuracy. Additionally, we
conduct three user studies, providing insights into the
calibration process and illustrating practical applications
of unified uncertainty. An ablation study further analyzes
the contributions of individual components in UUPL.
II. RELATED WORK
This section first describes uncertainty quantification, fol-
lowed by past work on modeling robot and human uncertain-
ties in HRI, and finally discusses preference learning and its
connection to uncertainty.
A. Uncertainty Quantification and Modeling
Uncertainty can broadly be categorized into two types:
Aleatoric Uncertainty (AU) and Epistemic Uncertainty (EU)
. AU, often referred to as stochastic or statistical uncertainty,
represents inherent randomness in the system and cannot be
mitigated through additional data or experiments. In contrast,
about the system or environment and can potentially be
reduced through further data collection. From the robots
probabilistic methods like Monte Carlo simulations, and EU
is typically estimated in Bayesian approaches. In Fig. 1, for
every possible trajectory. The uncertainties of trajectory x(1)
or x(2)s reward values are dominated by AU because they
have been observed, while the uncertainties of other unseen
trajectories reward values are dominated by EU. For human
uncertainty in HRI, various methods have been explored
to model AU. Laidlaw and Russell  employed inverse
decision theory (IDT) to estimate human uncertainty, while
Fisac et al.  utilized a noisy-rationality model to predict
human motion under uncertainty, recursively updating model
confidence. Additionally, Xu et al.  incorporated human
uncertainty into the Conditional Value-at-Risk (CVaR) frame-
work to design an uncertainty-aware policy, and Holladay
et al.  proposed the Comparison Learning Algorithm for
Uncertain Situations (CLAUS) based on the Luce-Shepard
Choice Rule . While significant progress has been made
in AU modeling for both robots and humans, EU modeling
for humans remains underexplored due to its inherently un-
measurable nature. Nevertheless, there is an emerging interest
in how robot behaviors might provide humans with additional
information about the environment, thereby indirectly reducing
human EU. Importantly, most existing studies focus on either
robot or human uncertainty in isolation, leaving a notable gap
in achieving an integrated approach to uncertainty modeling
in HRI. Although some work [13, 17, 18] attempt to consider
both human and robot uncertainties, they exclusively focus
on the AU part, achieving only partial unification. Our work
extends this by incorporating both aleatoric and epistemic
certainty representation framework.
B. Preference Learning
Preference learning has become increasingly popular in
lowing Furnkranz and Hullermeier , we define preference
learning as the task of learning a reward function from a
set of comparison pairs with known preference relationships,
formally defined in Section III-A1. In the context of reinforce-
ment learning (RL), preference learning can be seen as a gener-
alization of inverse reinforcement learning (IRL), wherein the
goal is to infer a reward function based on human feedback.
Several approaches have been proposed in preference learn-
ing. Sadigh et al.  represented the reward function as a
weighted sum of features, updating the weights using Bayesian
inference from human preferences. Nevertheless, this linear
feature assumption limits the generalizability of the model.
Chu and Ghahramani  employed Gaussian processes (GPs)
to model reward functions, capturing nonlinearity. However,
this approach lacked a strategy for generating next preference
query. Later works, such as Byk et al.  and Byk
et al. , addressed this limitation by integrating predictive
I prefer 20C than 18C
and Im very confident!
Do you prefer 18C or 20C,
and how confident are you?
Human Preference Uncertainty Model
Uncertainty-weighted GMM
User-specic
Uncertainty
Calibration
Very condent:
Very uncertain:
Temperature (C)
Reward value
Reward value dierence
Choice probability
Temperature (C)
GMM weight
Temperature (C)
Reward value
Temperature (C)
Reward value
Temperature (C)
Reward value
Overview of UUPL. Imagine a robot inferring a users preferred room temperature. For each query, we collect the users preference with the
associated uncertainty level. To begin, a calibration process (blue box) interprets the users definitions of confident and uncertain, ensuring these subjective
assessments are accurately quantified with uncertainty factors u. Then, we construct the human preference uncertainty model as a probit model using Gaussian
uncertainty level, enhancing its interpretability. Through this approach, UUPL effectively integrates human uncertainty into both the GP mean and variance,
achieving comprehensive uncertainty unification, and thus provides a more accurate, interpretable, and user-aligned learning result (rightmost picture).
entropy to guide query selection, leveraging information gain
principles as proposed by Houlsby et al. . Despite these
man uncertainty. Some studies have explored weak preference
modeling for human uncertainty. For example, Wilde et al.
used continuous-scale preference feedback to weight
linear feature models, and Cao et al.  incorporated weak
and equal preference options into deep neural network-based
frameworks. However, these preference learning approaches
focus solely on human uncertainty without considering robot
of uncertainty unresolved. We bridge this gap by unifying
uncertainties from both human and robot to achieve better
preference learning results.
III. PROBLEM FORMULATION AND METHODS
In this section, we begin by formulating the problem of
preference learning and uncertainty unification. Then we de-
scribe how we model human uncertainty, providing an intuitive
explanation of the approach. Finally, we introduce the GP
framework for preference learning, and detail our uncertainty
unification techniques with the calibration process.
A. Problem Formulation
1) Preference Learning: Given: (1) A sample space O such
that the instances Oi O are any comparable data type
(e.g., trajectory, state, object, . . . ), (2) a feature extraction
xi  (Oi), xi Rn, and (3) a human internal reward
function R(Oi) : O R assigning each instance a real value.
The goal of preference learning is to learn a function f(xi) :
Rn R to approximate R(Oi) from N pairs of comparison
data X  {(x(1)
N )} and the
corresponding human choices C  {C1, C2, . . . , CN}, where
Ci  {x(1)
i } if R(O(1)
), and vice versa.
i } denotes feature x(1)
is preferred over feature
preferred over option O(2)
. In later sections, we use O and
R(O) for models from human perspective, x and f(x) for
models from robot perspective.
2) Uncertainty Unification: In general, when an HRI task
say an algorithm f is uncertainty unified if it utilized both uH
and uR, i.e. f(X, uH, uR)  y, where X, y are the original
model input and output for the HRI task. This formulation
is fundamentally different from uncertainty-aware algorithms
expressed as f(X, uR)  y, which takes only robot uncer-
tainty without considering human uncertainty [27, 28]. The
intuition behind uncertainty unification is inspired by human-
human interactions, where individuals often infer each others
confidence or uncertainty through verbal and non-verbal cues,
using this information to guide joint decisions [29, 30].
to model and mimic such communication, integrating both
human and robot uncertainties into decision-making processes.
B. Human Preference Uncertainty Modeling
To achieve uncertainty unification in preference learning,
it is essential to model the human uncertainty behind their
choices. Given a query with two options {O(1), O(2)}, in
addition to collecting human choice C, we also collect the
human uncertainty level l and get the corresponding human
uncertainty factor ul regarding this choice. In this work,
we categorize human uncertainty levels into four discrete
4 (very uncertain)}. These levels reflect the perceived similar-
ity between O(1) and O(2). For instance, very uncertain im-
plies that the options are nearly indistinguishable, while very
confident indicates a strong preference. The uncertainty factor
ul maps human uncertainty level to robot model parameters,
and is further introduced in Section III-C5. To represent the
probability of a human choosing O(1) with uncertainty level l,
we introduce a reward uncertainty residual r N(0, (ul)2):
P(O(1) being chosen)
Intuition behind our human preference uncertainty modeling.
The x-axis represents the reward residual R(O(1)) R(O(2)), while the y-
axis indicates the probability of selecting O(1). For a given query (marked by
the red dashed line), confident human choices (low uncertainty level l  small
u) correspond to high probabilities, represented by the intersection between
the red dashed line and the green CDF with u  0.1. Conversely, uncertain
human choices (high l  large u) lower the modeled probability, as shown by
the intersection between the red dashed line and the yellow CDF with u  3.
where  is the cumulative distribution function (CDF) of the
standard normal distribution. The model is shown as the left
part of the purple box in Fig. 2.
Fig. 3 further illustrates the relationship between the re-
ward residual R(O(1)) R(O(2)) and the choice probabil-
ity P(O(1)being chosen) with multiple human uncertainty u
(modeled as the Gaussian variance ). The more the human
is uncertain, the less likely O(1) is chosen, as explained in the
caption of Fig. 3.
C. Gaussian Process for UUPL
GPs are employed to model the human reward function,
with their predictive variance serving as an indicator of un-
certainty. In the context of preference learning, GPs estimate
the uncertainty of the reward function across the entire feature
uncertainty model described above. We develop a generalized
framework for preference learning, built upon the Gaussian
processes methodology introduced by Chu and Ghahramani
and Byk et al. , with these prior works emerging as
special cases (i.e., zero human uncertainty) within our broader
approach. In this section, we briefly introduce GPs, during
which we emphasize how the human uncertainty is integrated
into the framework to achieve unification. For more details on
how GPs work in general, please refer to .
1) Kernel: In a GP, the kernel k(x(1)
i ) is a positive
semi-definite function that defines the covariance between any
two feature points. In this work, we choose the most common
radial basis function (RBF) kernel which is defined as
i )  ex(1)
where  is a hyperparameter controlling kernels smoothness.
2) Prior:
the corresponding predicted reward values are denoted by
N )]T . Assuming a
zero mean for f, the prior is fully specified by the covariance
matrix K, which is a 2N 2N matrix with the (ij)th element
be the kernel k
x(2(i mod 2))
, x(2(j mod 2))
. The prior can
then be expressed as a multivariate Gaussian:
2 efT K1f
3) Likelihood: The likelihood function takes the form of
human preference uncertainty model as introduced in Sec-
tion III-B. To express it from the robots GP perspective, the
likelihood for the ith comparison can be rewritten as
i (li)x(2)
i f(x(1)
i ) f(x(2)
N (lN )x(2)
N )}, the likelihood for the dataset is the joint
probability of observing each choice with reward f, which
can be written as the product of individual likelihood
i ) f(x(2)
4) Posterior: Based on Bayes theorem, the posterior can
be represented as
P(fD)  P(f)P(Df)
Since a closed-form solution for this posterior is not avail-
able due to the existence of Gaussian CDFs in the likeli-
the new Gaussian refers to the maximum a posteriori estimate,
and the variance refers to the inverse Hessian of the negative
log-likelihood with respect to reward f. Next, we focus on
introducing the integration of human uncertainty into Laplace
approximation framework.
5) Posterior Mean Approximation: Let the approximated
GP mean be fLap. Then, combining with Eq. 6, we have
fLap  arg max
i ) f(x(2)
2f T K1f
For more detailed derivation please refer to Appendix A.
In Eq. 7, the first part is a summation of N monoton-
ically increasing functions over differences of f, and the
second term is a quadratic penalization over f. For sim-
preference data is collected. Then f  [f(x(1)
Relationship between Laplace posterior mean difference fLap
and human uncertainty u. u1, u2, u3, u4 are determined by varying the
posterior mean difference proportionally, ensuring the model taking human
uncertainty and producing more accurate estimated posterior mean.
R22. Let
S(f)  ln
2f T K1f.
the difference of their internal reward function values of the
be closer. To achieve this in Eq. 7, we need to find a set of
ul  {u1, u2, u3, u4}, such that for each query (x(1)
larger uli results in a smaller fLap  fLap(x(1)
1 )fLap(x(2)
which requires us to find the relationship between u and the
posterior mean difference fLap.
Since solving
df S(f)  0 directly is intractable due to the
Gaussian CDF , we approach it analytically by determining
fLap against various u. Denote d as the monotonically
decreasing part of fLap (i.e., where u 10), then we design
ul  {u1, u2, u3, u4} to be the four percentage points of
d(u) as shown in Fig. 4. When human uncertainty level l
1 (very confident), u1 takes d1(dmax). When human uncer-
tainty level l  2 (confident), u2 takes d1( 2
3dmax). When hu-
man uncertainty level l  3 (uncertain), u3 takes d1( 1
u4 takes d1(1e 3). This mapping ensures that as human
uncertainty l increases, the posterior mean difference between
rewards fLap decreases proportionally, aligning with the
intuition of uncertain preferences. The resulting posterior GP
mean with different human uncertainty levels is shown as the
right part of the purple box in Fig. 2.
User-specific Uncertainty Calibration: Given individual
differences in defining confident or uncertain, a fixed
mapping from l to ul may be inadequate. Thus, we propose
a calibration process to tailor ul values to individual users,
by asking each user to describe a given reward function and
customize their uncertainty factors from their answers. Specifi-
collection process of M iterations: At iteration i, a feature pair
i ) is sampled and the user is told to assume the func-
tion values (fcalib(x(1)
i ), fcalib(x(2)
i )) reflect their internal re-
ward values, i.e., fcalib(x(k)
)  R(x(k)
), k {1, 2}. Then the
Algorithm 1: User-specific Uncertainty Calibration
: fcalib; fLap(u) as in Fig 4; Xcalib with
uniformly generated (x(1), x(2))
1 Init listu1 [], listu2 [], listu3 [], listu4 []
2 for (x(1)
) in Xcalib do
Collect user uncertainty level li
listuli .add(fcalib(x(1)
) fcalib(x(2)
5 for l in {1, 2, 3, 4} do
Get mean value
calib avg(listul)
Get quantile
calibmin(fcalib)
max(fcalib)min(fcalib)
Get ul with the same quantile
d the monotonically decreasing part of fLap
ul d1(ql  dmax)
user specifies their uncertainty level l, and the corresponding
value difference fcalib  fcalib(x(1)
i ) fcalib(x(2)
i ) is saved
together with their chosen uncertainty level li {1, 2, 3, 4}.
After collecting M pairs of data, the mean value difference for
each uncertainty level f
calib and the corresponding quantiles
calibmin(fcalib)
max(fcalib)min(fcalib), l {1, 2, 3, 4} are calculated. We then
Lap and further more the corresponding ul with the same
quantile on the function as in Fig. 4. The pseudo-code for the
calibration process is provided as Algorithm 1.
6) Posterior Covariance Approximation: To approximate
the GP covariance, we take the negative inverse Hessian of
the logarithm of the un-normalized posterior with respect to
f [23, 31, 32]. Let the approximated covariance be KLap, then
KLap  ((ln P(Df)  ln P(f)))1
where W is the negative Hessian of the log-likelihood, and
K is the 2N  2N covariance matrix of N preference pairs.
7) Uncertainty-unified Prediction: One benefit of GP is that
it provides a closed-form solution for prediction. Suppose we
have a test preference pair xt  (x(1)
t ), let the predictive
mean be t  [f(x(1)
t )]T , and the covariance be
, then we have:
t K1fLap
t  Kt kT
t (W  K1)1kt
where kt
By integrating fLap, the predictive mean t in Eq. 9 has
already taken human uncertainty into account, which can
generate more accurate reward values. While for the predictive
covariance t in Eq. 10 we introduce a Gaussian Mixture
Model (GMM) based method to unify human uncertainty with
robot uncertainty, as introduced below.
Uncertainty-weighted
Gaussian
Model Un-
der the uncertainty-unification setting, the GP covariance
Cov(f(x(1)
t )) should reflect both robot and human
uncertainty for any test feature pair (x(1)
t ). Robot
uncertainty is expressed by the predictive covariance t
as Eq. 10, and we design human uncertainty to be a
GMM G built upon all the N (query, uncertainty) pairs
N ), ulN )}.
w(uli)N(x(1)
t ; x(k)
w(uli)N(x(2)
t ; x(k)
bijection
{w1, w2, w3, w4} representing decreasing weights for increas-
ing uncertainty level1. Then, we define the uncertainty-unified
predictive covariance matrix
or equivalently
Var(f(x(m)
))  G(x(m)
Cov(f(x(1)
t )1G(x(2)
An example of uncertainty-weighted GMM G and the scaled
predictive variance Var is shown in the red box in Fig. 2.
uncertainty levels impact the variance of f(x(m)
If many query points are observed near x(m)
(i.e., large value
for N(x(m)
, 2)), then Var(f(x(m)
)) should be scaled
down; If the human has relatively low uncertainty level l (i.e.,
high confidence) for the preferences, then x(m)
should also
reflect a relatively low uncertainty from the robots perspective
(i.e., large value for w(uli)). Conversely, if the observed points
are all far away from x(m)
, or the human demonstrates high
uncertainty (i.e., low confidence), then the uncertainty of the
test point should also be high. We claim that this unified design
provides a rational variance estimation that benefits both theo-
retical acquisition functions (described in the next section) and
practical applications in robotic experiments (Section IV-C).
Variance analyses in Section IV-A2 and Fig. 5 further validate
the effectiveness of uncertainty-weighted GMM.
8) Acquisition Function: The acquisition function gener-
ates a new data pair to query the user preference and uncer-
tainty. Byk et al.  proposed an acquisition function that
maximizes the query information gain and at the same time
minimizes humans burden of answering the question .
w2 > w3 > w4 > 0 and G(x(m)
variance Var and covariance Cov. Let the next query be
) and H be the information entropy, then:
x arg max
x(1),x(2) I(f; CQ, D)
x(1),x(2) (H(CQ, D) EfP (fD)[H(CQ, f)]) (14)
which can be further written as
where h is the binary entropy function, g and m can be written
g(x(1), x(2))  Var(f(x(1)))  Var(f(x(2)))
2Cov(f(x(1)), f(x(2)))
ln(2)(ul)2 exp
ln(2)(ul)22g(x(1),x(2))
where Var and Cov are defined in Eq. 13. For a more detailed
derivation of such result, please refer to Byk et al. .
It is important to emphasize that this acquisition function is
designed to select the next query based on two key criteria:
similar reward values and high variances. In traditional binary
preference settings, users may struggle to make selections
when presented with two options with similar rewards (e.g.,
x(1) and x(2) in Fig. 1). By incorporating uncertainty, our
approach improves the user experience by allowing users to
express their confidence levels alongside their choices. This
claim will be supported later in user study sections.
the GP variance is further enriched by integrating human un-
certainty. This enhances the representativeness of the selected
query by maximizing information gain, taking into account the
joint uncertainties of both humans and robots. For example,
when faced with two queries observed the same number of
times (i.e., identical robot uncertainty), our model prioritizes
the one with higher human uncertainty, as it exhibits greater
entropy. Intuitively, this means the robot is more likely to
generate queries labeled as uncertain rather than confident,
thereby focusing on grounding the uncertain areas. As a result,
the proposed method selects more informative queries, which
we hypothesize will accelerate the convergence of the GP
model. This claim is validated in the experiment section.
IV. EXPERIMENT
The primary contribution of this work is demonstrating
that uncertainty unification significantly enhances the perfor-
mance of preference learning tasks. We conduct comprehen-
sive experiments to validate the effectiveness of our method
(UUPL) both quantitatively and qualitatively. First, we design
three simulation experiments to illustrate the rationality of
our uncertainty-unified framework, and analyze its accuracy
compared to other baseline methods. Following, we present
(b) Chu and Ghahramani
(c) Benavoli and Azzimonti
(d) Byk et al.
(e) UUPL
(a) Ground truth function
Reward value
Reward value
Reward value
Reward value
Reward value
Temperature (C)
Temperature (C)
Temperature (C)
Temperature (C)
Temperature (C)
GP variance visualizations. The ground truth function and the learned GPs (mean  1.96std) of three baseline methods and UUPL are provided.
The data comes from comparisons of 19C with all other integer temperatures, and the results showcase the rationality of our learned variance.
an ablation study which investigates the contributions of indi-
vidual components. Finally, we present three user studies to
demonstrate the efficacy of UUPL, especially our user-specific
uncertainty calibration (Section III-C5) in real-world settings,
and analyze user feedback through Likert scale ratings.
A. Simulation Experiments
For simulation experiments, we design ground truth func-
tions to simulate human reward functions and decide choices
C with uncertainty levels l based on Eq. 1. The human
uncertainty u  {u1, u2, u3, u4} is determined proportionally
according to Section III-C5. Each experiment is repeated six
1 and 2, N  100 for Simulation 3). In each iteration, C is
generated from the relative function values, l is calculated from
the function value differences, and finally, the GP is updated.
We record the accuracies and their variances across the 6 trials.
The goal is to compare the learned functions from different
methods with the ground truth function.
1) Metrics  Baselines: With a fine granularity, we dis-
cretize the learned GP mean and the ground truth function such
that Fpred  [pred(x1), pred(x2), . . . , pred(xn)] and Fgt
[fgt(x1), fgt(x2), . . . , fgt(xn)], with the corresponding means
be pred and f gt. Then we use the sample correlation coefficient
r to be our accuracy metric for simulation experiments:
i1(pred(xi) pred)(fgt(xi) f gt)
i1(pred(xi) pred)2 Pn
i1(fgt(xi) f gt)2
The sample correlation coefficient is scale-invariant and
focuses on the trend of the data. This property is ideal for
evaluating preference pairs, as the absolute alignment of Fpred
and Fgt is less critical than their relative ordering and overall
trajectory (i.e., ensuring that the functions evolve similarly).
For the baseline methods, we choose three other GP-
based preference learning methods: Baseline 1 as Chu and
and Baseline 3 as Byk et al. , each with different kernel
Note that none of these baselines are uncertainty unified.
2) Simulation 1  Thermal Comfort: The first simulation
experiment focuses on a conceptual scenario of learning a
users preferred room temperature, inspired by Benavoli and
Azzimonti . The feature is a 1D scalar representing the
temperature in Celsius: xi [10, 26], and the ground truth
function f(xi) is illustrated in Fig. 6(a). This function reflects
localized preferences for room temperature based on activities
such as cooking, working, and sleeping.
We first demonstrate the rationality of our uncertainty-
unified predictive variance (Section III-C7) qualitatively, lever-
aging the intuitiveness of the 1D variance. The results are
shown in Fig. 5. Suppose in (a) the feature x  19 (i.e.,
19C) has been observed 17 times, with comparisons made
against uniformly selected features x  10, 11, . . . , 26. Chu
and Ghahramani  assign zero variances to all the observed
and Azzimonti  assign uniformly high variances across
all points, ignoring the observation effects. Byk et al.
initialize a reference point f(17) with zero variance due to
its kernel design, which does not contain insightful mean-
ing and requires careful hyperparameter tuning. In contrast,
UUPL is tuning-free and assigns the smallest variance to
f(19), effectively capturing the impact of observations on
variance reduction and improving the variance interpretability.
the learned function to quickly identify two local maxima,
whereas baseline methods only detect a single maximum. This
demonstrates the representational power of our uncertainty-
unified framework.
3) Simulation 2  Tabletop Importance: This simulation
explores a more realistic setting: a tabletop scenario similar
to Fig. 1. Imagine a user sitting at a table cluttered with
various objects, such as electronics, stationery, and snacks,
while a home robot attempts to deliver a cup of coffee from the
opposite side of the table. To successfully complete the task,
the robot must learn the users preferences to minimize the
risk of spilling coffee onto critical objects. Directly assigning
scores to all points on the tabletop is impractical for users,
and general common sense rules (e.g., electronics are often
considered to be more important than snacks) often fail to
account for individual preferences, making preference learning
with GP an appropriate choice.
The task, illustrated in Fig. 6(b), models the tabletop as a
2D feature space where xi ([5, 5]  [5, 5]). Three hills
of varying shapes and scales represent objects on the table,
with the function value at each point indicating the importance
at the corresponding location  higher values correspond to
higher importance. It is worth noting that, compared to directly
comparing trajectory features as in Fig. 1, our design offers
additional insights, such as the uncertainty in specific areas of
the tabletop. This information can be further leveraged and will
be demonstrated in the first robot experiment in Section IV-C2.
(d) Sim 1 result
(e) Sim 2 result
(f) Sim 3 result
Accuracy (correlation)
Number of preferences
(a) Sim 1  Thermal Comfort
(b) Sim 2  Tabletop Importance
(c) Sim 3  Driving Choice
Temperature (C)
Reward value
Distance to the front car
Heading angle
Distance to the closest lane center
Accuracy (correlation)
Accuracy (correlation)
Number of preferences
Number of preferences
Three simulation experiments and their results. (a) The 1D thermal comfort function used in Simulation 1, with results shown in (d). (b) The 2D
tabletop importance function for Simulation 2, with results plotted in (e). (c) Simulation 3, where the left panel depicts two possible trajectories for the red
user car based on the blue agent cars action, and the right panel shows the functions of the four trajectory-related features. Results are presented in (f).
Accuracies (mean  std) of three baseline methods and UUPL on the three simulation experiments.
Baseline 1
Baseline 2
Baseline 3
Sim 1  Thermal Comfort
Sim 2  Tabletop Importance
Sim 3  Driving choice
the-shelf path planning algorithms, seamlessly integrating the
learned reward function into the robots planning process.
4) Simulation 3  Driving Choice: The last simulation
builds on scenarios described by Sadigh et al.  and Byk
et al. . As illustrated by Fig. 6(c), an agent (blue) car and a
user (red) car navigate a three-lane road. The agent car intends
to switch from the left to the middle lane, prompting the user
car to respond by slowing down, switching lanes, or adopting
other behaviors. At each iteration, two possible trajectories
(green and orange arrows) are presented. The features are
designed to consider four important aspects: Distance to the
front car, speed, heading angle, and distance to the closest lane
center (all normalized to [0, 5]), so xi ([0, 5][0, 5][0, 5]
[0, 5]). We design four complex functions for each feature as
shown on the right part of Fig. 6(c), and the ground truth
reward function f(xi) : R4 R is the sum of four function
values for each feature. Due to the experiments complexity,
the number of preference collection was increased to 100.
5) Simulation Results Analysis: The mean and variance
of the accuracy for all three simulations are presented in
Fig. 6(d)-(f) and Table I. To provide a comprehensive evalua-
the highest final accuracy, achieving 0.9949, 0.9653,
and 0.8764 for Simulation 1, 2, and 3, respectively.
This superior performance is largely attributed to the
human preference uncertainty model. By allowing users
to express uncertainty levels and integrating this informa-
tion into the Laplace approximation, UUPL refines the
posterior mean estimation. As shown in the purple box
in Fig. 2, this integration ensures the posterior mean is
scaled more accurately based on the uncertainty levels,
leading to improved accuracies across tasks.
For Simulation 1, UUPL reaches an accuracy of 0.7
within just 5 iterations. Although Baseline 2 achieves a
comparable performance of 8 iterations, its results deteri-
orate after reaching 0.8 accuracy. Baseline 3 and 1 require
13 and 30 iterations respectively to reach 0.7 accuracy.
In Simulation 2, UUPL attains 0.7 accuracy in only 5
(a) Ablation study results: The red curve represents UUPL, the purple curve excludes the uncertainty-weighted GMM, the yellow curve omits
the human uncertainty model, and the green curve is the baseline without modeling uncertainty. (b) Calibration evaluation: The No uncertainty method
shows the lowest accuracy and highest variance, while our Calibrated uncertainty method achieves the highest accuracy and lowest variance. (c) User study
30 iterations, and Baseline 1 fails to converge. For the
more complex Simulation 3, only UUPL and Baseline 3
achieve accuracies exceeding 0.7, with UUPL requiring
32 iterations compared to Baseline 3s 62 iterations. This
efficiency is attributed not only to the human uncertainty
model but also to the uncertainty-weighted GMM. By
synergizing with the entropy-based acquisition function,
UUPL selects queries based on human and robot joint
informative for the preference learning process. Con-
convergence rate significantly.
methods by analyzing the variance of accuracy. UUPL
shows a clear trend of decreasing accuracy variance as
more data is collected, ultimately achieving low variance,
as indicated in Table I. In contrast, the baseline methods
do not exhibit consistent variance reduction. This result
highlights that the uncertainty-unified framework pro-
duces more stable preference learning outcomes, making
the system reliable and applicable in real-world scenarios.
izing the learned reward functions for all four methods in
the thermal comfort simulation and the tabletop importance
simulation is omitted, because the 4D feature space makes
visualization less practical. Those figures provide a more
intuitive understanding of the superior performance of UUPL.
B. Ablation Study
In this section, we delve deeper into the individual contribu-
tions of the human preference uncertainty model proposed in
Section III-B and the uncertainty-weighted GMM introduced
in Section III-C7. To avoid potential biases arising from the
simplicity of the 1D thermal comfort function, we conduct
the evaluation on the more complex and realistic tabletop
importance function.
By systematically removing each uncertainty-related com-
ponent and analyzing their impact, we obtain the results shown
in Fig. 7(a). These findings highlight that both components
significantly enhance the overall performance of the system.
optimizes query selection by generating more informative
preferences that effectively incorporates human and robot
compared to the baseline (green curve). However, the ac-
curacy variance remains considerable, primarily due to the
forced binary choice in uncertain scenarios. Meanwhile, the
human preference uncertainty model (purple curve) improves
the accuracy of the learned posterior mean by incorporating
user-expressed uncertainty levels, and the accuracy variance
is significantly reduced by allowing users to describe their
preferences with greater precision. Together, these two com-
ponents synergize to enable the unified uncertainty framework
to achieve state-of-the-art performance (red curve).
C. User Studies
We conducted three user studies with eight participants to
evaluate both our uncertainty-unified framework, especially
the user-specific uncertainty calibration process. In this sec-
accuracy and reliability in capturing individual uncertainty
levels. Following this, we performed two robot experiments
with different uncertainty usage to evaluate user experience.
The three methods compared were: Byk et al. , referred
to as no uncertainty (NU), which does not include uncer-
tainty modeling; our method without user-specific uncertainty
our full method, referred to as calibrated uncertainty (CU).
1) Calibration Process Evaluation: We chose the thermal
comfort simulation function to be the calibration function
Fifty preference pairs were collected per participant, and
their uncertainty factor values ul  {u1, u2, u3, u4} were
calibrated individually using Algorithm 1. To evaluate the
calibrated uncertainties, we designed a different function ft
and conducted preference learning with all three methods
(NU, UU, CU) for all users, suggesting ft to be their reward
function. Each method was repeated three times, with twenty
preferences collected per trial. UU and CU experiments were
randomized to eliminate potential bias.
One example for the tabletop importance task. The red box
represents the user interface, and green box illustrates the tabletop setup,
with the robot trying to move from the blue star to the red star.
The results are presented as the box plot in Fig. 7(b).
NU exhibits the lowest accuracy alongside high variance,
indicating the limited effectiveness without uncertainty. While
UU achieves higher accuracy due to the inclusion of un-
certainty options, it still displays considerable variance, sug-
gesting that participants interpret confident and uncertain
inconsistently across individuals. In contrast, CU demonstrates
both the highest accuracy and significantly reduced variance,
validating that UUPL effectively standardizes uncertainty in-
terpretations and better describes the underlying function
across participants. Additionally, user ratings for the learned
clear preference for CU (7.2  1.0) over both NU (5.5  1.3)
and UU (5.0  1.5). These ratings further reinforce the im-
portance of user-specific uncertainty calibration in achieving
more accurate and consistent results.
2) Robot Experiments: To further evaluate the utility of our
Kinova Gen 3 manipulator and a Stretch 2 mobile manipulator.
These experiments also demonstrate some practical benefits of
unified uncertainty.
Tabletop Importance Task: We replicated the tabletop
importance simulation task in a real-world setting, where
each participant had a unique tabletop configuration. One
experiment setup is provided as Fig. 8. A Kinova Gen 3 was
holding a cup of coffee, aiming to transport it to the other side
of the table. A top-down image of the tabletop was taken,
and participants were instructed to provide preferences and
uncertainties for pairs of red and blue points on the image.
For a complete set of tabletop configurations, please refer
to Appendix C. During the experiment, each method (NU,
preference pairs collected per trial. Robot trajectories were
generated based on the learned reward functions using the A-
star planning algorithm. In this case, we used the uncertainty
to scale the motion velocity: in regions with low variance, the
robot moved at a normal speed, while in high-variance regions,
it slowed down, mimicking cautious exploration like humans
would do in unfamiliar environments.
For each trial, we collected user ratings for both the learned
function and the robots trajectory (including its uncertainty-
scaled velocity). The results are presented in the second and
Illustration of the apple pick-and-place task. The dashed pink line
is one possible trajectory. The robot starts at the blue star, passes the yellow
third group columns of Fig. 7(c) (User Study 2.1 and
User Study 2.2, respectively). In both cases, CU achieved
the highest average human ratings and the lowest overall
7.51.0 for User Study 2.2. In contrast, users did not express
a specific preference for NU and UU. These results proved
the importance of user-specific uncertainty calibration process,
which enables participants to articulate their preferences more
and user-aligned reward values. Additionally, users perceived
the uncertainty-scaled robot motion as being both safer and
more natural, further validating the advantages of incorpo-
rating calibrated uncertainty into the UUPL framework.
Apple Pick-and-Place Task: We further designed an apple
pick-and-place experiment using the Stretch 2 mobile manip-
ulator. As shown in Fig. 9, the robot first traversed through
a row of plants to pick up the apple at the yellow star, then
it bypassed a rack to the red star and placed the apple on
a table. During each trial, users were presented with two
were collected. To encode the trajectories, we extracted three
meaningful features: (1) Path through the plants: Whether to
take the shortest route, risking the robot becoming stuck or
completion time. (2) Clearance from the rack: The distance
maintained while bypassing the rack, balancing the risk of
pushing it over with the route efficiency. (3) Traversal velocity:
The speed of movement along the path.
As in previous experiments, each method (NU, UU, CU)
was tested three times per participant, with twenty preference
pairs collected per trial. To address the potential insufficiency
of twenty pairs in a higher dimension, we implemented an
adaptive query method based on corrected GP variances.
overall GP variance. If the variance drop is below than
a threshold, indicating a consistent high uncertainty about
the learned reward function, five more pairs are collected,
repeating until the variance drop reaches the threshold. Since
our uncertainty-weighted GMM ensures GP variance reflects
a rational and meaningful uncertainty considering both human
and robot, UUPL is able to utilize the variance drop as a
criterion to stop querying adaptively, which balances model
performance and human burden.
We compared NU, UU and CU in this user study from two
and the other one using the adaptive query method, shown
as User Study 3.1 and User Study 3.2 respectively in
Fig. 7(c). In the first comparison, UU and CU reached 6.51.2
and 7.5  1.3, both outperforming NUs 5.3  1.3. In the
second user study, UU and CU improved by approximately
1.0, but NU remained unchanged. NUs lack of improvement
can be attributed to its variance decreasing too rapidly in the
absence of uncertainty scaling, causing the threshold to be
met consistently after collecting only twenty preference pairs.
In contrast, CU effectively regulated the number of queries
based on the uncertain extent in users answers, soliciting
additional responses when uncertainty was high. This adaptive
query based UUPL achieved the highest rating of 8.6  0.9.
8.5 for the uncertainty option design, indicating that it not
only facilitates easier and more concise expression of their
preferences but also significantly enhances the learning results.
Failure Case Analysis: We identify several failure cases
that may degrade system performance. First, if users consis-
tently select extreme responses (e.g., very confident), the
framework effectively reduces to prior methods that ignore
uncertainty modeling . Second, when users interpret the
notions of confident and uncertain inconsistently across
tuations in the underlying uncertainty levels ul. Finally, if
users revise their preference opinions within a single trial,
additional noise is introduced into the Gaussian Process model,
undermining the learned function.
V. CONCLUSION
In this work, we introduced the concept of uncertainty
unification in human-robot interaction (HRI) and proposed
uncertainty-unified preference learning (UUPL) within the
domain of preference learning. Specifically, we developed a
human preference uncertainty model with discrete uncertainty
imation for Gaussian Process (GP) mean estimation to improve
its accuracy. For the GP variance, we proposed an uncertainty-
weighted Gaussian Mixture Model (GMM) to unify human
uncertainty with robot uncertainty, resulting in interpretable
variances that enhance the acquisition functions effectiveness.
We conducted three simulation experiments, three user studies,
and an ablation study to evaluate UUPL comprehensively. The
results consistently support our claim that unifying human
and robot uncertainties improves preference learning perfor-
mance. Beyond preference learning, we hope the idea of
explicit uncertainty unification can inspire more natural HRI in
real-world scenarios, while encouraging further research into
human-robot joint uncertainty estimation.
VI. LIMITATION AND FUTURE WORK
Our work has some limitations that point to opportunities for
future research. First, when designing the human preference
uncertainty model, we choose four uncertainty level options.
While Wilde et al.  suggests that humans generally prefer
discrete over continuous options, future work is encouraged
to determine the optimal number of uncertainty options or
adapt this number to individual users. It is worth noting that
our framework can adapt to individual preferred granularity
by adjusting the number of ul during calibration without
altering the core UUPL framework, ensuring the flexibility and
user-friendliness. Besides, it would be beneficial to integrate
human uncertainty into the kernel function, which provides
more information on representing the similarity of two feature
points. Lastly, the weights for uncertainty-weighted GMM
are predefined. A promising avenue would be to establish
a deterministic relationship between wl and ul, enabling the
GMM to better describe unified uncertainty.
Beyond preference learning, uncertainty unification also
has potential applications in other domains. For instance, in
developing large language models (LLMs), inferring human
uncertainty from linguistic cues such as phrases like maybe,
LLM uncertainty models like . Similarly, in human-robot
informative and could be utilized to enhance interaction. From
a game theory perspective, uncertainty unification could serve
as an objective function, where its minima represent the mutual
awareness of actions between humans and robots. Finally, a
key challenge for future work lies in effectively conveying
unified uncertainty to humans, ensuring that it is interpretable
and actionable in real-world interactions.
ACKNOWLEDGMENTS
This work was supported in part by the National Science
Foundation under Grant No. 2143435 and AIFARMS through
the Agriculture and Food Research Initiative (AFRI) grant
no. 2020-67021-32799project accession no.1024178 from the
USDANIFA. We would like to thank Dongping Li and
Kaiwen Hong for their help in conducting experiments, Neeloy
Chakraborty and Shuijing Liu for their valuable feedback
during proofreading.
REFERENCES
S Battula, SN Alla, EV Ramana, N Kiran Kumar, and
S Bhanu Murthy. Uncertainty quantification for digital
twins in smart manufacturing and robotics: A review.
In Journal of Physics: Conference Series, volume 2837,
page 012059. IOP Publishing, 2024.
Ransalu Senanayake. The role of predictive uncertainty
and diversity in embodied ai and robot learning. ArXiv,
Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet
that ask for help: Uncertainty alignment for large lan-
guage model planners. In Proceedings of The 7th Con-
ference on Robot Learning, volume 229 of Proceedings
of Machine Learning Research, pages 661682. PMLR,
Allen Ren, Jaden Clark, Anushri Dixit, Masha Itkina,
Anirudha Majumdar, and Dorsa Sadigh. Explore until
answering.
In Robotics: Science and Systems (RSS),
Neil A Macmillan.
Signal detection theory.
handbook of experimental psychology: Methodology in
experimental psychology, 4:4390, 2002.
Darius-Aurel Frank, Polymeros Chrysochou, Panagiotis
intelligence under self-threats and high-stakes decisions.
Technology in Society, 79:102732, 2024.
ISSN 0160-
Yaohui Guo and X. Jessie Yang. Modeling and predicting
trust dynamics in humanrobot teaming: A bayesian
inference approach.
International Journal of Social
Jose Ramon Medina, Tamara Lorenz, and Sandra Hirche.
Synthesizing anticipatory haptic assistance considering
human behavior uncertainty. Trans. Rob., 31(1):180190,
February 2015.
Luyao Yuan, Xiaofeng Gao, Zilong Zheng, Mark Ed-
Yixin Zhu, and Song-Chun Zhu.
In situ bidirectional
human-robot value alignment. Science Robotics, 7(68):
Cassidy Laidlaw and Stuart Russell. Uncertain decisions
facilitate better preference learning. In Proceedings of
the 35th International Conference on Neural Information
Processing Systems, NIPS 21, Red Hook, NY, USA,
2024. Curran Associates Inc. ISBN 9781713845393.
Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
Mohsin Ali, Jongseo Lee, Matthias Humt, Jianxiang
Ribana Roscher, M. Shahzad, Wen Yang, Richard Bam-
neural networks. Artificial Intelligence Review, 56:1513
Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting
Uncertainty is fragile: Manipulating uncertainty in large
language models. ArXiv, abs2407.11282, 2024.
Jaime Fisac, Andrea Bajcsy, Sylvia Herbert, David
Probabilistically safe robot planning with
confidence-based human predictions. In Robotics: Sci-
ence and Systems, 2018.
Sheng Xu, Bo Yue, Hongyuan Zha, and Guiliang Liu.
Uncertainty-aware preference alignment in reinforcement
learning from human feedback. In ICML 2024 Workshop
on Models of Human Feedback for AI Alignment, 2024.
Rachel Holladay, Shervin Javdani, Anca Dragan, and
Siddhartha Srinivasa. Active comparison based learning
incorporating user uncertainty and noise. In RSS Work-
shop on Model Learning for Human-Robot Communica-
Gordon D. Logan. An instance theory of attention and
memory. Psychological review, 109 2:376400, 2002.
Changliu Liu and Masayoshi Tomizuka.
Safe explo-
robot interactions. 2015 American Control Conference
(ACC), pages 465470, 2015.
Haimin Hu and Jaime F. Fisac. Active uncertainty reduc-
tion for human-robot interaction: An implicit dual control
approach.
In Steven M. LaValle, Jason M. OKane,
Michael Otte, Dorsa Sadigh, and Pratap Tokekar, editors,
Algorithmic Foundations of Robotics XV, pages 385401,
Johannes Furnkranz and Eyke Hullermeier. Preference
Dorsa Sadigh, Anca D. Dragan, S. Shankar Sastry, and
Sanjit A. Seshia.
Active preference-based learning of
reward functions.
In Robotics: Science and Systems,
Wei Chu and Zoubin Ghahramani. Preference learning
with gaussian processes.
In Proceedings of the 22nd
International Conference on Machine Learning, ICML
ation for Computing Machinery. ISBN 1595931805. doi:
Erdem Byk, Malayandi Palan, Nicholas C. Landolfi,
Dylan P. Losey, and Dorsa Sadigh. Asking easy ques-
In Proceedings of the Conference on Robot Learning,
volume 100 of Proceedings of Machine Learning Re-
Erdem Byk, Nicolas Huynh, Mykel J. Kochenderfer,
and Dorsa Sadigh.
Active preference-based gaussian
process regression for reward learning and optimization.
The International Journal of Robotics Research, 43(5):
Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and
Mate Lengyel. Bayesian active learning for classification
and preference learning. ArXiv, abs1112.5745, 2011.
Nils Wilde, Erdem Biyik, Dorsa Sadigh, and Stephen L.
Smith. Learning reward functions from scale feedback.
In Proceedings of the 5th Conference on Robot Learn-
Zehong Cao, Kaichiu Wong, and Chin-Teng Lin. Weak
human preference supervision for deep reinforcement
learning.
IEEE Transactions on Neural Networks and
Learning Systems, 32:53695378, 2021.
Taekyung Kim, Jungwi Mun, Junwon Seo, Beomsu Kim,
and Seong II Hong.
Bridging active exploration and
uncertainty-aware deployment using probabilistic ensem-
ble neural network dynamics.
Huan Nguyen, Rasmus Andersen, Evangelos Boukas,
and Kostas Alexis. Uncertainty-aware visually-attentive
navigation using deep neural networks. The International
Journal of Robotics Research, 43(6):840872, 2024. doi:
Wei-Fen Hsieh, Youdi Li, Erina Kasano, Eri-Sato
cation based on the combination of verbal and non-verbal
factors in human robot interaction. In 2019 International
Joint Conference on Neural Networks (IJCNN), pages 1
Tomislav Pejsa, Dan Bohus, Michael F. Cohen, Chit W.
munication about uncertainties in situated interaction.
In Proceedings of the 16th International Conference
on Multimodal Interaction, ICMI 14, page 283290.
Association for Computing Machinery, 2014.
Carl Edward Rasmussen. Gaussian Processes in Machine
Springer Berlin Heidelberg,
Bjrn Sand Jensen and Jens Brehm Nielsen. Pairwise
judgements and absolute ratings with gaussian process
Technical University of Denmark (DTU), De-
partment of Applied Mathematics and Computer Science,
Tech. Rep, 2011.
Alessio Benavoli and Dario Azzimonti.
A tutorial on
learning from preferences and choices with gaussian
processes. arXiv preprint arXiv:2403.11782, 2024.
Dorsa Sadigh, Shankar Sastry, Sanjit A. Seshia, and
Anca D. Dragan.
Planning for autonomous cars that
leverage effects on human actions. In Robotics: Science
and Systems, 2016.
APPENDIX A
DERIVATION OF POSTERIOR MEAN APPROXIMATION
Let the Laplace-approximated GP mean be fLap, then integrate Eq. 3 and Eq. 5 into Eq. 6, we have
fLap  arg max
(ln(P(Df))  ln(P(f)))
i ) f(x(2)
2 ln K 1
2f T K1f))
i ) f(x(2)
2f T K1f)
which is exactly Eq. 7.
APPENDIX B
VISUALIZATION OF LEARNED FUNCTIONS
Here we provide qualitative results for our simulation experiments.
Temperature (C)
(b) Chu and Ghahramani
(c) Benavoli and Azzimonti
(d) Byk et al.
(e) UUPL
Reward value
Temperature (C)
Reward value
Reward value
Reward value
Temperature (C)
Temperature (C)
Temperature (C)
Reward value
(a) Ground truth function
Fig. 10.
Visualization of Sim 1  Thermal Comfort. (a) represents the ground truth function, (b)- (d) illustrate the results obtained using the three baseline
methods. (e) presents the outcome of our UUPL approach. Notably, UUPL accurately captures the three local minima along with their relative magnitudes.
(b) Chu and Ghahramani
(c) Benavoli and Azzimonti
(d) Byk et al.
(e) UUPL
(a) Ground truth function
Fig. 11.
Visualization of Sim 2  Tabletop Importance. The first row is the top-down view of the functions in the second row. Column (a) depicts the
ground truth function, columns (b)(d) show the results of the three baseline methods, and column (e) displays the results of UUPL. UUPL successfully
identifies the locations and even the shapes of the three objects.
APPENDIX C
TABLETOP IMPORTANCE USER STUDY CONFIGURATIONS
Fig. 12.
The set of configurations for the tabletop importance task. Each participant was assigned a unique configuration featuring different objects,
including electronics, snacks, toys, and stationery.
