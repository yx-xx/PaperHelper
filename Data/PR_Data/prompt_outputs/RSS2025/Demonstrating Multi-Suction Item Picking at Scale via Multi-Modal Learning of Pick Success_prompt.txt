=== PDF文件: Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success.pdf ===
=== 时间: 2025-07-22 15:50:33.259648 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating Multi-Suction Item Picking at Scale
via Multi-Modal Learning of Pick Success
Che Wang, Jeroen van Baar, Chaitanya Mitash, Shuai Li, Dylan Randle,
Weiyao Wang, Sumedh Sontakke, Kostas E. Bekris, and Kapil Katyal
Amazon Robotics
Fig. 1: Given RGB, depth, pick location and semantic data for a picking scene as well as features of candidate, multi-suction
representations are used to evaluate candidate picks via a cross-attention mechanism and a pick success prediction head. All
input image modalities on the left (RGB, Depth, Pick Location, Semantic) can also be cropped to produce a local image (right
side of the input images pairs). Operating over local image crops helps boost performance. The modalities marked with
are used in the default approach. The modalities marked with  are utilized during pretraining. The modalities marked with
(opt.) are optional during the finetuninginference but can enable higher performance if deployed. The demonstrated strategy
is trained using picks executed on an operating, industrial setup. They are generated by a previously deployed engineered
approach. The strategy achieves improved performance relative to the engineered approach and learning-based alternatives.
AbstractThis work demonstrates how autonomously learning
aspects of robotic operation from sparsely-labeled, real-world
data of deployed, engineered solutions at industrial scale can pro-
vide with solutions that achieve improved performance. Specif-
comprehensive study on the application of multi-modal visual
encoders for predicting the success of candidate robotic picks.
Picking diverse items from unstructured piles is an important and
challenging task for robot manipulation in real-world settings,
such as warehouses. Methods for picking from clutter must
work for an open set of items while simultaneously meeting
latency constraints to achieve high throughput. The demonstrated
approach utilizes multiple input modalities, such as RGB, depth
and semantic segmentation, to estimate the quality of candidate
multi-suction picks. The strategy is trained from real-world
to pick items. The training picks have been generated by an
engineered strategy. A real-world limitation when learning in
such live, industrial setups is that only a single or a few
picks can be attempted per scene. The learning strategy first
pretrains multi-modal visual models in a self-supervised manner
to effectively reconstruct the input modalities in the target
domain. A downstream model is then trained to evaluate the
quality of multi-suction picks given the learned multi-modal
The manuscript provides comprehensive experimental evaluation
performed over a large item-picking dataset, an item-picking
dataset targeted to include partial occlusions, and a package-
picking dataset, which focuses on containers, such as boxes
and envelopes, instead of unpackaged items. The evaluation
measures performance for different item configurations, pick
of in-domain pretraining, the impact of different modalities and
the importance of finetuning. These ablations reveal both the
importance of training over multiple modalities but also the
ability of models to learn during pretraining the relationship
between modalities so that during finetuning and inference, only
a subset of them can be used as input.
I. INTRODUCTION
Picking items from unstructured piles is an important yet
challenging task for robotic manipulation in real-world set-
from clutter have to be robust to an open set of items while
simultaneously meeting latency constraints to achieve high
throughput. While robot manipulation in clutter has long been
approached via model-based reasoning [12, 15, 7, 13], the
recent focus has been on data-driven approaches and their
potential benefits. In particular, learning-based methods have
been introduced to address the large variety of items, for
pinch grasping [19, 22, 10], suction-based grasping [21, 3], or
both [23, 24]. Suction-based grasping, the modality considered
in this work, is popular in real-world settings (e.g., logistics
and fulfillment) as it simplifies the attachment of a target object
to a robots end-effector.
This work is inspired by data-driven methods for robot
picking as well as the progress in multimodal, multi-task mod-
els . It aims to explore the impact of such models in robot
picking given the availability of large-scale data obtained from
real-world industrial robotic deployments. The accompanying
experiments use large real-world picking datasets. In contrast
to prior work that is often limited either to a closed set of items
or training in simulation, this work aims to address the case
of picking an open-set of possible items that can appear in
any possible configuration, i.e., all possible products and con-
figurations that arise in a real-world warehouse environment,
using a multi-suction end-effector.
Recent work  proposed a shallow model for pick success
experts. The model was similarly derived from large-scale,
in-domain multimodal data for a multi-suction end-effector.
It focused, however, on picking packages instead of items
from an open-set. The variety in appearance for packages,
such as boxes, or envelopes, is considerably smaller compared
to an open-set of items, considered here. The prior work
demonstrated that the shallow model with engineered features
achieved stronger performance than a deep learning model1.
The current study first confirms the prior finding in the
context of open-set item picking, i.e., it is not trivial to develop
a deep architecture that learns effective visual representations
that outperform a shallow model using features engineered
by an expert (see Fig. 3). Motivated, however, by the need
to automate and simplify the development and training of
picking solutions, this work demonstrates that recent deep vi-
sual architectures allow the automated learning of multimodal
representations from real-world data.
In particular, we propose a model for pick success prediction
(see Fig. 1) that operates over a multimodal visual encoder,
such as the Multimodal Multi-Task Masked Autoencoder
(MultiMAE)  trained on RGB, depth and semantic data.
This encoder can be pretrained to correlate information across
different modalities, resulting in a representation that captures
1That model used a convolutional visual encoder using as input an RGB
image but without access to multimodal data or engineered features.
information about items and their relation in a cluttered pile.
combined with pick features via a cross-attention mechanism
and is finetuned to predict the quality of the pick. Experiments
show that this architecture achieves improved performance
over a highly-tuned, shallow model as well as against deep
model baselines that utilize a generic, frozen encoder. The
demonstrated architecture also consistently outperforms the
alternatives when applied to different types of picks as well as
to scenes corresponding to package picking. All experiments
presented are focused on multi-suction end effectors as they
work well for the warehouse setting.
In summary, the contributions of this work are the following:
1. The combination of multimodal pretraining and fine-
tuning of the MultiMAE, properly coupled with an encod-
ing of the pick candidate information, such as position and
outperforms the previously demonstrated shallow model.
2. We present experiments showing the learned multimodal
representation works on a standard item picking setting, item
picking with increased occlusion and random pick samples,
and a setting where we pick packages instead of items.
3. Extensive ablations of the demonstrated architecture
reveal which technical components are critical to achieving
the best performance and a series of insights in the large-scale
robot picking setting. For instance, while many efforts in the
literature advocate for frozen visual representations trained on
large generic datasets [28, 29, 33], we show that considerable
performance gains can be achieved through in-domain pre-
training and finetuning. The majority of these gains can be
achieved even with small exposure to in-domain multimodal
pretraining (e.g., just seeing 1 data of the available data).
In the appendix, we provide additional results, in the sup-
plementary materials, we provide videos of picking items from
clutter to illustrate the challenging nature of the task.
II. RELATED WORK
This section first reviews work on robot picking and then
discusses learning visual representations for robotic tasks.
A. Robotic Item Picking
Traditional methods for robot grasping  involve geo-
metric reasoning, planning and optimization methods. They
often calculate the poses and forces for robotic contacts so
as to satisfy certain mechanical constraints, such as force or
form closure [27, 25]. The requirement, however, for accurate
geometric and physical object models often limits the effec-
tiveness of these solutions in unstructured setups involving an
open-set of objects, which is the focus of this work.
This motivated the introduction of learning-based ap-
proaches for robot grasping more than a decade ago . Such
data-driven methods brought the promise of more effective
grasping in challenging, cluttered setups with unknown ob-
jects. The majority of these works focus on generating grasps
for parallel, pinch grippers [10, 9, 34]. Since the Amazon
Picking Challenge , however, it has been well understood
that suction-based grippers can be very effective in real-
world picking setups by simplifying pick reasoning. While the
underlying representation learning tools and insights of this
work are not necessarily limited only to such end-effectors,
the accompanying experiments have been performed using
a multi-suction gripper. Suction-based grippers have been
deployed in real-world production environments and allow fast
and robust picking of items that can potentially be heavy.
Some of the data-driven solutions for robot picking have
been extended to address suction-based grippers, such as the
Dex-Net family of solutions [23, 3]. In Dex-Net, a pick
candidate is evaluated using an expert-designed evaluation
system. In this work, we directly train the model to learn pick
success prediction given success labels of past production data,
and demonstrate that recent deep architectures have the ability
to generate informative representations for this task. This
direction minimizes the need for expert knowledge, giving a
more effective solution to the picking problem.
In the context of approaches that have been tested on
demonstrated that a shallow model can have good performance
in terms of pick success prediction as part of a larger grasp
planning pipeline for package picking. Packages tend to have
more limited geometries and physical features relative to
picking an open set of items, which is the focus of this work.
The shallow model in prior work outperformed a deep learning
model with a convolutional visual encoder. This work shows
that with proper pretraining and finetuning, a deep model with
a multimodal visual encoder can in fact outperform the shallow
model as well as alternative deep architectures in terms of
picking items from an open-set. Thus, a critical objective is
to identify appropriate intermediate visual representations that
allow the evaluation of robotic picks.
B. Learning Visual Representations
The value of a visual representation for control has been rec-
ognized previously. Pretrained vision models for control
exploit a visual representation for learning a motor policy.
Alternatives focused on manipulation  have proposed a
representation for video image data for learning a variety
manipulation tasks. A Masked Auto-Encoder (MAE)  was
proposed to be used to learn a control policy . In order
to exploit the multimodal nature of the available data, this
work adopts Multi-Modal Multi-Task Masked Autoencoder
(MultiMAE) , the multimodal extension of MAE instead.
In addition, the above mentioned approaches use generic
robotic data for pretraining and freeze the visual encoder
during downstream operation. This work identifies that when
large amounts of in-domain data are available, pretraining and
finetuning in-domain data provides improved performance.
There are also other recent efforts that investigate using
multimodal input for robotic control [17, 34, 18, 30]. We focus
on MultiMAE due to:
(1) Multiple visual modalities are often available in robotic
for depth images, and a segmentation model that provides
semantic segmentation information. To generate the robot
picking actions, the demonstrated architecture reasons about
the 3D surface of the items as well as the segment boundaries
of the items. So we expect additional modalities including
depth and segment can help the model to better predict pick
outcomes. The accompanying ablations show that multimodal
pretraining and finetuning (with RGB, depth and semantics),
which MultiMAE can leverage , bring significant perfor-
mance improvements compared to using a single modality.
(2) We are interested in a representation that can be pre-
trained in a self-supervised manner. This allows us to operate
over large-scale data without additional curation and labeling.
MAEs lend themselves well to this desirable objective. On top
of that, they are also simple, robust and efficient .
To the best of our knowledge, this is the first application of
a multimodal model for prediction of pick success at scale.
III. SETUP
Robotic work cells in the industry are instrumented with
sensors of various modalities, such as color and depth. The
sensors are typically providing a top-down observation of the
cluttered items to be picked. An example top-down view is
shown in Fig. 1. The visible items in the color images are
segmented using previously trained segmentation models, pro-
viding semantic information. A set of pick candidate locations
is generated across the items, where a pick candidate is defined
by a number of pick features, including X, Y, Z coordinates
of the pick location, which pistons of the multi-suction gripper
are activated during the pick, orientation of the end-effector,
among others. To achieve high throughput, items with a high
probability of pick success for a particular pick candidate,
should be picked. The goal is thus to train a pick success
prediction model, which given the multimodal image data,
provides a prediction of success for a given pick candidate.
A. Datasets
We have access to two datasets for training models for item
pick success prediction: the first dataset, the standard dataset,
contains 343K multimodal images of items in unstructured
clutter for pretraining the MultiMAE. It also has nearly 275K
training examples which contains both multimodal images
along with pick features for the specific pick candidate of
an item, for which the grasp was executed in deployment. For
the executed grasp, we also have an annotation of pick success
or failure. The second dataset, which we will refer to as the
random dataset, is a much smaller dataset containing the same
type of data as the 275K training examples mentioned above.
The difference with the standard dataset is that the items were
picked according to randomly selected pick candidates. So this
dataset contains more picks of items that are partially occluded
by other items. We also present results on a dataset obtained
for a different domain involving the picking of packages.
B. Nature of items
In Figure 2, we further emphasize the challenging nature of
our setting with example pick scene images.
(a) Big heavy box
(b) Book; irregular shaped item
(c) Pack of cans
(d) Deformable bags
(e) Washer pods container
(f) Detergent
(g) Glass bottles
(h) Bucket; bag of pastry
(i) Plastic cups
(j) Transparent box; glasses
(k) Plush
(l) Large protein powder bottle
(m) Spatula without packaging
(n) Tape; irregular shaped item
(o) Transparent toy box
(p) Transparent container
Fig. 2: Example pick scenes from the open-set item manipulation task. Some difficult items are highlighted in white boxes.
Items such as the big box in (a) can be hard to manipulate
due to its heavy weight; some items are deformable such as
the plastic bags in (d), the plush toy in (k); some items are
irregular or round shapes, making it hard to find a proper pick
bottle (f), glass bottles (g), plastic bucket (h), spatula (m),
and tape (n); some items can be difficult to pick for multiple
irregular surface and can potentially be heavy, and the toy
box in (o) has both deformable and rigid parts, with some
transparent packaging. The protein powder bottle in (l) is
both round and heavy. It is also important to note that some
items can be damaged if not picked properly, such as the
glasses in (j), and the book in (b). Additionally, boxes that
contain smaller items could open and plastic packaging can
be damaged when picked incorrectly. These examples show
that reliably picking from an open set of real world items is
a very challenging task.
IV. METHODOLOGY
A. Preliminary Evaluation of Visual Encoders
tures was proposed, which outperformed a deep model with
a CNN-based visual encoder . However, several works in
the literature, e.g.,  have proposed to use pretrained visual
encoders. We first trained a model similar to that shown in
Fig. 1. Instead of the MultiMAE encoder, however, we used
a pretrained visual encoder whose weights are frozen, along
with token mean pooling for the token weighting (Fig. 4). We
train the model on the datasets with annotated pick success (as
described above), and unimodal input, i.e., RGB only. Fig. 3
shows the performance of the shallow model compared to
a visual encoder with randomly initialized weights (ViT no
pretrain), and various visual encoders pretrained on generic
datasets. Results show that using a frozen generic encoder
clearly achieves higher performance over an encoder with
randomly initialized weights. Nevertheless, the shallow model
with expert-engineered features significantly outperforms the
pretrained visual encoder models.
Fig. 3: Performance of a shallow model compared with pre-
trained visual encoders. From left to right: shallow model
(XGBoost) baseline that uses expert features; using a ViT
model  with random initial weights; ViT model with
MAE  pretraining on ImageNet; ViT model with DINO
pretrain; and ViT model with MVP  pretraining.
B. Demonstrated Strategy
To close the performance gap with the shallow model,
it is evident that pretraining with generic data by itself is
not sufficient. We propose several improvements over these
models with pretrained visual encoders. Again referring to
Fig. 1), we propose the use of a MultiMAE for visual encoding
to exploit the availability of multimodal image input. Each new
modality will be processed into additional image tokens with a
different input adapter. Furthermore, instead of mean pooling,
the pick features are input to a cross-attention layer for the
Token Weighting. The cross-attention layer aims to learn to
relate the encoded pick features over the tokens. Finally, we
propose to perform two stages of training: first we train the
MultiMAE in a pretraining stage with pixel reconstruction
with the pick success prediction objective. During the second
stage we update the weights of the MultiMAE while training.
We refer to this second stage as the finetuning stage. Before
presenting results and our ablation study, we will first describe
the pretraining and finetuning stages in more detail.
C. Pretraining the MultiModal Autoencoder
We first pretrain the MultiMAE on 343K examples of
deployment data in the standard dataset. The nature of the
Fig. 4: Given visual encoder tokens outputted by the Multi-
MAE encoder, we implement two types of token weighting:
either using a simple mean pooling of all tokens, or weighting
via cross-attention. In the latter case, the encoded pick features
are also provided as input to the cross-attention block.
multimodal data is similar to that in the original MultiMAE
work . During pretraining, the model receives as input
a similar schedule as in  for randomly masking a portion
of the tokens from the three modalities, and the MultiMAE
learns to reconstruct the pixels in the missing image patches.
When pretraining on the deployment dataset, the MultiMAE
is initialized with weights from a publicly available version
pretrained on ImageNet, then pretrained for 800 epochs.
At pretraining, the RGB and depth images are resized to
224  224. For the semantic segmentation input images, we
first define 9 classes of semantic categories for the items
encountered. Instance segmentation images in our dataset,
obtained from a previously trained segmentation model, are
directly converted into semantic segmentation images. The
images are downsampled by a factor of 4 compared to the
RGB and depth data to reduce computation cost . Thus, the
resulting dimensions are 5656. At the start of pretraining, we
initialize a different class embedding for each semantic class,
which is then updated during pretraining. When reconstructing
the semantics, MultiMAE will predict a value for each class,
i.e., the semantics output has nine channels per pixel. An
example of the resulting models ability to reconstruct all three
modalities with randomly masked input is shown in Fig. 5.
When we visualize the reconstruction, for each pixel, we show
the class with the highest prediction value.
D. Fine-tuning for Pick Success Prediction
We next train the MultiMAE along with the additional
network components on 275K training examples to perform
pick success prediction. We refer to this stage as finetuning. We
use 69K validation examples to decide when to stop training.
The model is then evaluated on 86K test data, which have not
Masked Inputs
Semantic
Original Reference
Fig. 5: Example of MultiMAE reconstruction for all three
left column shows the ground truth reference images.
been seen during both stages of training. For each split the
success to fail ratio is about 11:1. We use a weighted loss to
account for this high class imbalance. We also incorporate pick
information into the model by using a cross-attention module
as shown in Fig. 4 bottom. The cross-attention results in a
weighted sum of the image tokens based on the pick features.
For simplicity, we discuss the case where we only consider
one attention head. We first project the pick embedding into
a query token Q, and project the encoder tokens (we omit the
global CLS token) into value tokens V and key tokens K. The
are trained with the pick success objective. Following self-
scale value, followed by a softmax, resulting in an attention
weight vector. We then take a weighted sum of the encoder
tokens using the attention weight vector. The intuition behind
using a cross-attention layer is the fact that we aim to combine
information from two entirely different domains: images and
pick features. The MultiMAE provides a visual encoding of the
multimodal images, which in turn is queried with pick features
for pick success. In the next section, we will compare cross-
attention with the more naive setting of using mean pooling,
where each token has the same weight for the attention weight
vector. Finally, the weighted sum is then input to a MLP
prediction head, together with the projected pick features, to
obtain pick success prediction.
Next we will present our experimental results that reflect on
the effectiveness of the demonstrated approach.
V. EXPERIMENTAL RESULTS
We use the Area Under the Receiver Operating Character-
istic Curve (ROC AUC) as our main metric for evaluating
the performance. This metric indicates how well the model
performs under different decision thresholds and is more
suitable for the case where the data is imbalanced . The
expert-engineered features are always provided for the shallow
model baseline, but not to the demonstrated MultiMAE model.
A. Experiments
For our main experimental evaluation, we will adopt the
best performing model among the variants of the MultiMAE
we have experimented with. We will refer to this as the
demonstrated approach. The settings for the demonstrated
approach are:
We pretrain on in-domain RGB (R), depth (D) and
semantic segmentation (S) images (Pretrain R-D-S);
For the finetuning stage we use RGB and depth images
(Finetune R-D), along with an image of the pick location.
The token weighting is done using cross-attention.
An additional pick location image is added to further
boost performance.
During finetuning, the MultiMAE encoder weights are
updated according to the pick success loss.
according to a bounding box with additional padding
derived from the target items segmentation mask. We
refer to this as the local crop. The local crops are
augmented with random offsets for additional robustness.
Ablation studies that follow will evaluate the importance of
the above implementation choices.
1) Largely Unoccluded Item Picks - Standard Dataset:
Fig. 6 shows a performance comparison of the demonstrated
MultiMAE approach against the previously best performing
method to date on this dataset, i.e., the shallow model , as
well as a baseline version of a (unimodal) mask autoencoder
(MAE Base). The demonstrated approach has a significantly
higher performance than the MAE Base variant (79.1 vs 90.6).
Their differences are studied further in the ablations. The
demonstrated approach also outperforms the shallow model by
about 4 points without the need to access the expert-engineered
features that the shallow model uses.
We also compared against a learn-from-scratch, point-
cloud baseline. This baseline adopts the PointTransformerV3
(PTv3) model  as point cloud encoder. Each encoder
block receives a point cloud as input and employs: 1) a 3D
sparse convolution layer  to serve as conditional positional
created using space filling curves. An action is represented by
a set of pick tokens. We use a linear layer to embed a pick
feature vector containing the discretized approach angle, wrist
and store this for each pick token. The pick token is then
used as query in the cross attention and the average location
of activated suction cups is used as location for this pick token
when performing 3D rotary positional encoding for the cross
attention. A linear projection head is then applied to the output
of this decoder layer. This baseline results in a test AUC of
84.6, which is stronger than the MAE baseline, but is lower
than both the shallow model and the demonstrated MultiMAE
model. This again highlights that multimodal pretraining can
provide significant benefits in terms of pick success prediction.
Note that this result simply shows the scene encoder in the
demonstrated approach can also be a point cloud model.
Although this paper is focused on 2D multimodal data, we
believe proper 3D pretraining as well as combining 3D and 2D
modalities can be beneficial, and we plan to further investigate
3D modalities in future work, as discussed in Section VI.
Fig. 6: Performance comparison of the shallow model
(relies on expert features) against a PTv3 baseline, a MAE
of largely unoccluded item picks. The demonstrated approach
outperfoms the shallow model by about 5 in test AUC.
2) Partially Occluded Item Picks - Random Dataset: In
order to evaluate the robustness of the demonstrated approach
and learned representation, we also evaluate the demonstrated
approach against the shallow model on a random pick dataset.
The item configuration distribution of this dataset, with many
picks for items that are partially occluded (shown in Fig 7),
is very different from the standard dataset, where we mostly
pick unoccluded items. This random dataset contains 8,461
training examples, 2,115 validation and 2,644 test examples.
The success-failure ratio on each split is approximately 4.4:1.
Table I compares the shallow model and the demonstrated
approach in three different settings: (a) Pretrain on the standard
random dataset (PT: STD, FT: RND, Test: RND); (b) Pretrain
on standard, finetune on standard, and then zero-shot test on
the random dataset (PT: STD, FT: STD, Test: RND); and (c)
Pretrain on standard, finetune on standard first, and then further
finetune on the random dataset, then test on the random dataset
(PT: STD, FT: BOTH, Test: RND). Note that the shallow
model does not have pretraining, and for case (c), the shallow
model is finetuned (trained) on the combination of the standard
and random datasets.
The demonstrated approach outperforms the shallow model
consistently in all settings. This demonstrates that the repre-
sentations learned from the demonstrated approach are robust
on different item configurations.
Fig. 7: Examples of picking partially occluded items. Columns
and corresponding image with the target item mask and pick
point location. In both cases, the target item is partially
occluded by a larger item at the top of the unstructured pile.
TABLE I: Performance comparison between the shallow
model and the demonstrated approach on a different dataset
where random items that are partially occluded are picked in
different settings. The model is pretrained (PT) on standard
dataset (STD). And then can be finetuned (FT) on the random
dataset (RND), or first finetuned on the standard dataset then
on the random dataset (BOTH). Then the model is tested on
the random dataset.
Inference
Performance
Demonstrated
Demonstrated
Demonstrated
3) Package Picking Dataset: We also investigate whether
the demonstrated approach can work on a different pick scene
containing packages rather than items. Here we pretrain and
finetune on the multimodal data derived from the package
manipulation task described in . This is quite different
from the item manipulation setting, and the majority of the
items to pick are packages such as boxes, bags and envelopes,
as shown in Fig. 8. Here we consider a package picking dataset
with 100K training, 20K validation and 20K test examples,
each with 2:1 success-failure ratio.
Fig. 8: Example pick scenes with packages. There is less
variety in object appearance relative to the domain of item
Table II shows when we perform multimodal pretraining and
finetuning on this package picking dataset, we can also out-
perform the shallow model that relies on expert features, and
obtain the best performance. This demonstrates the demon-
strated approach also works with a different pick scene with
a different item distribution.
TABLE II: Package picking dataset experiment. Results show
that the demonstrated approach can also outperform the shal-
low model for this different setting where the majority of the
items to pick are packages such as boxes, bags and envelopes.
Package Picking Dataset Experiment
Performance
Shallow Model
No Pretrain
Generic RGB Pretrain, Frozen
In-domain Multimodal Pretrain, Finetune
Item Picking Data Pretrain
Item Picking Data Pretrain, Frozen
B. Ablation Studies
We present a series of ablations to better understand what
the most important factors are for the performance of the
demonstrated method, and how performance differs with vari-
ations of data, input modalities and other settings.
1) Effect of Visual Modalities: Table III shows the effect
of having different combinations of the visual modalities, i.e.,
RGB (R), depth (D) and semantics (S), at the pretraining and
finetuning stages. The results show that pretraining with more
modalities can bring performance gain even when finetuning
with RGB only. When pretrained with all three modalities,
having depth and semantics as additional input at the finetun-
ing stage can also further improve performance. When only
a single modality is used at finetuning, RGB has the best
our demonstrated approach, we do not use semantics during
while providing only minimal performance improvement.
2) Effect of In-Domain Pretraining: Many popular works in
the literature advocate for the use of frozen visual representa-
tions that are pretrained on large generic datasets [33, 29, 28].
This is reasonable for tasks where only a small amount of in-
domain data is available. Table IV shows how pick prediction
success performance can be affected heavily by the pretraining
dataset. Although pretraining on generic datasets (ImageNet)
with either RGB only or all three modalities will improve
performance over not pretraining the visual encoder at all,
pretraining on in-domain data with all three modalities can
further boost performance (row 4 in Table IV).
3) Effect of Local Crop Sizes: For the visual input at the
finetuning stage, we can either use the image of the entire pick
around the target item. The size of the crop is determined
by the segment bounding box and a padding value. We note
that the model is pretrained on both global and local crop
due to random crop augmentation following . A padding
TABLE III: Visual modality ablation. Comparison of perfor-
mance for pretraining and finetuning with different modali-
ties. The modalities are RGB (R), depth (D), and semantic
segmentation (S) images. The effect is measured with respect
to the default setting, denoted with . The default setting
is also described in Figure 1. The improvement of using all
three modalities for both pretraining and finetuning (last row)
is minimal. The best performance is highlighted in bold.
Visual Modality Ablation
Performance
TABLE IV: Pretrain domain ablation. Comparison of perfor-
mance for different pretraining settings. Generic: pretrain on
with RGB, depth, and semantic segmentation on in-domain
data (indicated by ) achieves highest performance.
Pretrain Domain Ablation
Performance
No pretrain
of 0 means the crop is tight around the target item segment,
larger values for the padding will include more surrounding
information. When we have multiple visual modalities, we
crop all of them the same way. Fig. 9 shows the global scene
image and the same image with different local crop sizes.
The performance we get when using them as input is shown
directly below each image. A local crop is better than a global
4) Different Ways to Incorporate Pick Features: The pick
success prediction model needs to integrate information from
very different domains: visual modalities and encoded pick
to the pick, e.g., activated suction cups of the multi-suction
cup end effector. How we combine the images and the pick
information can affect the performance. In this ablation we
study three different ways to incorporate the pick features:
(1) use a cross-attention module for learned token weighting;
(2) use a local crop of the input images, centered around the
target item, instead of the entire (global) image; (3) mark the
pick point on another 2D image, and use it as an additional
visual input modality. Table V shows how different ways of
(a) Global
(b) Local0
(c) Local50
(d) Local100
Fig. 9: Global image vs local crop centered around the target
item with different padding values. This is also described in
Figure 1. Performance (in parenthesis) with local crops is
better compared to the global image. A padding of 50 is the
best. gives the best performance.
incorporating pick location affect performance.
TABLE V: Pick incorporation ablation. Comparison of perfor-
mance for different ways to incorporate pick location informa-
tion. Here cross-attn means use cross attention to compute
weighted tokens; pick loc image means marking the pick
point on a 2D image, and using it as an additional visual input
modality. The effect is measured with respect to the default
Pick Incorporation Ablation
Performance
Global mean pool wo pick loc image
Global cross-attn wo pick loc image
Local mean pool wo pick loc image
Local cross-attn wo pick loc image
Local mean pool w pick loc image
Local cross-attn w pick loc image
With global scene multimodal images, without providing a
pick location image, and using mean pooling of the encoded
image tokens as the weighting (refer to Fig. 4), it is hard
for the model to identify the target item when there are
multiple items in the scene (row 1 in Table V). Adding the
cross attention module allows the model to associate the pick
location information of the pick features with the pick point
location and target item in the image. Fig. 10 shows evidence
that in this case, the model is able to learn (through the pick
success prediction loss) which image tokens to pay attention
The right two columns of Fig. 10 show the model initially
has random attention, but learns to focus more on the target
item and image patches near the pick point. We also visualize
the pick point as a small square in the second column, together
with the target item segmentation mask. The crosshair in the
three columns on the right is only used for visualization
purpose in the visualized attention map. Note that in this
particular experiment, the target item mask and the pick point
square are only provided as a reference and are not available
to the model during training or inference.
If we switch to using a local crop image centered around
the target item, the model can more easily combine the pick
Target Item (Ref)
Initial Attention
Learned Attention
Fig. 10: Different examples of visualization of the learned
attention. In this particular case, only the RGB and the pick
coordinates are provided to the model. The target item mask
and pick point in the second column are only for reference.
The model is able to learn to pay attention to regions near the
target item and pick point.
features with the encoded images, leading to performance
improvement (row 3, Table V). However, notice that the
positive effect of cross attention is reduced in the local input
setting (compare rows 3 and 4).
Inspired by recent work that shows explicitly marking object
location with a single pixel can help manipulation , we also
tried providing the pick location explicitly as an additional 2D
pick location is marked by a square of pixels on a 224  224
single channel image, as shown in Fig. 11. The last two rows
in Table V show this can bring performance to 90.60 (Local
cross attention  pick loc image). Again notice that the effect
of cross attention is weaker when we use a local crop together
with the pick location image.
Fig. 11: Left: RGB input image. Middle: pick location and
target item mask for our reference. Right: the pick location
image which is used as an additional input.
These results show that using cross attention, local crop
and pick location image can all improve performance, and
their effects are partly overlapping. In practice, the best input
setting can depend on the use case, e.g. when evaluating large
amounts of pick candidates in a scene, using a global image
with cross attention and without pick location image can give
the lowest overhead. So we include all three components in
our demonstrated approach for more robustness.
5) Effect of Finetuning the Visual Encoder: We found that
finetuning the visual encoder with pick success prediction
leads to improved performance compared to freezing it: 87.89
vs 90.6 as shown in Table VI. While it can be good to use a
frozen visual encoder in a small data setting, this result shows
we should further finetune when a large dataset is available.
TABLE VI: Performance of the demonstrated method with or
without encoder finetuning at finetuning stage.
Encoder Finetuning
Performance
Frozen visual encoder
Finetuned visual encoder
6) Effect of Data Augmentation: In our experiments we also
explored whether using data augmentation during finetuning
helps performance. The data augmentation is a random shift of
the local crop. The crop is done on images of shape 512612
(done before they are resized to 224  224). The center of
the crop is initially the center of the item segment. When
data augmentation is used, the center can be shifted to four
directions with a random value (-25 to 25 pixels). We also
use a random crop padding of up to 150 pixels (in 50 pixel
increments) to change the size of the crop. We then ensure
the target item is always in view, so if the crop region is
shifted too much that the item is missing in the view, then the
crop region is enlarged to include the target item. The same
crop is applied to all visual inputs (RGB, depth, semantics,
and pick location image). Augmentation is only applied for
training examples and not used during validation or testing.
Table VII shows that although our dataset is fairly large, data
augmentation still can provide marginal improvement.
TABLE VII: Performance of the demonstrated method with or
without data augmentation.
Data Augmentation Ablation
Performance
Without data augmentation
With data augmentation
7) Effect of Pretraining Epochs: Table VIII shows how
much performance changes as we pretrain for a larger number
of epochs on in-domain data. We always start in-domain
pretraining with generic weights pretrained on ImageNet con-
taining three modalities. On row 1, 0 epochs means we
use the generic pretrained weights without any in-domain
results show that performance improvement gains are largest in
earlier epochs of training, and after pretraining for 200 epochs
the performance starts to improve more slowly. Nevertheless,
the best performance is obtained with the most epochs, which
is 800 in our experiments.
TABLE VIII: Performance of the demonstrated method with
different number of pretraining epochs on in-domain data.
Numbers after the dashed line indicate higher performance
compared to the shallow model.
Pretrain Epoch Ablation
Performance
0 epochs
20 epochs
100 epochs
200 epochs
400 epochs
800 epochs
8) Effect of Pretraining Data Ratio: Table IX shows how
much performance changes as we pretrain on different ratios
of the in-domain dataset. These results show that even when
pretrained on 1 of the in-domain data (3.4K), there is already
a significant benefit, and the performance can be further
improved when we have more in-domain data for pretraining.
TABLE IX: Performance of the demonstrated method when
pretrained on different data ratios. By default, we pretrain
on 100 of the data (343K). Numbers after the dashed line
indicate higher performance than the shallow model. With just
1 of in-domain the model can outperfom the shallow model.
Pretrain Ratio Ablation
Performance
9) Alternative Representation Learning Methods with In-
domain Data: To further understand how other representa-
tion learning methods perform w
