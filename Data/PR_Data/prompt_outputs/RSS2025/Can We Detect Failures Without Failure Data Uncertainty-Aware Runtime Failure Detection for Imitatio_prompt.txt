=== PDF文件: Can We Detect Failures Without Failure Data Uncertainty-Aware Runtime Failure Detection for Imitatio.pdf ===
=== 时间: 2025-07-22 15:48:17.626644 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Can We Detect Failures Without Failure Data?
Uncertainty-Aware Runtime Failure Detection for
Imitation Learning Policies
Chen Xu1, Tony Khuong Nguyen1, Emma Dixon1, Christopher Rodriguez1, Patrick Miller1, Robert Lee2,
Paarth Shah1, Rares Ambrus1, Haruki Nishimura1, and Masha Itkina1
1Toyota Research Institute (TRI), 2Woven by Toyota (WbyT)
chen.xutri.global
AbstractRecent years have witnessed impressive robotic
manipulation systems driven by advances in imitation learning
and generative modeling, such as diffusion- and flow-based
approaches. As robot policy performance increases, so does
the complexity and time horizon of achievable tasks, induc-
ing unexpected and diverse failure modes that are difficult
to predict a priori. To enable trustworthy policy deployment
in safety-critical human environments, reliable runtime failure
detection becomes important during policy inference. How-
knowledge of failure modes and require failure data during
and scalability. In response to these limitations, we present
in imitation learning-based robotic manipulation. To accurately
identify failures from successful training data alone, we frame
the problem as sequential out-of-distribution (OOD) detection.
We first distill policy inputs and outputs into scalar signals
that correlate with policy failures and capture epistemic un-
certainty. FAIL-Detect then employs conformal prediction (CP)
as a versatile framework for uncertainty quantification with
statistical guarantees. Empirically, we thoroughly investigate both
learned and post-hoc scalar signal candidates on diverse robotic
manipulation tasks. Our experiments show learned signals to be
mostly consistently effective, particularly when using our novel
flow-based density estimator. Furthermore, our method detects
failures more accurately and faster than state-of-the-art (SOTA)
failure detection baselines. These results highlight the potential
of FAIL-Detect to enhance the safety and reliability of imitation
learning-based robotic systems as they progress toward real-
world deployment. Videos of FAIL-Detect can be found on our
I. INTRODUCTION
Robotic manipulation has applications in many important
dous success in learning complex manipulation skills from
human demonstrations using stochastic generative modeling,
such as diffusion- [12, 65] and flow-based methods [9,
45]. However, despite their outstanding results, policy net-
works can fail due to poor stochastic sampling from the
action distribution. The models may also encounter out-of-
distribution (OOD) conditions where the input observations
deviate from the training data distribution. In such cases,
the generated actions may be unreliable or even dangerous.
ensure the safety and reliability of the robotic system.
Detecting failures in robotic manipulation tasks poses sev-
eral challenges. First, the input data for failure detection, such
as environment observations, is often high-dimensional with
complicated distributions. This makes it difficult to identify
discriminative features that distinguish between successful and
failed executions, particularly in the imitation learning setting
where a reward function is not defined. Second, there are
countless opportunities for failure due to the complex nature
of manipulation tasks and the wide range of possible environ-
mental conditions (see Fig. 2). Consequently, failure detectors
must be general and robust to diverse failure scenarios.
In imitation learning, training data naturally consists of
successful trajectories, making failed trajectories OOD. Prior
work often tackles failure detection through binary classifi-
cation of ID and OOD conditions . Thus, many of these
methods [24, 17, 18, 33] require OOD data for training the fail-
ure classifier. This poses significant challenges since collecting
and annotating a comprehensive set of failure examples is
often time-consuming, expensive, and even infeasible in many
real-world scenarios. Moreover, classifiers trained on specific
sets of OOD data may not generalize well to unseen failure
modes. To address these limitations, we develop a failure
detection approach that operates without access to OOD data,
overcoming the reliance on failure examples while maintaining
robust performance.
We propose FAIL-Detect: Failure Analysis in Imitation
Learning  Detecting failures without failure data. FAIL-
Detect is a two-stage approach (see Fig. 1) to failure detection
in generative imitation-learning policies. In the first stage, we
extract scalar signals from policy inputs andor outputs (e.g.,
robot states, visual features, generated future actions) that are
discriminative between successes and failures during policy
inference. We investigate both learned and post-hoc signal
for failure detection. A key novelty of our method is the
ability to learn failure detection signals without access to
failure data. Aside from being performant, our method enables
faster inference than prior work , which requires sampling
multiple robot actions during inference. In the second stage,
Stage 2: Threshold Calibration
Stage 1: Score Learning
Runtime Failure Detection
Fig. 1: FAIL-Detect: Failure Analysis in Imitation Learning  Detecting failures without failure data. We propose a two-stage approach to
failure detection. (Left - Stage I) Multi-view camera images and robot states are distilled into failure detection scalar scores. Images are first
passed through a feature extractor and then, along with robot states, constitute observations Ot. Both Ot and generated future robot actions
At can serve as inputs to a score network DM. This network outputs scalar scores DM(At, Ot) that capture characteristics of successful
demonstration data. (Middle - Stage II) Scores from a calibration set of successful rollouts are then used to compute a mean t and band
width ht to build the time-varying conformal prediction threshold. (Right - Runtime Failure Detection) A successful trajectory (bottom)
has scores that consistently remain below the threshold. When a failure occurs (top), such as failure to fold the towel, the score spikes above
the threshold, triggering failure detection (red box).
we use conformal prediction (CP) [52, 47] to construct a
time-varying threshold to sequentially determine when a score
indicates failure with statistical guarantees on false positive
rates. By integrating adaptive functional CP  into our
dynamics of manipulation tasks unlike static thresholds used
in prior work .
Our contributions are as follows. We present FAIL-Detect,
a modular two stage uncertainty-aware runtime failure detec-
tion framework for generative imitation learning-based robotic
manipulation. First, we construct scalar scores representative
of task successes. Second, we use CP to build a time-varying
threshold with stochastic guarantees. FAIL-Detect is flexible
to incorporate new score and threshold designs. We thoroughly
test learned and post-hoc score candidates that rely solely on
successful demonstrations. FAIL-Detect with our novel learned
score (logpZO) derived from a flow-based density estimator
surpasses other methods. We show that FAIL-Detect identifies
failures accurately and quickly on diverse robotic manipulation
SOTA failure detection baselines.
II. RELATED WORK
Imitation Learning for Robotic Manipulation. Imitation
learning has emerged as a powerful paradigm for teaching
robots complex skills by learning from expert demonstrations.
Diffusion policy (DP)  using diffusion models  has
emerged as highly performant in this space. DP learns to
denoise trajectories sampled from a Gaussian distribution,
effectively capturing the multi-modal action distributions often
present in human demonstrations [63, 12]. Diffusion mod-
els have been used to learn observation-conditioned poli-
cies [12, 49], integrate semantic information via language
conditioning [63, 10, 44], and improve robustness and gen-
eralization [29, 54, 56]. Concurrently, vision-language-action
models like Octo  and OpenVLA  have shown promise
in generalist robot manipulation by leveraging large-scale
pretraining on diverse datasets. More recently, flow matching
(FM) generative models have been proposed as an alternative
to diffusion in imitation learning [4, 23, 6, 64], offering
faster inference and greater flexibility (i.e., extending beyond
Gaussian priors ), while achieving competitive or superior
success rates. We test FAIL-Detect on DP and FM imitation
learning architectures.
OOD Detection. The task of detecting robot failures can be
viewed as anomaly detection, which falls under the broader
framework of OOD detection . Ensemble methods ,
which combine predictions from multiple models to improve
robustness and estimate uncertainty, have long been regarded
as the de facto approach for addressing this problem. However,
they are computationally expensive as they require training and
running inference on multiple models. Another popular ap-
proach frames OOD detection as a classification problem .
This formulation learns the decision boundary between ID
and OOD data by training a binary classifier [53, 15], but
requires OOD data during training. In contrast, density-based
approaches [35, 16, 60], one-class discriminators based on
random networks [13, 21], and control-theoretic methods
aim to model information from ID data without relying on
OOD data during training. Density-based methods attempt to
capture the distribution of ID data, yet they can be challenging
to optimize. One-class discriminators have shown superior per-
formance over deep ensembles in practice but can be sensitive
to the design of the discriminator model. Control-theoretic
approaches use contrastive energy-based models; however,
they often require a representation of the systems dynamics.
learn parameters for second-order distributions (e.g., Dirichlet)
to approximate epistemic uncertainty (due to limited model
knowledge or OOD inputs) from aleatoric uncertainty (due to
inherent randomness in the data). Lastly, distance-based ap-
proaches [3, 51, 27] identify OOD samples by computing their
distance to ID samples in the input or latent space, avoiding the
need for training but exhibiting limited performance compared
to other approaches. We consider many of the listed model
variants as score candidates in FAIL-Detect.
(a) Slipped out early. (b) Slipped out late.
(c) Tilted upward.
(d) Tilted downward.
(e) Tilted slightly.
(f) Not picked up.
Fig. 2: Diverse failure types observed for a single trained policy g on a simple pick-and-place task (put square on peg). These failures
occurred at different time steps across multiple rollouts and include the square slipping out of the gripper or being misplaced (e.g., with
tilted position) on the peg. FAIL-Detect is able to handle the wide range of failures observed at test time.
Failure Detection in Robotics. Detecting failures in robotic
systems is important for ensuring safety and reliability, as
failures can lead to undesirable behaviors in human envi-
ronments [39, 42, 34]. Various approaches have been pro-
LLM embeddings  and using the reconstruction error
from variational autoencoders (VAE) to detect anomalies in
behavior cloning (BC) policies for mobile manipulation .
formal prediction for actions generated by an LLM-based plan-
These works do not consider failure detection in the setting
of generative imitation learning policies. On the other hand,
Gokmen et al.  learn a state value function that is trained
jointly with a BC policy and can be used to predict failures.
Liu et al.  propose an LSTM-based failure classifier for a
BC-RNN policy using latent embeddings from a conditional
VAE. Given a Transformer-based policy and a world model to
predict future latent embeddings, Liu et al.  train a failure
detection classifier on the embeddings. To handle previously
unseen states, they also propose a SOTA OOD detection
PCA-kmeans in Section V). However, unlike FAIL-Detect,
these methods require collecting failed trajectories a priori to
detect failures. Meanwhile, Wang et al.  uses self-reset to
collect additional failure data and train a classifier to identify
failure modes. In their on-robot experiments, approximately
2000 trajectories (roughly 2 hours) had to be collected using
prediction intervals for rewards of predicted trajectories. He
et al.  propose using random network distillation (RND)
to detect OOD trajectories and select reliable ones. These
works do not directly consider runtime failure detection. Our
two-stage solution for this problem combines the advantages
of both approaches. The closest SOTA method to FAIL-
Detect by Agia et al.  introduces a statistical temporal
action consistency (STAC) measure in conjunction with vision-
language models (VLMs) to detect failures within rollouts at
runtime. STAC does not require failure data, consists of a score
computed post-hoc from a batch of predicted actions and a
constant-time CP threshold to flag failures, and is evaluated
in the context of DP. We demonstrate improved empirical per-
formance over STAC by integrating learned failure detection
scores with a time-varying CP band.
III. PROBLEM FORMULATION
Our focus in this work is to detect when a generative
imitation learning policy fails to complete its task during
execution. We define the following notation. Let g(At  Ot)
denote the generator, where Ot represents the environment
observation (e.g., image features and robot states) at time t,
and g is a stochastic predictor of a sequence of actions
At  (Att, At1t, . . . , AtH1t) for the next H time steps.
The first H < H actions At:tHt are executed, after which
the robot re-plans by generating a new sequence of H actions
at time tH. Recent works have trained effective generators g
via DP  and FM . Given an initial condition O0 and the
generator g to output the next actions, we obtain a trajectory
t  (O0, A0, OH, AH, . . . , Ot, At) up to t  kH (k 1)
execution time steps. Failure detection can thus be framed
as designing a decision function D(; ) : t {0, 1} with
a decision. If the decision D(t; )  1, the rollout is flagged
as a failure at time step t. For instance, in a pick-and-place
the object or misses the target position.
IV. FAILURE DETECTION FRAMEWORK
Given action-observation data (At, Ot), we propose a two-
stage framework to design the decision function D(; ):
1) Train a scalar score model DM(; ) : (At, Ot) R
(for score method M) on action andor observation
pairs from successful trajectories only.
2) Calibrate time-varying thresholds t based on a CP band.
The final decision D(t; )  1(DM(At, Ot; ) > t) raises
a failure flag if the scalar score DM(At, Ot; ) exceeds the
threshold t at time step t. This two-stage framework is
flexible to incorporate new scores in Stage 1 or new thresholds
in Stage 2. See Fig. 1 for an overview of the framework.
A. Design of Scalar Scores
To construct scores indicative of failures, we propose a
novel score candidate and several adaptations of existing
approaches originally developed for other applications. See
Table I for an overview of the scoring methods we consider.
TABLE I: Overview of score methods evaluated in this work. The input was selected either based on the structure and
requirements of each method or, when multiple input combinations were possible, based on empirical performance. All
methods except STAC (which proposes a different calibration method; see Section V) use time-varying CP bands described in
Section IV-B.
Category
Original application
Density estimation
Density estimation
Likelihood estimation on tabular data
Second-order
OOD detection for classification and regression
(Ot, At)
Second-order
OOD detection for human pose estimation
(Ot, At)
One-class discriminator
Reinforcement learning ; OOD detetcion
One-class discriminator
Efficient sampling of flow models
Post-hoc
Smoothness measure
Smoothness analysis for time series data
Post-hoc
Statistical divergence
Baseline
Failure detection for generative imitation learning policies
PCA-kmeans
Post-hoc
Clustering
Baseline
OOD detection during robot execution
When designing a scalar score that is indicative of policy
The method should not require failure data during training as
it may be too diverse to enumerate (see Fig. 2). (2) Light-
real-time robot manipulation. (3) Discriminative: The method
should yield gaps in scores for successful and failed rollouts.
To avoid overfitting on historical data, the score network DM
only takes the latest TO steps (TO  2 following ) of past
observations Ot alongside future action At as inputs, rather
than the growing trajectory history. To meet our desiderata,
we select and build on the following approach categories.
(a) Learned data density: we fit a normalizing flow-based
density estimator to the observations, where data far from the
distribution of successful trajectory observations may indicate
failure. The approach we term lopO  fits a continuous nor-
malizing flow (CNF) f to the set of observations {Ot}t0. A
low log p(Ot) for a new observation Ot implies it is unlikely,
indicating possible failure. Note the computation of log p(Ot)
requires integration of the divergence of f over the ODE
leverages the same CNF f to evaluate the likelihood of a noise
estimate ZOt (conditioned on an observation Ot). Using the
forward ODE process, we compute ZOt by integrating f over
the unit interval [0,1], starting from Ot as the ODE initial
condition. When Ot is ID, ZOt is approximately Gaussian,
leading to p(ZOt)  C exp(0.5ZOt2). Thus, a high value
2 corresponds to a low likelihood p(ZOt) in the noise
space. Further details on logpZO are described in Appendix A.
The key distinction between lopO and logpZO lies in their
vation space, while the latter does so in the latent noise space.
We expect the latter to be better because its computation does
not require the divergence of f integrated over [0, 1], a hard-
to-estimate quantity in high dimensions.
(b) Second-order: these methods learn parameters for
second-order distributions that can separate aleatoric and epis-
temic uncertainty . NatPN  imposes a Dirichlet prior
on class probabilities and optimizes model parameters by
minimizing a Bayesian loss. To use NatPN, we discretize
the observations Ot using K-means and apply NatPN to
the discretized version. We also consider multivariate deep
evidential regression DER , which assumes AtOt follows
a multivariate Gaussian distribution with a Wishart prior and
learns its parameters.
(c) One-class discriminator: we consider methods that
learn a continuous metric, but do not directly model the dis-
tribution of input data. The one-class discriminator RND
initializes random target fT () and random predictor f(; )
networks. The target is frozen, while the predictor is
trained to minimize E(At,Ot)ID trajectory[DM(At, Ot; )] for
DM(At, Ot; )  fT (At, Ot)f(At, Ot; )2
2 on successful
demonstration data. Intuitively, RND learns a mapping from
the data (At, Ot) to a preset random function. If the learned
mapping starts to deviate from the expected random output,
the input data is likely OOD. In this category, we also consider
consistency flow matching (CFM) , which measures tra-
jectory curvature with empirical variance of the observation-
to-noise forward flow. The intuition is that on ID data, the
forward flow is trained to be straight and consistent. Thus,
high trajectory curvature indicates the input data is OOD.
(d) Post-hoc metrics: we investigate methods that compute
a scalar score analytically without learning. We use SPARC
to measure the smoothness of predicted actions. We expect
SPARC to be useful for robot jitter failures, which are em-
pirically frequent in OOD scenarios. The recent SOTA in
success-based failure detection, STAC , falls in the post-
hoc method category. However, since it comes with its own
statistical evaluation procedure, we describe it as one of our
main baselines in Section V. Additionally, we term the OOD
detection method by Liu et al.  as PCA-kmeans, which
also falls in this category. We retrofit it within our two-stage
framework as another baseline in Section V.
B. Sequential Threshold Design with Conformal Prediction
We design a time-varying threshold t such that a failure
is flagged when DM(At, Ot; ) exceeds t. To do so, we
leverage functional CP , a framework that wraps around a
time series of any scalar score DM(At, Ot; ) (higher indicates
(a) Before disturbance
(b) After disturbance
(c) Successful rollout
(d) Failed rollout
(e) ID initial condition
(f) OOD initial condition
(g) Successful rollout
(h) Failed rollout
(i) ID initial condition
(j) OOD initial condition
(k) Successful rollout
(l) Failed rollout
Fig. 3: Robot hardware experiment scenarios. (Top row) FoldRedTowel with Disturbance: In (b), the human pulls the towel from the
position in (a) towards the bottom during a policy rollout. We note that such recovery behavior is sometimes present in the training data,
so the task may succeed as in (c). A failure case is shown in (d). (Middle row) FoldRedTowel OOD: Compared to ID (e), we start with
a crumpled towel with a blue spatula distractor to the right of the towel as in (f). Neither condition is present in the training data, thus
although the task could succeed as in (g), the success rate is low and the robot typically fails like in (h). (Bottom row) CleanUpSpill OOD:
Compared to ID (i), we start with a green towel as in (j). The training data only contains white and gray towels and, therefore, although the
task could succeed as in (k), the robot typically fails like in (l) with a low success rate.
failure) and yields a distribution-free prediction band C with
user-specified significance level  (0, 1). Under mild con-
ditions [52, 58, 59], C contains any ID score DM(At, Ot; )
with probability of at least 1  for the entire duration of
the rollout. If DM(At, Ot; ) C, we can confidently reject
that (At, Ot) is ID.
For sequential failure detection, we build C as a one-
sided time-varying CP band. The band is one-sided as we
are only concerned with high values of the scalar score
DM(At, Ot; ), which indicate the trajectory is OOD (i.e.,
a failure). Given N successful rollouts as the calibration
i  1, . . . , N and t  1, H, . . . , T}. The CP band is a set
of intervals C  {[lowert, uppert] : t  1, H, . . . , T},
where lowert
min(Dcal) since the band is one-sided.
To obtain the upper bound, we follow , computing the
time-varying mean t and band width ht, so that uppert
t  ht. Further details of upper bound construction are in
Appendix B. Theoretically, for a new successful rollout T
(O0, A0, . . . , OT , AT ), with probability at least 1, the score
DM(At, Ot; ) [lowert, uppert] for all t  1, H, . . . , T. By
defining the threshold t  uppert and setting failures to one,
the decision rule 1(DM(At, Ot; ) > t) controls the false
positive rate (successes marked as failures) at level .
V. EXPERIMENTS
We test our two-stage failure detection framework in both
simulation and on robot hardware. Our experiments span
multiple environments, each presenting unique challenges in
terms of types of tasks and distribution shifts. We empirically
investigate an extensive set of both learned and post-hoc scalar
scores (see Table I) within our FAIL-Detect framework (see
results in Section VI). We refer to Appendix C for more details
on policy training, the CP band calibration procedure, and the
learned scalar score architectures.
a) Tasks:
consider
Robomimic benchmark1 . In the robot hardware experi-
Panda robot station that are significantly more challenging:
FoldRedTowel and CleanUpSpill (see Fig. 8). We construct
OOD settings for each task. In simulation, we adjust the third-
person camera 10 cm upwards at the first time step after t  50
to simulate a camera bump mid-rollout2. For the on-robot
1We omit the Lift task as both FM and DP policies achieve 100 success.
2We use t  15 for Can, which has the shortest task completion time.
(a) Transport ID
(b) Transport OOD
(c) Square ID
(d) Square OOD
(e) Can ID
(f) Can OOD
(g) Toolhang ID
(h) Toolhang OOD
Fig. 4: Quantitative failure detection results for simulation tasks on FM policy (best, second, third); results with TPR and TNR are in Fig. 11
and results on DP are in Fig. 12. For balanced accuracy and weighted accuracy, higher is better and for detection time, lower is better.
The CP band for each task is calibrated with successful rollouts under ID initial conditions only (i.e., the same band is used for ID and
OOD test cases). We group together post-hoc (STAC, PCA-kmeans, SPARC), density-based (logpO, logpZO), second-order (DER, NatPN),
and one-class (CFM, RND) methods and show barplots with standard errors. The dashed line in the Detection Time plots represents the
average successful trajectory time in that setting with standard error. Overall, learned methods outperform post-hoc ones in failure detection.
In terms of combined accuracy (balanced accuracy and weighted accuracy), logpZO and RND are the best two methods, reaching top-1
performance in 1016 and 516 cases, respectively. Moreover, logpZO reaches top-3 performance in 1416 cases, while RND does so in 916
cases. In comparison, the baselines STAC and PCA-kmeans reach top-1 performance in 316 and 016 cases, respectively. Note that STAC
reaches top-3 performance in 816 cases, while PCA-kmeans does so in 316 cases. The learned methods also achieve the fastest detection
logpZO is the fastest method in 38 cases, RND in 08 cases, and the PCA-kmeans baseline does so in 18 cases. In contrast, STAC is the
slowest in nearly all cases, detecting failures only after the average success trajectory time, rendering the detection not practical.
FoldRedTowel task, we disturb the task after the first fold
(challenging ID scenario) and create an OOD initial condition
by crumpling the towel (seen in less than 15 of the data)
and adding a never before seen distractor (blue spatula). For
the CleanUpSpill task, we create an OOD initial condition by
changing the towel to a novel green towel (see Fig. 3).
b) Baselines: We baseline FAIL-Detect against STAC
and PCA-kmeans  as SOTA approaches in success-based
failure detection for generative imitation learning policies.
STAC operates by generating batches (e.g., 256) of predicted
actions at each time step. It then computes the statistical
distance (e.g., maximum mean distance (MMD)) between
temporally overlapping regions of two consecutive predictions,
where the MMD is approximated by batch elements. Intu-
over the rollout and subsequently, STAC makes a detection
using CP. Note that instead of computing a CP band for
a temporal sequence, STAC computes a single threshold
based on empirical quantiles of the cumulative divergence
in a calibration set. We reproduce the method and adopt
hyperparameters used in their push-T example, where we
generate a batch of 256 action predictions per time step. We
did not employ the VLM component of the STAC failure
detector to remain as real-time feasible as possible. Due to
the long STAC inference time (even after parallelization) and
resulting high system latency, we omit its comparison on the
two robot hardware tasks. In our second baseline, Liu et al.
tackle failure detection by training a failure classifier,
which requires the collection of failure training data. However,
this approach is not applicable to our setup as we assume
access to only successful human demonstrations for training
and successful rollouts for calibration. Instead, we incorporate
their proposed OOD detection method as a post-hoc scalar
score in the first stage of FAIL-Detect to construct a fair
baseline. We use the performant time-varying CP band to
obtain thresholds in the second stage. The method measures
the distance of a new observation Ot at test time index
t from the set of training data {Ot}t0, which consist of
(a) FoldRedTowel with FM: (Setting-dependent band) ID  Disturb (b) FoldRedTowel with FM: (ID-only band) ID  Disturb
(c) FoldRedTowel with FM: (Setting-dependent band) OOD
(d) FoldRedTowel with FM: (ID-only band) OOD
(e) CleanUpSpill with DP: (Setting-dependent band) OOD
(f) CleanUpSpill with DP: (ID-only band) OOD
Fig. 5: Quantitative results for the robot hardware experiments across two tasks with policies trained using FM and DP. We consider two
different ways to compute the CP band: setting-dependent using successful trajectories from each OODID environment and ID-only
using only the trajectories from the ID environment. For balanced accuracy and weighted accuracy, higher is better and for detection time,
lower is better. Additional metrics are reported in Fig. 13 and Fig. 14. The figure layout is the same as Fig. 4 (best, second, third), and NaN
detection time indicates that no test rollout was detected as failed. Once again the learned approaches outperform the post-hoc methods.
Note we do not present STAC here as it was slow to run on hardware in real-time. In the small sample size regime, logpZO remains robust
in combined accuracy, achieving top-1 performance in the highest number of cases (812) and top-3 performance in 1112 cases. RND
underperforms by never reaching top-1 performance, yet it always achieves top-3 performance. In contrast, the PCA-kmeans baseline reaches
top-1 performance in 412 cases and top-3 performance in 1012 cases. In detection time, the post-hoc SPARC method is the fastest in 46
remains practical with detection times well below the average success trajectory completion time.
visual encoded features jointly trained with the policy on the
demonstration data. PCA-kmeans first uses PCA to embed
the training features and then applies K-means clustering
to the embedded data to obtain K  64 centroids. After
embedding Ot using the same principal components, the
method computes the smallest Euclidean distance between the
embedding and the K centroids. This distance serves as the
OOD metric (higher values indicate greater OOD). We omit
comparison against ensembles , a popular OOD detection
over ensembles in prior work  and their prohibitively high
computational cost.
c) Evaluation Protocol: To quantify failure detection
cessful rollouts as zero. We then adopt the following
standard metrics: (1) true positive rate (TPR), (2) true
negative rate (TNR), (3) balanced accuracy  (TPR
TNR)  2, (4) weighted accuracy  TPR  (1 )
Successful rollouts
Rollouts
, and (5) detection time
E(At,Ot)test rollouts[arg mint1,H,...,T 1(DM(At, Ot; ) >
t)], which computes the average failure detection time from
the start of the rollout. The balanced accuracy metric equally
represents classes in an imbalanced dataset (e.g., few success-
ful rollouts in an OOD setting). Weighted accuracy represents
how well a method matches the true success  failure distri-
bution. Due to the high human time cost of performing real-
robot rollouts, we evaluate FAIL-Detect and the baselines on
significantly fewer rollouts in the robot hardware tasks (i.e., 50
rollouts) compared to the simulation tasks (i.e., 2000 rollouts).
VI. RESULTS
We present our experimental findings addressing the follow-
ing research questions:
A. How performant is failure detection without failure data?
B. What is the impact of learned vs. post-hoc scores on
failure detection?
C. Do failure detections align with human intuition?
A. How performant is failure detection without failure data?
A key question we consider is whether failure detection
is possible and performant without enumerating all possible
failure scenarios, which is practically infeasible. We conduct
extensive experiments across simulation and robot hardware
tasks to answer this question. We evaluate balanced accuracy,
weighted accuracy, and detection time to assess whether
failures can be identified reliably and quickly.
FAIL-Detect achieves high accuracy with fast detection.
Our two-stage framework demonstrates strong performance
across both accuracy metrics and detection speed. For exam-
score candidates is 78 in simulation (Fig. 4) and 72
on the robot hardware tasks (Fig. 5). This performance shows
the capacity of failure-free failure detection methods to ro-
bustly identify failures across many scenarios. Notably, FAIL-
Detect maintains viable detection time across various score
(a) STAC
(b) PCA-kmeans
(c) logpZO
(d) NatPN
Fig. 6: Qualitative results of failure detection scores overlaid with CP bands. The curves are colored by the ground truth successfailure status
of the rollout (failure  red and success  blue). We show 150 test rollouts on Square ID across post-hoc baselines (STAC, PCA-kmeans) and
learned FAIL-Detect methods (logpZO, NatPN, RND). We use the constant CP threshold for STAC as per . Note that post-hoc baseline
methods mark most trajectories as successes due to the poor failuresuccess separation. In comparison, learned metrics have tight CP bands
and higher failuresuccess separation.
trajectory completion.
B. What is the impact of learned vs. post-hoc scores on failure
detection?
Learned scores outperform post-hoc scores. Looking at
performance across simulation and robot hardware tasks, we
find that learned scalar scores hold an advantage over post-hoc
scores in failure detection. In simulation (Fig. 4), logpZO and
RND are the best two methods, achieving top-1 performance
in 1016 and 516 cases, respectively. STAC is the best in the
post-hoc category for top-1 accuracy in 316 cases, yet PCA-
kmeans is never the best. Overall, there is a large performance
gap between the learned and post-hoc methods, especially
in terms of the best overall accuracy. We did notice that
post-hoc methods perform better in the OOD cases than in
ID scenarios. We hypothesize this may be due to a clearer
distinction between successful ID trajectories and failed OOD
exhibits significant jitter.
In terms of detection time, logpZO is the most efficient,
achieving the fastest time in 38 cases, while PCA-kmeans
does so in only 18 cases. Notably, STACs detection time
consistently exceeds practical limits, surpassing the average
success trajectory time.
For the robot hardware experiments (Fig. 5), with a much
lower rollout data regime for calibration than in simulation
(see Table III) and a wider diversity of behaviors and obser-
top-1 highest balanced accuracy and weighted accuracy in
812 scenarios. The PCA-kmeans baseline is second best with
412 top-1 ranking. RND underperforms by never achieving
top-1 performance, yet it always ranks among the top-3
best methods. Additionally, most methods, including logpZO
and RND, maintain practical detection times well below the
average successful trajectory time. SPARC is on average the
fastest as it attains top-1 performance in 46 cases, but it
exhibits poor accuracy.
logpZO score within the FAIL-Detect framework to be most
consistent in performance. The post-hoc methods are often
at the extremes of performance (either doing well or poorly)
depending on the particular setting.
Qualitative score trends. Visualization of the detection
scores (Figs. 6 and 9) confirms that learned methods are more
discriminative with better score separation between successful
and failed trajectories compared to post-hoc approaches. STAC
also suffers from a single calibration threshold that is time
invariant. In Appendix D, we present comprehensive ablation
studies examining performance sensitivity to CP significance
level  (Fig. 10).
Computational advantage. Some post-hoc methods require
sampling from the stochastic policy repeatedly to achieve a
performant failure score. For example, STAC requires gen-
erating 256 action predictions per time step. Although the
computational efficiency could be improved by generating
fewer predictions, this compromises its statistical reliability.
On the other hand, our learned scores offer significantly faster
inference speeds compared to STAC. For instance, testing on
an A6000 GPU with 50 rollouts, logpZO score computation
takes 0.04 s (Square) and 0.033 s (Transport) per time step,
while STAC requires 1.45 s for both tasks, amounting to a
36-44 times slowdown.
C. Do failure detections align with human intuition?
FAIL-Detects alerts demonstrate strong correlation with
observable failure indications in the environment (Fig. 7).
When scores exceed the decision threshold, these moments
often align with meaningful changes in the physical state of the
task. In simulation environments, the detection scores capture
distinct failure patterns with high precision. For instance,
for the Square task, abrupt increases in scores coincide
with the moment the gripper loses its hold on the square.
instant when the hammer slips during inter-arm transfer. Real-
world applications demonstrate similarly compelling results:
the system effectively detects both human-induced disruptions
leading to an incomplete second towel fold (FoldRedTowel
ID  Disturb) and OOD initial conditions resulting in an
improper first towel fold (FoldRedTowel OOD).
This correspondence between score spikes and physical
events is encouraging for FAIL-Detects capacity to capture
(a) Square
(b) Transport
(c) FoldRedTowel ID  Disturb
(d) FoldRedTowel OOD
Fig. 7: Physical interpretation of logpZO, the most successful and robust learned score method. Failed trajectory scores are in red and
successful ones are in blue. Each figure shows the failure detection time and the corresponding camera view. (Simulation) In Fig. 7a, failure
is flagged when the square slips from the gripper. In Fig. 7b, failure is detected when both arms drop the hammer. (On-robot) In Fig. 7c,
failure is alerted as the second fold attempt fails. In Fig. 7d, failure is detected as the left robot arm fails to complete the first fold.
task-relevant subtlety. The framework successfully translates
complex environmental changes into quantifiable metrics, with
score variations serving as reasonable indicators of failure
events. Moreover, this correlation offers valuable diagnos-
tic capabilities for potential policy improvement. Instead of
requiring an exhaustive a priori enumeration of potential
failure modes, which is an inherently challenging endeavor,
our approach enables potential targeted analysis of observed
failures. By examining executions within temporal windows
surrounding a failure detection, one can efficiently identify
failure types for subsequent analysis.
Environment-dependent thresholds aid performance on
robot hardware tasks. For simulation tasks (Fig. 4), ID-
only bands computed on successful ID rollouts work well for
failure identification in both the ID and the OOD camera bump
scenarios. This setting is practically preferable as it does not
require collecting successful rollouts in each new environment
to calibrate the failure detection threshold. However, on-robot
tasks with OOD initial conditions (Figs. 5, 13 and 14), most
methods show degraded performance with ID-only bands, at
times yielding low or close to zero TNR due to most rollouts
being marked as failure. This over-conservative behavior likely
occurs because OOD successful trajectories are less perfor-
mant. We find on-robot policies to be more sensitive to the en-
vironmental changes we employed than the simulation-based
policies in response to the camera bump. Even when tasks
are completed successfully in these challenging conditions,
trajectories often exhibit poor quality (e.g., slow execution,
jitter) and higher (worse) scores compared to ID successful
failures.
VII. LIMITATIONS
With FAIL-Detect, we demonstrate the promising potential
of a failure detection method without access to failure data.
The proposed two-staged method does have several limita-
served that, at times, the learned scores focus on simpler robot
state information (e.g., gripper closed or open) over the higher-
dimensional visual features. Finding score architectures that
maximally make use of the visual information could improve
failure detection performance. Meanwhile, we acknowledge
the existence of false positives, especially in OOD settings
where the trajectories degrade in performance. Our proposed
setting-dependent CP band is meant to account for these
environmental and behavioral shifts. However, more advanced
techniques such as adaptively adjusting the CP significance
level during inference could be helpful. Additionally, our score
candidates do not consider long sequences of temporal data.
Although the CP band is temporally-aware, having the learned
scores distill temporal patterns from historical time series
data may lead to better failures predictions. Lastly, the timely
detection of failures may be further improved by considering
multimodal sensory information such as sound or tactile.
VIII. CONCLUSION
We present a modular two-stage uncertainty-aware runtime
failure detection approach for generative imitation learning in
manipulation tasks. Our method combines a learned scalar
score and time-varying conformal prediction to accurately and
efficiently identify failures while providing statistical guaran-
tees. Through extensive experiments on diverse tasks, we show
that the novel logpZO score consistently achieves the highest
performance among the considered candidates. Crucially, we
demonstrate that unseen failures can be effectively detected
without access to failure data, which may be difficult to collect
to cover all possible failure scenarios. Our results highlight the
potential of our method to enhance the safety and reliability
of robotic systems in real-world applications.
ACKNOWLEDGEMENT
This work was primarily done during an internship at Toyota
Research Institute (TRI). We thank Vitor Guizilini, Benjamin
feedback. We thank the robot teacher team at the TRI head-
quarters for the data collection, specifically Derick Seale and
Donovon Jackson.
REFERENCES
Christopher Agia, Rohan Sinha, Jingyun Yang, Ziang
Unpacking Failure Modes of Generative Policies: Run-
time Monitoring of Consistency and Progress. In Con-
ference on Robot Learning (CoRL), 2024.
Sivakumar
Alejandro
Melendez-
the Analysis of Movement Smoothness.
Journal of
Neuroengineering and Rehabilitation, 12:111, 2015.
Steven Basart, Mazeika Mantas, Mostajabi Moham-
Out-of-Distribution Detection for Real-world Settings. In
International Conference on Machine Learning (ICML),
Kevin Black, Noah Brown, Danny Driess, Adnan Es-
Vision-Language-Action Flow Model for General Robot
Control. arXiv preprint arXiv:2410.24164, 2024.
Lennart Bramlage, Michelle Karg, and Cristobal Curio.
Plausible Uncertainties for Human Pose Regression. In
International Conference on Computer Vision (ICCV),
pages 1508715096. IEEE, 2023.
Max Braun, Noemie Jaquier, Leonel Rozo, and Tamim
Riemannian Flow Matching Policy for Robot
Motion Learning.
arXiv preprint arXiv:2403.10672,
Fernando Castaneda, Haruki Nishimura, Rowan McAllis-
Barrier Functions: Self-Supervised Policy Filters that
Avoid Out-of-Distribution States. In Learning for Dy-
namics  Control Conference (L4DC), volume 211,
pages 286299. PMLR, 2023.
Bertrand Charpentier, Oliver Borchert, Daniel Zugner,
Simon Geisler, and Stephan Gunnemann. Natural Pos-
terior Network: Deep Bayesian Predictive Uncertainty
for Exponential Family Distributions.
In International
Conference on Learning Representations (ICLR), 2022.
Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen,
and Harold Soh. Dont Start from Scratch: Behavioral
Refinement via Interpolant-based Policy Diffusion.
Lili Chen, Shikhar Bahl, and Deepak Pathak. PlayFu-
Annotated Play.
In Conference on Robot Learning
(CoRL), 2023.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt,
and David K Duvenaud.
Neural Ordinary Differential
Equations. Advances in Neural Information Processing
Systems (NeurIPS), 31, 2018.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
fusion Policy: Visuomotor Policy Learning via Action
Diffusion. In Robotics: Science and Systems (RSS), 2023.
Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja
Estimation By Fitting Prior Networks. In International
Conference on Learning Representations (ICLR), 2020.
Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini,
et al. The Importance of Being a Band: Finite-Sample
Exact Distribution-Free Prediction Sets for Functional
Data. Statistica Sinica, 1:141, 2024.
Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and
Rosanne Liu. Extremely Simple Activation Shaping for
Out-of-Distribution Detection. In International Confer-
ence on Learning Representations (ICLR), 2023.
Xuefeng Du, Zhaoning Wang, Mu Cai, and Sharon Li.
Towards Unknown-aware Learning with Virtual Outlier
Synthesis.
In International Conference on Learning
Representations (ICLR), 2022.
Matt Foutter, Rohan Sinha, Somrita Banerjee, and Marco
Self-Supervised Model Generalization using
Out-of-Distribution Detection. In First Workshop on Out-
of-Distribution Generalization in Robotics at CoRL 2023,
Cem Gokmen, Daniel Ho, and Mohi Khansari.
ing for Help: Failure Prediction in Behavioral Cloning
Through Value Approximation. In International Confer-
ence on Robotics and Automation (ICRA), pages 5821
Martin Hagele, Klas Nilsson, J Norberto Pires, and
Rainer Bischoff. Industrial Robotics. Springer Handbook
of Robotics, pages 13851422, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Deep Residual Learning for Image Recognition.
In Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR), pages 770778. IEEE,
Nantian He, Shaohui Li, Zhi Li, Yu Liu, and You He.
with Confidence Estimation.
In International Confer-
ence on Machine Learning (ICML), volume 235, pages
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
Diffusion Probabilistic Models. Advances in Neural In-
formation Processing Systems (NeurIPS), 33:68406851,
Xixi Hu, qiang liu, Xingchao Liu, and Bo Liu. AdaFlow:
Imitation Learning with Variance-Adaptive Flow-Based
Policies. In Advances in Neural Information Processing
Systems (NeurIPS), 2024.
Arda Inceoglu, Eren Erdal Aksoy, and Sanem Sariel.
Multimodal Detection and Classification of Robot Ma-
nipulation Failures.
Robotics and Automation Letters,
Masha Itkina and Mykel Kochenderfer.
Interpretable
Self-aware Neural Networks for Robust Trajectory Pre-
diction. In Conference on Robot Learning (CoRL), pages
Michael Janner, Yilun Du, Joshua Tenenbaum, and
Sergey Levine.
Planning with Diffusion for Flexible
Behavior Synthesis.
In International Conference on
Machine Learning (ICML), pages 99029915. PMLR,
Ramneet Kaur, Kaustubh Sridhar, Sangdon Park, Yahan
Insup Lee. CODiT: Conformal Out-of-Distribution De-
tection in Time-Series Data for Cyber-Physical Systems.
In Proceedings of the ACMIEEE 14th International
Conference on Cyber-Physical Systems (with CPS-IoT
Week 202
