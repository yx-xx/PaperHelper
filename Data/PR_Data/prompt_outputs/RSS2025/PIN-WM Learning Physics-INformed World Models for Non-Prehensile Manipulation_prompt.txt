=== PDF文件: PIN-WM Learning Physics-INformed World Models for Non-Prehensile Manipulation.pdf ===
=== 时间: 2025-07-22 15:48:39.601421 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：for Non-Prehensile Manipulation
Wenxuan Li1,
Hang Zhao2,
Zhiyuan Yu2
Qin Zou2,4
Ruizhen Hu3,
Kai Xu1,
1National University of Defense Technology
2Wuhan University
3Shenzhen University
4Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)
Equal contributions
Corresponding author
Project page:
AbstractWhile non-prehensile manipulation (e.g., controlled
pushingpoking) constitutes a foundational robotic skill, its learn-
ing remains challenging due to the high sensitivity to complex
physical interactions involving friction and restitution. To achieve
robust policy learning and generalization, we opt to learn a
world model of the 3D rigid body dynamics involved in non-
prehensile manipulations and use it for model-based reinforce-
ment learning. We propose PIN-WM, a Physics-INformed World
Model that enables efficient end-to-end identification of a 3D
rigid body dynamical system from visual observations. Adopting
differentiable physics simulation, PIN-WM can be learned with
only few-shot and task-agnostic physical interaction trajectories.
by Gaussian Splatting without needing state estimation. To
bridge Sim2Real gaps, we turn the learned PIN-WM into a
group of Digital Cousins via physics-aware randomizations which
perturb physics and rendering parameters to generate diverse
and meaningful variations of the PIN-WM. Extensive evaluations
on both simulation and real-world tests demonstrate that PIN-
learning robust non-prehensile manipulation skills with Sim2Real
I. INTRODUCTION
Non-prehensile robotic manipulation [50, 23, 86], which in-
volves moving an object by pushing or poking, finds extensive
applications in many real-world scenarios where grasping is
infeasible due to the weight, size, shape, or fragility of the
simpler end effectors, making systems more cost-effective and
easier to deploy in certain environments [18, 67]. However,
significant challenges arise from the difficulty of fully dictating
the motion and pose of the object being pushed. The complex
underlying dynamics, caused by factors such as friction,
complicate motion planning and control.
Some studies tackle non-prehensile manipulation with im-
itation learning [11, 77], where the reliance on expensive
expert demonstrations limits their scalability. Others explore
deep reinforcement learning (DRL) , leveraging trial-and-
error in simulations to learn policies [39, 80]. However, the
discrepancy between simulation and reality hinders the transfer
of the learned policy to real-world environments [12, 45]. A
promising alternative is to learn world models  of the
environment dynamics in a data-driven manner, which can be
used for predictive control or employed in model-based RL
(a) Few-shot task-agnostic data collection for world model learning
(c)  Sim2Real policy transfer
(b) Physics-aware digital cousins
Fig. 1: PIN-WM is learned from few-shot and task-agnostic physical
interaction trajectories (random pushes of the blocks in this example),
through end-to-end differentiable identification of 3D physics param-
eters essential to the push operation (a). The learned PIN-WM is then
turned into a group of digital cousins via physics-aware perturbations
(b). The resulting world models are then used to learn the task-specific
policies with Sim2Real transferability (c).
for better data-efficiency and Sim2Real generality [26, 28].
quantity and quality of training data and struggle to generalize
to out-of-distribution (OOD) scenarios .
It is well-recognized that incorporating structured priors into
learning algorithms improves generalization with limited train-
ing examples . A number of studies [53, 5, 67] have sought
to integrate principles of physics into the development of world
models. In doing so, two critical aspects require particular
consideration. The first is the differentiability of the physics
parameter identification process. ASID  identifies physics
parameters for an established simulator using gradient-free
optimization . Baumeister et al.  adopt a similar method
to learn dynamics for model predictive control (MPC) .
The absence of gradient feedback renders these methodologies
critically dependent on data quality. However, collecting high-
quality real-world trajectories itself is a challenging task .
Song and Boularias  employ a differentiable 2D physics
simulator for learning planar sliding dynamics. However, 2D
physics is insufficient to capture complex motions such as
flipping an object through poking. The second consideration
lies in the necessity of state estimation in the optimization
of world models. While most existing methods involve state
estimation with additional modules [53, 5, 67], the recent
advances in differentiable rendering [42, 75, 57] make it
possible to optimize against an observational loss directly, thus
saving the effort on state estimation.
We introduce PIN-WM, a Physics-INformed World Model
that allows end-to-end identification of a 3D rigid body
dynamical system from visual observations. First, PIN-WM
is a differentiable approach to the identification of 3D physics
interaction trajectories. Our method systematically identifies
critical dynamics parameters essential to non-prehensile ma-
WM learns physics parameters by optimizing the rendering
loss  induced by the 3D Gaussian Splatting  scene
without additional state estimation modules. Consequently, the
learned world model, with identified physics and rendering
policies of non-prehensile manipulation using RL.
The learned PIN-WM, representing a digital twin  of the
real-world rigid body system, may still exhibit discrepancies
against reality due to the inaccurate and partial observa-
tions . To bridge the Sim2Real gap, we turn the identi-
fied digital twin into plenty of digital cousins  through
physics-aware perturbations which perturb the physics and
rendering parameters around the identified values as means.
Such purposeful randomization creates a group of physics-
aware digital cousins obeying physics laws while introducing
adequate varieties accounting for the unmodeled discrepancies.
The resulting world model group allows learning robust non-
prehensile manipulation policies with Sim2Real transferability.
Through extensive evaluation across diverse task scenarios,
we demonstrate that PIN-WM is fast-to-learn and accurate,
making it useful in learning robust non-prehensile manipu-
lation skills with strong Sim2Real transfer. The overall per-
formance surpasses the recent Real2Sim2Real state-of-the-
arts [67, 53, 45] significantly. Our real-world experiments
further showcase that PIN-WM facilitates Sim2Real policy
transfer without real-world fine-tuning and achieves high
success rates of 75 and 65 in the Push and Flip tasks,
respectively. Our contributions include:
We propose PIN-WM for accurate and efficient identifica-
tion of world models of 3D rigid body dynamical systems
from visual observations in an end-to-end fashion.
We turn the identified digital twin into a group of physics-
aware digital cousins through perturbing the physics and
rendering parameters around the identified mean values,
to support learning non-prehensile manipulation skills
with robust Sim2Real transfer.
We conduct real robot implementation to demonstrate that
our approach enables learning control policies with min-
imal task-agnostic interaction data and attains high per-
formance Real2Sim2Real without real-world fine-tuning.
II. RELATED WORK
A. Non-Prehensile Manipulation
Non-prehensile manipulation [50, 23, 86] refers to con-
trolling objects without fully grasping them. While offering
policies. Mason  presents a theoretical framework for
pushing mechanics and derive the planning by predicting the
rotation and translation of an object pushed by a point contact.
Akella and Mason  use a theoretical guaranteed linear
programming algorithm to solve pose transitions and generate
open-loop push plans without sensing requirements. Dogar
and Srinivasa  adopt an action library and combinatorial
search inspired by human strategies. The library can rearrange
cluttered environments using actions such as pushing, sliding,
and sweeping. Zhou et al.  model pushing mechanisms
use sticking contact and an ellipsoid approximation of the
limit surface, enabling path planning by transforming sticking
contact constraints into curvature constraints. These methods,
parameters or idealized physical models, which are often
violated in practice .
Deep learning methods have recently been applied to train
non-prehensile policies. Some studies focus on imitation learn-
Young et al.  emphasize the importance of diverse demon-
stration data for generalizing non-prehensile manipulation
achieves high success rates in real-world robotic pushing.
Chi et al.  utilize diffusion models multimodal action
distribution capabilities  to imitate pushing T-shaped ob-
imitation learning relies heavily on the quantity of real-world
data. Otherwise, it is prone to state-action distribution shifts
during sequential decision-making , which is a critical issue
in non-prehensile tasks requiring precise contact point selec-
tion and control . Hu et al.  conclude that imitation
generalization follows a scaling law  with the number of
environments and objects, recommending 50 demonstrations
per environment-object pair. Such data requirements can be
costly and prohibitive for scalability. Alternatively, deep rein-
forcement learning (DRL) can learn policies through trial and
error in simulated environments [39, 80]. However, the large
gap between simulation and reality poses significant chal-
lenges for transferring these policies to the real world [12, 45].
Building an interactive model that accurately captures real-
world physical laws is crucial for learning feasible non-
prehensile manipulation policies in real world.
B. World Models for Policy Learning
World models , which learn the environment dynamics
in a data-driven manner, provide interactive environments for
effective policy training [26, 28]. Hafner et al.  pro-
pose Dreamer, a world model that learns a compact latent
representation of the environment dynamics. The following
work  applies Dreamer to robotic manipulation tasks,
demonstrating fast policy learning on physical robots. DINO-
WM  leverages spatial patch features pre-trained with
DINOv2 to learn a world model and achieve task-agnostic be-
havior planning by treating goal features as prediction targets.
TD-MPC [28, 29] uses a task-oriented latent dynamics model
for local trajectory optimization and a learned terminal value
function for long-term return estimation, achieving superiority
on image-based control. Building on the success of learning
from large-scale datasets [7, 61], Mendonca et al.  leverage
internet-scale video data to learn a human-centric action space
grounded world model. However, purely data-driven world
models rely heavily on the quantity and quality of training
data and struggle to generalize to out-of-distribution (OOD)
scenarios [79, 62]. This lowers the robustness of the learned
policies transferred to the real world.
Incorporating structured priors into learning algorithms
is known to improve generalization with limited training
data [63, 8]. Recent advances in differentiable physics have
opened up new possibilities for incorporating physical knowl-
edge into world models. Lutter et al.  introduce a deep
network framework based on Lagrangian mechanics, effi-
ciently learning equations of motion while ensuring physi-
cal plausibility. Heiden et al.  augment a differentiable
rigid-body physics engine with neural networks to capture
nonlinear relationships between dynamic quantities. Other
works  demonstrate analytical backpropagation through
a physical simulator defined via a linear complementarity
problem. Sim  combine differentiable physics [16, 68]
and rendering [9, 37, 38, 76] to jointly model scene dynamics
and image formation, enabling backpropagation from video
pixels to physical attributes. This approach was soon followed
by improvements with advanced rendering techniques [46, 8],
or enhanced physics engines .
Despite those advances, only a few studies [53, 5, 67] incor-
porate physical property estimation into world models for non-
prehensile manipulation, relying on gradient-free optimization
or simplified physical models that fail to effectively handle
complex interactions. Gradient-free methods rely on high-
quality trajectories for system identification; lacking such data,
they are prone to local optima, as demonstrated by ASID using
CEM . Simplified physics models, such as the 2D physics
engine adopted by Song and Boularias , inherently struggle
to capture the full 3D dynamics of real-world interactions,
leading to inaccurate predictions. In contrast, PIN-WM enables
end-to-end identification of 3D rigid-body dynamics from
visual observations using few-shot, task-agnostic interaction
lation policies with RL. PIN-WM aligns with the original,
narrow-scope definition of a world model : a dynamics
model tailored to a specific environment for precise model-
based control. This contrasts with general-purpose world foun-
dation models like Cosmos .
C. Domain Randomization
Domain Randomization trains a single policy across a range
of environment parameters to achieve robust performance
during testing. Peng et al.  enhance policy adaptability to
varying environmental dynamics by randomizing the environ-
ments dynamic parameters. Miki et al.  train legged robots
in diverse simulated physical environments. During testing, the
robots first probe the terrain through physical contact, then pre-
emptively plan and adapt their gait, resulting in high robustness
and speed. Tobin et al.  introduce randomized rendering,
e.g., textures, lighting, and backgrounds, in simulated envi-
ronments to enhance real-world visual detection. Yue et al.
randomize synthetic images using real image styles from
auxiliary datasets to learn domain-invariant representations.
Dai et al.  introduce an automated pipeline to transform
real-world scenes into diverse, interactive digital cousin envi-
rates compared to digital twins .
While these randomization methods provide a simple ap-
proach for efficiently transferring simulation-trained policies
to the real world, their uniform sampling of environment
parameters lacks proper constraints. This results in a generated
space far larger than the real-world space, increasing learn-
ing burdens  and often producing conservative policies
with degraded performance . In contrast, we perturb the
physics and rendering parameters around the identified values
as means. Such purposeful randomization creates a group
of physics-aware digital cousins obeying physics laws while
introducing adequate varieties accounting for the unmodeled
discrepancies. The developed world model facilitates the learn-
ing of robust non-prehensile manipulation policies that transfer
effectively from simulation to real-world environments.
III. METHOD
A. Overall Framework
We develop real-world non-prehensile manipulation skills
through a two-stage pipeline: Real2Sim system identification
via our physics-informed world model, and Sim2Real policy
transfer enhanced by physics-aware digital cousins. We pro-
vide an overview of our framework in Figure 2.
Real2Sim System Identification: The Real2Sim stage
constructs our physics-informed world model, which identifies
the physics parameters of the target domain from visual
observations. A world model  predicts the next system
observation ot1 based on the current observation ot and
actions at:
ot1  W(ot, at, ),
where t denotes the time step and  represents the learnable
parameters. A comprehensive physical world for robot inter-
action should account for visual observations, physics, and
prehensile manipulation works [67, 86] that the geometry is
assumed to be known. Therefore, our PIN-WM W  I g
focuses on learning visual observations and physics of the
target domain, where I is the differentiable rendering function
(a) Rendering alignment
Multi-view observation
2D Gaussian Splats
Simulate
Next real observation 1
(c) Policy learning with physics-aware digital cousins
(d) Transfer to the target domain without fine-tuning
(b) Identification of physics parameters
Differentiable
Physics (LCP, )
Source domain (, )
Target domain (, )
Rendered image
Ground truth
Rendering loss
Next renderings 1
Source Domain
Target Domain
A. Real2Sim System Identification
B. Sim2Real Policy Transfer
Target domain
Differentiable
Render (2DGS, )
Fig. 2: Our Real2Sim2Real framework for learning non-prehensile manipulation policies. (a) The robot in the target domain moves around
the object, capturing multi-view observations to estimate the rendering parameters  of 2D Gaussian Splats. (b) Once optimized,  is frozen.
Both source and target domains apply the same task-agnostic physical interactions at. In the source domain, dynamics are computed via
LCP with physical parameters  to update the rendering.  is then optimized with the rendering loss between two domains. (c) The identified
world model is then used for policy learning. Physics-aware perturbations are introduced to  and  to mitigate the remained discrepancies
from inaccurate observations. (d) This ensemble of perturbed world models enhances the Sim2Real transferability of learned policies.
and g is the differentiable physics function. In more detail, the
world model can be rephrased as:
It1  I(g(xt, at, ), ),
where g, parameterized by , predicts the next state xt1 from
current state xt and action at, and I, parameterized by ,
generates the image It1 corresponding to xt1. Hence,
{, } forms all learnable parameters for W. The goal of
system identification is to optimize  so that the generated
images resemble those observed in the target domain.
Sim2Real Policy Transfer: After system identification,
we obtain the world model W as an interactive simula-
tion environment. We can learn non-prehensile manipulation
skills through reinforcement learning, where the learned pol-
icy is expected to achieve Sim2Real transfer without real-
world fine-tuning. However, the identified world model may
deviate from the real world due to inaccurate and partial
observations . We enhance policy transfer performance
by introducing physics-aware digital cousins (PADC). PADC
perturbs the identified system to generate meaningful train-
ing variations, which share similar physics and rendering
properties while introducing distinctions to model unobserved
discrepancies. This approach improves policy transferability
and reduces the learning burden. The learned policy is then
directly deployed in the target domain for manipulation tasks.
B. Physics-INformed World Model
In this section, we provide a detailed description of learning
PIN-WM W  I g. To fully characterize the dynamics g of
our system, we adopt rigid body simulation  to formulate
the dynamics of scene components that satisfy the momentum
conservation. Therefore, we include the target object, the end-
xt  {pt, qt, t}, where pt, qt, and t represents their
We account for joint, contact, and friction constraints for
rigid body simulation, and include the physical properties of
those scene components that are most concerned by non-
prehensile manipulation tasks [67, 53] into our physical pa-
rameters   {M, k, }, where M represents the mass
and inertia, k represents restitution, and  represents friction
coefficients. Under the rigid-body assumption where object
motion follows the Newton-Euler equations, these parameters
are sufficient to define collision, inertial response, and contact
behavior [20, 68]. Properties like elasticity or plasticity fall
outside the rigid-body scope.
The differentiable rendering function I is used to align the
visual observations between the source domain and the target
domain. Note that from a differentiable physics perspective,
the floor is stationary, and the robot is the one applying
force actively, whose dynamics will not be affected by other
observed and aligned. Therefore, our rendering function only
consider the target object, that is I(xt, )  I(xo
represents the rendering parameters specifically defined for
the target object, and the rendered image will change with the
update of object pose.
The learning process of our PIN-WM starts with optimizing
for rendering alignment, and uses optimized to guide
the identification of physical parameters  for simulation.
Rendering Alignment: To optimize the rendering param-
eters  for the target object o, the robot end-effector moves
around o in its initial state xo
0 and captures multiple static
scene images with an eye-in-hand camera Is  {Is
as demonstrated in Figure 2(a). To make sure that the rendering
function I can generalize to new viewpoints or object poses,
we adopt 2D Gaussian Splatting (2DGS)  as the render.
Compared to 3D Gaussian splatting , 2DGS is more
effective in capturing surface details.
2DGS renders images by optimizing a set of Gaussian
elliptical disks, which are defined in local tangent uv planes
in world space:
P(u, v)  pk  sutuu  svtvv  H(u, v, 1, 1),
where pk is the central point of the k-th 2D splat. tu and tv
are principal tangential vectors, and tw  tu tv presents the
primitive normal. R  [tu, tv, tw] is a 3  3 rotation matrix
and S  (su, sv) is the scaling vector. The 2D Gaussian for
static object representation can be equivalently represented by
a homogeneous matrix H0 4  4:
During the optimization, we randomly splat 2D disks onto
the object surface G for high-quality rendering initialization.
For an image coordinate (x, y), volumetric alpha blending
integrates alpha-weighted appearance to render the image I:
i Gi(u(x, y))
jGj(u(x, y))
i are the color and opacity of the i-th
ray emitted from the camera viewpoint through the image pixel
(x, y) and the plane where the 2D Gaussian distribution resides
in 3D space, G(u) is the 2D Gaussian value for intersection
This differentiable rendering models the visual observation
of the target object o in its initial state, and the corresponding
with the following loss function:
L  Lc  dLd  nLn,
where Lc combines rendering loss  Lr  I Is2
the D-SSIM term . Ld and Ln are regularization terms for
depth distortion and normal consistency , respectively.
Once the optimal parameters are obtained for the target
object o in its initial state xo
can lead to the rendering updating, achieved by transforming
the 2DGS accordingly. In more detail, for any new object state
matrix To
t 4  4 . We then apply To
t to initial Gaus-
sian splats represented by H0, resulting in the transformed
homogeneous matrix:
where To
0 represents the static object pose in its initial state xo
as the reference. Ht can be used to render the new image for
the target object with an updated state, denoted as I(xt, ).
Identification of Physics Parameters: With the opti-
mized rendering parameter , we further estimate the physics
properties  for simulation, based on the gradient flow from
visual observations established by the differential render. The
robot interacts with the object in state xt through a set of
task-agnostic actions A  {at, ..., atn1} to collect a video
tn} capturing dynamics, as shown in Fig-
ure 2(b). The transformed observations I  {I(xti, )}n
are then obtained in simulation with Equation 5, where xti
g(xti1, ati1, ) is the updated state when applying action
ati1. The physics parameter  is then estimated by minimiz-
ing the discrepancy between the generation I and observation
Id. Therefore, we can represent the objective of the physics
estimation as:
I(g(xti1, ati1, ), ) Id
What remains now is to develop a differentiable physics
model xt1  g(xt, at, ) for simulation, predicting the
next object pose xt1 based on current state xt and action
at. Note that previous work estimates physics parameters by
differentiating the impact of external wrenches on objects .
robot parts support wrench measuring. Therefore, we choose
the translation dt of robot end-effector as the action, i.e.,
We formulate this system identification process as a
velocity-based Linear Complementarity Problem (LCP) [16,
68] which solves the equations of motion under global con-
straints. Here, we use LCP to first estimate t1 from xt, and
then further use those two together to update the remaining
pt1 and qt1. In more detail, given a time horizon H which
describes the duration of an actions effect, LCP updates twist
velocities of each scene component t to t1 after H, where
t includes linear velocities vt and angular velocities t.
The updated velocity t1  {vt1, t1} is then used to
calculate the updated pose {pt1, qt1} integrated by the
semi-implicit Euler method :
pt1  pt  H  vt1,
qt1  normalize(qt  H
where pt and qt are the objects position and orientation
represented by a quaternion. [0, n1] represent quaternion
constructed from the angular velocity n1, and denotes
the quaternion multiplication.
The LCP is solved following the framework by Cline ,
where the goal is to find velocities t1 and Lagrange mul-
tipliers e, c, f,  satisfying the momentum conservation
when a set of constraints is included:
Mt1  Mt  f g  H  Jee  Jcc  Jff,
(Rigid Body Dynamics Equation)
Jet1  0,
(Joint Constraints)
Jct1 kJct c,
(Contact Constraints)
Jft1  E 0,
(Friction Constraints)
where f g is the gravity wrench, e, c, f,  are constraint
impulse magnitudes, E is a binary matrix making the equation
linearly independent at multiple contacts, and Je, Jc, Jf are
input Jacobian matrices describing the joint, contact, and
friction constraints, please refer to  for construction details.
a specific relative pose, contact constraints prevent interpene-
dissipation principle.
We adopt the primal-dual interior point method  as the
LCP solver to obtain the solution t1 while establishing
gradient propagation from t1 to . We then apply the
method described in  to derive the gradients of the solution
with the objective in Equation 8. The output of the physics
model g is contributed by both  and the last state xt, where
xt also depends on . Therefore, the gradients with respect to
can be expressed as:
(I(g(xti1, ati1, ), ) Id
As LCP is widely adopted by mainstream simulators [14, 49],
the estimated  can be compatible with existing simulation
environments [36, 10] as well.
Since we use a velocity-based LCP, the end-effector trans-
lation d can be converted into velocity e  dH, where
H is the action time horizon. This equation holds because
the robots mass is typically much greater than the objects
and velocity of the floor is kept stationary during the whole
used for solving the dynamics equation. Moreover, in robot
alent to simulation step size h, while the latter is set small
enough to ensure accurate object pose integration (Equation 9).
The input sub-action for each recursion is derived by dividing
the original action at into Hh segments. We propagate
recursive derivatives of Equation 11 across Hh simulation
time steps and optimize .
C. Physics-aware Digital Cousins
The learned world model W reduces the gap with real
worlds and provides an interactive environment for manip-
ulation policy learning. However, inconsistencies with the
real world remain due to inaccurate and partial observations.
Domain randomization  randomizes system parameters in
the source domain during training to cover the problem space
of the target, but it often lacks adequate constraints, increasing
training burdens and reducing policy performance. Therefore,
We propose physics-aware digital cousins, which perturb the
rendering and physics near the systems identified parameters,
as illustrated in Figure 2(c).
We adopt all estimated rendering parameters and physics
parameters for generating digital cousins. For rendering, we
adopt the spherical harmonics (SH) parameters sh ,
which represent the directional rendering component of the
2D Gaussian. Perturbing sh allows modeling of lighting
and material variations. We randomize the system parameters
r  {sh, } by sampling r from a uniform distribution:
where  indicates perturbation magnitude. For SH parame-
ensure zero-shot policy transfer, we perturb the identified
parameters with   0.1 to generate digital cousins. Our
physics-informed world model is compatible with arbitrary
reinforcement learning methods; we adopt Proximal Policy
Optimization (PPO)  for its ease of implementation. The
learned policy is then directly deployed in the target domain
world for manipulation tasks, as shown in Figure 2(d).
IV. RESULTS AND EVALUATIONS
With our experimental evaluations, we aim to answer the
following questions:
Does our method outperform other Real2Sim2Real meth-
ods in learning deployable manipulation policies?
Does PIN-WM achieve more accurate system identifica-
tion compared to existing approaches?
Does the proposed physics-aware digital cousins (PADC)
help with policy transfer?
Can our method deliver superior performance in real-
world settings?
We conduct experimental evaluations in both simulation and
the real world. Simulators provide ground truth for evaluating
system identification accuracy and hence offer comprehensive
answers to the first three questions, while the real-world tests
are used to validate the effectiveness of policy deployment
regarding the last question.
We evaluate our method on rigid body motion control. The
robots objective is to perform a sequence of non-prehensile
actions to move an object into a target pose. Actions are
specified as translations of the end-effector. We set up two
object on a plane to a target pose, involving 2D translation in
the xy-plane and 1D rotation around the z-axis. The flip task
Push tasks
Flip tasks
Fig. 3: Manipulation trajectories in simulation obtained by our
method for both push and flip tasks.
is to poke an object to turn it from a lying pose to an upside-
down pose, which requires 3D rotation and 3D translation.
A. Evaluations in Simulation
Experiment setup: In simulation, we collect a single
task-agnostic trajectory that the target object is pushed forward
along a straight line by the robot end-effector for a predefined
distance in the target domain. After that, any access to the
target domain is prohibited. Since our estimated parameters
are compatible with existing simulators, we integrate estima-
tions to the Bullet engine  for high-performance physics
simulation. With the learned simulator, we train manipulation
policies with only RGB images as input. We maintain 32
parallel threads for efficient training, each running an inde-
pendent physics-aware digital cousin. The initial object pose is
randomized for each episode. After the episode terminates for
each thread, the environment is replaced with a newly sampled
digital cousin. Trained policies are then directly deployed to
the target domain for evaluation. For both push and flip tasks,
we set a relatively low friction in the target domain to highlight
the importance of physics identification. Figure 3 demonstrates
several manipulation trajectories obtained by our method for
both push and flip tasks with different initial states.
Evaluation metrics: To answer the first three questions,
our evaluation metrics focus on both the manipulation policies
and the world models. We measure the success rate Succ
of a policy if the task is completed within a threshold of
100 steps for push and 25 steps for flip. We also consider
the required number of steps to complete a task, denoted as
Steps. We evaluate the accuracy of a world model using
one-step error  which measures the distance between the
final object states after applying one sampled action to the
TABLE I: Comparisons on policy performance in the target domain.
Dreamer V2
Diffusion Policy
RoboGSim
Domain Rand   I
2D Physics   I
ASID   I
PIN-WM wo PADC
PIN-WM w PADC
identified model and the target-domain simulator. This error
is computed separately for translation and rotation differences,
measured in meters and radians, respectively.
Baseline methods: We compare our method with various
types of approaches for training non-prehensile manipulation
Methods that rely purely on data. A representative is the
well-known Dreamer V2 , which is a latent-space dynam-
ics model from data for handling high-dimensional observa-
tions and learning robust policies. Given their strong reliance
on data quantity, we provide 100 task-agnostic trajectories.
Based on the learned expert policy, we train non-prehensile
manipulation skills via imitating expert demonstrations
of 100 task-completion trajectories similar to Chi et al. .
Methods with pre-defined physics-based world models.
We use Bullet  as the default simulator. Following the
standard domain randomization approach , we randomize
the physics parameters in Bullet, including mass, friction,
We employ our learned rendering function I as the renderer.
We set a variant with fixed, random physics and rendering
parameters where no system identification or randomization
is involved, denoted as Random. We also compare with
using 3DGS  but not physics parameters.
Methods with learned physics-based world models. We
compare with ASID  and the method of Song and Boular-
ias . The former performs system identification using
gradient-free optimization. The latter leverages differentiable
2D physics (thus referred to as 2D Physics). Since neither of
the two methods learns rendering parameters and their trained
policies cannot work without aligned visual input, we add our
rendering function I to enhance these two methods.
Note that all physics-based methods being compared are
trained with the same task-agnostic trajectories as PIN-WM,
for fair comparison. All policies are trained until no significant
success rate performance can be gained and are then deployed
directly to the target domain for evaluation. We also conduct
an ablation study of our method that trains policies without
PADC. More implementation details of baseline methods are
provided in Appendix A.
Comparisons on policy performance: We conduct 100
episodes of tests for each method and report the comparison
results in Table I. Our method achieves the best performance
for both non-prehensile manipulation tasks, thanks to the
accurate system identification of PIN-WM and the meaningful
digital cousins of PADC. Without PADC, our method still
outperforms others, although with a performance decrease.
The purely data-driven world model Dreamer V2 ,
albeit having access to more task-agnostic data, fails to
accurately approximate the dynamics of the target domain,
resulting in poor performance of the trained and deployed
policies. Diffusion Policy , relies on more expensive task-
completion data, also presents inferior performance due to
the limited training data quantity and hence poor out-of-
distribution generalization. These results highlight the impor-
tance of incorporating physics priors in learning world models.
For those methods with pre-defined physics-based world
detailed explanation in Appendix B of why Domain Rand
I struggles; smaller-scale randomizations around ground-truth
physical parameters improve its effectiveness, though knowing
these parameters is unrealistic. RoboGSim  optimizes only
for rendering parameters but not physics ones, also leading
to performance degradation. In contrast, our physics-aware
digital cousin design, perturbing the physics and rendering
parameters around the identified values as means, creates
meaningful digital cousins allowing for learning robust poli-
cies with Sim2Real transferability.
tives exhibit unsatisfactory performance in the target domain.
One reason is that their world models failed to effectively
capture the target-domain dynamics. ASID  leads to sub-
optimal solutions due to its inefficient gradient-free optimiza-
tion. Although 2D Physics  accounts for 2D differentiable
physics and achieves satisfactory push performance, its per-
formance on the flip task degrades since it involves 3D rigid
body dynamics. These experiments collectively demonstrate
that an accurate identification of both physics and rendering
parameters is crucial for learning non-prehensile manipula-
tion skills. Although PIN-WM performs physical parameter
identification under the assumption of perfect geometry, we
also provide experiments in Appendix B showing that, even
with noise, the training environment constructed by PIN-WM
effectively supports policy training.
Comparisons on system identification: We compare the
accuracy of system identification of both data-driven and
physics-based approaches. This is done by measuring the one-
step error after applying the same randomly sampled action to
the same surface point of the target object. The results are
reported in Table II.
We observe that the data-driven method Dreamer V2 ,
as expected, suffer catastrophic performance degradation when
generalizing to new state and action distributions. ASID
shows lower accuracy compared to PIN-WM in both push and
flip tasks, since it is difficult for gradient-free optimization to
TABLE II: Comparisons on system identification accuracy across
different methods, using one-step error of the predicted trajectory.
Trans. and Rot. are translation and rotation errors, respectively.
Dreamer V2
2D Physics
Translation Error
Dreamer V2
2D Physics
Orientation Error
Fig. 4: Transition and orientation errors of push task during training.
find a good solution in a finite time due to the large search
space. Although 2D Physics  adopts a differentiable frame-
our method learns 3D rigid body physics parameters through
differentiable optimization, achieving superior performance in
both push and flip scenarios. We present the learning curves
of the push task in Figure 4, demonstrating the stability and
efficiency of PIN-WM during training. We can observe that
Dreamer V2 quickly converges on the training dataset, but it
does not generalize well on the test dataset. We also provide
the physical parameters identified by each method, along with
the ground truth parameters, in Appendix B.
B. Evaluations in Real-World
Fig. 5: Our real-world experiment setup.
Experiment setup: Our hardware setup consists of a
shown in Figure 5. Given a real-world object o and its mesh
geometry G, we use FoundationPose  to estimate initial
object pose To
0 in the world coordinate system. We set the
mesh geometry G with pose To
0 in the simulator, and sample
a series of surface points on the transformed mesh To
tG as the
initialization for 2D Gaussian Splatting. The robot then moves
around the object and captures the time-lapse video sequence
TABLE III: Real-world deployment performance.
Domain Rand   I
RoboGSim
2D Physics   I
ASID   I
PIN-WM wo PADC
PIN-WM w PADC
n}. We segment the region of interest with SAM
2  and optimize the 2D Gaussian with the objective in
Equation 6, aligning rendering with the real world. After that,
we apply a straight line of translational actions to push the
object o in the real world. A dynamic video Id is captured by
the eye-to-hand camera to be used for optimizing the physics
parameters  with Equation 8.
Baseline methods: In the real-world setting, collecting
a large amount of trjactories, either task-agnostic or task-
pare with policy learning methods requiring no or few-shot
real-world data, thus excluding data-driven methods such as
Dreamer V2  and Diffusion Policy .
Comparisons on policy performance: We evaluate real-
world performance with both push and flip tasks under iden-
tical initial conditions across 20 trials. The push task requires
pushing the T-shaped object to the red target position, while
the flip task involves flipping a mug from its side to an upside-
down pose. The results are summarized in Table III, showing
that PIN-WM can complete the task with higher success rates
and fewer steps.
Conventional simulators with random parameters fail to pro-
duce transferable policies due to physical and rendering mis-
alignment. Although having integrated our rendering function
noisy variations that degrade policy learning. This is also
demonstrated by the comparisons with RoboGSim , which
aligns rendering but not physics parameters. With a more
accurate estimation of physics parameters, 2D Physics  and
ASID  obtain slightly better results but are still inferior to
PIN-WM. By learning both physical parameters and rendering
representations through differentiable optimization, together
with our physics-aware digital cousins design, our approach
attains much better performance in real-world deployments.
We show comparisons of real-world trajectories of push
task in Figure 6. Our method successfully pushes the T-
shaped object to the target pose with a few steps. In contrast,
alternative approaches either require longer trajectories or fail
to complete the task. We also verify the effectiveness of our
method by demonstrating how it completes the push task
on a larger T-shaped object in Figure 7, demonstrating its
adaptability to varied shapes and sizes. Appendix C provides
real-world comparisons for pushing objects on a slippery glass
We provide trajectories about flipping a mug in Figure 8 and
a cube object in Appendix C.
V. CONCLUSIONS
We have presented a method of end-to-end learning physics-
informed world models of 3D rigid body dynamics from
visual observations. Our method is able to identify the 3D
physics parameters critical to non-prehensile manipulations
with few-shot and task-agnostic interaction trajectories. To
realize Sim2Real transfer, we turn the identified digital twin
to a group of physics-aware digital cousins through perturbing
the physics and rendering parameters around the identified
mean values. Experiments demonstrate the robustness and
effectiveness of our method when compared with different
types of baseline methods.
Limitation and future work: We see several opportuni-
ties for future research. First, we use visual observations to
guide the optimization of physical parameters, thus rendering
alignment places a key role here. We find that the various
shadows generated with the robots movement can distort ren-
dering loss estimation, compromising the accuracy of learned
physical properties. This issue could potentially be resolved
by incorporating differentiable relighting  into Gaussian
Splatting to better model lighting conditions. Additionally, our
current framework focus on rigid-body dynamics, and it would
be interesting to explore ways to integrate more advanced
differentiable physics engines, such as the Material Point
Method (MPM) , to extend PIN-WMs capability to handle
deformable objects. We are also engaged in applying PIN-WM
to real-world applications in industrial automation [70, 82, 83].
VI. ACKNOWLEDGEMENTS
This work was supported in part by the NSFC (62325211,
Laboratory (23XJ01009), Key RD Program of Wuhan
(2024060702
