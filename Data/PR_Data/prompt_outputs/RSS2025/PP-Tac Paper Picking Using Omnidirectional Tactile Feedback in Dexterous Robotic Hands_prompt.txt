=== PDF文件: PP-Tac Paper Picking Using Omnidirectional Tactile Feedback in Dexterous Robotic Hands.pdf ===
=== 时间: 2025-07-21 14:59:45.211567 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Dexterous Robotic Hands
Pei Lin1,2
Yuzhe Huang1,3
Wanlin Li1
Jianpeng Ma1
Chenxi Xiao2
Ziyuan Jiao1
1Beijing Institute for General Artificial Intelligence
2ShanghaiTech University
3Beihang University
equal contributors
corresponding authors
Fig. 1: Overview of PP-Tac. The system leverages tactile feedback from the proposed hemispherical sensor (R-Tac), integrated into a
dexterous robotic hand, to grasp thin, deformable, paper-like objects. (a) Hand motions are generated by a diffusion-based policy inspired by
human strategies, such as sliding and pinching. (b) The hardware setup includes a robotic arm, a dexterous hand, and four fingertip-mounted
tactile sensors that simultaneously detect force and slip events.
AbstractRobots are increasingly envisioned as human com-
ulating deformable objects. Despite recent advances in robotic
hardware and embodied AI, existing systems continue to struggle
with handling thin, flat, and deformable objects such as paper
and fabric. These limitations stem from the lack of robust
perception techniques for reliable state estimation under diverse
visual conditions and the absence of planning methods capa-
ble of generating effective grasping motions. To address these
pick up paper-like objects. PP-Tac incorporates a multi-fingered
robotic hand equipped with high-resolution, hemispherical tactile
sensors (R-Tac) that provide omnidirectional tactile feedback.
This hardware configuration enables real-time slip detection and
online force control to mitigate slippage during manipulation.
Grasp motion generation is accomplished through a trajectory
synthesis pipeline, which constructs a dataset of pinching motions
and trains a diffusion-based policy to control the hand-finger
simultaneously. Experiment results show that PP-Tac successfully
grasps paper-like objects with varying material, thickness, and
of our knowledge, this is the first system to successfully grasp
paper-like deformable objects using a tactile dexterous hand.
I. INTRODUCTION
Robots are increasingly popular as assistive agents in ev-
eryday life, particularly within household environments .
These robots are designed to perform various domestic tasks,
often involving the grasp of thin, deformable objects such as
paper and fabric . For instance, clothes-folding tasks
require high dexterity and adaptability to accommodate vari-
ations in fabric size, texture, and stiffness, while document
organization tasks  demand precise picking capabilities
for diverse paper types and form factors. Beyond domestic
and logistical applications, such as fabricating fabrics  and
packing objects using plastic bags and cardboard .
Despite their significance, picking up paper-like objects
remains challenging in robotics . In particular, the main
challenges are three-fold: 1) Vision systems, commonly used
for manipulation, struggle to perceive contact information
during interactions with deformable objects due to limited
sensing modalities and occlusion, resulting in an inaccurate
environment model for motion planning ; 2) These objects
are often flat in shape, lacking salient features for contact
points and thus hindering the synthesis of stable grasps ;
3) These objects exhibit high appearance variability due to
continuous and unpredictable deformations during manipula-
based methods.
In contrast, humans excel at picking up paper-like objects
by leveraging coordinated multi-fingered motion and tactile
sensing. As shown in Fig. 1(a), the process typically begins
with establishing contact with the fingers, followed by sliding
motions to deform the material and to generate a contact
point for the pinched grasp. Such motion is made possible by
the hands high Degree of Freedoms (DoFs), which enables
establishing multiple contact points adaptively during sliding
motion. During this process, tactile sensing is also crucial as it
allows humans to perceive the objects deformation and decide
the appropriate forces. These real-time adjustments ensure the
successful execution of the picking-up action.
Inspired by human strategies, this paper introduces a
robotics system coined PP-Tac: Paper-like object Picking
using Tactile feedback. The PP-Tac system consists of two
key components: 1) A dexterous robotic hand equipped
with hemispherical, high-resolution Vision-Based Tactile
Sensors (VBTS) sensors (R-Tac): Mounted on the fingertips,
these tactile sensors provide real-time omnidirectional tactile
feedback during grasping. With a hemispherical sensing sur-
face and a high-frame-rate monochrome camera, the design
offers faster response and simplified calibration compared to
conventional RGB-based tactile sensors. An overview is shown
in Fig. 1(b). 2) A diffusion-based motion generation policy
(PP-Tac policy): This policy imitates human picking strategies
by first generating expert demonstrations through trajectory
optimization that replicate sliding and pinching behaviors. It
then trains a diffusion model on these trajectories, enabling
the robotic hand to adaptively grasp diverse flat objects using
proprioceptive and tactile feedback.
In a series of comprehensive real-world experiments, PP-Tac
achieved an overall success rate of 87.5 in grasping everyday
thin and deformable paper-like objects, including plastic bags,
paper bags, and silk towels on flat surfaces. Examples of
successful grasps are shown in Fig. 1(a). The system also
demonstrated strong adaptability to previously unseen uneven
surfaces. An ablation study further confirmed the significance
of each system component, highlighting the essential roles of
tactile feedback and the motion generation policy in enabling
coordinated handfinger motion in the dexterous robotic hand.
To the best of our knowledge, this work presents the first
demonstration of using a dexterous hand with tactile feedback
to pick up thin, flat, deformable paper-like objects. The main
contributions of this work are fourfold:
1) We present R-Tac, a novel hemispherical tactile sensor
designed for ease of fabrication, calibration, and scalable
deployment. R-Tac is integrated into each fingertip of a
fully actuated dexterous robotic hand, providing real-time
contact feedback during manipulation tasks.
2) We propose a novel trajectory optimization framework for
data generation that avoids using computationally expen-
sive tactile or soft-body simulations, while enabling robust
sim-to-real transfer.
3) We present the PP-Tac policy, a diffusion-based control
strategy that simultaneously generates coordinated hand
and finger motions, relying solely on tactile and propriocep-
tive feedback to manipulate paper-like objects. The policy
demonstrates robust generalization across a wide range of
materials and surface conditions.
4) We provide a full implementation and systematic evaluation
of the proposed algorithms on a physical robotic system.
Both the hardware design and code for the PP-Tac sys-
tem are publicly released to support further research and
community development.
II. RELATED WORK
A. Deformable Object Manipulation
Deformable Object Manipulation (DOM) tasks involve ma-
nipulating soft objects that deform during interaction, pre-
senting long-standing challenges in robotic research. These
difficulties primarily arise from uncertainties in perception
and the complex dynamics of soft bodies [16, 3, 31]. Early
approaches addressed these issues using vision-based percep-
tion for state estimation [51, 37], enabling tasks such as rope
handling [33, 37], cloth folding [42, 27], and picking up paper
with visual markers . However, vision-based methods
often struggle in real-world DOM tasks due to variability in
object appearance, unknown physical properties, visual occlu-
sions [25, 6], and inconsistent lighting conditions [48, 22].
These limitations hinder the scalability of vision-based DOM
solutions in diverse and unstructured environments.
Tactile sensing, particularly Vision-Based Tactile Sensors
(VBTS), has demonstrated significant potential for solving
DOM tasks . Leveraging their high-resolution tactile feed-
shape reconstruction [34, 10, 30, 47], localization [20, 26, 7],
and slip detection [44, 13]. Prior work has explored VBTS
for deformable object manipulation , but existing imple-
mentations rely on gripper-mounted sensors, which lack the
dexterity of multi-fingered hands due to limited DoF. Our
experiments reveal that gripper-based approaches struggle with
thin deformable objects and lack sufficient adaptability for
objects placed on non-flat surfaces, which highlights the need
for integrating dexterous robotic hands with VBTS for robust
manipulation .
B. Dexterous Robotic Hand with Tactile Sensing
Existing dexterous hands are often equipped with tactile
designs support a variety of form factors, enabling integra-
tion with different robotic fingers. However, their underlying
sensing principles often limit spatial resolution and robustness
under varying environmental conditions. To improve sensing
Fig. 2: The hardware design of the R-Tac and its integration into the four-fingered dexterous robotic hand system. (a) illustrates the
pipeline of depth reconstruction. (b) illustrates the exploded view of the sensor, detailing each component. (c) shows the dimensions of the
sensor. (d) shows the schematic design. (e) illustrates the robotic hand equipped with four sensors as its distal links.
particularly those with curved elastomer surfaces [10, 46, 23,
designs remain non-commercial and are difficult to deploy
at scale, especially in dexterous hands. A major challenge
lies in sensor calibration. Illumination from RGB chromatic
light sources can create uneven light intensity distributions
on curved elastomer surfaces, necessitating extensive data
collection for accurate calibration. This process often depends
on specialized test bedssuch as those fabricated via CNC
machiningfurther increasing complexity [10, 46, 23, 2]. Ad-
high bandwidth demands, potentially limiting frame rates in
multi-sensor deployments. To address these limitations, we
propose R-Tac, a structurally simple, compact, and easily
calibratable tactile sensor optimized for scalable deployment.
Dexterous robotic hands equipped with VBTS have been
successfully applied to grasping and in-hand manipulation
tasks. For example, Do et al.  utilize DenseTact
on an Allegro Hand  to grasp and manipulate small
perform object reorientation. However, using VBTS-equipped
dexterous robotic hands for picking up thin, flat, deformable
paper-like objects, such as paper sheets, remains unexplored.
III. HARDWARE DESIGN
To meet the dexterity requirements of paper-picking tasks,
we designed and fabricated a set of hemispherical VBTS
Allegro Hand.
A. Fingertip-shaped Tactile Sensing
The design of R-Tac is guided by five key principles to
support effective manipulation:
omnidirectional tactile perception.
High resolution: Supports accurate depth reconstruction
and reliable slip detection during interaction.
Low-cost and easy fabrication: Comprised of off-the-
shelf or easily fabricated components, with a total cost of
approximately 60.
Efficient calibration: The monochrome sensing principle
simplifies lighting control and minimizes manual calibration
Lightweight data transmission: The monochrome camera
produces less data volume per frame, enabling high-speed
data transfer.
Following these principles, the sensors design and integration
into the dexterous hand are shown in Fig. 2. We now detail
the sensor components and the calibration process.
1) Contact and Illumination Module: The core of the
sensor is a contact module (elastomer) with a uniformly illu-
rigidity during contact. Inspired by the monochrome sensing
we developed a hemispherical structure comprising a white
LED ring, a stiff transparent internal skeleton, a soft semi-
transparent perception layer, and a thin opaque protective layer
that achieves the desired optical characteristics.
The LED ring (LUXEON 2835 4000K SMD LED) and a
diffuser (double-sided frosted diffuser sheet) are first installed
within the sensor shell. The skeleton is then manufactured
from PDMS (Dow Corning Sylgard 184 with Shore hardness
50 A) using a two-piece molding technique. The mixture
(base: catalyst  10:1) is degassed and poured into the mold,
and cured for 24 hours at room temperature. The perception
layer is then manufactured similarly, using semitransparent
silicone (Smooth-On Ecoflex with Shore hardness 00-10), and
the layer is peeled off after 4 hours. Note that the measured
depth range relies on the thickness of this layer, which is
set to 2 mm. Finally, a silicone coating (Smooth-On Psycho
Fig. 3: Slip detection. The left tactile image represents a no-contact
The network estimates the slip probability Pslip by processing the no-
contact reference image with the five most recent tactile frames.
Paint) is airbrushed onto the perception layer to form the
opaque protective layer. The entire manufacturing process can
be completed within three days.
2) Camera Module: A micro black-and-white CMOS cam-
era (OV9281) with a wide 160lens is used to capture the
light intensity data. The camera operates up to 120Hz with a
resolution of 640480 and a latency of approximately 100ms.
3) Calibration: The uniform optical properties of the elas-
tomer and illumination module (with a capture standard devi-
ation as low as 6) enable the 3D geometry of the round shape
sensor to be computed from single-channel pixel intensity in
simply two steps using only 30 captures, without the need for
a CNC machine. First, given the known intrinsic parameters
3D-printed indentation-based setup to estimate the extrinsic
parameters of rotation matrix A and translation vector b, as
well as the sensor surface reference projection D. Next, the
depth mapping function M is calibrated by capturing a single
image of a ball of known size pressed onto the sensor .
The complete mapping function from the pixel coordinates
(u, v) to the sensor coordinates (x, y, z) can be expressed as:
(D(u, v) M(I(u, v)))K1
which transforms grayscale intensity images to a depth map
expressed in the sensor coordinates. A detailed explanation of
camera calibration is provided in Appendix A. Reconstruction
results and qualitative analysis are presented in Section VI-B.
4) Contact Force Estimation  Slip Detection: Our sensors
are capable of detecting both contact forces and slip events.
The contact force is modeled based on elasticity theory, where
it is linearly proportional to the deformation depth. The slip
detection module operates as follows:
Detection Model: As shown in Fig. 3, slip events are
characterized by the appearance of distinct wrinkle patterns
in the tactile images. We employ a lightweight neural
network composed of a convolutional neural network (CNN)
followed by a multilayer perceptron (MLP) to detect these
events. The network takes as input a temporal sequence
of five tactile frames along with a reference no-contact
image. The CNN extracts features from each frame, and the
resulting feature maps are concatenated and passed through
Fig. 4: Force analysis during grasping flat objects. The grasping
process relies on three key forces: 1) The contact normal force exerted
by the sensor on the object. 2) the static friction force (f1, f
1) between
fingers and the object, and 3) the dynamic friction force (f2, f
between the object and the supporting terrain. A successful grasp
occurs when the static friction (f1, f
1) exceeds the critical buckling
resistance of the paper, causing the sheet to deform and form a stable
pinch region.
the MLP to estimate the slip probability Pslip.
utes of tactile data collected from four sensors. The dataset
includes 40 slip and 60 non-slip samples, with each
frame manually annotated. Binary cross-entropy is used as
the loss function.
to determine slip events. Empirical evaluation shows that a
threshold of 0.75 offers a good balance between sensitivity
and precision, achieving a slip detection accuracy of 86.
B. Robotic Hand System
We integrated the proposed R-Tac sensors into a fully
actuated dexterous robotic hand, with each sensor mounted
at the distal end of the fingertips to enable contact sensing
for subsequent paper-picking tasks. The hand comprises 16
controllable DoFs, including the DIP, PIP, MCP, and MCP-2
joints for the index, middle, and ring fingers, as well as the
provided by Dynamixel XC330-M288-T motors, multiplexed
through a U2D2 Hub. Each tactile sensor communicates with
the PC via a USB interface. The robotic hand is mounted on a
Franka Emika Research 3 robotic arm, which interfaces with
the PC through a high-speed Ethernet connection.
IV. PROBLEM STATEMENT
deformable paper-like objects from flat surfaces. This appears
as a commonly seen scenario in everyday tasks, such as
organizing scattered document pages or retrieving napkins
from dining plates. Although creases or irregularities in the
material can sometimes provide grasping points, a particularly
challenging scenario arises when the object is extremely flat
and lacks discernible edges or salient grasping features. This
research introduces a novel approach to tackle the paper-
picking problem that was previously unexplored.
Motivated by the human strategy for grasping flat objects,
our work is based on a biomimetic grasping pose optimized for
paper picking, as illustrated in Fig. 4. By applying sufficient
inward force, the robotic fingers can induce buckling of the
material against the supporting surface. This buckling effect
dynamically generates a pinchable region, enabling subsequent
grasp execution.
During buckling, the distance between contact points be-
neath the fingers decreases. When this reduction rate matches
the fingertips closure speed (i.e., no relative motion between
fingertips and material), two frictional forces govern the sys-
1) between the fingers and material,
and dynamic friction (f2, f
2) between the material and the
supporting surface. Their magnitudes depend on the applied
normal force and the respective coefficients of friction.
In particular, the above analysis assumes that the static
friction between robotic fingers and the material exceeds both
the maximum static friction at the material-terrain interface
and the critical buckling resistance of the material. This
framework can also be extended to scenarios with uneven
supporting terrains. Without loss of generality, we assume that
height variations in the terrain are less than 3 cm.
One challenge is determining the control inputs for all
finger joints and the hand pose (i.e., the end-effector pose of
the manipulator). Intuitively, this resembles human grasping
the hand must first elevate and then lower to establish stable
contact. However, a hand-finger coupling issue arises: the
motion of one finger requires a specific hand state, which in
turn affects the movement of other fingers. In practice, our
approach solved this problem by adopting a learning-based
policy rather than a model-based optimization paradigm. This
is due to its superior efficiency in deployment, as we found
model-based optimization is too computationally expensive to
adapt for online execution.
V. POLICY LEARNING FOR PAPER-PICKING
Manipulating paper-like objects with visual perception re-
mains challenging due to difficulties in detecting thickness
and textural variability. To address this, we propose a vision-
independent tactile-based approach. The core idea leverages
tactile feedback to maintain contact conditions (as defined
in Section IV), facilitating the creation of a buckling region
for successful grasping. We implement this through the PP-
Tac policy, developed in two stages: 1) Trajectory Optimiza-
optimization. 2) Diffusion Policy Training: Train a policy
on this dataset to infer motions from tactile feedback and
proprioceptive states, ensuring generalization to real-world
robotic systems.
A. Grasp Motion Dataset Synthesis
We synthesize grasping motions via trajectory optimization
in simulation, eliminating the need for complex teleoperation
interfaces. Although reinforcement learning (RL) presents an
Fig. 5: Fingertip trajectories from data synthesis. Trajectories
ensure fingertip sliding along the terrain surface. Adjusting the
distance between waypoints and terrain affects sensor deformation.
The right figure projects trajectories of two fingers onto the m-z and
n-z planes, where m and n are straight-line projections of fingertip
trajectories on the palm-aligned x-y plane, and the z-axis extends
outward from the hand.
the dynamics of deformable objects and the behavior of VBTS
to ensure fidelity. In contrast, our method relies on rigid-body
dynamics and demonstrates direct sim-to-real transfer, as val-
idated through physical experiments. The grasping procedure
begins by initiating contact between the fingertips and the
objects surface (see Appendix B for implementation details).
Upon contact, the fingers close gradually to pinch the object,
each following an independently optimized trajectory while
applying a target normal force (Figs. 4 and 5).
To generate diverse fingertip trajectories, we first con-
structed randomized terrain profiles and manually designed
initial pinching motions that emulate natural, intuitive grasping
behaviors. As illustrated in Figs. 4 and 5, the (x, y) coordinates
of the fingertip paths were extracted from these handcrafted se-
quences as target trajectories. The corresponding z-coordinates
were obtained by projecting these points onto the terrain
surface and sampling the local height at each location. This
process yields the final target fingertip trajectories, denoted as
eetarget R43, for all four fingers.
Given ee
are solved through the following optimization problem:
ee  L LR
ee  wee MSE(fk(),ee
eetarget),
L wMSE (,) ,
( RRR, pppwrist),
(RRR,pppwrist)) ,
where  is the optimization variables consisting of Ndata
frames finger joint angles qqq, hand (i.e., wrist, mounted to
the end-effector of the arm) rotation RRR and hand translation
along the z-axis in world coordinates pppwrist. Ndata is the
sequence length. The forward kinematics fk computes the
four fingertips trajectories by giving . MSE denotes mean
squared error. Lee
ee can minimize the error between the fingertip
positions and their targets, while Lregularizes the motion to
remain close to the initial pose. Furthermore, LR
excessive wrist movement, helping to keep the arm within
its reachable workspace. We use stochastic gradient descent
(SGD) for optimization. After filtering out collision-prone
each consisting of Ndata  100 frames.
B. PP-Tac Policy
Once the dataset is prepared, we employ a diffusion policy
to jointly control the hand and arm, enabling adaptation
to varying terrain shapes and contact force conditions. We
adopt a Denoising Diffusion Probabilistic Model (DDPM)
framework [17, 18, 8, 41], which predicts Npred future states
conditioned on Nprefix historical states. The state variables are
written as:
xxx  (ppp, ppp,qqq, qqq, R, , pwrist, pwrist,dddtac)  N
where ppp R173 is hand joints position in world coordinate,
ppp R173 is the linear velocity of the hand joints relative to
each parent frame, qqq R16 is the rotation angle of controllable
hand joints, qqq R16 is the angular velocity of controllable
hand joints, R R6 is 6D rotation (represented as two-row
vectors of rotational martix, which is from ) of wrist(end
effector of arm), R6 represents the angular velocity of
wrist rotation, pwrist R is the wrists height along arms
represents the deformation depth readings from four fingertip
tactile sensors. Table II summarizes the notations used in this
paper. The total state dimension is D  152. Such an over-
parameterized input allows the network to extract more robust
and expressive latent features for the diffusion policy.
The overall pipeline is illustrated in Fig. 6, with the right
figure depicting a single denoising diffusion step in detail.
We apply an encoder-only transformer to predict future robot
motion xxxpred
given prefix motion xxxprefix, diffused future motion
, diffusion step t, current frame index i, and target
deformation depth dddtac. The input sequence is encoded into
a latent vector of dimension R(1NprefixNpred)D, comprising:
1) A latent vector of D-dimensional features representing t,
Nprefix  D dimensions corresponding to the prefix states of
Nprefix time steps. 3) Npred  D dimensions for the predicted
states of Npred time steps. Instead of predicting t (formulated
by ), we follow  to predict the state sequence itself
. Predicting xxxpred
is found to produce better results for
the state sequence which contains motion data, and enables us
to apply a target loss for each denoising step as follows:
L  xxxpred
2  consistLconsist,
Lconsist  fk(qqqpred
) ppppred
where Lconsist enforces consistency between joint angles and
During inference, we set t  1000 and the diffused xxxpred
N(0, I) and iteratively denoise it to produce xxxpred
. To ensure
real-time performance, we reduce denoising steps to 10 and
set Npred  Nprefix  5, achieving motion generation in 11
ms on an RTX4090 GPU. The predicted qqq controls the hand,
while R and pwrist control the arm.
During grasping, preventing slip between the object and the
fingertips is essential to maximize material deformation. To
achieve this, a fingertip contact force controller is introduced,
which adjusts the fingertips deformation depth dddtac. If slip
is detected by the tactile sensors, we increase the desired
deformation depth by a small increment dddtac.
To deploy diffusion policy to real robots, we also need to
tackle the domain gap between the real world and simulation.
This is achieved by introducing four distinct ways to incorpo-
rate disturbances into xxxprefix during training:
Add random Gaussian noise to  to simulate various control
errors that may occur in real-world situations.
Add Gaussian noise to the first frame and gradually amplify
it in subsequent frames, simulating the fingers moving
across a rising or descending terrain.
Randomly choose from 2 to Nprefix temporal consistent
frames to be static, simulating fingers getting stuck due to
excessive pressure on complex terrain. And dddtac is set to its
maximum threshold. The reason for adding the index of the
frame into the input is also to avoid issues caused by the
fingers getting stuck.
VI. EXPERIMENTS
In this section, we present comprehensive experiments to
evaluate our proposed PP-Tac pipeline. First, we detail the
implementation of our algorithm (Section VI-A). Next, we
show the quantitative and qualitative results of the depth
reconstruction of our VBTS (Section VI-B). Then, we per-
form systematic comparisons of our system on different flat
materials and supporting terrains (Section VI-C). We also
compare our system with various manipulators to highlight
its advantages and limitations (Section VI-D). Last, ablation
studies are conducted to examine the influence of parameters
and the necessary training steps (Section VI-E).
A. Implementation Details
For reproducibility, we provide the implementation details
of the PP-Tac algorithm. Our diffusion policy is implemented
as a four-layer Transformer encoder with a latent dimension
of 512 and four attention heads. We split each synthesized
data sequence into subsequences of length 10 for the diffu-
sion process, and train the model for approximately 600,000
iterations on a single RTX 4090. During training, the diffusion
step t is uniformly sampled from 0 to 1000. During inference,
an acceleration technique is applied as follows. First, t is
initialized to 1000 and directly denoised to xxxpred
. Subsequently,
noise is added to the t  1000 100Ni level and denoised
again to xxxpred
, where Ni is the inference step number. Thus,
the entire inference process consists of 10 steps.
For terrain generation, we model the terrain beneath each
finger as a cubic spline with a trajectory length of 100.
Control points are placed at intervals of 25 along the trajectory,
resulting in a total of 5 control points. To simulate ramps,
the height of each control point is randomized by sampling
uniformly within the range of [0, 3] cm.
Fig. 6: Inference pipeline of the proposed PP-Tac policy. Conditioned on robot proprioception and the target force that needs to be exerted,
PP-Tac can infer the action of the next steps. If a slip is detected between the finger and the flat object underneath, an incremental amount
of force will be exerted by the finger.
Fig. 7: Reconstruction results. (a) Gallery of reconstructed depth
and normal maps from tactile images. (b) Depth reconstruction error
of the indentation test.
B. Depth Reconstruction of VBTS
To evaluate the performance of the tactile sensor in depth
qualitative results of the sensor output are shown in Fig. 7,
which demonstrates the raw captured image from the sensor,
the ground truth depth maps, predicted depth maps, and the
corresponding calculated normal maps, respectively. These
results demonstrate that the sensor can fully reconstruct fine
surface details.
We quantify the reconstruction error using a violin plot,
leveraging ground truth indentation information obtained from
3D-printed hemispherical shape indicators containing various
testing indenters. We collected 215 testing configurations,
each with paired sensor outputs and ground truth reprojection
images. The sensor achieves a mean absolute error (L1 error)
reconstruction loss of 0.35 mm, and a median loss of 0.28
terms of computational speed, the depth mapping process takes
less than 10 ms, ensuring real-time performance for robotic
applications.
C. Evaluation of PP-Tac Policy on Materials and Terrains
We conducted experiments to evaluate the systems ability
to handle flat objects under varying conditions. The qualitative
and quantitative results are shown in Fig. 8 and Fig. 9
respectively. Fig. 8 shows the typical successful grasp cases,
highlighting that our hardware and PP-Tac algorithm can
successfully handle flat objects placed above both the flat
and uneven object surface. During the grasping process, the
fingertip first contacts the material, followed by a gradual
finger closure that buckles the material and creates pinchable
regions. Finally, the object is pinched and lifted.
Fig. 9 provides quantitative analysis of the success rate with
respect to the object material and the complexity of the terrain
beneath. To facilitate this analysis, we conducted experiments
using four flat objects in daily life: paper, plastic bag, cloth,
and kraft paper bag, each of which presents unique challenges.
The paper is extremely flat with no detectable hold points.
Plastic bags, commonly encountered in daily life, are difficult
to locate using conventional visual pipelines because of their
transparency. The cloth is thick and highly deformable, while
the kraft paper bags are stiff and have a multilayered structure.
To assess the systems robustness, we also varied the terrain
beneath the objects. The four types of terrain used include: a
flat plane, a slope (10 degrees), a plane with a 2 cm thick book
randomly placed on it, and an uneven terrain with random
curvatures. The terrain shapes are shown in Fig. 9.
For statistical significance, we performed 20 grasping at-
tempts for each combination of terrain and object. From results
in Fig. 9, cloth and plastic bags are relatively easy to grasp
Fig. 8: Gallery of Grasping Different Objects in Real-World Evaluations. This figure demonstrates successful grasps of five flat objects
on four different types of terrains, highlighting the effectiveness of our hardware and the PP-Tac algorithm. (a) A paper on a flat desktop.
(b) A stiff Kraft paper bag on a flat desktop. (c) A soft napkin on a plate. (d) A paper sheet on a randomly arranged book. (e) Paper sheet
on a random terrain. These evaluations showcase the robustness and adaptability of our approach.
due to their low stiffness, which allows them to buckle more
easily under force. In contrast, paper and kraft paper bags are
stiffer and resist buckling, leading to lower success rates.
The terrain beneath the object also significantly impacts
grasp success. On flat terrains, such as a plane or a tilted
relatively high. This suggests that flat surfaces usually generate
consistent frictional forces essential for a successful grasp.
as kraft paper bags. These stiff flat objects usually lack of
initial buckling when placed on a flat surface, making it more
challenging to form reliable grasp points afterward.
For uneven surfaces, the success rates varied according to
the shape of the terrain. When a book was placed underneath
the flat object, all objects maintained high success rates. These
results can be attributed to the edge of the book and the partial
void space created beneath the material, which made it easier
for the materials to buckle and separate with the terrain. In
rate dropped for all objects. This is likely due to the challenges
added to our force controllers, which increased the likelihood
of the fingers slipping away from the material.
TABLE I: Experimental results for varying paper quantities: The
systems performance was evaluated on paper materials with different
buckling strengths, achieved by bonding 1, 3, 5, and 7 layers of
paper with adhesive. For each configuration, 20 trials of grasps were
conducted. The average number of slip events detected (No. Slip)
and the final success rate (Succ. Rate) were recorded.
Paper Layers
No. Slip
Succ. Rate ()
D. Comparison with Other System Configurations
To assess whether PP-Tacs system setup leveraging dexter-
ous hand and tactile sensors can offer advantages, systematic
comparisons with other robot configurations were conducted.
trial allowed only one grasp attempt.
Bi-finger grippers controlled via human teleoperation with a
camera mounted on the wrist to provide an egocentric view
which can mimic the vision-based method . This baseline
can demonstrate the effectiveness of our hardware design.
Open-loop
generated trajectories using the ground truth shape of the
Fig. 9: Experiment results. Evaluations were conducted to quantify the success rate of grasping four different flat objects (paper, plastic bag,
trajectory with compliant finger control via tactile feedback; (3)Model based force tracking": combines the PP-Tac-derived hand trajectory
with compliant finger control via tactile feedback; (4) Non-disturbance: grasp using our dexterous hand with tactile sensors, where the
diffusion policy was trained without domain randomization disturbances; and (5) PP-Tac(ours): grasp using our full PP-Tac pipeline. Each
condition was repeated 20 times. Note that open-loop grasp control is not feasible on uncertain terrains; these scenarios are marked as NA.
terrain and then replayed these trajectories rather than using
the PP-Tac policy. Note that this trajectory-replay setting is
unattainable in scenarios with high variations, such as the
book setting and the complex terrain scenario in which the
terrain shape is unknown.
Model based force tracking: due to the challenges outlined
in Section IV, we employ the wrist trajectory generated by
PP-Tac while actively controlling only the fingertips through
real-time tactile feedback.
The evaluation results in Fig. 9 show that the PP-Tac
pipeline outperforms all baselines. We observed that the tele-
operation baseline using a gripper achieved some successful
cases in grasping cloth and plastic bags, albeit with lower
performance than PP-Tac. This is due to the ease of detect-
ing the initial grasp point on these soft materials through
human perception, and combined with human intelligence
enabling grasp adjustments through visual feedback. However,
for stiffer materials like paper and kraft paper, the bi-finger
gripper failed completely. Therefore, we conclude that the PP-
Tac pipeline is the most suitable configuration for handling flat
objects. The open-loop baseline achieved a lower success rate
compared to PP-Tac. The suboptimal performance primarily
stems from control error. As mentioned in , Allegro Hand
exhibits joint angle errors exceeding 0.1 radians, which will
be further accumulated across the kinematic chain. These
errors critically degrade performance in precision-sensitive
tasks such as paper picking, highlighting the necessity of
tactile feedback for robust control. While the "Model-based
force tracking" achieves satisfactory performance in structured
terrains by leveraging wrist trajectories generated by PP-
irregular or complex terrains. This underscores the need for
enhanced adaptability in unstructured environments.
E. Ablation Studies
1) Influence of Material Stiffness: We found that the ma-
terials stiffness (represented by its thickness), significantly
influences the tasks success rate. To demonstrate this effect,
we created flat objects by stacking paper pages bonded with
adhesive. The experimental results are shown in Table I. As
the number of paper pages increased, the grasp success rate
decreased significantly. Additionally, the increase in material
stiffness also led to a higher number of detected slips.
2) Influence of Data Disturbance: We emphasize the im-
portance of the data disturbance technique for domain random-
ization (introduced in Section V-B). To quantify its impact,
we conducted ablation studies comparing grasp performance
before and after adding four types of disturbances to the
prefix motion xxxprefix. Experimental results demonstrate that
this technique significantly enhances performance. As shown
in the Non-disturbance baseline in Section VI-C, removing
data disturbance led to a notable performance drop across all
stiff objects, such as kraft paper bags. This underscores
the improved generalization and higher grasp success rates
enabled by domain randomization. However, a drawback of
this technique is the increased training time, requiring approx-
imately 400,000 additional iterations to achieve the same loss
as training without data disturbance.
VII. LIMITATIONS
We have observed the following limitations in our system.
One limitation is determining the initial force (sensors target
deformation depth) required for successful grasping. While our
algorithm can adaptively adjust the force magnitude online,
an appropriate initial value must still be manually set, which
remains an empirical parameter-tuning process. If the initial
value is too small, the grasp is more likely to fail due to
the additional time and finger sliding distance needed for
adaptation to a reasonable value. Conversely, if the initial value
is too large, excessive friction may exceed the load capacity of
the hand motors. In addition to the initial value, the adaptive
algorithm for adjusting force also has room for improvement,
particularly with highly stiff materials such as kraft paper bags
on non-flat surfaces.
VIII. CONCLUSIONS
This paper presents PP-Tac, a coordinated hand-arm system
designed to manipulate thin, flat objects such as paper and
fabric. The system is equipped with a multi-fingered, vision-
based tactile sensor that is easy to fabricate and deploy on
the hands fingertips. The sensor can detect contact on its
curved surfaces, enabling the system to measure force and
friction during contact. This capability helps minimize slip
and increases the likelihood of material deformation when
handling flat materials. Based on this hand design, the grasping
motion is planned using a data-driven approach. We developed
an efficient synthesis algorithm to generate sliding trajectories
across various terrain shapes and sensor deformation condi-
Using this dataset and a domain randomization technique, we
trained a diffusion policy that enables adaptation to diverse
terrains in real-world settings. Experimental results show that
our system can successfully grasp flat objects of varying
thicknesses and stiffness, achieving a success rate of 87.5.
external disturbances and adapts well to different support
terrain surfaces.
IX. ACKNOWLEDGMENT
We thank Changyi Lin (Carnegie Mellon University) for
insightful discussions. This work was supported in part by
the National Natural Science Foundation of China (Grant
No.52305007), by the State Key Laboratory of Mechanical
System and Vibration (Grant No. MSV202519), and by Shang-
hai Frontiers Science Center of Human-centered Artificial
Intelligence (ShangHAI), MoE Key Laboratory of Intelli-
gent Perception and Human-Machine Collaboration (KLIP-
HuMaCo).
REFERENCES
Enrique Amig, Julio Gonzalo, and Felisa Verdejo. A
general evaluation measure for document organization
In Proceedings of International ACM SIGIR
Conference on Research and Development in Information
Retrieval (SIGIR), pages 643652, 2013.
Iris Andrussow, Huanbo Sun, Katherine J Kuchenbecker,
and Georg Martius. Minsight: A fingertip-sized vision-
based tactile sensor for robotic manipulation. Advanced
Intelligent Systems, 5(8):2300042, 2023.
Veronica E Arriola-Rios and Jeremy L Wyatt. A multi-
modal model of object deformation under robotic push-
ing. IEEE Transactions on Cognitive and Developmental
Osher Azulay, Nimrod Curtis, Rotem Sokolovsky, Guy
Sintov. Allsight: A low-cost and high-resolution round
tactile sensor with zero-shot learning capability. IEEE
Robotics and Automation Letters (RA-L), 9(1):483490,
Aude Billard and Danica Kragic. Trends and challenges
in robot manipulation.
Tara Boroushaki, Junshan Leng, Ian Clester, Alberto
occluded objects using rf perception.
In International
Conference on Robotics and Automation (ICRA), pages
Arkadeep Narayan Chaudhury, Timothy Man, Wenzhen
Using collocated
vision and tactile sensors for visual servoing and local-
ization. IEEE Robotics and Automation Letters (RA-L),
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran
Diffusion policy: Visuomotor policy learning
via action diffusion. International Journal of Robotics
Research (IJRR), page 02783649241273668, 2023.
Xinke Deng, Yu Xiang, Arsalan Mousavian, Clemens
6d object pose estimation for robot manipulation.
International Conference on Robotics and Automation
(ICRA), pages 36653671. IEEE, 2020.
Won Kyung Do and Monroe Kennedy. Densetact: Optical
tactile sensor for dense shape reconstruction. In Interna-
tional Conference on Robotics and Automation (ICRA),
pages 61886194. IEEE, 2022.
Won Kyung Do, Bianca Aumann, Camille Chungyoun,
and Monroe Kennedy. Inter-finger small object manipula-
tion with densetact optical tactile sensor. IEEE Robotics
and Automation Letters (RA-L), 2023.
Mehmet Remzi Dogar and Siddhartha S Srinivasa.
framework for push-grasping in clutter. In Proceedings
of Robotics: Science and Systems (RSS), volume 2, 2011.
Siyuan Dong, Daolin Ma, Elliott Donlon, and Alberto
Rodriguez. Maintaining grasps within slipping bounds
by monitoring incipient slip. In International Conference
on Robotics and Automation (ICRA), pages 38183824.
Christof Elbrechter, Robert Haschke, and Helge Ritter.
Bi-manual robotic paper manipulation based on real-time
marker tracking and physical modelling. In International
Conference on Intelligent Robots and Systems (IROS),
pages 14271432. IEEE, 2011.
Satoshi Funabashi, Tomoki Isobe, Shun Ogasa, Tetsuya
Shigeki Sugano.
Stable in-grasp manipulation with a
low-cost robot hand by using 3-axis tactile sensors with
a cnn. In International Conference on Intelligent Robots
and Systems (IROS), pages 91669173. IEEE, 2020.
nipulation of deformable objects. In Proceedings of IEEE
International Conference on Emerging Technologies and
Factory Automation (ETFA), pages 977984. IEEE, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models. Advances in Neural In-
formation Processing Systems (NeurIPS), 33:68406851,
Jonathan Ho, William Chan, Chitwan Saharia, Jay
and Tim Salimans.
Imagen video: High definition
video generation with diffusion models. arXiv preprint
Mohsen Kaboli, Rich Walker, Gordon Cheng, et al.
In-hand object recognition via texture properties with
robotic hands, artificial skin, and novel tactile descriptors.
In Proceedings of IEEE-RAS International Conference
on Humanoid Robots (Humanoids), pages 11551160.
Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque,
Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg.
Self-supervised visuo-tactile pretraining to locate and
follow garment features.
In Proceedings of Robotics:
Science and Systems (RSS), 2023.
Gagan Khandate, Siqi Shang, Eric T. Chang, Tristan Luca
and Matei Ciocarlie.
Sampling-based Exploration for
Reinforcement Learning of Dexterous Manipulation. In
Proceedings of Robotics: Science and Systems (RSS),
Michael Krawez, Tim Caselitz, Jugesh Sundram, Mark
Van Loock, and Wolfram Burgard.
Real-time outdoor
illumination estimation for camera tracking in indoor
environments.
IEEE Robotics and Automation Letters
Mike Lambeta, Tingfan Wu, Ali Sengul, Victoria Rose
Haozhi Qi, Alexander Sohn, Byron Taylor, et al. Digitiz-
ing touch with an artificial multimodal fingertip. arXiv
preprint arXiv:2411.02479, 2024.
Vincent Lepetit, Francesc Moreno-Noguer, and Pascal
Ep n p: An accurate o (n) solution to the p n
p problem.
International Journal of Computer Vision
Mengdi Li, Cornelius Weber, Matthias Kerzel, Jae Hee
Robotic occlusion reasoning for efficient object existence
prediction.
In International Conference on Intelligent
Robots and Systems (IROS), pages 26862692. IEEE,
Rui Li, Robert Platt, Wenzhen Yuan, Andreas Ten Pas,
Nathan Roscup, Mandayam A Srinivasan, and Edward
Adelson. Localization and manipulation of small parts
using gelsight tactile sensing. In International Confer-
ence on Intelligent Robots and Systems (IROS), pages
Yinxiao Li, Danfei Xu, Yonghao Yue, Yan Wang, Shih-
Fu Chang, Eitan Grinspun, and Peter K Allen. Regrasp-
ing and unfolding of garments using predictive thin shell
modeling. In International Conference on Robotics and
Automation (ICRA), pages 13821388. IEEE, 2015.
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenen-
for manipulating rigid bodies, deformable objects, and
fluids. arXiv preprint arXiv:1810.01566, 2018.
Changyi Lin, Ziqi Lin, Shaoxiong Wang, and Huazhe
Xu. Dtact: A vision-based tactile sensor that measures
high-resolution 3d geometry directly from darkness. In
International Conference on Robotics and Automation
(ICRA), pages 1035910366. IEEE, 2023.
Changyi Lin, Han Zhang, Jikai Xu, Lei Wu, and Huazhe
Xu. 9dtact: A compact vision-based tactile sensor for
accurate 3d shape reconstruction and generalizable 6d
force estimation. IEEE Robotics and Automation Letters
Pei Lin. Handdiffuse: generative controllers for two-hand
interactions via diffusion models. In AAAI Conference on
Artificial Intelligence (AAAI), volume 39, pages 5280
Xiaofei Liu, Wuqiang Yang, Fan Meng, and Tengchen
Sun. Material recognition using robotic hand with capac-
itive tactile sensor array and machine learning. Transac-
tions on Instrumentation and Measurement (TIM), 2024.
Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola,
Pieter Abbeel, Jitendra Malik, and Sergey Levine. Com-
bining self-supervised learning and imitation for vision-
based rope manipulation.
In International Conference
on Robotics and Automation (ICRA), pages 21462153.
Kei Ota, Devesh K Jha, Hsiao-Yu Tung, and Joshua
Tenenbaum. Tactile-Filter: Interactive Tactile Perception
for Part Mating. In Proceedings of Robotics: Science and
Systems (RSS), 2023.
Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta,
Yi Ma, Roberto Calandra, and Jitendra Malik. General in-
hand object rotation with vision and touch. In Conference
on Robot Learning (CoRL), pages 25492564. PMLR,
Wonik Robotics. Allegro Hand, 2024. URL
allegrohand.comah-v4-main.
Jose Sanchez, Juan-Antonio Corrales, Belhassen-Chedli
and sensing of deformable objects in domestic and in-
dustrial applications: a survey. International Journal of
Robotics Research (IJRR), 37(7):688716, 2018.
Brian Scassellati, Henny Admoni, and Maja Mataric.
Robots for use in autism research.
Annual Review of
Biomedical Engineering, 14(1):275294, 2012.
Kenneth Shaw, Ananye Agarwal, and Deepak Pathak.
Leap hand: Low-cost, efficient, and anthropomorphic
hand for robot learning.
In Proceedings of Robotics:
Science and Systems (RSS), 2023.
Yu She, Shaoxiong Wang, Siyuan Dong, Neha Sunil,
Alberto Rodriguez, and Edward Adelson.
Cable ma-
nipulation with a tactile-reactive gripper. International
Journal of Robotics Research (IJRR), 40(12-14):1385
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential
equations.
In International Conference on Learning
Representations (ICLR), 2021.
Li Sun, Gerardo Aragon-Camarasa, Simon Rogers, and
J Paul Siebert. Accurate garment surface analysis using
an active stereo robot head with application to dual-arm
flattening. In International Conference on Robotics and
Automation (ICRA), pages 185192. IEEE, 2015.
Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha
Neuralfeels with neural fields: Visuotactile perception for
in-hand manipulation. Science Robotics, 9(96):eadl0628,
Ian H Taylor, Siyuan Dong, and Alberto Rodriguez.
Gelslim 3.0: High-resolution measurement of shape,
force and slip in a compact tactile-sensing finger.
International Conference on Robotics and Automation
Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
fusion model. In International Conference on Learning
Representations (ICLR), 2023.
Megha H Tippur and Edward H Adelson. Rainbowsight:
A family of generalizable, curved, camera-based tactile
sensors for shape reconstruction. In International Confer-
ence on Robotics and Automation (ICRA), pages 1114
Benjamin Ward-Cherrier, Nicholas Pestell, Luke Cram-
Jonathan Rossiter, and Nathan F Lepora.
The tac-
tip family: Soft optical tactile sensors with 3d-printed
biomimetic morphologies. Soft robotics, 5(2):216227,
Xiaolong Wu and Cdric Pradalier. Illumination robust
monocular direct visual odometry for outdoor environ-
ment mapping. In International Conference on Robotics
and Automation (ICRA), pages 23922398. IEEE, 2019.
Zhengyou Zhang. A flexible new technique for camera
calibration. Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 22(11):13301334, 2002.
Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
Hao Li. On the continuity of rotation representations in
neural networks. In Conference on Computer Vision and
Pattern Recognition (CVPR), pages 57455753, 2019.
Jihong Zhu, Andrea Cherubini, Claire Dune, David
Fanny Ficuciello, Kensuke Harada, Jens Kober, Xiang
of deformable objects. IEEE Robotics and Automation
Magazine (RA-M), 29(3):6777, 2022.
APPENDIX
A. Detail of Camera Calibration
In this section, we introduce the camera calibration process
as part of the overall sensor calibration. Since the tactile sensor
is enclosed by an opaque, rounded membrane, conventional
calibration board methods cannot be used to determine the
pinhole cameras extrinsic parameters. To address this, we
designed an indentation setup (as shown in Fig. 10) to capture
a sufficient number of spatial points in a known sensor frame,
identify their corresponding 2D-pixel coordinates in the image,
and establish the mapping between the sensor frame and the
image frame. First, the cameras intrinsic parameters K was
using high-precision calibration boards . Next, we define a
three-dimensional coordinate system, referred to as the sensor
frame (x, y, z) with its origin at the center of the elastomer, as
shown in Fig. 10(a). To facilitate the calibration, A custom 3D-
printed holder secures the sensor (Fig. 10(b)), while another
3D-printed hemispherical indicator is attached to the holders
groove (Fig. 10(c)). Small pins with a diameter of 1.5mm,
serving as indenters, are inserted into pre-defined holes within
the indicator for 28 trials. For each trail, the contact positions
are recorded both in the camera image as pij  (uij, vij)
and in the sensor frame as Pi,j  (xij, yij, zij), where i
denotes the trail index and j denotes the contact point index
within the trail. The contact positions in the camera image are
detected by subtracting the captured image from a reference
image without indentation. We use solvePnP  to calculate
the extrinsic parameters that includes rotation matrix A and
translation vector b such that:
pij  K[A  b]Pij
After obtaining the intrinsic and extrinsic parameters of
the camera, we can project the sensors curved surface from
the sensor frame onto the image frame, obtaining the sensor
Fig. 10: Camera calibration using an indentation setup: The sensor
frame is first defined in (a). A holder is designed and 3D-printed
to secure the sensor, as shown in (b). A hemispherical indicator
is designed and 3D-printed to attach to the sensor holder. Pins are
inserted into pre-defined holes to serve as indenters for recording
contact locations in the sensor frame, as shown in (c).
Fig. 11: Example of establishing contact: First, the hand descends
until a finger makes contact with the surface. A fixed-point rotation
is performed around the contacting finger, as shown in (a). The hand
then continues to rotate until a second finger makes contact, triggering
a fixed-axis rotation around both contacting fingers, as shown in (b).
The process is complete when three or more fingers are in contact,
as shown in (c).
surface reference projection D (Equation (9)), by which the
depth value on the pixel (u, v) can be queried.
where [u v 1]T and Zc are given as:
(A[x,y,z]T b)x
(A[x,y,z]T b)y
, Zc  (A[x, y, z]T  b)z. (10)
B. Detail of Establish Contact
In this section, we detail our approach to generate contact
with a flat object using the fingertips. The goal is to control
the hand to ensure that at least three fingertips are in contact
with the surface. We denote the four fingertips as ft (thumb),
fi (index), fm (middle), and fr (ring). The contact states
are represented by two sets: CCC, which includes the fingers
in contact, and NNN, which includes the fingers not in contact.
The complete process is illustrated in Fig. 11.
Establish First Contact: Starting from status when all
fingers are hovering (i.e., CCC  ,NNN  {ft, fi, fm, fr}), the
hand is controlled to move downward till one finger touches
the surface. For example, if the thumb touches the surface
(Fig. 11), the contact state sets are updated to CCC  {ft} ,NNN
{fi, fm, fr}
Establish Second Contact: Once the first contact is
to create the second contact point. To achieve this, we first
obtain the centroid point of the fingertip in contact (denoted
as (xc, yc, zc)), and compute the centroid point of fingertip
positions in NNN (denoted as (xn, yn, zn)). This allows us to
calculate the rotational axis as:
v1  Rz(90)(xn xc, yn yc, zn zc)T ,
where Rz(90) is the rotation matrix for a 90-degree rotation
around the z-axis. Given ,v1
v1 calculated before, robot arms
target end-effector pose b
eeT leading to such rotation can be
obtained via Rodrigues rotation formula:
v1) I  sin()
(1 cos())
The target end effector pose of the robot arm can be
calculated as:
where b denotes the base of the robot arm, ee and ee
represent the end effector before and after the movement, and
c and c represent the positions (xc, yc, zc) before and after the
rotation. The robot arm is then controlled to gradually increase
until the second fingertip contacts the object surface. Once
this occurs, we update the contact states to CCC  {ft, fi} and
NNN  {fm, fr}.
Establishing Third Contact: In this step, the hand
rotates around an axis defined by the first and second contact
points until the third fingertip makes contact. For instance, if
the thumb and index finger make contact, the rotation axis is
ftfi. The arms target end-effector pose for this rotation
where c and ee are c and ee after rotation specified
v2. During execution, the angle  is gradually increased
until a new fingertip contacts the surface, achieving the desired
target end-effector pose b
eeT. Note that these steps may not
always be required. In some cases, we observe that the third
finger may already be in the contact state when we attempt to
establish contact with the second finger.
C. List of Symbols
The definition of symbols can be found in Table II.
TABLE II: Summary of symbols and notations.
Descriptions
Pixel coordinates in VBTS.
Camera coordinates in VBTS.
Sensor coordinates in VBTS.
The intrinsic parameters of the camera in VBTS.
The extrinsic parameters of the camera in VBTS.
Sensor surface reference projection in VBTS.
Depth mapping function in VBTS.
Rotation angle of controllable hand joints.
Angular velocity of controllable hand joints.
Positional coordinate of hand joints in arms base
Linear velocity of hand joints in arms base axis.
Wrists (end effector of arm) 6D rotation.
Angular velocity of hand pose.
Wrist (end-effector of arm)s height along arms
Linear velocity of pppee.
The deformation depth readings from four finger-
tip tactile sensors.
The target deformation depth.
State variables dimension.
Hand joint angles qqq1:Ndata, wrists (end effector of
arm) 6D rotation R1:Ndata and wrists translation
along z-axis p1:Ndata
for overall trajectory.
Length of synthesis motion sequence.
Length of predicted actions.
Future motion predicted by PP-Tac policy.
Length of historical actions.
The historical action sequence.
Diffusion step.
