=== PDF文件: Optimal Interactive Learning on the Job via Facility Location Planning.pdf ===
=== 时间: 2025-07-22 15:57:12.554261 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Optimal Interactive Learning on the Job via
Facility Location Planning
Shivam Vats
Michelle Zhao
Patrick Callaghan
Mingxi Jia
Maxim Likhachev
Oliver Kroemer
George Konidaris
Brown University
Carnegie Mellon University
Equal contribution
AbstractCollaborative robots must continually adapt to novel
tasks and user preferences without overburdening the user. While
prior interactive robot learning methods aim to reduce human
are not well-suited for sustained, multi-task collaboration. We
propose COIL (Cost-Optimal Interactive Learning)a multi-
task interaction planner that minimizes human effort across a
sequence of tasks by strategically selecting among three query
types (skill, preference, and help). When user preferences are
(UFL) problem, which enables bounded-suboptimal planning in
polynomial time using off-the-shelf approximation algorithms.
We extend our formulation to handle uncertainty in user
preferences by incorporating one-step belief space planning,
which uses these approximation algorithms as subroutines to
maintain polynomial-time performance. Simulated and physical
experiments on manipulation tasks show that our framework
significantly reduces the amount of work allocated to the human
while maintaining successful task completion.
I. INTRODUCTION
Collaborative robots hold the promise of reducing human
life by assisting with tedious and labor-intensive tasks. For
human-robot collaboration to be truly effective, robots must
safely complete assigned tasks according to individual user
preferences. Although robots can be equipped with prior
knowledge about the user and a library of common skills,
they will inevitably face novel situations in practical long-term
deployments. How can they learn and adapt on the job when
faced with tasks that are beyond their capabilities or when
the user preference is unclear? We explore these questions
in the context of multi-task domains, such as factories and
during deployment. The central challenge in this setting is
to enable robots to learn interactively from humans while
ensuring that all tasks are completed with minimum human
Prior interactive learning approaches leverage active query-
ing strategies to proactively identify queries that result in
maximum uncertainty reduction , volume removal  or
information gain . Such active learning strategies have been
shown to reduce the number of queries. Furthermore, the use
of multiple query types, such as demonstrations and preference
accounting for the cost associated with each query type .
Correspondence to shivamvatsbrown.edu.
Fig. 1: A human-robot team working together to pick objects off a
conveyor and pack them in bins. The robot queries the human online
to learn motor skills and user preference about how each object should
be grasped and where it should be placed. We propose a planning
algorithm COIL that optimizes how the robot allocates tasks and uses
these different types of queries to minimize human effort during the
course of its deployment.
do not guarantee minimization of human effort across multiple
tasks in extended collaboration. The latter challenges the robot
to reason proactively about the expected utility of its queries
so that it does not overburden the user by seeking to learn all
the tasks.
We propose a novel multi-task interaction planner COIL
(Cost-Optimal Interactive Learning) that uses three types of
queries skill, preference, and human helpto complete a
given sequence of tasks, simultaneously satisfying hidden
user preferences and minimizing user burden. Our key the-
oretical contribution is a novel formulation of multi-task
interactive robot learning as an instance of the uncapacitated
facility location (UFL) problem. Different from prior works,
our formulation is cost-optimal and jointly minimizes human
burden during learning and deployment. Our formulation has
significant theoretical and practical implications. Theoretically,
it shows that interactive robot learning in our setting can
be approximated, i.e, a bounded sub-optimal solution can be
computed in polynomial time. Practically, it enables the use
strong off-the-shelf UFL solvers for planning. We believe
that our UFL formulation opens exciting avenues for future
research by leveraging rich theory and algorithms developed
by the operations research community, without the need to
develop solutions from scratch. Furthermore, we extend our
UFL formulation to incorporate multiple query types by
proposing a simple but effective one-step belief space planning
algorithm that can request user preferences when uncertain.
problem is NP-hard. Our UFL formulation enables us to
leverage fast approximation algorithms as sub-routines to plan
efficiently. We extensively evaluate and analyze the behavior of
our algorithm in simulated and physical manipulation domains.
We find that COIL minimizes user burden significantly better
than existing approaches, can adapt to failures in teaching,
and efficiently scales to long-duration collaboration. COIL
results in interactions with 12 20 and 23 lower cost
than the best performing baseline in simulated manipulation
experiments and physical conveyor experiments respectively.
Additional details and videos of our experiments are available
II. RELATED WORK
Learning through multiple interaction modalities is an
active area of robotics research [14, 23]. Byk et al.  first
passively collect demonstrations to initialize a belief about the
humans reward function and then actively probe the user with
preference queries to zero-in on their true reward. Fitzgerald
et al.  also aim to learn a humans reward function,
but they do so by actively selecting the query from among
Rewards expected to be most informative to the learner. In a
similar vein, Mehta and Losey  use multiple modalities
of physical interaction queries to first learn a reward model
online using a neural network representation and then apply
constrained optimization to identify an optimal trajectory. Like
our work, these works aim to learn from multiple interaction
types. However, the aforementioned methods apply to singular
that will be useful to the robot (and can do so while consid-
ering the cost of querying), they do not plan for the future
utility of learning those tasks in the way our method does.
Human preference is another wide-spread topic of interest
in robotics [5, 24, 25, 26, 32]. Jeon et al.  present a
unifying formalism for preference-based learning algorithms
through multiple types of interaction with the understanding
that a preference is implicit in the feedback provided by the
human and the skill being taught. Moreover, these preferences
must be learned through multiple interaction instances. In
action parameters by which a skill must abide and are learned
through just one interaction instance. Hadfield-Menell et al.
introduce cooperative inverse reinforcement learning to
learn a humans reward function (i.e., preference). While their
POMDP-based approach could be applied in our paradigm,
our use of a HiP-MDP  makes the problem tractable and
enables us to design an efficient, effective planner. Work like
Bajcsy et al.  isolates a users preferences by learning
features one-at-a-time until converging to the complete reward
and without considering future utility. Finally, reinforcement
learning from human feedback (RLHF) methods have achieved
state of the art results in sample complexity and reward
learning for complex tasks [1, 8, 13, 28]. Even so, these
successes come with a substantial cost of requiring hundreds
or even thousands of preference queries and human-hours,
rendering them infeasible for on-the-job learning from people.
On-the-job collaborative methods automate task alloca-
tion between machine and human either through modeled or
learned approaches [3, 15, 30, 37, 36]. For instance, Liu et al.
propose a learning and deployment framework in which
the human monitors the robots performance to intervene and
provide corrections. Data from deployment is then used in
subsequent rounds to improve the robots policy. Vats et al.
use a mixed integer program to decide when to learn
new skills and when to delegate tasks to the user. While their
method accounts for the future utility of learning a task, they
do so without also considering the humans preferences. Racca
et al.  argue that active learning methods can increase the
cognitive effort of human teachers by asking difficult questions
that require frequent context change. This point suggests that
sample-efficient learning cannot be the sole end goal; the
difficulty of answering each question should be taken into
account as well. While some of the aforementioned methods
do model different costs associated with querying, our method
is the first to do so across a stream of incoming tasks while
considering human preferences as well.
III. PROBLEM STATEMENT
A robot is asked to collaborate with a user to jointly com-
plete a sequence of physical tasks (i)n
picking objects from a conveyor and sorting them into bins
according to hidden user preferences. Each task is described
by a vector i T and has an associated reward function:
task  ri
safe  ri
cskill-fail  I[safety violated]  ri
The safety reward rsafe models user-agnostic safety con-
straints that the robot must not violate (e.g., avoid collisions,
do not drop objects). The preference reward rpref measures
how closely robot actions match user preferences (e.g., placing
a plate on the right shelf, not placing fingers inside a mug).
querying the human. In addition, the robot starts with a base
set of skills L0 : {0} which may not cover all tasks. Hence,
the robot should either request the user to handle such tasks
or ask the user to teach it new skills such that it can then
complete them autonomously. Our goal is to complete all the
tasks while minimizing the burden placed on the user during
the interaction. The human effort required to respond to each
type of robot query is quantified using costs: chum for assigning
a task to the human, cskill for requesting the human to teach
a motor skill, and cpref for requesting their preference. Hence,
the overall objective is to maximize J:
query ci
where ci
query  ci
pref measures the cost of querying
the human, and ci
rob is a fixed cost incurred if the robot
undertakes the task. Note that max J min J. We will
use this fact later to convert this into a minimization problem.
IV. APPROACH
We model each task as a hidden-parameter MDP (HiP-
MDP), where some parameters of the reward function that
capture user preferences are uncertain. Each task HiP-MDP
is M : (S, A, , rtask(s, a; z), T, P), where  is a discrete
set of possible reward parameters and P is the prior over
these parameters. The robot updates its posterior b over the
parameters based on its interaction with the human. z is a
hidden distribution over reward parameters in  and represents
the users preferences for the given task. The reward for each
task rtask(s, a; z) is thus a function of the hidden preference
distribution. We model the choice of acting autonomously and
the three types of queries that the robot can make as four
distinct actions that comprise the robots action space A:
1) Execute Skill. The robot executes a skill to complete the
task. This is modeled by a rob action which has a fixed
cost crob and a variable task reward rtask that captures
how well the robot completed the task.
2) Query type 1: Request Skill. Ask the user to teach a
new motor skill for a task via demonstrations. The users
effort to perform this query is captured by the cost cskill.
If the robot fails to learn from the human, then the query
is deemed a teaching failure.
3) Query type 2: Request Human. Ask the user to complete
the task. The user effort required to fulfill this request
is captured by the cost chum.
4) Query type 3: Request Preference. Ask the user for
their preference about how a task should be done. The
user effort is captured by the cost cpref. Some prior
work define preference query as a comparative choice
between two possible options. In our case, the user
chooses a preferred option from all available options.
We will refer to it as a preference request to avoid
confusion.
Robot Skills. The robot learns a library L of task-specific
skills (s, ) that are parameterized by task and user prefer-
ence parameters. Define the expected return for executing skill
with a specific preference parameter  to complete task i
task(s, a).
To complete i, the robot must not only select which skill
to use, but also which preference parameter to execute said
skill with. Let the robot have skills L : {1, . . . , k}. Then,
the robot must choose a combination of skill and preference
,  arg max
Skill Return Model. The true reward function is unknown
to the robot at planning time. Hence, the robot must use
a reward model to predict the expected return from skill
execution before learning it. Let the robots belief about the
users preference for the current task be b. We provide
the robot with a domain-specific function safe
to predict
the probability of safe execution. safe
( , ) predicts the
probability of safe execution of a skill  when deployed on
a task   with parameter . This function is defined based
on similarity between ( , ) and the conditions (, ) that
will be learned for: safe
Appendix E for our instantiation in experiments. Intuitively,
the probability of skill success is high if both tasks and
preferences are similar and low otherwise. The preference
belief b models the probability of preference satisfaction.
The skill return model is then:
R(, )  [cskill-fail  (1 teach  safe
cpref-fail  (1 b())].
The first term predicts the expected penalty for safety viola-
tion and the second terms predicts the penalty for preference
violation. teach estimates the probability that the robot will
successfully learn a skill from the human. We model the
success of teaching as a Bernoulli process and use Bayesian
inference to compute the posterior distribution with a Beta
distribution Beta(, ) as the prior. We use the mean of the
distribution in the return model as the estimated probability of
successful teaching:
teach  E[Beta(, )]
This enables the robot to identify difficult-to-learn tasks and
adapt its plan online if teaching fails so that it does not waste
human time. Our experiments will show that non-adaptive
methods expend significant human effort trying to learn such
tasks even after repeated teaching failures whereas COIL
assigns them to the human and focuses on learning feasible
A. Skill Learning Under Known Preferences: Facility Loca-
tion Formulation
We first consider the case in which the robot cannot query
the human about their preference online and must plan with its
belief about the preferences. Previous work by Vats et al.
proposed a mixed integer programming formulation ADL to
plan in a similar setting. Unfortunately, solving MIP optimally
is NP-hard which makes it difficult to scale ADL to larger
problems. We propose a novel uncapacitated facility location
(UFL) formulation that has tight polynomial-time approxima-
tion algorithms which can efficiently compute high-quality
solutions even for large problems. Intuitively, the challenge
of identifying the minimal cost set of interactive actions that
cover the full task sequence maps nicely onto the UFL problem
which seeks to service demands (tasks) by allocating facility
resources (interactive actions) while minimizing costs.
Facility Location Problem. We first briefly introduce the
uncapacitated facility location (UFL) problem. Please refer to
Williamson and Shmoys [35, chapter 4] for a more detailed
Fig. 2: Facility location formulation. Tasks 1, . . . , 5 are demands to
be satisfied. Facilities correspond to interactive actions available for
every task. We highlight facilities for 2: Human facility can only
service 2. Skill facility can service similar future tasks 2, 4, 5.
Robot facility cannot service any task as the robot hasnt learned a
skill yet. Furthermore, none of the facilities can service past tasks.
treatment. The facility location problem has a set of demands
D  {1, . . . , m} and a set of facilities F  {1, . . . , n}. There
is facility cost fi associated with opening each facility i F
and an assignment or service cost cij of serving demand j
by facility i. The goal is to serve all the demands by opening
a subset of facilities F  F such that the overall cost of
opening the facilities in F  and the cost of assigning each
demand j D to the nearest facility i F  is minimized:
facility cost
service cost
We formulate the interaction planning problem as a facility
location problem by defining demands, facilities, facility costs
and service costs D, F, fi, cij.
Tasks as Demands. Define the set of demands as the set
of tasks F  {1, . . . , N}. A solution to UFL has to satisfy
all the demands. Hence, this guarantees that it will complete
all the tasks.
Robot Actions as Facilities. In the following, we define
facilities corresponding to all the actions for every task i.
1) Human Facilities. Define one human facility ihum with a
facility cost of chum that captures the cost of asking the
user to complete the task. The service cost to serve i
is 0 as human effort is modeled by chum and for all
the other tasks as this action cannot not complete them.
2) Skill Facilities. Define one skill facility iskill with a
facility cost of cskill that corresponds to requesting the
user to teach the robot a new skill i for i. The service
cost for completing a task j consists of a fixed robot
execution cost crob and the task reward
crob max
Rj(i, ).
Algorithm 1 Facility location formulation for COIL.
demands and facilities
for i 1,    , N do
F.insert(ihum), f hum
human facility
0 if i  j else
F.insert(iskill), f skill
skill facility
for j i, . . . , N do
crob max Rj(i, )
for i L do
skills already learned
F.insert(irobot), f robot
robot facility
for j 1, . . . , N do
crob max Rj(i, )
return (ai)N
for   do
possible query response
i1 UPDATEPREFBELIEFS()
J COIL-KNOWNPREFS((b
J  J  b1()  J
if cpref
P b1 J then
a1 apref
return (ai)N
i1 INITPREFBELIEF()
current task index
while all tasks are not done do
plan COIL((bi)N
execute the first action and update k
update teach
The robot executes i with preference parameters
that maximize the expected reward under its preference
belief distribution.
3) Robot Facilities. Define one robot facility irob with a
facility cost of 0 that corresponds to the robot using a
previously learned skill i. The service cost for com-
pleting a task j consists of a fixed robot execution cost
crob and the task reward as above.
Approximation Algorithms. We implement the approxi-
mation algorithm proposed by Jain and Vazirani . In the
worst case, this algorithm is 3-suboptimal when the facility
location problem is metric and log(n)-suboptimal otherwise.
This algorithm has a run-time of O(n2 log(n)), where n is the
number of demands. In practice, we found it to be near-optimal
for our problems. Let X be the set of facilities opened so far,
and let U be the set of demands that are not served by open
facilities. Then, the algorithm iteratively picks a demand i F
and Y U that minimizes the ratio
and sets fi
to 0. Intuitively, it opens a facility that has the minimum cost
per demand for some subset of demands and assigns those
demands to the opened facility.
B. Planning for Preference Requests
Equipped with a provably bounded-suboptimal plan for
learning under known preferences as a reference point, COIL
then determines when preference requests are needed to clar-
ify user preferences before execution. For each task, COIL
evaluates the expected change, under its current beliefs b,
in overall plan cost should it request the users preference
for the current task. If the expected plan cost plus preference
request cost is lower than the current plan (line 22), COIL
elects to first reduce uncertainty in the preference parameters
for the current task. In this way, COIL augments its bounded-
suboptimal reference plan with uncertainty-guided preference
requests when necessary, towards improving the plan to handle
uncertainty in preference parameters. During the interaction,
we replan after every interactive action execution to take into
account the latest information. This is enabled by the fast run-
time of COIL which is polynomial in the number of tasks and
preference parameters.
Theorem 1. The worst-case runtime of COIL is polynomial
in n and k, where n is the number of tasks and k   is
the number of possible preference parameters.
facility
location
1 times (see line 18) in total by call-
ing the function COIL-KNOWNPREFS. The complexity of
COIL-KNOWNPREFS is polynomial when SOLVEUFL is an
approximation algorithm. In particular, we use an approxi-
mation algorithm with runtime O(n2 log(n), where n is the
number of tasks. Hence, the overall complexity of COIL is
O(k(n2 log(n)), which is polynomial in n and k.
V. EXPERIMENTAL SETUP
To evaluate the efficacy of our proposed approach, we run a
series of quantitative experiments to study how COIL affects
the costs of learning on the job during task execution compared
with baselines. We further investigate quantitatively the nature
of queries throughout the interaction to understand how COIL
and other approaches induces different interactive behaviors.
Domains. We study the combined learning and execution costs
incurred by our approach through experiments in three con-
trolled environments: an object pickup and dropoff Gridworld
environment implemented using MiniGrid , a simulated
7DoF Manipulation environment implemented using robo-
Brief descriptions of each domain are below; please see the
appendix B for details.
1) Gridworld is a discrete, 17x17 grid comprised of a
sequence of 15 total objects of nine varieties distin-
guished by object type, color, and position. Each object
defines a task the agent must execute, and each task is
to navigate to an object, pick it up, and transport it to
the users preferred location for that object. There are
three possible goal locations.
2) 7DoF Manipulation is a simulated bin-packing task
where a Franka robot manipulator must pick up and put
30 total objects of seven different varieties (e.g., milk
defines a task the agent must execute.
3) Conveyor is a real-world instantiation of a factory set-
ting in which a Franka tabletop manipulator and a human
employee work side by side to sort objects arriving on a
conveyor belt into three containers. Some of the objects,
such as the mug, have two possible grasps: handle and
rim. The robot must pick each object using the preferred
grasp and place it in the preferred bin. We randomly
sample 5 task sequences consisting of 20 tasks each.
We pretrain GEM  using human demonstrations for
every task and use it to provide demonstrations in our
experiments.
Baselines. We compare COIL with state-of-the-art approaches
for interactive robot learning and task allocation. Implemen-
tation details for each baseline can be found in the Appendix
1) Confidence-based ADL (C-ADL). ADL  addresses
the similar problem of planning for skill learning and
task allocation and thus makes for a strong baseline.
and thus lacks the ability to reason jointly over prefer-
ence and skill learning. To ensure a fair comparison,
the C-ADL baseline makes preference queries when the
robots confidence over the humans preference is below
some predefined threshold.
2) Information Gain (IG). Because COIL plans over mul-
tiple query types (skill, preference) and multiple human
contribution types (human, robot), it makes sense to de-
sign a baseline inspired by interactive learning methods
that incorporate multiple types of human feedback (e.g.,
a widely-used approach is to select the query which
provides the greatest expected information gain .
While these methods typically learn reward functions for
a single task and consider neither human contributions
nor the future utility of learning particular skills, we
design an information gain objective which does both
as the IG baseline in our interaction paradigm.
3) Confidence-based Autonomy (CBA). Inspired by Cher-
nova and Veloso , CBA requests skill and preference
teaching if the robot is uncertain about user preferences
or skills and otherwise assigns the task either to itself
or the human.
Human Cost Profiles. The costs a human might associate with
depend on domain-specific variables such as the cost of labor
and the difficulty of teaching. We study the performance of our
approach under multiple simulated users with three different
cost profiles. In our evaluations, we assign a cost of crob  10
to each robot execution, chum  80 to human task execution,
and cpref  20 to each preference request. The robot incurs a
TABLE I: In the Gridworld domain, we find that COIL makes fewer
preference queries than the confidence-based baselines because COIL
only asks for human preferences if it believes that this information
will be useful later. Format is mean(standard deviation).
penalty cost of cskill-fail  100 if it fails to successfully execute
any skill, and a penalty of cpref-fail  100 if it successfully
executes a skill by placing the object in a goal undesired by
the user. To compare the profiles with minimal cost tuning,
we examine profiles across the cost of human skill teaching.
1) Low-Cost Teaching (cskill  50): This profile simulates
an experienced teacher for whom providing demonstra-
tions of robot skills is not burdensome.
2) Med-Cost Teaching (cskill  100): This profile simulates
a teacher for whom teaching is moderately more bur-
densome than performing the task themselves.
3) High-Cost Teaching (cskill  200): This profile simulates
a novice teacher for whom providing demonstrations of
robot skills is highly burdensome.
Preference Belief Estimation. The robot maintains a belief
estimate b of the users hidden personal preferences for
every task. The probability that each preference parameter
satisfies user preference, i.e., b(), is modeled as a Bernoulli
distribution. These belief estimates are updated based on
feedback from the user using a Bayesian filter (described in
Appendix section A).
VI. EXPERIMENTAL RESULTS
We summarize our results through four major takeaways
derived from five distinct experimental focuses: (A) cost
comparisons under different human cost profiles when all
task skills are learnable by the robot, (B) scalability and
computational costs of COIL under long task sequences,
(C) online-adaptiveness of COIL when challenging-to-learn
skills necessitate online replanning, and (D) a real-world
instantiation of COIL that enables the human-robot team to
teach and learn as they manipulate objects which arrive on a
conveyor.
A. COIL outperforms myopic interactive learning.
COIL significantly outperforms myopic interactive learning
methods that do not consider plan over the future utility of all
possible plans (Figure 3). In the Gridworld domain instantiated
such that all objects are learnable by the robot, we evaluate
the incurred plan costs over 30 task sequences and randomized
human object arrangement preferences. Under the low-cost
Plan Cost
Low Cost Teaching
High Cost Teaching
Medium Cost Teaching
Fig. 3: Under Med- and High-Cost teaching cost profiles, COIL
consistently chooses the lowest cost plan compared with baselines.
We highlight the qualitative behavior of COIL compared to baselines:
COIL under the medium cost teaching profile assigns singleton
tasks to the human when the cost of learning is high. Error bars
indicate standard error over 30 randomized task sequences and true
human preferences in the Gridworld domain.  represents p < 0.01
significance.
COIL-NoAd
COIL-NoAd
COIL-NoAd
TABLE II: Results on the manipulation domain. On average, COIL
plans interactions that result in 7 to 18 reduction in cost compared
to the best performing baseline. The improvement over baselines is
particularly marked when the cost of teaching is more expensive
than assigning the task to the human, i.e, medium and high cost
profiles. The reported statistics are averaged over 10 interactions with
30 randomly sampled tasks each.
significance). As the cost of human demonstrations increases
(med-cost and high-cost), COIL consistently achieves the
lowest-cost plans of all the methods. We compared statistical
differences using a one-way ANOVA  between costs
achieved for each algorithm, and evaluated pairwise differ-
ences using pairwise t-tests  with Bonferroni correction
if significant main effects were present. For the Med-
Cost human cost profile in the Gridworld domain, we found
significant effects of planner (F  23.48, p < 0.01), and found
significant (p < 0.01) pairwise differences between COIL
and all baseline algorithms. For the High-Cost human cost
profile in the Gridworld domain, we found significant effects
of planner (F  120.58, p < 0.01), and found significant
(p < 0.01) pairwise differences between COIL versus IG
Fig. 4: COIL
computes
near-optimal
solutions
significantly
mixed integer
programming.
and CBA. Though we did not find a pairwise significant
cost on average compared with C-ADL (Figure 3). We provide
all statistical testing values in Appendix E-D.
To understand the differences between the behaviors in-
duced by each algorithm, the bottom four rows of Figure 3
highlight a task sequence of 10 objects when each method
collaborates with the med-cost human profile. COIL identifies
the lowest-cost plan by requesting to be taught the skills and
preferences associated with repeated instances of the same
object and delegating singleton tasks to the human partner. C-
ADL identifies a plan similar to COIL, but it asks for prefer-
ence queries for singleton tasks that are eventually assigned to
the human, incurring additional preference cost. Under Med-
Cost teaching, IG and CBA opt to learn every preference and
skill (Table I). On the other hand, under Low-Cost teaching,
all approaches choose to learn all preferences and skills, given
that the total cost of doing so is not expensive. These results
highlight a particular strength of COIL to balance learning,
requests for human contribution, and execution especially
when the right course of action under nuanced costs is not
easily known.
B. COIL efficiently scales to long task sequences without
compromising plan quality.
approximation
algorithm
COIL with an optimal mixed-integer programming (MIP)
approach.The
formulation
implemented
state-of-the-art
algorithm is implemented in Python, leaving room for further
improving run-time. COIL efficiently computes near-optimal
solutions significantly faster than MIP. The speedup is
defined as the ratio of MIP runtime to COIL runtime, while
sub-optimality is measured as the ratio of the cost of plans
generated by COIL to those produced by MIP. In Figure 4,
we observe that the computational advantage of COIL over
MIP gets more pronounced for longer task sequences without
compromising on solution quality.
C. Adapting to teaching failures online reduces human bur-
Thus far, our assumption that the robot can reliably learn
skills for all tasks enabled us to evaluate and compare the
plan costs incurred by COIL and baselines. While accurately
modeling the feasibility of skill learning is not the focus of
Low Cost Teaching
High Cost Teaching
Medium Cost Teaching
Plan Cost
COIL-NoAdapt
Fig. 5: In the Gridworld environment, when half of the objects are
COIL-NoAdapt and other baselines across teaching profiles. In high
cost teaching, COIL and COIL-NoAdapt often assign tasks to the
human off the bat, reducing the impact of adaptivity. Error bars
indicate standard error over 30 randomized task sequences and true
human preferences.  represents p < 0.01 significance.
this work, we investigate how COIL can adapt to failures in
learning with online replanning. Importantly, these challeng-
ing skills are comparatively difficult to learn and are revealed
as such only after the robot tries to acquire the skill via
human demonstrations. We demonstrate these results in both
the Gridworld and Simulated 7DoF Manipulation domains.
1) COIL Adaptivity in Gridworld: In the simulated Grid-
world domain, we control for the presence of challenging
skills by varying the number of skills that are challenging-to-
learn (i.e., can never be learned), and examine the relationship
between number of challenging skills and the benefit of esti-
mating teach online in the COIL framework. As the proportion
of challenging skills increases (from 10 to 50 (Fig 5) to
90), the added benefit of replanning based on the observed
feasibility of teaching enables COIL to identify lower-cost
plans than COIL-NoAdapt which optimistically assumes that
all skills are learnable. Baselines that do not adapt to teaching
failures also incur higher costs than COIL. (Fig 5). Refer
to Appendix F for reported statistical analyses and detailed
analysis of results on 10 and 90 challenging-to-objects.
2) COIL Adaptivity in Simulated 7DoF Manipulation: We
next study the adaptivity of COIL in the simulation manip-
ulation domain. We consider task sequences of 30 objects,
sampled from 7 unique varieties. The mug object is too wide
for the robot gripper which results in a teaching failure when
the human tries to teach the robot. COIL takes this failure
into account by updating teach for all mugs and adapts its
plan to assign mugs to the human. This results in statistically
significantly lower costs than baselines (figure 6, table II). In
this experiment, we compare with our 3 strongest baselines,
derived from our earlier results, removing CBA from our
analysis. Statistical analyses are detailed in Appendix G.
D. COIL succeeds in real-world operations.
We evaluate COIL on a physical conveyor domain designed
to emulate collaboration in factories. We ran the experiments
with a medium cost profile with chum  50, cskill  100,
5 different task sequences, each with 20 objects randomly
Fig. 6: In the Simulated 7DoF Manipulation domain, COIL achieves
significantly lower plan costs than baseline methods. Error bars
represent standard error over 10 randomized initialization of the 30-
object task sequence.
Algorithm
COIL (teach fail)
CADL (teach fail)
TABLE III: Results on a physical conveyor. We ran experiments
with 5 different task sequences, each with 20 objects, with COIL
and CADL. We observed teaching failure on the white mug (pos-
sibly because its shiny surface made camera-based pose estimation
difficult). Hence, we report the run with teaching failure separately
from the other 4 runs. COIL was able to achieve significantly lower
cost than the baseline in both situations. CADL especially struggled
in the case of teaching failure as it repeatedly requested to be taught
the mug skill.
sampled from a set of 12 objects. Each sequence consisted of
one high-frequency object which was five times more likely
to be sampled than the other objects. The objects appeared
one-by-one in front of the robot, at which point it needed to
decide whether to autonomously pick up the object using an
appropriate grasp and place it in the correct bin, or request help
from the human. Some objectssuch as mugs and bottles
had two feasible grasps, of which only one was preferred by
the human. Similarly, the human had hidden preferences about
which bin each object belonged to. The robot queried the
human to understand their preferred grasp and target bin for
each object.
We report the results from our experiment in table III.
Compared to C-ADL, COIL made fewer skill and preference
requests. C-ADL has a tendency to overburden the user with
preference requests while COIL learns user preference for a
task only when it intends to complete the task autonomously.
A priori, it is desirable to learn how to manipulate the high-
frequency object in each experiment. However, we observed
that the robot was unable to learn a skill for the white mug,
possibly because its shiny surface made our camera-based
pose estimation difficult. Because of its adaptive nature, COIL
changed its plan and assigned all mugs to the human. By
how to manipulate the mug and expended significant human
effort in the process.
Limitations. To plan for a set of tasks, COIL must know
the composition of that set beforehand. This prior knowledge
will be difficult to come by reliably in all real-world settings.
Fig. 7: In the real-world conveyor task sequence, COIL minimizes
costs relative to C-ADL, requesting user preferences when uncertain,
executing known skills, and requesting demonstrations to learn new
skills. We highlight three tasks along the plan to learn generated by
COIL. C-ADL requests unneeded preferences before assigning tasks
to the human.
be discrete and stationary, but prior work reveals that a
humans preferences take a variety of form and are subject to
change over time. Furthermore, we use prior knowledge about
similarities between tasks to predict the generalization of skills
to future tasks. In many complex domains, such a prior may be
inaccurate. For example, while the robot may be confident that
its learned skill can be executed again on the next instance of
the same skill, perturbations in the environment (e.g., slight
differences in object orientation), unobserved environment
variables (e.g., lighting), and other factors may cause the robot
to fail when rolling out its learned skill.
VII. CONCLUSION
Teams are best positioned to succeed when each mem-
ber accounts for their teammates abilities and preferences.
Robot teammates will need to do the same if human-robot
teams are to succeed, and COIL is our proposed means of
enabling this capability. By planning over multiple interaction
humans associated task preferences and contribution costs,
COIL identifies plans which minimize the burden placed upon
the human on its way to achieving task success. COIL does so
by formulating the interaction as a facility location problem
which identifies the optimal sequence of robot actions and
human contributions and then deducing if that optimal plan
could be improved with more certain beliefs about the human
teammates preferences. COIL identifies plans which cost less
as compared to baselines and efficiently scales to long task
sequences. Moreover, COIL can learn human preferences for
both task completion and teammate task contributions, and it
can re-plan when skills prove too difficult to learn. Finally,
COIL demonstrates its effectiveness in a real-world conveyor
factories of the future. In our future work, we are interested
in extending our planner to multi-step tasks. One possibility
would be to decompose a multi-step tasks into a sequence of
sub-tasks which can then be handled by our planner. Another
direction of interest is to plan for unordered sets of tasks which
introduces an additional challenge of scheduling.
ACKNOWLEDGMENTS
We thank Prof. Reid Simmons and Prof. Henny Admoni
for their valuable feedback, Prof. Anupam Gupta for in-
sightful discussions and Lakshita Dodeja for her help with
our real-world robot experiments. This work was supported
by the Office of Naval Research (ONR) under REPRISM
MURI N000142412603 and ONR grant N00014-22-1-2592,
as well as by the National Science Foundation (NSF) via grant
1955361. Partial funding was also provided by the Robotics
and AI Institute.
REFERENCES
Nichola Abdo, Cyrill Stachniss, Luciano Spinello, and
Wolfram Burgard. Robot, organize my shelves! Tidying
up objects by predicting user preferences.
IEEE International Conference on Robotics and Automa-
tion (ICRA), pages 15571564.
document7139396.
Andrea Bajcsy, Dylan P. Losey, Marcia K. OMalley, and
Anca D. Dragan. Learning from Physical Human Correc-
International Conference on Human-Robot Interaction
(HRI), pages 141149. URL
document9473611?arnumber9473611.
Connor Basich, Justin Svegliato, Kyle Hollins Wray,
Stefan Witwicki, Joy-Deep Biswas, and Shlomo Zilber-
Learning to optimize autonomy in competence-
aware systems.
In Proceedings of the International
Joint Conference on Autonomous Agents and Multiagent
ISBN 978-1-4503-7518-4. URL www.ifaamas.org.
Malayandi
Nicholas C Landolfi, Gleb Shevchuk, and Dorsa Sadigh.
Learning reward functions from diverse sources of human
erences. The International Journal of Robotics Research,
Erdem Biyik, Nima Anari, and Dorsa Sadigh.
active learning of reward functions from human prefer-
ences. ACM Transactions on Human-Robot Interaction,
Sonia Chernova and Manuela Veloso.
Confidence-
based policy learning from demonstration using Gaussian
mixture models. 5:13151322. doi: 10.11451329125.
Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers,
Rodrigo de Lazcano, Lucas Willems, Salem Lahlou,
Suman Pal, Pablo Samuel Castro, and Jordan Terry.
Minigrid  miniworld: Modular  customizable rein-
forcement learning environments for goal-oriented tasks.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
Shane Legg, and Dario Amodei.
Deep reinforcement
learning from human preferences. Advances in neural
information processing systems, 30, 2017.
Finale Doshi-Velez and George Konidaris.
parameter markov decision processes: A semiparamet-
ric regression approach for discovering latent task
parametrizations. In IJCAI: proceedings of the confer-
Tesca Fitzgerald, Pallavi Koppol, Patrick Callaghan, Rus-
sell Q Wong, Reid Simmons, Oliver Kroemer, and Henny
aware Informative REasoning.
Gurobi Optimization, LLC. Gurobi Optimizer Reference
Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel,
and Stuart Russell. Cooperative Inverse Reinforcement
Learning. URL
Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea
Contrastive prefence learning: Learning from human
feedback without rl. arXiv preprint arXiv:2310.13639,
Minyoung Hwang, Luca Weihs, Chanwoo Park, Kimin
able behaviors: Personalizing multi-objective rewards
from human preferences.
In Proceedings of the
IEEECVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1621616226, June 2024.
Toshiyuki Inagaki. Adaptive Automation: Sharing and
Trading of Control.
jsmetld.2001.10.79.
Kamal Jain and Vijay V Vazirani.
Primal-dual ap-
proximation algorithms for metric facility location and
k-median problems.
In 40th annual symposium on
foundations of computer science (Cat. No. 99CB37039),
pages 213. IEEE, 1999.
Hong Jun Jeon, Smitha Milli, and Anca Dragan.
Reward-rational
(implicit)
unifying
formalism
learning.
Advances
Neural Information Processing Systems, volume 33,
2f10c1578a0706e06b6d7db6f0b4a6af-Abstract.html.
Mingxi Jia, Haojie Huang, Zhewen Zhang, Chenghao
Robin Walters, Robert Platt, and Stefanie Tellex. Open-
vocabulary pick and place via patch-level semantic maps.
arXiv preprint arXiv:2406.15677, 2024.
Tae Kyun Kim. T test as a parametric statistic. Korean
journal of anesthesiology, 68(6):540546, 2015.
Pallavi Koppol, Henny Admoni, and Reid G Simmons.
Interaction considerations in learning from humans. In
Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao
in-the-Loop Autonomy and Learning During Deploy-
ment. URL
Shaunak A. Mehta and Dylan P. Losey. Unified Learn-
ing from Demonstrations, Corrections, and Preferences
during Physical Human-Robot Interaction, January 2024.
URL  arXiv:2207.03395
Yannick Metz, David Lindner, Raphael Baur, and Men-
natallah El-Assady.
Mapping out the space of hu-
man feedback for reinforcement learning: A conceptual
Austin Narcomey, Nathan Tsoi, Ruta Desai, and Marynel
Vazquez.
Learning human preferences over robot be-
havior as soft planning constraints.
arXiv preprint
Heramb Nemlekar, Robert Ramirez Sanchez, and Dy-
lan P. Losey.
through a learned canonical space, 2024.
URL https:
arxiv.orgabs2407.16081.
Andi Peng, Yuying Sun, Tianmin Shu, and David
Abel. Pragmatic feature preferences: Learning reward-
relevant preferences from human input. arXiv preprint
Mattia Racca, Antti Oulasvirta, and Ville Kyrki. Teacher-
aware active robot learning.
In 2019 14th ACMIEEE
International Conference on Human-Robot Interaction
(HRI), pages 335343. IEEE, 2019.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is
secretly a reward model. Advances in Neural Information
Processing Systems, 36, 2024.
Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit
Active Preference-Based Learning of Reward
Functions. In Robotics: Science and Systems XIII. ISBN
Christopher J Shannon, David C Horney, Kimberly F
using flexible human performance models: An initial
pilot study. Advances in Human Factors in Robots and
Unmanned Systems, page 211, 2017.
Lars St, Svante Wold, et al. Analysis of variance (anova).
Chemometrics and intelligent laboratory systems, 6(4):
Linda van der Spaa, Jens Kober, and Michael Gienger. Si-
multaneously learning intentions and preferences during
physical human-robot cooperation. Autonomous Robots,
Shivam Vats, Oliver Kroemer, and Maxim Likhachev.
Synergistic scheduling of learning and allocation of tasks
in human-robot teams. In 2022 International Conference
on Robotics and Automation (ICRA), pages 27892795.
Weisstein.
Bonferroni
correction.
wolfram. com, 2004.
David P Williamson and David B Shmoys. The design of
approximation algorithms. Cambridge university press,
Michelle Zhao, Reid Simmons, Henny Admoni, Aaditya
imitation learning: Handling expert shift and intermittent
Michelle D Zhao, Reid Simmons, and Henny Admoni.
Learning human contribution preferences in collaborative
human-robot tasks.
In Jie Tan, Marc Toussaint, and
Kourosh Darvish, editors, Proceedings of The 7th Confer-
ence on Robot Learning, volume 229 of Proceedings of
Machine Learning Research, pages 35973618. PMLR,
v229zhao23b.html.
Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto
Abhishek
Yifeng Zhu, and Kevin Lin.
simulation framework and benchmark for robot learning.
In arXiv preprint arXiv:2009.
