=== PDF文件: Uni-NaVid A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks.pdf ===
=== 时间: 2025-07-22 09:42:37.835091 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Model for Unifying Embodied Navigation Tasks
Jiazhao Zhang1,2
Kunyu Wang3
Shaoan Wang1,2
Minghan Li2
Haoran Liu1,2
Songlin Wei1,2
Zhongyuan Wang3
Zhizheng Zhang2,3,
He Wang1,2,3,
3Beijing Academy of Artificial Intelligence
Language Instruction
Egocentric Video
Vision-and-language Navigation
doors on the left.
Next Actions: <Forward>...
Object Goal Navigation
stop by it.
Next Actions: <Left>...
Embodied Question Answering
bed located in?
Next Actions: <Right>...
Human Following
Next Actions: <Stop>...
Zero-shot deployment
Training Data
Vision-and-language Navigation
doors on the left.
Next Actions: <Forward>...
Vision-and-language Navigation
doors on the left.
Next Actions: <Forward>...
Object Goal Navigation
stop by it.
Next Actions: <Left>...
Object Goal Navigation
stop by it.
Next Actions: <Left>...
Embodied Question Answering
bed located in?
Next Actions: <Right>...
Embodied Question Answering
bed located in?
Next Actions: <Right>...
Human Following
Next Actions: <Stop>...
Human Following
Next Actions: <Stop>...
Move to the man on the right side. Then follow that man until you see a sofa. Turn right and search for
a TV, stop by the TV. Finally, what is the color of the sofa? Uni-NaVid: Beige
Fig. 1: Uni-NaVid learns general navigation skills across four embodied navigation tasks using 3.6 million navigation samples.
Uni-NaVid only takes online RGB video frames and language instructions as input and output actions, achieving general
navigation ability in a real-world deployment.
AbstractEmbodied Navigation is a fundamental capability for
intelligent robots, requiring robots to follow human commands
and move autonomously within physical environments. Despite
significant advancements, most existing navigation approaches are
tailored to specific navigation tasks, such as instruction following,
searching objects, answering questions, tracking people, and
more. However, the increasing demands on advanced embodied
navigation pose the challenge of designing a practical navigation
agent that can incorporate multiple navigation tasks naturally
and benefits from the synergy between these tasks. To this end, we
present Uni-NaVid, a video-based vision-language-action (VLA)
model to unify different paradigms of navigation tasks and
improve navigation performance by encouraging the synergy
among different navigation sub-tasks. This VLA model can directly
take natural language instructions and RGB video streams as
inputs and output low-level robotic actions in an end-to-end
manner. To efficiently process extensive RGB video streams,
we propose an online token merge strategy that spatially and
temporally consolidates similar visual information which improves
the inference speed to 5 Hz. For training Uni-NaVid, we collect
3.6 million navigation data samples across different navigation
tasks. Extensive experiments on diverse navigation benchmarks
demonstrate that Uni-NaVid achieves state-of-the-art performance
indicates corresponding authors. Contact authors at (zhngjizhgmail.com,
zhangzzgalbot.com, hewangpku.edu.cn).
within a unified framework by using only ego-centric RGB
video as inputs. Additionally, real-world experiments confirm
the models effectiveness and efficiency, shedding light on its
strong generalizability.
I. INTRODUCTION
Embodied navigation [101, 79] is a critical capability for
intelligent robots and has drawn significant attention in the
robotics community. For successful embodied navigation,
robots must be able to move autonomously within physical
environments based on human instructions. However, nav-
igation tasks vary significantly, and most existing studies
are designed for specific tasks, e.g., vision-and-language
navigation [42, 44], object goal navigation , embodied
question answering [21, 84], and following [102, 34, 65].
only one type of navigation task, often relying on specialized
modules and task-specific datasets. This narrow scope limits
their applicability to multi-purpose navigation applications and
prevents these methods from leveraging potential synergies
across diverse navigation tasks.
Developing a versatile navigation model presents significant
modeling and the integration of heterogeneous data for joint
use. Initial efforts adopt imitation learning (IL) [79, 87, 63] or
reinforcement learning (RL) [97, 90] to learn general navigation
skills in simulation environments or limited diverse real-world
environments. However, due to the limited rendering quality
and diversity of simulators, these approaches often encounter
the sim-to-real gap and suffer from poor generalization across
diverse navigation tasks [27, 5, 38]. Recent studies [108, 103,
unification using pre-trained large language models (LLMs).
simplify the problem to some extent by adopting discretized
modeling approaches. They rely on pre-defined graphs for
decision-making learning, which sacrifices output flexibility
and introduces additional challenges for real-world deployment.
In this work, we propose Uni-NaVid, a video-based Vision-
Language-Action (VLA) model for unifying diverse commonly
demanded navigation tasks (Tab. I). Uni-NaVid takes egocen-
tric RGB video streams and natural language instructions as
in continuous environments. To achieve multi-task navigation
while supporting efficient navigation, Uni-NaVid extend video-
based VLM  by incoprating two key components: (1)
an efficient VLA architecture based on an online token
merge mechanism, which enables efficient processing of
online-captured video streams for LLM inference; and (2)
an extensive collection of 3.6M samples across four widely
studied navigation tasks. We provide a detailed elaboration
During navigation, the agent is required to process a
substantial volume of online captured frames, which results
in memory overload and computational latency, particularly in
LLM-based approaches [100, 58]. To this end, we propose an
online token merging mechanism to compress near historical
frames with a relatively low ratio while compressing far
historical frames with a relatively high ratio. This merging
mechanism operates in an on-the-fly manner, maximizing
the reuse of previous navigation history. In this way, Uni-
NaVid learn compact representations that maintain not only
fine-grained spatial information but also structured temporal
the token number. Besides, Uni-NaVid adopts a foresight
prediction to generate actions for a future horizon at once
instead of step-by-step. This enables Uni-NaVid to achieve
5Hz inference, facilitating the deployment of a non-blocking
navigation robot powered by a VLA model in real-world
environments (Please refer to the supplementary video).
We aim to build Uni-NaVid as a versatile multi-task navi-
gation agent, incorporating four widely demanded navigation
embodied question answering, and human following. These
tasks are distinct from each other, with varying task settings
and objectives. Specifically, for the human-following task, we
construct a new language-guided human-following benchmark
for data collection and evaluation. Finally, we collect 3.6M
navigation samples based on diverse navigation tasks with
Embodied Navigation Tasks
D.E. C.E. VLN  ObjNav  EQA  Follow
InstructNav
Poliformer
Uni-NaVid
TABLE I: Task and setting comparison. Uni-NaVid is de-
veloped to address four embodied navigation tasks, generating
action outputs in continuous environments. C.E.: Continuous
Environment; D.E.: Discrete Environment.
different simulation environments. Additionally, inspired by the
success of manipulation VLAs , we further integrate 2.3M
real-world internet data samples for Video Question Answering
(VQA) [7, 48] and video captioning  as auxiliary tasks. This
integration aims to enhance scene understanding and promote
sim-to-real generalization.
We conduct extensive experiments on benchmarks across
the aforementioned four navigation tasks and compared our
method with strong baselines specifically designed for each
task. Utilizing only RGB video streams and instructions as
VLA model across diverse benchmarks, achieving SOTA or
SOTA-comparable performance. Furthermore, comprehensive
ablation studies validate the synergistic benefits of learning
multiple navigation tasks jointly. Finally, real-world exper-
iments demonstrate that Uni-NaVid achieves non-blocking
navigation exhibiting impressive robustness in handling diverse
instructions and environments. We believe our work serves
merely as a starting point for general-purpose navigation.
II. RELATED WORKS
Multi-Task Embodied Navigation. Embodied navigation [2,
based on human instructions. There is extensive literature on
embodied navigation; here, we focus on four mainstream tasks
that involve both visual information and language instructions:
Vision-and-Language Navigation [4, 42, 44], Object Goal Nav-
igation [12, 40], Embodied Question Answering , and Hu-
man Following [35, 65, 106, 107]. Early efforts [79, 87, 63, 90]
towards a generalist-embodied navigation model involved multi-
task navigation datasets and directly learning navigation skills,
showing initial success in multi-task performance. However,
these methods experienced performance drops when deployed
in novel environments, especially in real-world settings. In
recent years, advanced approaches [103, 58, 33, 57, 109, 72]
have leveraged the generalization capabilities of large language
models to improve multi-task navigation. These models show
promising generalizability across navigation tasks but rely on
extensive prompting, which impacts time efficiency. In contrast,
our video-based large language model is trained end-to-end
for multi-task navigation, offering robust generalization and
computational efficiency for tasks like human following.
Embodied Navigation Datasets. To train and evaluate the
performance of a policy for embodied navigation tasks, a
Grid pooling
Grid pooling
History Video
Current Observation
Language
Short-Term Memory
Online Visual Token Merging
Long-Term Memory
Large Language Model
Language Token
Image T
