=== PDF文件: Interruption Handling for Conversational Robots.pdf ===
=== 时间: 2025-07-22 15:57:58.038065 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Interruption Handling for Conversational Robots
Shiye Cao1, Jiwon Moon1, Amama Mahmood1, Victor Nikhil Antony1,
Ziang Xiao1, Anqi Liu1, and Chien-Ming Huang1
Equal Contribution 1 Johns Hopkins University, Baltimore, Maryland, 21218, USA
{scao14, jmoon47, amama.mahmood, vantony1, ziang.xiao, aliu.cs, chienming.huang}jhu.edu
Cooperative agreement
How about a ashlight next?
It provides 24-hour signaling capability
and can be used for various purposes.
Participant clicked to add overcoat to the list
it might be challenging to use
effectively.
[I still think a]
navigate at night. It requires some
knowledge of constellations...
Cooperative clarication
Disruptive interruption
can also be used as a signaling item,
since the sound is very large, and I
think it'll do way better than the
cosmetic mirror.
taking a break and shifting our focus
can do wonders for our mood...
We present an interruption handling system that classies user-initiated interruptions into 1) cooperative agreement, 2) cooperative assistance, 3)
cooperative clarication, and 4) disruptive interruption and adapts different strategies to handle the interruptions accordingly. This gure illustrates how the
system responded to three user-initiated interruptions using different handling strategies (see Fig. 3 for details). We highlight overlapping speech in blue. R
denotes robot and P denotes participant. Participants consent was obtained for images in this publication.
involved. Despite advancements in robotic systems, state-of-the-
art systems still have limited capabilities in handling user-
initiated interruptions in real-time. Prior research has primarily
focused on post hoc analysis of interruptions. To address this
and manages them in real-time based on the interrupters intent
(i.e., cooperative agreement, cooperative assistance, cooperative
signed based on interaction patterns identied from human-
human interaction data. We integrated our system into an LLM-
powered social robot and validated its effectiveness through a
timed decision-making task and a contentious discussion task
with 21 participants. Our system successfully handled 93.69
(n104111) of user-initiated interruptions. We discuss our learn-
ings and their implications for designing interruption-handling
behaviors in conversational robots.
I. INTRODUCTION
Robots are envisioned to become assistants, teammates, and
companions in peoples everyday lives, capable of taking on
complex social and physical tasks. To achieve effective and
intuitive human-robot interaction (HRI), it is crucial that robots
possess natural conversation capabilities .
Interruptionsoccurring when a listener attempts to take
the oor before the speakers utterance is completeare a fun-
damental aspect of human communication . Interruptions
can be used to signal understanding (cooperative agreement),
assist the speaker (cooperative assistance), seek clarication
when needed (cooperative clarication), or express disagree-
ment and shift the topic (disruptive) , leading to more
uid and fast-paced conversations when effectively used and
handled. However, interruptions can disrupt the ow of the
and the interrupter feeling excluded if addressed inadequately.
and manage interruptions on-the-y .
Humans are naturally adept at handling interruptions, but
state-of-the-art conversational robots, which have only recently
shifted from scripted to more natural interactions, still lack
the ability to handle user-initiated interruptions effectively.
Existing spoken dialogue systems mostly rely on explicit turn-
taking signaling (e.g., push-to-talk mechanism  and wake-
words ) to detect user-initiated interruptions. Moreover,
existing systems are designed to always yield the speaker
turn immediately after an interruption is detected . Previous
work emphasizes the importance of classifying the intent of
the interrupter to enable the system to respond accordingly to
the interruption to allow more natural and uid conversations
[24, 19]. Towards this goal, studies have explored the use of
multi-modal inputs (i.e., , acoustic features, facial expressions,
and head movements) to classify interruptions as either co-
operative or disruptive during post hoc analysis of human-
human conversation data . However, to the best of our
classication into its interruption-handling framework, and
little work has explored how conversational robots should
manage different types of interruptions. When should robots
yield to users? When should robots hold their turn?
As a rst step towards answering these questions, in this
interaction patterns identied from human-human interaction
data. We integrated our interruption handling system into a
social robot and showed that our system can successfully
handle 93.69 of user-initiated interruptions (e.g., Fig. 1) in
timed decision-making and contentious discussion tasks.
II. RELATED WORK
A. Interruptions in Human-Human Interaction
Interruptions occur when a listener tries to take the oor
before the speakers utterance is complete . They are a com-
mon aspect of human conversations and can occur more than
once per minute in dyadic  and group conversations .
While most interruptions involve overlapping speech, brief
overlapping speech that does not purloin the speakers oor
such as during speaker exchanges  or verbal backchannels
(conversation continuers signaling attention, understanding,
and agreement, such as uh-huh or yeah) is not
considered an interruption. Interruptions can be categorized
as cooperative or disruptive based on the intention of the
interrupter . Cooperative interruptions are intended to aid
the completion of the current turn by expressing concurrence
or understanding (cooperative agreement), providing a word
or idea that the speaker may need (cooperative assistance), or
asking the speaker to clarify or elaborate on previous informa-
tion (cooperative clarication) . Disruptive interruptions
occur when the listener challenges the speakers control and
disrupts the conversational ow to express an opposing opinion
(disagreement), further develop the current topic (oor taking),
change the subject (topic change), or summarize the speakers
point to end the turn and avoid unwanted information (tangen-
tialization) . Prior work found that the type and frequency
of interruptions are shaped by the conversational and power
dynamics of interaction . However, limited work inves-
tigated how humans handle interruptions in conversations.
with varying power dynamics between speakers to understand
how people handle different types of interruptions.
B. Interruptions in Human-Agent Interaction
Existing conversational robots have very limited capabilities
in handling interruptions. Many systems ignore overlapping
speech altogether (e.g., ), while others treat any overlapping
speech as interruption (e.g., ) or rely on designed explicit
turn-taking signaling (e.g., user touches the robots head )
to detect user-initiated interruptions. Popular commercial voice
assistants take a step further by relying on wakewords (e.g.,
Alexa) to detect user-initiated interruptions . However,
these approaches remain error-prone due to their rigidity (may
mistake ambient microphone noise as interruption  and rely
on users to remember the wakeword ), and are unnatural
in human conversations. Recent advances, such as Alexas
follow-up mode and Siris ability to handle back-to-back
the uidity of the conversation. However, these systems still
struggle with managing complex, multi-turn interruptions ,
yielding the oor to all interruptions equally without assessing
intent. Prior work in human-agent interaction recommended
distinguishing between cooperative and disruptive interrup-
tions to help determine how the agent should deal with the
interruption . Multi-modal features (i.e., acoustic proles,
head activity, gaze behavior, lexical features, and facial expres-
sions) have been used to automatically classify interruptions as
cooperative or competitive . However, prior efforts for han-
dling these cooperative and disruptive interruptions have been
limited to post hoc evaluation of a dataset ; to the best of
our knowledge, no existing conversational agent has the ability
to handle user-initiated interruptions based on the context of
the interruption in real-time. In this work, we leverage natural
language understanding capabilities of large language models
(LLMs) to classify and handle user-initiated interruptions in
real-time. By categorizing interruptions into four ner-grained
categoriescooperative agreement, assistance, clarication,
and disruptiveour system tailors handling strategies to match
the context and user intention behind the interruption.
III. INTERRUPTION HANDLING IN HUMAN INTERACTION
In HRI, robot behaviors are often designed based on human
behavior patterns observed in human-human interaction (e.g.,
[2, 11]), as this can make robot behavior feel more natural and
align better with user expectations. However, there is limited
prior research on interruption handling in both human-robot
and human-human interaction, providing little guidance for
designing conversational robots. To address this, we analyzed
natural human conversations to identify human behavior pat-
terns when handling interruptions.
A. Human Conversational Data
As the frequency and type of interruption correlate with
power dynamics in conversations , we selected the top ve
YouTube videos for three categories of conversations with
varying power dynamics, ltered by view count and duration
(420 minutes), resulting in 246 interruptions.
Discussions (nD
106 interruptions) represent casual
settings with no clear power dynamics. We analyzed two
dyadic conversations and three multi-party discussions.
Talk show interviews (nT  122 interruptions) represent
casual settings with moderate power dynamics, exemplied
by clips from The Tonight Show Starring Jimmy Fallon.
While the interviewer can control the conversation, they
typically refrain due to the low-stakes nature.
Press briengs (nP  18 interruptions) featuring former
U.S. White House press secretary Jen Psaki are high-
stakes settings where the speaker holds signicant power,
controlling the narrative.
Handling Strategies
Interruption
Cooperative
agreement
Hold floor
Cooperative
assistance
Cooperative
clarification
Disruptive
interruption
Interaction patterns of interruption handling in human conversations.
The highlighted strategies were implemented in our system (Fig. 3). Ack
denotes acknowledge and Cont. denotes continue.
B. Analysis
Informed by the taxonomy of interruption types from prior
work and research highlighting the need to tailor interruption
handling to the intention (type) of the interruption (detailed
in Section II-A), we coded the videos for 1) intention of the
interruptereither cooperative (i.e., agreement, assistance, or
clarication) or disruptiveand 2) the strategy individuals use
to manage interruptionsyield or hold the oor. Our goal
was to identify interaction patterns that reveal how humans
handle different types of interruptions. Interaction patterns
were created based on the sequence of these two states.
One coder transcribed and coded the interruptions, while a
secondary coder independently coded 10 to assess inter-
coder reliability (Cohens Kappa  1).
C. Interaction Patterns
We identied the following interaction patterns (Fig. 2):
1) Cooperative: agreement hold oor: When speakers
encountered cooperative agreement, they typically acknowl-
edged the interruption and continued speaking (nD  26,
nT  48, nP  2). A deviation from this pattern was noted
in only one case, where the speaker yielded the oor to the
interrupter upon agreement during a discussion (nD  1).
2) Cooperative: assistance hold oor: When the inter-
rupter offered assistance, the speaker typically acknowledged
words), then resumed speaking (nD  7, nT
deviation occurred a few times, where the speaker yielded the
oor to the interrupter (nD  4, nT  2).
3) Cooperative: clarication hold oor or yield: When
the interrupter sought clarication, the speaker typically ac-
knowledged the question, paused for the interrupter to nish,
nT  8, nP  1). However, in contentious discussions, the
speaker sometimes yielded the oor (nD  3, nP  1), show-
ing that clarifying questions can be used to assert alternative
points and take control of the oor from the speaker.
4) Disruptivehold oor or yield: Disruptive interrup-
tions were the most common (130 of 246) and showed
different patterns across tasks. Speakers either held the oor
(nD  19, nT  37, nP  7) or yielded it (nD  41,
7). In debates, speakers often yielded
due to the polarized nature of the discussions, while in less
polarized settings (talk shows) they typically held the oor,
where disruptive interruptions can be more acceptable.
We identied three strategies speakers used to hold the
acknowledge the interruption while implicitly signaling intent
to retain oor by repeating a few words spoken at the time
of interruption and continue speaking, and 3) explicitly state
intent to continue (e.g., Let me nish my thought) and
then continue speaking. The rst two strategies were more
common for mild interruptions later in the speakers turn, after
part of their message had been conveyed. The third strategy
was used by the speaker to defend their turn from more
aggressive disruptive interruptions (e.g., disruptive interruption
early in their turn or repeated interruption attempts). In multi-
party discussions, other participants sometimes intervened to
protect the speakers turn during such aggressive interruptions.
Speaker acknowledgments to interruptions were mostly non-
verbal (i.e., nods and mutual gaze). Verbal acknowledgements
(e.g., sure or yeah) occurred in only 13 cases. Similar to
prior work , speakers averted gaze to hold oor, and held
prolonged mutual gaze until interrupter took over to yield oor.
We used these interaction patterns to guide the design of
our interruption handling system for conversational robots.
IV. INTERRUPTION HANDLING SYSTEM FOR
CONVERSATIONAL ROBOTS
Our interruption handling system has three main modules:
interruption detection, intent classication, and interruption
handling. Fig. 3 illustrates how user input ows through
it. We provide an example implementation of our system
integrated into an LLM-powered social robot built on Platform
for Situation Intelligence (psi) 1.
A. User-Initiated Interruptions Detection
We detect interruptions by monitoring for simultaneous
speech. As overlapping speech towards the end of turn is
common during speaker oor exchanges and is not considered
an interruption . So, if at the time of the overlapping
speech left, the system simply ignores it and nish-up (Line
4 ). Verbal backchannels are also not considered interruptions;
interruption (e.g., No) requires identifying the intent of the
1 Supplemental materials (contains additional implementation details):
Is robot
talking?
talking?
Is robot
left < 2 s
classification
immediately
finish-up
Generate
behavior
behavior
Generate
behavior
Generate
behavior
since start
of robot turn
1 or 2 words
Disruptive
interruption
yield immediately
Generate
behavior
ack and wrap-up
Summarize
behavior
Cooperative
agreement
continue
behavior
ack and continue
behavior
Cooperative
assistance
ack and continue
behavior
Cooperative
clarification
to clarifying
question
behavior
clarify and continue
Metro-map-inspired diagram of the interruption handling system, illustrating how user speech ows through the interruption detection, intent
overlapping speech between the user and the robot. We use Line <color> to refer to different interruption handling paths in the gure. Ack denotes
acknowledge and Cont. denotes continue.
dictability in human-agent interactions than human-human
triggered interruptions. When wakeword (i.e., the robots
name) or stop is heard as overlapping speech, the system
always yield immediately (Line 4 ).
B. Interrupter Intention Classication
In human communication, the speakers choice of inter-
ruption handling strategy depends on the intention of the
interrupter. So, we prompt-engineered a large language model
(GPT-4o-mini) to classify the intention of the interrupter into
cooperative agreement (includes backchannels), cooperative
given the conversational history and the amount of time
elapsed in the turn since the robot started talking.
Both backchannels and cooperative agreements are used to
support the speaker by expressing attention, understanding,
and agreement. In this work, we differentiate them by overlap
C. Interruption Handling
Following the interaction patterns identied in Section II-B,
the system adopts different interruption handling strategies
based on the predicted intention of the interrupter:
Cooperative Agreement: If the cooperative agreement is
only one or two words, it is likely a verbal backchannel,
which the system disregards it and continues with the
remaining planned content from the last punctuation mark
(continue, Line 4 ). If the cooperative agreement contains
more than two words, the system acknowledges verbally
(with ya, yes, uhhum, or sure), nods, and then
continues with the remaining planned content (ack and
Cooperative Assistance: The system acknowledges ver-
bally (with yeah, yes, or thanks), nods, and continues
with the remaining planned content from the last punctuation
mark. (ack and continue, Line 4 ).
Cooperative Clarication: Cooperative clarications are
handled by an LLM prompted to address the clarication
requested and then continue with the remaining previously
planned content (clarify and continue, Line 4 ).
Disruptive Interruption: Disruptive interruptions occurring
within 5 seconds of the robot starting to speak are consid-
ered aggressive disruptive interruptions by our system. In
such cases, an LLM is prompted to generate behavior for
the robot to express its intent to maintain the turn (e.g., Let
me nish my thought in Fig. 1), provide a summary of the
remaining content, and then yield (ack and wrap-up, Line
4 ). For milder disruptive interruptions, the system yields
the oor immediately and a new robot response is generated
based on the content of the interruption (yield immediately,
Line 4 ).
V. EVALUATION STUDY: METHODOLOGY
We conducted a user study to validate our system by inte-
grating the interruption handling system into an LLM-powered
social robot. We implemented basic social robot interaction
behavior by hand-crafting a bank of facial expressions (happy,
positions (left gaze, right gaze, look at screen, left nod, right
LLM (GPT-4o-2024-05-13) to generate contextualized robot
speech and select tting facial expressions, head movements,
and task actions (additional details provided in supplemental
materials1 Section II).
A. Inducing User-Initiated Interruptions
To encourage user-initiated interruptions, we made the fol-
lowing design choices in our implementation:
1) Task: We contextualized our system in a timed decision-
making and a contentious discussion task, anticipating
that time pressure and controversy can encourage inter-
ruptions. In the decision-making task, participants had ve
minutes to select seven out of fteen items to help them
survive in a desert survival simulation. The robot was
instructed to persuade, rather than simply agree, with users.
A countdown timer was shown to add time pressure. In the
discussion task, participants prepared for a 2-minute pre-
sentation on whether the federal government should abolish
capital punishment. The robot remained neutral until the
participant clearly stated their position, at which point it
adopted an opposing stance to encourage consideration of
alternative perspective.
2) Robot persona: We designed the robot to have a feminine
name (Luna), a feminine voice (Google en-US-Standard-
H), gaze aversion while it is talking, and generally positive
facial expressions (see supplemental materials1 Fig. 1).
Prior research highlights that gender signicantly affects
interruption patterns; women are more likely to be inter-
rupted than men, particularly when they avoid direct eye
contact with their conversation partners . Additionally,
women tend to smile more during conversations, which has
been linked to a higher likelihood of being interrupted .
3) Pre-programmed events: We designed two pre-programmed
events 1 per task where the robot holds the oor for roughly
one minute to provoke user-initiated interruptions. In event
introduces itself and provides disclaimers. In event two,
triggered 3.5 minutes into the discussion, the robot tells an
unrelated fun story or facts.
B. Study Procedure
We set up two cameras to record the participants interac-
tions with the robot from both the front-view and back-view
during the study (see Fig. 4). After providing consent, par-
ticipants completed a demographic questionnaire and watched
a 1-minute sample interaction video. They then engaged in
a practice task, designed to familiarize them with the robot,
answering four factual questions about space exploration.
Following the practice task, participants completed two tasks
(in random order) and completed a post-task questionnaire
after each task. The study concluded with a semi-structured
interview aimed to understand their overall experience working
with the robot and to collect feedback on robot interruption
handling.
C. Participants
We recruited 21 participants (11 female, 10 male), aged
18 to 30 (M22.38, SD3.58), through convenience sam-
pling from the local community via electronic newsletters
and mailing lists. Most participants had limited experience
using speech-based AI technology (M2.81, SD1.25, Min1,
5 being extensive experience) and reported that they have
only used it for simple commands, i.e., setting alarms and
checking the weather. The study took roughly 45 minutes, and
participants were compensated at 15.00 per hour. The study
was approved by our institutional review board (IRB).
D. Metrics
We constructed two subjective scales (see supplemental ma-
terials1 Section III for details) from the post-task questionnaire
to explore how interruption handling might affect the users:
Perceived inclusion in discussion (rating scale 15): Three-
item scale (Cronbachs   .78) measuring how heard,
Overall satisfaction with discussion (rating scale 15): Nine-
item scale (Cronbachs   .77) assessing users enjoyment,
willingness to engage in future discussions.
E. Coding
One coder transcribed all interactions and extracted con-
versation snippets with overlapping speech. For ground truth
and ne-grained understanding of interruptions, two coders
independently analyzed 100 of the interruptions, classifying
them by type (disruptive, cooperative agreement, assistance,
or clarication) and by attributes such as statement (opinion
or non-opinion), question (opinion or factual), or verbal back-
fully addressed, meaning that the robot effectively responded
to the interrupter without causing a conversation breakdown.
Cohens kappa was 0.92, and coders resolved conicts through
discussion.
VI. EVALUATION STUDY: FINDINGS
We identied 121 user-initiated interruptions from 206.59
minutes of interaction data (e.g., Fig. 4) involving 525 user
query-robot response pairs, or turns (decision-making: 312
some people argue that knowing
the perpetrator has been executed
can bring a sense of finality and
justice. They believe it can help
families move forward by ensuring
the criminal cannot harm anyone
[else. However, I]
that not]
give me] (1.5)
data? Like (.) has it actually
has it [worked as a deterrent?]
Repeated I
Participant interrupted.
System classified user speech
as cooperative agreement.
Robot stopped briefly and
continued. Line
Participant interrupted.
System classified user speech
as cooperative clarification.
Robot stopped and answered
the question. Line
the data on whether capital punishment
acts as a deterrent is mixed. Some
studies suggest... Overall, the evidence
is inconclusive...
Speech overlapped.
Not a user-initiated
interruption.
Robot generated new
behavior (response). Line
Cut off planned speech
Because of the 1.5 s delay in
participant speech, this was
treated as a new question.
Example conversation where the system handles various types of compounded interruptions. Overlap 3 is an example of a robot-initiated interruption.
We used the Jeffersonian transcription system . R denotes robot and P denotes participant.
(n  86) had more than twice as many user-initiated interrup-
tions as the contentious discussion task (n  35).
TYPES OF INTERRUPTIONS OBSERVED AND EXAMPLES.
Speech act
Attribute Example
Cooperative agreement (n  18)
Verbal back-channel
Statement
Thats a good idea
gestions
Cooperative assistance (n  2)
Statement
Another thing that I was
thinking was jack knife.
Cooperative clarication (n  9)
Question
and the parachute?
What percent?
Disruptive interruption (n  92)
Question
about adding the pistol to the
How many states have capital
punishment?
Statement
I do not think that the cos-
metic mirror is necessary. I
think we should change it into
45 caliber pistol
A. The majority of interruption attempts were disruptive
Based on the interruption intent coded by the research team,
the most common type of interruption was disruptive (n  92,
76.03), followed by cooperative agreement (n  18), coop-
erative clarication (n  9), and rarely, cooperative assistance
(n  2). Participants attempted disruptive interruptions in
various ways, including asking opinion (n  42 out of 92,
45.65) and factual (n  16) questions or issuing opinion
(n  7) and non-opinion (n  27) statements. See Table I for
more details and example interruptions.
B. 88.78 of interruption intent was correctly classied
Five interruptions triggered pre-programmed events (see
description in Sec. 3), bypassing the interruption handling
system. During a 35-second time frame in one study, there
was an abnormally large delay in the speech-to-text API, likely
caused by a network error, causing the system to be unable
to detect user speech. Hence, our system did not handle the
ve interruptions (all disruptive) during this time. We exclude
these ten cases from the rest of our analysis.
Amongst the interruptions handled by the system (n  111),
participants used wakeword(s) in 47 interruptions (decision-
to immediately yield when a wakeword is heard during
overlapping speech (n  43). It treats all interruption attempts
containing wakeword(s) as strong disruptive interruptions
without passing the intent classier module (SR2, Line 4 in
Fig. 3). Due to speech recognition errors, wakewords were
not heard in 4 out of 47 cases. Additionally, interruptions
made when less than two seconds of robot-planned speech
remained (n  13) also bypassed the intent classier module
(SR3, Line 4 in Fig. 3). Of the 55 interruption attempts that
passed through the intent classication module2, the module
correctly classied the intent of 44 interruptions (80.00).
2111 total handled interruptions minus 43 with wakeword(s) detected, and
13 occurring near the end of robot-planned speech.
(n  43), our system incorrectly classied the intent in 11 out
of 98 (11.22) interruptions, meaning 88.78 were correctly
classied.
C. 93.69 of the interruptions were successfully handled
Not all interruption intent classication errors resulted in un-
successful handling, nor were all correctly classied interrup-
tions handled successfully. Of the 111 interruptions handled,
107 cases were the users rst interruption attempt during that
Among the seven unsuccessful initial attempts, the robot
continued speaking in four cases due to intent classication
attempts were successful. Overall, 104 out of 111 (93.69)
interruptions were successfully handled.
TABLE II
TYPES OF INTERRUPTIONS AND HANDLING STRATEGIES (MISCLASSIFIED
INTERRUPTIONS ARE HIGHLIGHTED).
Interruption
type (coded)
(by model)
Interruption
handling
<2s of plan-
ned speech
Finish-up3
Continue
Agreement
Ack and continue
Agreement
Disruptive
Yield immediately
Assistance
Assistance
Ack and continue
Clarication
Clarify and continue
Clarication
Disruptive
Yield immediately
Yield immediately
Disruptive
Ack and wrap-up
Agreement
Ack and continue
Assistance
Ack and continue
Clarication
Clarify and continue
Disruptive
wakeword
not used
Clarication
LLM error handling4
Yield immediately
wakeword
Agreement
Continue
D. Speech recognition errors caused the majority of interrup-
tion handling failures
To better understand the cause of unsuccessfully handled
interruptions (n  7), we analyze their potential links to intent
classication errors (n  11), as shown in Table II.
Misclassied disruptive interruption as cooperative agree-
ment (n  3). The robot continued speaking due to mis-
classication; two of which contained a wakeword but were
still misclassied due to speech recognition error.
Misclassied disruptive interruption as cooperative assis-
tance (n  1). The participant attempted to interrupt
3Two cooperative agreements, one assistance, one clarication, and nine
disruptive interruptionstwo used wakewords but were not heard by system
due to speech recognition errorwere handled through nish-up.
4LLM failed to generate correctly formatted robot behavior, so the robot
performed default LLM error handling. See supplemental materials 1 Section
II for details.
with The rst aid kit but abandoned the interruption
assistance. Five seconds later, the participant reattempted a
disruptive interruption, and the robot yielded immediately.
Misclassied disruptive interruption as cooperative clar-
ication (n  4). Despite the miss-classication, these
interruptions were considered successfully handled, as the
robot answered the users questions.
Misclassied cooperative agreement as disruptive (n  2).
Wh
