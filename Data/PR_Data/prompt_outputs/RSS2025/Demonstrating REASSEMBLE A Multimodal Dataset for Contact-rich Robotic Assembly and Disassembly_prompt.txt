=== PDF文件: Demonstrating REASSEMBLE A Multimodal Dataset for Contact-rich Robotic Assembly and Disassembly.pdf ===
=== 时间: 2025-07-22 15:46:46.383706 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating REASSEMBLE: A Multimodal
Dataset for Contact-rich Robotic Assembly and
Disassembly
Daniel Sliwowski, Shail Jadav, Sergej Stanovcic, Jedrzej Orbik, Johannes Heidersberger, Dongheui Lee
Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria.
Institute of Robotics and Mechatronics (DLR), German Aerospace Center, Wessling, Germany.
jedrzej.orbik, johannes.heidersberger, dongheui.lee}tuwien.ac.at
Assembly
Disassembly
NIST Task Board 1
Robotic assEmbly
disASSEMBLy datasEt
REASSEMBLE
4551 Contact-rich Task
Demonstration
4035 Successful
516 Failed
External View
Wrist View
Temporal Action
Segmentation
Motion Policy
Learning
SuccessAnomaly
Detection
Multimodal data
Event camera
ForceTorque
3 RGB cameras
3 Microphones
Proprioception
Multiple Task annotations
Motion Policy
Learning
Temporal Action
Segmentation
Success  Anomaly
Detection
Pick Ethernet
Insert Ethernet
Approach
b. Align
Hierachical action
annotations
4551 high level
demonstrations
10195 low level
demonstrations
Fig. 1: Overview of the REASSEMBLE dataset. In REASSEMBLE, we focus on creating a dataset for contact-rich manipulation tasks.
We leverage the well-established NIST Assembly Task Board 1  to facilitate deployment of learned algorithms across different research
institutes. The dataset includes data from various modalities, such as RGB cameras and robot proprioception, which are common in other
works. Additionally, we incorporate event cameras, a force and torque sensor, and microphones, which are less common in manipulation
Segmentation (high-level actions and low-level skills), Motion Policy Learning, and SuccessAnomaly Detection.
AbstractRobotic manipulation remains a core challenge in
assembly and disassembly. Existing datasets have significantly
advanced learning in manipulation but are primarily focused
on simpler tasks like object rearrangement, falling short of
capturing the complexity and physical dynamics involved in
assembly and disassembly. To bridge this gap, we present
REASSEMBLE (Robotic assEmbly disASSEMBLy datasEt), a
new dataset designed specifically for contact-rich manipulation
tasks. Built around the NIST Assembly Task Board 1 bench-
and place) involving 17 objects. The dataset contains 4,551
total of 781 minutes. Our dataset features multi-modal sensor
and multi-view RGB cameras. This diverse dataset supports
research in areas such as learning contact-rich manipulation, task
condition identification, action segmentation, and task inversion
learning. The REASSEMBLE will be a valuable resource for
advancing robotic manipulation in complex, real-world scenarios.
The dataset is publicly available on our project website1.
I. INTRODUCTION
Robot learning, and deep learning more broadly, has made
remarkable progress in recent years, driven by the increasing
availability of high-quality, large-scale datasets . These
datasets have enabled state-of-the-art algorithms to achieve
impressive performance across diverse tasks, including motion
anomaly detection , .
Most previous work in robot learning has focused on rela-
tively simple manipulation tasks, such as object rearrangement,
and limited interaction. In contrast, long-horizon, contact-rich
manipulation tasks, such as object assembly and disassembly,
remain underexplored. These tasks demand a deeper under-
standing of interaction dynamics and the ability to plan and
execute precise, goal-oriented sequences. Recent work
has shown that current algorithms struggle with such tasks,
largely due to the lack of datasets tailored for long-horizon,
contact-rich scenarios. Without suitable data, models fail to
generalize effectively in these challenging settings.
While industrial environments mitigate these challenges
through specialized tooling and structure, robots in homes
or unstructured settings face greater complexity. Tasks often
involve diverse objects, irregular conditions, and unexpected
events. For example, a household robot assembling furniture
must handle uncertainties in alignment, material tolerances,
and force application, without the aid of precise jigs or fixtures.
In such cases, small deviations can easily lead to task failure.
Many approaches tackle these challenges by decomposing
complex actions into simpler skills that can be sequenced to
perform long-horizon tasks , . For instance, fastening
a screw can be broken down into approaching the socket,
aligning the screw, and twisting it until secured. This hier-
archical learning improves generalization, enabling the reuse
of previously learned skills to solve new, unseen tasks .
Although much of the focus has traditionally been on
enabling robots to assemble products, there exists an equally
critical need to develop robots capable of disassembling
objects. Disassembly is a cornerstone of sustainability, par-
ticularly in advancing the circular economy. According to
the United Nations Sustainable Development Goal 12, global
demand for natural resources is projected to triple by 2050,
far exceeding what the planet can sustainably provide under
current consumption patterns. This underscores the urgent
need for innovative solutions to improve resource efficiency
and increase recycling rates. Robots capable of disassem-
bling objects can play a transformative role in this effort.
For instance, they could be deployed to efficiently disas-
semble complex objects such as cars, electronic devices, or
EV batteries, which are currently dismantled manually in
labor-intensive and hazardous processes. Despite the critical
importance of disassembly in recycling, automation in this
sector remains limited. Additionally, this disassembly can
be facilitated through task inversion learning (TIL) using
assembly demonstrations. Developing robust, adaptive agents
capable of safely and efficiently disassembling products could
drastically reduce waste, conserve resources, and mitigate the
environmental impact of human consumption.
To bridge the gap between these pressing challenges, we
introduce REASSEMBLE, a comprehensive dataset tailored
to long-horizon and contact-rich manipulation tasks. While
it draws inspiration from industrial assembly and disassem-
for unstructured and everyday environments. Built upon the
standardized NIST Assembly Task Boards , specifically
Task Board 1, REASSEMBLE provides benchmarks for tasks
such as gear meshing, peg insertions, electrical connector
tasks mimic the intricate manipulation challenges that robots
face in real-world applications. What sets REASSEMBLE
apart from other robot manipulation datasets is its focus on
multimodal data for holistic learning frameworks. We pro-
vide a comparison of commonly used robot learning datasets
and their properties in Table I. The REASSEMBLE dataset
includes multi-view RGB images and robot proprioceptive
poses. Additionally, it incorporates less commonly utilized
sensory modalities, such as audio recordings from external
and wrist-mounted microphones, as well as six-axis force-
torque measurements. Importantly, REASSEMBLE also in-
cludes event camera data. Unlike standard RGB cameras,
event cameras capture changes in light intensity at each pixel
rather than recording entire frames. This low-latency, high-
precision information provides critical insights into object
improving robustness in dynamic environments. Furthermore,
REASSEMBLE distinguishes itself by providing multi-task
as hierarchical temporal action segmentation, motion policy
modal dataset, REASSEMBLE fosters the development of
adaptive and versatile robotic systems capable of tackling
the challenges of long-horizon, contact-rich manipulation. To
1) A multi-modal dataset designed for long-horizon, high-
2) A dataset with multi-task labels to support algorithm
development in various robot learning fields, like hi-
erarchical temporal action segmentation, motion policy
3) Data and annotations supporting both forward and in-
verse manipulation task learning.
4) Introduction of the, to the best of our knowledge, first
manipulation-focused dataset to include event camera
motion information.
II. RELATED WORKS
In robotic manipulation, most simulated environments and
datasets primarily focus on fundamental tasks such as picking,
, , as shown in Table I. While these tasks are essential
for understanding basic robotic capabilities, they are typically
limited to short-horizon primitive skills and fail to provide the
critical data necessary for precise, contact-rich manipulation.
In contrast, complex long-horizon tasks such as assembly
Collection method
1 RGB Camera, Robot
Proprioception
Everyday Robots
VR teleoperation
1 RGB Camera, Robot
Proprioception
Everyday Robots
VR teleoperation
Language Table
1 RGB Camera, Robot
Proprioception
VR teleoperation
BridgeDatav2
4 RGB Cameras, 1 Depth
VR teleoperation
4 RGBD Cameras, Robot
Proprioception
Franka Emika Panda
Reaserach 3
FurnitureBench
2 RGB Cameras, Robot
Proprioception
VR teleoperation
3 RGBD CAmeras, Robot
Proprioception
Franka Emika Panda
Reaserach 3
VR teleoperation
8-10 RGBD Cameras, 1
Multiple
haptic teleoperation
1 RGB Camera, Robot
Proprioception
Haptic Teleoperation
50SALADS
celerometers
Human demonstration
Assembly101
8 RGB Cameras, 4 Mono
Human demonstration
1 RGBD Camera, 1 Mi-
crophone
Scripted
(Im)PerfectPour
2 RGB Cameras
Franka Emika Panda
Reaserach 3
VR teleoperation
REASSEMBLE
3 RGB cameras, Robot
Event camera, 3 Micro-
Franka Emika Panda
Reaserach 3
Haptic teleoperation
Table I: Datasets comparison. We compare several commonly used datasets based on the number of demonstrations, the number of verbs
they contain, the sensors used during data collection, the robotic platform, the data collection method, and the tasks that can be learned from
the dataset. The tasks include TAS (Temporal Action Segmentation), MPL (Motion Policy Learning), AD (Anomaly Detection), and TIL
(Task Inversion Learning). We use "nr" to denote information that is not reported by the works and "NA" for cases where the category is
not applicable.
and disassembly, which require stringent tolerances, demand
accurate and high-resolution contact information, including
force-torque data, that visual sensing alone cannot reliably
capture. For instance, force-torque sensing plays a pivotal role
in assembly tasks involving a NIST assembly board, where
precise measurements are essential to detect and respond to
contact events with higher accuracy.
Existing datasets, such as the Furniture Benchmark ,
have made progress in addressing long-horizon tasks like
furniture assembly. However, they lack the high-quality force-
torque data required for the tight tolerances demanded in these
applications. Similarly, while datasets like RH20T include
manipulation data with audio and force-torque sensor infor-
needed for complex assemblies . Vision-based tactile
contact-rich manipulation, but they too fall short in capturing
the extended temporal dynamics and multi-modal sensing
required for precise assembly and disassembly tasks .
To address these limitations, REASSEMBLE introduces a
novel dataset incorporating high-resolution force-torque sens-
ing specifically tailored for tight-tolerance, high-precision, and
long-horizon tasks.
The increasing prevalence of automation in robotic manip-
ulation tasks highlights the necessity of effective skill assess-
performance and reliability. Temporal action segmentation
(TAS) plays a critical role in achieving these objectives by
identifying and classifying distinct actions within continuous
temporal sequences. Numerous datasets have been developed
to support temporal action segmentation , , . For
where humans perform salad preparation tasks . This
dataset provides temporally labelled actions for long-duration
action segmentation. These models can learn to segment com-
plex temporal sequences into meaningful sub-tasks. However,
such datasets primarily focus on human activity and often
lack relevance to robotic manipulation tasks. Robotics-specific
lored to specific robotic tasks , . While these datasets
are valuable for advancing task-specific learning, they are
not designed for comprehensive temporal action segmentation
across diverse manipulation contexts. Moreover, conventional
action segmentation datasets like 50Salads predominantly rely
on visual and temporal information, often overlooking critical
multimodal data, such as force-torque measurements, which
are essential for understanding robotic actions. Therefore,
REASSEMBLE also addresses robotic action segmentation
and incorporates multimodal data, including visual, force-
Advancements in large vision-language models (LVLMs),
such as PaLM-E, LLaVa, have shown impressive capabilities
in reasoning about task conditions by leveraging visual infor-
mation during robotic task execution , . These models
provide robust insights into system states, enabling robots
to monitor and adapt to dynamic conditions. However, their
significant computational requirements make them impractical
for real-time applications, where lightweight and efficient
models are essential for rapid decision-making. To address
these challenges, ConditionNet was introduced as a focused
resource for condition monitoring in robotic systems .
While ConditionNet dataset ((Im)PerfectPour) is not aimed
at contact-rich manipulation and long-horizon task. Building
on these limitations, REASSEMBLE is designed to address
the gaps in existing resources. It includes conditioned labeled
multi-modal data such as visual, force-torque, and temporal
effectively learn to detect, understand, and respond to failures
in real time.
III. METHODOLOGY
A. REASSEMBLE Tasks
The REASSEMBLE dataset is designed to address long-
horizon and contact-rich robotic manipulation tasks. Instead of
creating custom tasks, we leverage the well-established NIST
Assembly Task Board benchmark , using it as the basis for
our dataset. This benchmark includes four distinct task boards:
(1) peg and connector insertion tasks, (2) manipulation of
flexible belts and chains, and (34) cable routing tasks. For the
REASSEMBLE dataset, we focus on Assembly Task Board
In future expansions we plan to incorporate data from the
remaining boards.
The assembly instructions for these task boards are available
on the official NIST Assembly Task Boards website. For this
enhance compatibility with our experimental setup. Specif-
the Ethernet and USB holders to better accommodate the
plugs used in our experiments, ensuring the plugs remain
within the original tolerances specified by the benchmark. To
facilitate object manipulation, we 3D-printed custom holders
for pegs, Ethernet connectors, and USB plugs, allowing these
connectors to remain vertical when not inserted into the board.
This modification simplifies the picking and insertion tasks,
aligning better with the kinematics of the Franka robot, which
is more suited for top-down operations than lateral manipu-
lations. The 3D models for these modified components are
made publicly available on the project website. Furthermore,
reflective markers were attached to the boards corners to
enable accurate localization using a motion capture system.
Figure 1 illustrates both the assembled and disassembled
configurations of the task boards.
The REASSEMBLE dataset encompasses both assembly
and disassembly processes, with the associated actions defined
as follows: for assembly, the actions are picking and insert-
placing. These four actions represent the minimal set required
to comprehensively describe the assembly and disassembly
tasks. The manual labelling of demonstrations by human
operators necessitated limiting the action set to these four
The assembly board contains a total of 20 objects; however,
due to constraints in the robot and teleoperation setup, the
three nuts were excluded from the dataset (M12, M8, and
M4). As a result, the dataset comprises 17 objects. A detailed
breakdown of the objects included in the dataset is provided
in the Appendix in Figure 15.
B. Sensors
We focus on collecting a comprehensive range of sensory
information during task demonstrations to create a robust
dataset. Consistent with prior works, we capture multi-view
RGB video using two external cameras and one wrist-mounted
camera. The external cameras are HAMA C-600 Pro webcams,
while the wrist-mounted camera is an Intel RealSense D435i.
In addition, we record proprioceptive data from the robot, in-
cluding joint positions, velocities, efforts, end-effector position
and orientation (relative to the robots base frame), and gripper
To capture audio data, we utilize three microphones: two
mounted on the external cameras and one attached to the
robots gripper. The gripper microphone is an OSA K1T
wireless microphone. Interaction forces and torques are mea-
sured using a wrist-mounted 6-axis force-torque (FT) sensor
(AIDIN ROBOTICS AFT200-D80-C), as shown in Figure
2. For accurate FT readings in the robots base frame, we
perform a calibration procedure. This involves sampling 6
unique orientations uniformly distributed over a unit sphere,
moving the robot to these orientations, and recording FT
measurements. Using the sensor pose and these measurements,
we calibrate the grippers mass, center-of-mass pose, and
sensor bias, following the methodology in . The calibration
process is implemented using the force torque tools
ROS package.
In addition to the aforementioned sensors, the dataset in-
cludes an event camera. The sensor is mounted externally on
a tripod in front of the robot. Due to its limited resolution
and field-of-view, it is positioned at a distance that allows it
to capture the majority of the workspace of the robot while
still being able to perceive the smaller objects, such as small
gears or pegs.
To estimate the position and orientation of the cameras
relative to the robots base frame, we employ a motion capture
system. Custom 3D-printed brackets with attached reflective
Microphone
6-axis Force
Torque Sensor
RGB Camera
Event Camera
Fig. 2: Overview of the sensor placement. We use two external and
one wrist-mounted RGB cameras (marked in orange). Additionally,
we use an externally mounted event camera (in blue), three micro-
phones (in yellow), and one wrist-mounted forcetorque (FT) sensor
(in red). The omega.6 haptic teleoperation device is also visible.
markers are mounted on each camera, ensuring precise lo-
calization. Reflective markers are also attached to the robots
base frame to simplify the computation of relative poses. An
overview of the complete sensor setup is presented in Figure 2.
Event cameras are emerging as critical sensors in robotics,
providing a fundamentally different data acquisition paradigm
compared to traditional RGB cameras . Unlike frame-
based cameras that capture static images at fixed intervals,
event cameras asynchronously record changes in pixel inten-
Event cameras can provide valuable information for robotic
manipulation by capturing motion events with exceptional pre-
cision. For instance, during manipulation tasks, event cameras
can highlight fine-grained movements, such as the subtle shifts
of objects in the environment. An example of this is shown in
Figure 3, where the robot nudges a stuck peg. Once dislodged,
the pegs movement is clearly captured in the event data.
Event camera information has also been successfully used in
tasks such as slip detection , and has shown improved
performance in human action recognition compared to using
only RGB images . Motivated by these findings, we
incorporated event camera data into REASSEMBLEs multi-
modal framework.
(a) Before nudge.
(b) After nudge.
Fig. 3: Visualization of event camera data. In this example, a peg
becomes stuck after insertion, and the robot applies a nudge to
properly insert it. (a) shows a snapshot of the event stream before
the nudge, and (b) after. The motion of the peg is clearly visible in
the event camera stream.
C. Teleoperation Setup
During data collection, the operator controls the system via
a haptic device. Haptic feedback is especially important for
demonstrating contact-rich manipulation tasks, as the forces
perceived at the end effector are critical. The haptic device
allows the operator to feel the interaction forces with the
environment and adjust the demonstrated motion accordingly.
Without this feedback, the demonstrated forces could be
incorrect; for example, pulling a plug too hard might cause the
entire board to move, or exceeding the robots force thresholds
could damage the connectors during insertion. Haptic feedback
also provides cues to the operator about the success of the
attaching a nut to a bolt. The teleoperation system consists of
a haptic device (omega.6 from Force Dimension) as the master
and a Franka Emika FR3 robotic arm as the remote device. The
system operates bidirectionally, allowing the master device
to control the robots end-effector pose while relaying force
feedback from the robot back to the master device , see
Figure 4. The haptic feedback allows the operator to perceive
the robots interaction forces with the environment, enabling
them to adjust the motion accordingly.
To allow for intuitive user control and effectively utilize
the robots workspace, the haptic devices position xm R3
must be aligned with the robots base frame and its motion
appropriately scaled. This transformation is achieved through
an affine operation that combines uniform scaling, represented
by the diagonal matrix S  diag(4.5, 4.5, 4.5) R33,
and rotation alignment between the remotes and masters
base frames using the rotation matrix Rr0
m0 R33. The
transformed master device position is then used to compute
the command position for the robots end effector:
m0S(xm xinit
where xinit
R3 is the initial position of the robots end
effector. The relative orientation change between the masters
orientation qm  [wm, vT
m] R4 and its initial orientation
Fig. 4: Overview of the teleoperation control system. The operator
controls the robots motion through the haptic device, which simul-
taneously feeds back forces measured at the robots end effector.
m R4 is calculated as
qm  qm qinit1
where denotes the quaternion product defined as
w1  w2 vT
w2v2  w2v1  v1  v2
Given this quaternion difference qm and the robots initial
orientation qinit
is computed as:
The control input imp
R7 for the robots Cartesian
impedance controller is expressed as
imp  J(Kxe  De ),
R6 is the pose error vector derived from
Note that while r R7 and r,cmd R7 are the current
and commanded Cartesian poses (with position xr R3 and
quaternion orientation qr R4), the pose error e is computed
as a 6D vector by appropriate transformation of the orientation
difference. The matrices Kx R66 and D R66 represent
the stiffness and damping coefficients, respectively.
To provide the operator with haptic feedback, a six-axis
force-torque sensor mounted on the robots flange measures
the external force fr R3, which is transformed from the
sensor frame to the base frame of the haptic device
where Rr0
frame with the robots base frame. The force applied to the
haptic device, denoted as fm R3, is computed as:
fm  Bm xm  0.35f r0
where Bm  diag(40, 40, 40) N(ms) is the viscous damping
first term, Bm xm, provides stabilizing damping to suppress
cessed force feedback, which allows the operator to perceive
the interaction forces experienced by the robots end effector
in the remote environment.
For higher-DOF robots such as the FR3 robot, neglecting
nullspace optimization can lead to undesirable configurations
and complicate teleoperation. This is particularly problematic
near kinematic singularities or when approaching joint limits.
To address these issues, nullspace optimization is performed
to ensure smooth operation. The optimization leads to joint
torques null that do not affect the end-effector pose but opti-
mize the nullspace configuration. These torques are computed
to achieve two objectives: avoiding kinematic singularities and
maintaining joint positions within safe limits. The nullspace
where J() R67 represents the robots Jacobian matrix,
and V () R is a potential function designed to incorporate
multiple objectives. The objective function V () is defined as
V ()   det
term det
promotes the avoidance of singularities
by maximizing the manipulability measure. The joint limit
avoidance term JL() is expressed as
JL()  tanh (a( c)  b)  tanh (a( c)  b)  2,
where a, b < 0 R and c R7 are parameters that
define the joint limit boundaries and scaling. The velocity
damping term
R7 penalizes high joint velocities,
thereby ensuring smooth motion. By integrating the nullspace
optimization torque  n, the teleoperation system effectively
avoids undesirable configurations, respects joint limits, and
maintains high levels of manipulability. These enhancements
collectively provide the operator with a more intuitive and
reliable teleoperation experience.
D. Data Collection
Each demonstration begins by randomizing the position
and orientation of the board and objects within the robots
workspace. The operator is then instructed to assemble and
disassemble the board by teleoperating the robot. During
data collection, the operator simultaneously labels the actions
by verbally narrating their activity and marks the temporal
boundaries of task segments by pressing the appropriate key on
the keyboard. To convert the audio file containing the narrated
action descriptions, we use the Whisper  automatic speech
recognition algorithm. At the beginning of each demonstration,
we measure the poses of the board and cameras using the
motion capture system. We do not continuously track the
boards pose because the event camera erroneously reports
events when the motion capture systems illumination is turned
During teleoperation, all sensory information is recorded
using the rosbag tool. Subsequently, the raw recordings
are post-processed into HDF5 files, whose structure can be
found in the Appendix. We do not downsample or synchronize
any sensory data during postprocessing; instead, we save the
raw data along with timestamps. This approach allows for
later adjustments in synchronization or data downsampling
for specific downstream tasks. To optimize memory usage,
all video and audio recordings are stored as encoded MP4
and MP3 files, respectively, which reduces the dataset size
rosbag recordings.
The accuracy of the predicted text for action narration de-
pends on how clearly the operator spoke during data collection.
We observed that some errors occur in the transcriptions; for
errors can be easily corrected automatically by substituting
incorrect phrases with the correct ones. Additionally, we man-
ually validate the data stored in the recordings and the action
segment and success annotations. For this, we develop and
leverage a data visualization tool which we describe in more
detail in the Appendix. We verify that all data sources cover
the entire recording, are synchronized, and that the action and
success labels accurately reflect the events in the videos. If any
labelling errors are identified, we correct them. Additionally,
if any data is missing, we record the corresponding file name.
compute all valid annotations. We leverage this information to
validate whether all action annotations are correct and ensure
that no transcription mistakes remain in the dataset.
The skill annotations were manually added after record-
ing all demonstrations. We identified nine commonly shared
skills across high-level actions. Grasp refers to free-space
movement of the empty gripper until it closes on the object,
followed by Lift, which involves moving the object upward.
Approach denotes the free-space movement with an object
inside of the gripper until the object makes contact with the
position the object with respect to the socket. Release involves
letting go of the object, either after alignment or when placing
it on the table. Twist is used when rotating a BNC connector
or nut, regardless of direction. Push refers to inserting the
object into a socket, commonly seen with gears and waterproof
socket. Finally, Nudge captures a gentle push to dislodge a
peg that may be stuck after release. After completing the
and manually reviewed infrequent skill-action combinations to
ensure they were correctly labeled and not erroneous.
IV. DATASET STATISTICS
In this section, we conduct data analysis on the demonstra-
tions contained within the REASSEMBLE dataset. We focus
on four key aspects of the data: the number and distribution
of action occurrences, the amount of successful and unsuc-
cessful action demonstrations for each action, the diversity of
positions where the robot interacts with the objects, and the
patterns in the demonstrated force-torque profiles.
A. Action distribution
The number of demonstrations for each action-object pair
and their distribution provide insights into the diversity and
balance of the REASSEMBLE dataset. To achieve the best
performance across all actions, it is crucial that the dataset
contains a large and relatively equal number of demonstrations
for each action. This ensures that downstream models observe
each unique action an equal number of times, reducing per-
formance bias toward more frequently occurring actions. The
REASSEMBLE dataset includes four actions and 17 different
or 69 when including the "Idle" action. Figure 6 shows the
number of occurrences for each unique action. For clarity, we
present one plot per verb in the REASSEMBLE dataset.
Fig. 5: Sankey diagram showing the hierarchical structure and how
skills are distributed within actions. The REASSEMBLE dataset
contains 121 unique skill-object pairs.
The REASSEMBLE dataset contains a minimum of 55
demonstrations for the "Remove square peg 2" action and a
maximum of 86 demonstrations for the "Pick USB" and "Insert
BNC" actions. This demonstrates that the dataset is balanced.
Figure 5 illustrates the skill composition of each action.
The dataset contains a total of 4,551 actions, decomposed
into 10,195 skills with an average of 2.2 skills per action. On
other actions typically involve two. The Insert action, which
is the most challenging in the REASSEMBLE dataset, shows
the greatest variability, with some demonstrations requiring
up to ten steps. Insertion sequences with more than three
skills often involve objects like the BNC connector or the
nut. For instance, screwing the nut requires releasing it,
rotating the gripper back, and re-grasping it, due to the limited
rotational range of the robots final joint. Uncommon action-
skill combinations typically occur during failure recovery. For
the Ethernet connector gets stuck in the gripper. Similarly, an
Align skill appears during Place when a peg needed to be
reinserted into its holder.
B. Action difficulty and failure modes
The number of failed demonstrations per action can serve as
a metric for task difficulty, as operators are more likely to fail
when the motion is complex. Figure 7 shows the relationship
between successful and unsuccessful demonstrations for each
unique action. From the figure, we observe that the most
difficult action in the dataset is the "Insert" action, which has
the highest number of total failures (Figure 7, top right). This is
due to the complexity of the insertion motion, which typically
Ethernet
Large gear
Medium gear
Round peg 1
Round peg 2
Round peg 3
Round peg 4
Small gear
Square peg 1
Square peg 2
Square peg 3
Square peg 4
Waterproof
Fig. 6: Number of demonstrations of each action-object pair. In REASSEMBLE, we have 4 actions: pick, insert, remove, and place, and
17 objects, resulting in 68 unique action-object pairs. The number of executions of each unique action is almost equal, making it a balanced
dataset.
involves multiple steps: approaching the target, executing a
search pattern to align the object with its socket, and finally
applying downward force to insert the object.
Among all insertion tasks, inserting the BNC and nut 4 are
the most difficult. These tasks not only require aligning the
object with the socket but also rotating it around the normal
axis to the board to secure the object. The higher failure rate
for inserting objects like the Ethernet cable or USB cable
arises for two reasons: first, both plugs are directional and
must have a specific orientation relative to the socket; second,
the plugs are located on the edge of Task Board 1, increasing
the likelihood of being beyond the robots workspace.
The majority of failures in the "Pick" action (Figure 7, top
left) occur because the gripper either misses the object or the
object slips out of the gripper while it is closing. Failures
in the "Remove" action (Figure 7, bottom left) often result
from improper alignment of the gripper with the object during
released.
In contrast, we observe the fewest failures for the "Place"
action (Figure 7, bottom right). The only requirement is that
the object remains on the table after the action. However,
failures in this action do occur if the object slips prematurely
from the gripper and lands on the task board, which we classify
as a failure in this work, as an object on the board might
obstruct the sockets of other objects.
C. Interaction point diversity
Prior works have shown that diversity in the interaction
points of actions within a dataset improves generalization
and performance on downstream tasks . To ensure high
diversity in the collected data, we instructed the operator to
randomize the board and object poses for each trial during data
collection. Figure 8 shows the approximate interaction point
of all 4,551 demonstrations, obtained by sampling the final
end-effector position of each demonstration. The interaction
points approximately show the positions of the objects and
the board within the robots workspace during data collection.
The robot is positioned at the origin and faces the positive
x-axis direction. In most trials, the objects and the board were
placed in front of the robot, within its workspace.
D. Force and Torque patterns
We analyze whether any patterns occur in the demonstrated
force and torque profiles of the actions. Since the demonstra-
Waterproof
Ethernet
Large gear
Medium gear
Square peg 3
Square peg 4
Square peg 2
Round peg 3
Small gear
Round peg 1
Round peg 2
Square peg 1
Round peg 4
Successful
Unsuccessful
Round peg
Square peg 3
Waterproof
Large gear
Round peg 1
Round peg 2
Ethernet
Square peg 1
Medium gear
Round peg 4
Square peg 4
Square peg 2
Small gear
Successful
Unsuccessful
Waterproof
Ethernet
Round peg 3
Square peg 4
Square peg 2
Square peg 3
Medium gear
Round peg 2
Large gear
Square peg 1
Round peg 1
Small gear
Round peg 4
Successful
Unsuccessful
Large gear
Round peg 3
Round peg 4
Waterproof
Medium gear
Round peg 1
Round peg 2
Square peg 2
Square peg 1
Square peg 3
Square peg 4
Ethernet
Small gear
Successful
Unsuccessful
Fig. 7: Number of successful and unsuccessful demonstrations for each action-object pair. In REASSEMBLE, we also annotate the
success of each action demonstration. This can serve as a proxy for the action difficulty. The insert action is the most difficult, as it requires
the highest precision and longest horizon of all actions, followed by the remove, pick, and place actions.
Fig. 8: Approximate interaction point for each of the actions.
The robot is placed at the point (0,0), facing the positive x-direction.
REASSEMBLE has a large variety in the interaction points for each
tions of each action have different durations, for visualization
it is necessary to unify the number of the force and torque
measurements to identify patterns across all demonstrations
of a specific action. To achieve this, we normalize each
demonstration by its duration, converting the demonstrations
from the time domain to the "progress domain." A progress
of 100 indicates the action has finished. Next, we resample
each demonstration to a common number of samples (500 in
our case) by linearly interpolating the data. Finally, we plot
the mean force and torque values along with their standard
deviations for each action and visually search for meaningful
patterns. The most interesting findings are presented in Fig-
Figure 9A shows the Z-component of the measured force
for the Insert waterproof action. Three distinct phases of the
motion are visible: free-space movement from 0 to 20,
initial contact and alignment between 20 and 75, and the
pushing phase starting around 75. Around this point, the
force briefly drops as the plug aligns with the socket and is
released. From that moment until the end of the motion, the
gripper continues to apply downward force until the connector
snaps into place. Figure 9B shows the Z-component of the
measured force for the Place round peg 4 action. Here, the
effect of the objects weight is clearly observed. Around the
80 mark, the gripper opens and releases the peg, which
results in the force reading returning to nearly zero. Figure 9C
Fig. 9: Mean and standard deviation of force and torque measurements for selected actions. To analyze the temporal evolution of each
emerge in the force and torque signals across actions, and similar trends are observed in other actions throughout the dataset.
shows the torque around the Z-axis for the Remove small
gear action. Periodic changes in torque are observed, which
correspond to the operator twisting the gear back and forth
while pulling it, in order to loosen and remove it.
V. BENCHMARKS
A. Temporal Action Segmentation
One of our goals when developing the REASSEMBLE
dataset was to include multi-task annotations to enable the
development of robotic algorithms for addressing challenges
encountered at various stages of robotic system development.
One such challenge, typically faced early in robot learning
TAS is to determine temporal boundaries between actions and
label them in an untrimmed recording of a task demonstration.
By segmenting a long-horizon demonstration into shorter,
simpler actions, TAS simplifies policy learning for individual
actions. Instead of learning one complex policy from the entire
chained together.
More formally, TAS can be defined as follows: given a
dataset of N untrimmed task demonstrations D  {di, si}N
where di represents a demonstration of varying length con-
taining modalities such as vision, force, andor robot propri-
oception information, and si represents the ground truth seg-
such that M(di)  si. In the computer vision community,
TAS is typically posed as a supervised learning problem ,
where the model is trained using both data and ground truth
action segments. In contrast, the robotics community often
approaches TAS as an unsupervised problem , , where
patterns are learned only from demonstrations.
For benchmarking purposes, we evaluate the performance
of a state-of-the-art visual TAS model, DiffAct . DiffAct
leverages diffusion processes  to model and learn action
boundaries. Diffusion processes work by progressively adding
noise to the ground truth information and learning how to
iteratively remove this noise. In conditional diffusion pro-
information. For DiffAct, video visual features serve as the
conditioning information. We follow the original DiffAct paper
and use I3D video features .
To extract I3D features, we preprocess RGB videos and op-
tical flow data estimated using the RAFT  model. We use
only the wrist camera, as it provides the best perspective for
distinguishing manipulated objects, being the closest camera to
them during demonstrations. The video is downsampled from
30 to 10 frames per second to reduce computational time. We
extract the visual features in windows of 21 frames, and with
a stride of 1, resulting in one feature for each corresponding
frame in the video.
In the TAS domain, five metrics are commonly used to
evaluate and compare model performance:
Frame-level accuracy: Measures the proportion of cor-
rectly annotated frames relative to the total number of
EDIT score: Quantifies the number of label changes
needed to convert the predicted segmentation into the
ground truth.
F1 scores at 10, 25, and 50 overlap: Measure
Fig. 10: Temporal action segmentation results. In red, we highlight
instances where the "Pick" action was not predicted by DiffAct. In
DiffAct confused "round peg 1" with "square peg 1" and "square peg
the precision and recall of action segments, with the
overlap indicating the required temporal overlap between
predicted and ground truth segments for them to be
considered correct.
Among these, F150 is the most commonly used metric as
it is the most stringent.
We use the default hyperparameter settings provided for
the 50Salads dataset . The performance of DiffAct on
the REASSEMBLE dataset is as follows: Accuracy 61.5,
44.1. For comparison, DiffAct achieves significantly higher
performance on the 50Salads dataset: Accuracy 88.9, EDIT
We hypothesize that the lower performance of DiffAct on
the REASSEMBLE dataset is due to the increased challenges
it presents. Firstly, the REASSEMBLE dataset contains a
significantly higher number of unique actions (69 compared
to 19 in 50Salads), and many of the objects are less visually
distinguishable from each other, such as gears and pegs.
the median number of actions per video (36 compared to
19 in 50Salads). Furthermore, REASSEMBLE often includes
sequences where very long actions (e.g., Insert and Remove)
are separated by very short actions (e.g., Pick and Place).
DiffAct frequently misses these short actions, as illustrated
in Figure 10.
One area that remains relatively unexplored is the effective
fusion of multimodal data within TAS models, which has
been limited partly due to a lack of suitable datasets. In that
to explore various strategies for integrating multiple data
modalities. We further investigated strategies for fusing mul-
timodal data for robotic TAS models . Preliminary results
demonstrate improved performance through the integration
of visual, auditory, force-torque (wrench), gripper, and pose
information. These findings are promising, and we plan to
conduct a more comprehensive analysis in future work to
further validate and refine our approach.
The REASSEMBLE dataset presents new opportunities for
advancing temporal action segmentation (TAS) in robotics.
One underexplored area is the proper fusion of multimodal
data in TAS models, which has been limited partly due
to a lack of suitable datasets. Additionally, the challenging
nature of REASSEMBLE could facilitate research into embed-
ding common-sense reasoning and hierarchical understanding
within TAS models. For example, enabling models to learn
that before inserting an object, it must first be picked up.
B. Motion Policy Learning
The primary objective of this study is to introduce a novel
robot manipulation dataset specifically designed for contact-
rich manipulation tasks, rather than to develop a new policy
learning methodology. We train and evaluate a language- and
goal-conditioned diffusion model for MPL using the RE-
ASSEMBLE dataset. The model successfully captures motion
profiles for gear assembly and disassembly, achieving a goal-
point error of 1.3 cm, comparable to the SOTA method .
While this error is sufficient for tasks like pick and place, it
presents challenges for high-precision tasks such as insertion.
The contemporary models such as diffusion policies and action
chunking transformers have shown promising results across
various tasks, they have yet to be optimized for high-precision
applications such as assembly tasks involving the NIST board
, . Therefore, for the complete experimental evalua-
integrating Dynamic Movement Primitives (DMPs) into our
over several decades, providing a robust mathematical founda-
tion for learning and reproducing complex motion trajectories.
In our approach, DMPs are utilized to learn the weights
corresponding to specific motion conditions, particularly we
focus on Cartesian positions. The formulation of the DMPs
in our system is governed by the following set of differential
equations
z  z (z(g y) z)  f(x),
where y Rn represents the position, z Rn is the velocity,
x Rn is the canonical system state, g Rn denotes the
goal position, and z, z, x R are gain coefficients that
govern the systems convergence and stability properties. The
nonlinear forcing term f(x) is critical for learning complex
trajectories and is defined as
i1 wii(x)
where wi Rn are the learned weights, and i(x) are the
basis functions typically chosen as Gaussian functions
i(x)  exp
hi(x ci)2
with hi R representing the width parameters and ci R the
centers of the Gaussian functions. The weights wi are learned
from demonstration data by minimizing the error between the
demonstrated trajectories and those generated by the DMP
During inference, DMP framework generates motion trajec-
tories by adapting to updated start and goal points. The start
point y0 is extracted from the robots current pose at the time
of inference, while the goal pose g is determined using an
RGB-D-based object state estimation pipeline. This pipeline
employs a YOLO model trained to detect NIST objects,
Fig. 11: Large Gear assembly  disassembly The figure illustrates the trajectories generated by the DMP framework for robotic assembly
and disassembly of the large gear, including Pick, Insert, Remove, and Place motions. These trajectories are executed using an impedance
controller. Key points such as the start, end, and toggle positions for gripper actions (specifically opening and closing) are highlighted,
demonstrating the frameworks ability to adapt trajectories based on updated object poses and task requirements.
enabling accurate real-time object localization via an RGB-
D camera. Hand-eye calibration is performed to accurately
map detected object positions to the robots coordinate frame.
For non-symmetric objects such as the waterproof connector
and D-SUB connector, Principal Component Analysis (PCA)
is applied to the point cloud data captured by the RGB-D
camera. PCA identifies the primary axes of variance in the
from the point cloud data of the associated object.
The effectiveness of the proposed framework is evaluated
in large gear assembly and disassembly tasks serving as a
representative test case. Once the DMP generates a trajectory
based on the learned weights and the provided start and goal
controller. This controller ensures compliance and adaptability
to environmental uncertainties, as depicted in Figure 11.
Gripper actions are conditioned on the goal error, such that
upon reaching the goal pose, gripper operations are executed
according to task-specific conditions.
Performance demonstrations highlight successful object de-
tection and manipulation tasks, including the challenging
assembly and disassembly of gears. As shown in Figure 11,
the DMP framework effectively learns the required motion
trajectories for these tasks, facilitating precise operations such
as picking, inserting, removing, and placing the large gear. For
insertion 
