=== PDF文件: Demonstrating DVS Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks.pdf ===
=== 时间: 2025-07-22 15:46:01.457730 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating DVS: Dynamic Virtual-Real
Simulation Platform for Mobile Robotic Tasks
Zijie Zheng1
Zeshun Li1
Yunpeng Wang1
Qinghongbing Xie1
Long Zeng1
1Tsinghua University
Equal contribution
Corresponding author
Dynamic Virtual Real Simulation Platform Website
Custom Data
VR Interaction
Perception
Segmentation
Dynamic Data
Trajectory
Moving Pedestrians
Mobile Robot Agents
Dynamic Plug-in
Art Gallery
Restaurant
Static Scene
Exhibition Gallery
Supermarket
Dynamic  Scene
Application
Navigation
Trajectory Prediction
Robot Relocalization
Pick and Place
Motion capture
Robots Agents
Grasping
3D Asssets
Fig. 1: Overview of DVS platform, which offers a variety of large-scale indoor scene types and dynamic element plugins on
the left, enabling users to construct dynamic environments. In the middle, the platform supports various data types that can be
robots for tasks such as navigation, trajectory prediction, and grasping. Through a virtual-real fusion feedback mechanism, the
platform allows bidirectional mapping of the states of real and virtual agents, enriching the research scenarios.
AbstractWith the development of embodied artificial intel-
tasks. Existing simulation platforms, however, are often limited
to idealized environments, simple task scenarios and lack data
interoperability. This restricts task decomposition and multi-
task learning. Additionally, current simulation platforms face
challenges in dynamic pedestrian modeling, scene editability,
and synchronization between virtual and real assets. These
limitations hinder real world robot deployment and feedback.
To address these challenges, we propose DVS (Dynamic Virtual-
Real Simulation Platform), a platform for dynamic virtual-real
synchronization in mobile robotic tasks. DVS integrates a random
pedestrian behavior modeling plugin and large-scale, customiz-
able indoor scenes for generating annotated training datasets. It
features an optical motion capture system, synchronizing object
poses and coordinates between virtual and real world to support
dynamic task benchmarking. Experimental validation shows that
DVS supports tasks such as pedestrian trajectory prediction,
robot path planning, and robotic arm grasping, with potential
for both simulation and real world deployment. In this way, DVS
represents more than just a versatile robotic platform; it paves
the way for research in human intervention in robot execution
tasks and real-time feedback algorithms in virtual-real fusion
environments. More information about the simulation platform
is available on
I. INTRODUCTION
Robots capabilities are becoming increasingly powerful
with advances in perception, decision-making, and execution
technologies. These improvements have expanded their po-
tential applications in industrial manufacturing[23, 44], smart
rule-based operations to end-to-end learning has enabled
robots to tackle more complex tasks. However, achieving
high efficiency in real world scenarios requires a complete
robot deployment. Existing simulation platforms often fail
to effectively support this closed-loop research due to their
functional limitations.
Data collection in robotics typically relies on two ap-
or using virtual agents in simulated environments. Systems
such as Mobile ALOHA aim to reduce the cost of col-
lecting data in the real world. However, they still require
significant hardware investment and expert labor. Simulators
provide task-specific modeling tools for focused research. In
multitask and complex scenario investigations, allowing faster
iteration. Platforms such as Habitat, iGibson[17, 34], and
Arena have facilitated data collection and algorithm train-
ing. Nonetheless, their applicability is often constrained by
domain-specific assumptions or limited extensibility. Habitat,
while extended to support HITL (Human-in-the-Loop) and
HRC (Human-Robot Collaboration), focuses mainly on
navigation tasks. iGibson enhances data richness and realism
through interactive environments but lacks support for dy-
namic scenarios. Arena specializes in navigation, while tasks
like grasping rely on other simulators.
Existing simulation platforms are typically tailored to spe-
cific tasks or static environments, limiting their ability to
support complex, long-horizon scenarios that demand deep
environmental understanding and cross-domain collaboration.
For instance, completing a task like retrieving a bottle from
the fridge and placing it on a desk involves navigation,
decompose such tasks into sub-tasks across multiple simu-
such as modeling pedestrian behaviors, and do not incorporate
real world feedback. This limitation exacerbates the sim-to-
real gap, often leading to significant performance degradation
during deployment.
We propose DVS (Dynamic Virtual-Real Simulation Plat-
form), a novel framework tailored for dynamic, closed-loop
robotic research across diverse tasks. DVS addresses exist-
ing limitations through three key features. First, it supports
complex long-horizon tasks with dynamic pedestrian modeling
and flexible indoor scene editing. This enables high-fidelity
simulation environments for multi-stage operations. Second,
it establishes a virtual-real fusion workflow, combining high-
accuracy optical motion capture and ROS-based communica-
tion. This ensures synchronized validation between virtual and
physical robots, facilitating optimization based on simulated
feedback. Third, it introduces an intervention-based process.
Researchers can adjust virtual scenarios in real-time during
physical execution, enhancing task flexibility and robustness,
and extending HRC research capabilities.
Key contributions of this work are as follows:
We present a virtual-real fusion simulation platform
(DVS) for robotic research, which enables closed-loop
sim-to-real transfer validation through virtual-physical
synchronization and ROS-based communication. It sup-
ports a wide range of tasks.
We provide dynamic environmental modeling, including
pedestrian behavior simulation and flexible scene edit-
ing. These capabilities enhance complex task execution
through diverse and high-quality data generation.
We introduce an intervention-enabled workflow. This
supports real-time scenario adjustments during physical
deployment. The virtual-real synchronization mechanism
improves adaptability in dynamic environments, demon-
strated through manipulation tasks.
II. RELATED WORK
Simulation platforms have become integral to the develop-
ment and validation of embodied artificial intelligence algo-
controlled environments before deployment in real world tasks.
These platforms have seen significant advancements over the
past decade, particularly in the areas of physical modeling,
scene realism, and task-specific benchmarks.
The emergence of embodied intelligence has catalyzed sig-
nificant advances in robotics and AI, especially in tasks involv-
ing real world interaction and navigation. Such tasks ranging
from obstacle avoidance and path planning to human-robot
collaboration demand rigorous testing and training frame-
works. In this context, simulation environments have emerged
as indispensable tools, that offer safe, scalable, and cost-
effective platforms for developing and validating embodied AI
algorithms. These environments not only enable exploration
of high-risk scenarios and faster algorithm iteration but also
address the critical challenge of sim2real generalization, where
models trained in simulation must effectively transfer to real
world robotic systems.
During the past decade, the development of simulation
platforms has been instrumental in advancing embodied in-
telligence. Platforms such as Gazebo, MuJoCo, and
NVIDIA Isaac Sim have excelled in robotics control and
high-precision physical simulation, enabling accurate mod-
eling of robot dynamics and multi agents systems. Mean-
have prioritized photorealistic environments for navigation and
task planning, supporting benchmarks for tasks like object
Recent systems such as DialFRED and TEACh have
expanded the scope of these benchmarks by integrating natural
language dialogue, encouraging richer agent-environment in-
teractions. Despite these advancements, several persistent chal-
lenges remain unresolved, hindering the broader applicability
of these platforms to dynamic and real world scenarios.
A major limitation of existing platforms is their inability to
model dynamic, stochastic environments that capture realistic
human behaviors and evolving scene conditions. Platforms like
Habitat and AI2-THOR, while robust for static or semi-static
object interactions, which constrain their generalizability to
real world, unpredictable conditions. Another challenge is the
gap in the sim2real generalization. Although simulators like
Pybullet and MuJoCo excel in physical modeling, they
often lack the diversity and randomness required to robustly
train algorithms for real world deployment. Moreover, the
growing emphasis on human-robot collaboration has exposed
the limitations of existing platforms, which rarely support
real-time interactions such as gesture-based commands, shared
HumanTHOR and SEAN have made notable progress
toward dynamic interaction modeling, but their focus remains
limited to basic social navigation or static collaborative tasks,
TABLE I: Comparison of Simulation Platforms. For the sensor, S refers to semantic, L refers to Lidar
Simulation Platform
Dynamic Scenes
VR Interaction
Pedestrians
Gibson series
ThreeDWorld
VirtualHome
DVS(Ours)
leaving ample room for advancement. Moreover, most existing
platforms specialize either in physical modeling or in photore-
alistic simulation, yet rarely integrate both capabilities within a
unified frameworkhighlighting a critical gap in tools capable
of holistically supporting embodied intelligence research.
To address these limitations, we propose a novel virtual-
physical integration platform that combines the strengths
of high-fidelity physics, dynamic scene modeling, and real-
time human-robot collaboration. By introducing stochastic
pedestrian behavior modelingincluding adjustable avoid-
ance radii, randomized spawning points, and variable motion
patternsour platform supports dynamic and unpredictable
of robot algorithms. Additionally, a optical motion capture
system provides submillimeter precision data for sim2real
models to real world systems. Real time human-in-the-loop
(HITL) interactions, including gesture commands, natural
language dialogue, and shared workspace collaboration, fur-
ther enable realistic HRC experiments. Finally, the integration
of annotated synthetic data with real world motion capture
enables simultaneous development and validation across both
virtual and physical domains, effectively bridging the gap
between simulation and reliable real world deployment in
embodied AI.
By tackling key challenges in dynamic scene modeling, sim-
to-real transfer, and human-robot collaboration, our proposed
platform provides a unified solution for advancing embodied
intelligence. Its capacity to simulate complex, real world
environments and enable seamless robot deployment positions
it as a powerful enabler for future research in navigation,
of existing simulation platforms is delineated in Table I.
III. SYSTEM FRAMEWORK
In this section, we describe the key components of our
Dynamic Virtual-Real Simulation Platform, which integrates
virtual-real fusion and dynamic scene generation to support
advanced robotic research. These two capabilities are designed
to address the limitations of existing simulation platforms,
enabling more effective training and evaluation of algorithms
Fig. 2: Virtual-Real Data Synchronization Framework. The
central demonstrates the synchronization of object pose and
robot motion through VRPN and ROS. The left and right parts
depict the virtual simulation environment and physical real
world scene, respectively.
in real world conditions. By combining high-fidelity virtual
simulation with real-time interactions and dynamic scene
testing mobile robots.
A. Virtual-Real Fusion for Seamless Interaction
Virtual-real fusion is a core feature of DVS, enabling precise
bidirectional synchronization between the virtual and physical
environments. This synchronization is critical for ensuring that
algorithms trained in the virtual world can be directly applied
to physical robots, thus reducing the sim-to-real gap.
The virtual-real fusion module consists of two primary
nization. These components work together to ensure that both
the objects and robots in the simulation environment align
accurately with their real world counterparts.
1) Object Pose Synchronization: Object pose synchroniza-
tion is a critical feature for bridging the gap between virtual
and real environments, enabling accurate interactions between
robots and their surroundings in both domains. In DVS, we
achieve precise synchronization using 14 motion capture cam-
accuracy and 0.1  rotational precision. This allows for high-
fidelity pose alignment, essential for ensuring that physical
virtual counterparts in simulation.
The synchronization process begins with extrinsic cali-
bration of the motion capture system. By calibrating the
systems extrinsic parameters, we can establish a unified world
coordinate system that aligns the virtual and real spaces. This
calibration is achieved through the following transformation:
Tvirtual  R  Treal  t
R is the rotation matrix derived from the spatial cali-
bration process, defining how the real world orientation
maps to the virtual space.
Treal is the translation vector representing the position of
the real world object.
t is the translation vector that compensates for any
Through this method, the physical object trajectories, such
as those of a robots end-effector, are directly mapped into
the virtual environment. This enables precise interaction with
virtual objects, improving the realism of simulations and
ensuring the accuracy of robotic tasks that require interaction
between the real and virtual worlds.
B. Dynamic Scene Generation
The dynamic scene generation module of DVS significantly
enhances the realism and complexity of training environ-
world conditions. This module incorporates dynamic pedes-
trian agents and mobile robotic proxies, both of which are
key to simulating the unpredictability and complexity of real
world environments.
1) Dynamic Pedestrian Plug-in: DVS features a pedestrian
simulation plugin that introduces human-like agents into the
virtual environment. These agents feature variable motion
accelerations and socially compliant avoidance behaviors,
enabling dense, collision-free movement. By incorporating
mechanisms such as variable speeds, random spawning points,
and obstacle avoidance, the system realistically simulates
human interaction dynamics. Adding dynamic pedestrians to
different static scenes allows our platform to closely replicate
crowded environments like supermarkets and busy restaurants,
significantly enhancing the realism of indoor simulations. This
provides a richer learning environment for developing mobile
robots navigation and collaboration capabilities in human-
populated spaces.
Such dynamic pedestrian behaviors are crucial for training
indoor mobile robots in navigation and human-robot inter-
action tasks. The human-like agents interact with robots in
real time, enabling researchers to collect diverse and realistic
datasets for optimizing navigation strategies, path planning,
and collaborative algorithms. This feature is particularly valu-
able for enhancing robots ability to adapt to sudden environ-
mental changes or unpredictable pedestrian behaviors.
2) Multi-Robot Plug-in: In addition to pedestrian simula-
agents within the same environment. This capability allows
researchers to study multi-robot collaboration and competition
in dynamic settings. Simulation of multiple robots operating
in close proximity enables the development of cooperative
algorithms for tasks such as resource sharing, coordinated
The ability to simulate multi-robot environments in dy-
search. By mimicking real world challenges such as managing
crowded environments or dealing with unexpected obstacles,
DVS helps researchers develop more robust algorithms that
can handle complex tasks in unpredictable settings.
ensure that DVS provides a training environment that closely
mirrors real world operational conditions. These capabilities
are essential for developing robots that can navigate com-
plex spaces, collaborate with humans, and adapt to dynamic
changes in their environments.
IV. APPLICATIONS OF DVS PLATFORM
Our platform supports the full workflow, from data gen-
eration to real world validation. In the previous chapter, we
introduced two core modules of our system. This chapter
discusses the construction of a large-scale virtual-real fusion
dataset and explores the experimental data generation process,
along with its application in task training and testing.
A. Data Perception and Generation
Fig. 3: The interactive interface of the simulation platform:
The left panel adjusts dynamic pedestrian parameters while
the right selects perception data types.
In robotics research, virtual environments provide clear task
settings. Data generation is a core feature of simulation
platforms. As shown in Fig. 3, our platform facilitates the
generation and processing of various data formats, including
RGB images, depth maps, 2D3D bounding boxes, semantic
and instance segmentation, and trajectory data, all via a user-
friendly interface . These data types support foundational
tasks and enable complex research scenarios, such as path
To enhance data quality and usability in simulation, we op-
timize the data generation pipeline by ensuring smooth camera
trajectories and precise depth-to-RGB alignment. Specifically,
we employ Bezier curves to generate smooth camera motion,
minimizing abrupt directional changesparticularly at tra-
jectory cornerswhich significantly improves frame-to-frame
Fig. 4: The robotic arm is interrupted while executing Prompt
A and is requested to execute Prompt B. The first row shows
the robotic arm in the virtual platform, and the second row
shows the real robotic arm.
feature matching and point cloud reconstruction. Depth data
is temporally aligned with RGB frames to guarantee precise
accurate scene modeling.
We evaluated the impact of these optimizations by collecting
camera trajectories both with and without smoothing, and
analyzed feature matching performance using LightGlue
and SuperPoint. The results in Table II show that smoothed
trajectories yield a higher number of feature correspondences
between frames, particularly in small and cluttered indoor
environments such as bedrooms. As demonstrated in our
enhance downstream tasks like 3D reconstruction and semantic
research as task complexity increases.
TABLE II: Camera Trajectories With and Without Smoothing
Trajectory Type
Avg. Features
Straight
Livingroom
Straight
B. Robotic Tasks Learning
1) Virtual-Real Intervention Grasping: A key weakness
of learned policies in robotic manipulation [9, 46, 5, 47]
TABLE III: Task Success Rates by Module and Prompt Order
Prompt Order
Success Rate ()
First Task
Second Task
OpenVLA-7B
is that their success rate in task execution is low when
deployed in practice, even with domain adaptation . In
heterogeneous deployments using only pretrained weights,
the success rate of robots performing tasks across different
models tends to approach zero. Even when data collection for
specific tasks is done using the robot being deployed and fine-
due to the characteristics of our platform, which includes
virtual-real mapping and benchmark alignment between the
virtual environment and the real world, and the fact that the
robot has a ROS communication interface, we can supervise
and intervene in the robots tasks in the real world through
the platform to improve the success rate of task execution.
We set up experimental conditions based on the common
manipulation task of grasping. As shown in Fig. 4, in order
to reflect the characteristics of our platform supervision and
at the beginning of the experiment, and interrupt the task
and provide new tasks based on virtual scenes through the
platform when the gripper is performing the task. We utilized
a seven-degree-of-freedom Kinova Gen3 robotic arm to collect
nearly a hundred grasping data points on a planar surface. The
data was then fine-tuned on the pre-trained models released
by OpenVLA-7B and RDT-1B, enabling our robotic
arm to achieve a high success rate in performing tasks in
specific scenarios. At the beginning of the experiment, we
provided the robotic arm with prompts to grasp an apple and a
task on the platform and assigned a new task.The experiment
demonstrated that our platform effectively intervened in the
robotic arms task execution. The experimental results are
shown in the Table III. Prompts: A: Pick up the apple; B:
Pick up the banana.
2) Real to Sim to Real Learning: In autonomous robotic
task execution without human intervention, integrating phys-
ical robots and their sensory systems into a virtual-real
fusion architecture serves as a key technical strategy for
improving task success rates. The real-world component of
our platform includes the physical robot and sensing devices
used to collect feedback. A notable application of virtual-
real synchronization is digital twin monitoring, which enables
continuous evaluation and refinement of algorithms using real-
world dataeffectively bridging the sim-to-real gap.
To achieve precise synchronization, we employ ROS2 to
align the real robots motion, controlled via MoveIt, with that
of its virtual counterpart. This ensures the virtual environment
reflects physical priors such as joint friction and mechanical
latency. We validated this approach in a Virtual-Real As-
sisted (VLA) grasping task by comparing standard sim-to-
real transfer with a fine-tuned version incorporating real-world
feedback. As shown in Table IV, the virtual-real fusion method
led to significantly better performance, highlighting the value
of real-time feedback in simulation refinement.
Observed
Prediction
Ground Truth
Collision
Fig. 5: Visualization of pedestrian trajectory prediction, where
each color represents a different pedestrian. The accuracy of
the prediction is higher when the predicted trajectory (short
dashed line) closely aligns with the ground truth (GT, solid
line). In environments with dense static obstacles, such as
(red rectangular box).
TABLE IV: Comparative with Different Finetuning Data
Finetune Data
Successes
Pick the apple and place
at the target point
virtual-real
Pick the banana and place
at the target point
virtual-real
To support accurate virtual-real synchronization, we use
a motion capture system for high-precision calibration be-
tween real and virtual coordinate systems. This calibration
is essential for aligning the physical robots position with its
virtual counterpart, correcting errors such as odometry drift
and IMU noise. For tasks that are less sensitive to positional
the Quest system to achieve approximate alignmenta low-
cost yet sufficiently accurate alternative for many practical
applications.
3) Dynamic Indoor Pedestrian Trajectory Prediction:
Pedestrian trajectory prediction aims to forecast future tra-
jectories based on observed trajectories, while considering
complex interactions and environmental layouts . It serves
as a crucial connection between the perception system and the
planning system.
Three trajectory prediction algorithms, i.e. STGAT ,
Trajectron  and TUTR , are tested on our synthetic
indoor scenes (Gym, Office and Supermarket) as well as
the official public outdoor dataset (ETH ). We use ADE
(Average Displacement Error) and FDE (Final Displacement
Error) as evaluation metrics, where lower ADE and FDE
values indicate better performance. The experimental results
are depicted in Table V. Additionally, to analyze pedestrian
movement patterns and collision avoidance strategies, we
selected two dense indoor scenes (Gym and Office) and
visualized the predicted trajectories in Fig. 5.
mance decrease when applied to indoor scenes compared to
the outdoor ETH scene. Specifically, the ADE for STGAT
decreases from 0.79 to 1.42 (79.7) when generalizing from
the ETH scene to the Supermarket scene, while the FDE
for STGAT decreases from 1.48 to 2.88 (94.5) in the
TABLE V: Experiments on Pedestrian Trajectory Prediction.
while ETH  is the official public outdoor dataset.
Trajectron
Trajectron
Supermarket
Trajectron
Trajectron
same scenario. We analyze this performance drop from three
perspectives. First, compared to outdoor scenes, narrow indoor
spaces are often filled with numerous static obstacles, which
can interfere with human trajectory decision-making and lead
to collisions. Second, indoor human interactions are more
frequent due to communication or obstacles caused by people
standing in the way, making predictions more challenging.
making predictions more sensitive to small positional changes.
If the model was trained in larger, more open outdoor spaces,
it may not have learned to adapt to the smaller, more dynamic
movements of indoor environments.
The results also underscore the importance of robust
spatial-temporal modeling in trajectory prediction tasks. The
transformer-based architecture of TUTR appears particularly
well suited to capture intricate interactions over time, which
leads to its superior performance. Trajectoron provides a
balance of stability and accuracy, but lags behind in highly
dynamic environments. In contrast, STGATs graph-based
complex environments, highlighting its limitations in handling
high-dimensional spatial-temporal variability. These findings
offer valuable insights for future research, emphasizing the
need for models that can generalize effectively across diverse
scenarios while maintaining low computational overhead.
4) Dynamic Indoor Social Navigation: Social navigation
refers to the process by which agents use social cues and
prior experiences to determine paths, make decisions, and
navigate complex environments. It incorporates interpersonal
and collective information, including behavioral patterns, real-
time human feedback, and established social norms.
We evaluated three indoor social navigation algorithms
thetic indoor restaurant and store environments in our platform
with varying levels of dynamic complexity. Performance was
quantified using three primary metrics: Success Rate, average
Navigation Time (in seconds) for succ
