=== PDF文件: PINGS Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map.pdf ===
=== 时间: 2025-07-22 09:42:54.595476 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：within a Point-Based Implicit Neural Map
Xingguang Zhong
Liren Jin
Louis Wiesmann
Marija Popovic
Jens Behley
Cyrill Stachniss,
Center for Robotics, University of Bonn, Germany
Lamarr Institute for Machine Learning and Articial Intelligence, Germany
Neural Points
Distance Field:
Surface Mesh
Radiance Field:
Rendered Image
Large-scale SLAM
timestep
Fig. 1: We present PINGS, a novel LiDAR-visual SLAM system unifying distance eld and radiance eld mapping using an elastic point-
based implicit neural representation. On the left, we show a globally consistent neural point map overlaid on a satellite image. The map was
built using PINGS from around 10,000 LiDAR scans and 40,000 images collected by a robot car driving in an urban environment for around
5 km. The estimated trajectory is overlaid on the map and colorized according to the timestep. On the right, we show a zoomed-in view
of a roundabout mapped by PINGS. It illustrates from left to right the rendered image from the Gaussian splatting radiance eld, neural
points colorized by the principal components of their geometric features, and the reconstructed mesh from the distance eld (colorized by
the radiance eld). The red line indicates the local trajectory of the robot car (shown as the CAD model).
AbstractRobots benet from high-delity reconstructions of
their environment, which should be geometrically accurate and
photorealistic to support downstream tasks. While this can be
achieved by building distance elds from range sensors and
radiance elds from cameras, realising scalable incremental
mapping of both elds consistently and at the same time with
high quality is challenging. In this paper, we propose a novel
map representation that unies a continuous signed distance
eld and a Gaussian splatting radiance eld within an elastic
and compact point-based implicit neural map. By enforcing
geometric consistency between these elds, we achieve mutual
improvements by exploiting both modalities. We present a novel
LiDAR-visual SLAM system called PINGS using the proposed
map representation and evaluate it on several challenging large-
scale datasets. Experimental results demonstrate that PINGS can
incrementally build globally consistent distance and radiance
elds encoded with a compact set of neural points. Compared to
state-of-the-art methods, PINGS achieves superior photometric
and geometric rendering at novel views by constraining the radi-
ance eld with the distance eld. Furthermore, by utilizing dense
photometric cues and multi-view consistency from the radiance
improved odometry estimation and mesh reconstruction. We also
provide an open-source implementation of PINGS.
I. INTRODUCTION
The ability to perceive and understand the surroundings
is fundamental for autonomous robots. At the core of this
capability lies the ability to build a map  a digital twin of the
robots workspace that is ideally both geometrically accurate
and photorealistic, enabling effective spatial awareness and
operation of the robot [24, 41].
Previous works in robotics mainly focus on the incremental
mapping of an occupancy grid or a distance eld using range
based implicit neural representation can effectively model a
continuous signed distance eld (SDF) for LiDAR simultane-
ous localization and mapping (SLAM), enabling both accurate
localization and globally consistent mapping.
or distance elds [49, 51] fall short of providing photorealistic
novel view rendering of the scene, which is crucial for applica-
tions requiring dense photometric information. This capability
can be achieved by building an additional radiance eld with
visual data using representations such as neural radiance eld
(NeRF)  or a 3D Gaussian splatting (3DGS) model .
Recent works demonstrated the potential of radiance elds,
especially 3DGS, for various robotic applications including
human-robot interaction , scene understanding [92, 97],
simulation or world models for robotics learning [2, 12, 81],
visual localization [4, 42], and active reconstruction [26, 27].
image collections in bounded scenes with ofine processing,
limiting their applicability for mobile robotic applications.
In this paper, we investigate how to simultaneously build
elds as well as accurate distance elds for large-scale en-
vironments using LiDAR and camera data. Building upon
PIN-SLAMs  point-based neural map for distance elds
and inspired by Scaffold-GS , we propose a novel point-
based model that additionally represents a Gaussian splatting
radiance eld. By enforcing mutual supervision between these
elds during incremental mapping, we achieve both improved
rendering quality from the radiance eld and more accurate
distance eld for better localization and surface reconstruction.
The main contribution of this paper is a novel LiDAR-visual
SLAM system, called PINGS, that incrementally builds contin-
uous SDF and Gaussian splatting radiance elds by exploiting
their mutual consistency within a point-based neural map. The
distance eld and radiance eld infered from the elastic neural
points enable robust pose estimation while maintaining global
consistency through loop closure correction. The compact
neural point map can be efciently stored and loaded from
distance eld and high-delity real-time novel view rendering
from the radiance eld, as shown in Fig. 1.
In sum, we make four key claims: (i) PINGS achieves better
RGB and geometric rendering at novel views by constraining
the Gaussian splatting radiance eld using the signed distance
eld; (ii) PINGS builds a more accurate signed distance eld
for more accurate localization and surface reconstruction by
leveraging dense photometric cues from the radiance eld; (iii)
PINGS enables large-scale globally consistent mapping with
loop closures; (iv) PINGS builds a more compact map than
previous methods for both radiance and distance elds.
Our open-source implementation of PINGS is publicly
available at:
II. RELATED WORK
A. Point-based Implicit Neural Representation
Robotics has long relied on explicit map representations
with discrete primitives like point clouds , surfels [3, 73],
localization  and planning .
to model radiance elds  and geometric (occupancy or dis-
tance) elds [44, 49, 52] using multi-layer perceptrons (MLP).
These continuous representations offer advantages like com-
pact storage, and better handling of regions with sparse obser-
vations or occlusions, while supporting conversion to explicit
representations for downstream tasks.
Instead of using a single MLP for the entire scene, recent
methods use hybrid representations that combine local feature
vectors with a shared shallow MLP. Point-based implicit neural
representations [51, 79] store optimizable features in a neural
point cloud, which has advantages over grid-based alternatives
through its exible spatial layout and inherent elasticity under
transformations for example caused by loop closures.
Point-based implicit neural representations have been used
for modeling either radiance elds or distance elds for vari-
ous applications including differentiable rendering [8, 79], dy-
namic scene modeling , surface reconstruction , visual
odometry [56, 86], and globally consistent mapping . For
elds with neural points for odometry estimation and uses the
elasticity of these neural points during loop closure correction.
In this paper, we propose a novel LiDAR-visual SLAM
system that is built on top of PIN-SLAM  and encodes
a Gaussian splatting radiance eld within neural points while
jointly optimizing it alongside the distance eld. Compared to
NeRF-based approaches [8, 79], this offers faster novel view
rendering suitable for robotics applications.
B. Gaussian Splatting Radiance Field
NeRF  pioneered the use of MLPs to map 3D positions
and view directions to color and volume density, encoding
radiance elds through volume rendering-based training with
posed RGB images. More recently, 3DGS  introduced
explicit 3D Gaussian primitives to represent the radiance
to NeRF-based methods, 3DGS is more efcient by using
primitive-based differentiable rasterization  instead of ray-
wise volume rendering. The explicit primitives also enables
editing and manipulation of the radiance eld. These proper-
ties make 3DGS promising for robotics applications [2, 26,
geometric accuracy and scalability for incremental mapping.
We discuss the related works addressing geometric accuracy
in the following and addressing scalable mapping in Sec. II-C.
While 3DGS achieves high-delity photorealistic rendering,
it often lacks the geometric accuracy. To tackle this limitation,
SuGaR  uses a hybrid representation to extract meshes
from 3DGS and align the Gaussian primitives with the surface
meshes. To address the ambiguity in surface description,
another solution is to atten the 3D Gaussian ellipsoids to
2D disks [11, 23, 25, 85]. The 2D disks gradually align with
surfaces during training, enabling more accurate depth and
normal rendering. However, extracting surface meshes from
these discrete primitives still requires either TSDF fusion
with rendered depth or Poisson surface reconstruction .
Another line of works [58, 84] model discrete Gaussian
opacity as a continuous eld, similar to NeRF-based surface
reconstruction . Several works [6, 39, 83] jointly train a
distance eld with 3DGS and align the Gaussian primitives
with the zero-level set of the distance eld to achieve accurate
surface reconstruction. However, these methods rely solely on
image rendering supervision for both 3DGS and neural SDF
training without direct 3D geometric constraints, leading to
ambiguities in textureless or specular regions. The volume
rendering-based SDF training also impacts efciency.
While 3DGS originally uses structure-from-motion point
directly from LiDAR measurements [10, 21, 78]. Direct depth
measurements can further supervise depth rendering to im-
prove geometric accuracy and convergence speed [25, 42].
Our approach uniquely combines geometrically consistent
2D Gaussian disks with a neural distance 
