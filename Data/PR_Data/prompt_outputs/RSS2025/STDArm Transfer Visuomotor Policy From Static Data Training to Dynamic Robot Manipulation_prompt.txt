=== PDF文件: STDArm Transfer Visuomotor Policy From Static Data Training to Dynamic Robot Manipulation.pdf ===
=== 时间: 2025-07-22 15:49:53.138663 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Static Data Training to Dynamic Robot
Manipulation
Yifan Duan1, Heng Li1, Yilong Wu1, Wenhao Yu2, Xinran Zhang1, Yedong Shen1,
Jianmin Ji1 and Yanyong Zhang3
1School of Computer Science and Technology, University of Science and Technology of China, Hefei, China
2Institute of Advanced Technology, University of Science and Technology of China, Hefei, China
3School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, China
Equal Contribution Corresponding Author
Project page:
AbstractRecent advances in mobile robotic platforms like
quadruped robots and drones have spurred a demand for
deploying visuomotor policies in increasingly dynamic environ-
ments. However, the collection of high-quality training data, the
impact of platform motion and processing delays, and limited
onboard computing resources pose significant barriers to existing
solutions. In this work, we present STDArm, a system that directly
transfers policies trained under static conditions to dynamic
platforms without extensive modifications.
The core of STDArm is a real-time action correction frame-
work consisting of: (1) an action manager to boost control
frequency and maintain temporal consistency, (2) a stabilizer
with a lightweight prediction network to compensate for motion
calibrating system parameters. In this way, STDArm achieves
centimeter-level precision in mobile manipulation tasks.
comprehensive
evaluations
proposed
STDArm on two types of robotic arms, four types of mobile
the STDArm enables real-time compensation for platform motion
disturbances while preserving the original policys manipulation
ing robot motion.
I. INTRODUCTION
Visuomotor policies, which directly map visual observations
to robot actions, have emerged as an effective paradigm
for complex task acquisition . By integrating end-to-end
imitation learning frameworks trained on human demonstra-
modular pipeline design, translating raw sensor input into
robot actions without intermediate representations . Recent
advances have enabled robotic systems with remarkable capa-
bilities such as battery insertion , food preparation , and
textile handling , etc.
The acquisition of high-quality human demonstrations, typ-
ically achieved through teleoperation, plays a pivotal role in
the successful skill acquisition using imitation learning-based
visuomotor policies . This process is relatively simple with
stationary robotic systems, leading to common experimental
Action from
Static Policy
Action from
Flight Direction
Static Policy
Target Object
Success!
When deploying a policy trained on static data on computationally
constrained edge devices, action drift may occur due to delays like inference
and execution latency. Our system, STDArm, addresses this by direct rectifica-
tion of actions, enabling reliable task completion without policy modifications.
setups that use table-mounted manipulators. However, sig-
nificant challenges arise when working with dynamic robot
These robots exhibit both active movement for navigation and
wobble caused by their instability. The inherent mobility of
these platforms requires skilled operators who can maintain
stable action outputs while compensating for base movement
disturbances during data collection. This transforms what
would otherwise be a straightforward data collection step into
potentially the most challenging part of the entire process.
Beyond the challenges in data collection, directly deploying
policies trained in stationary conditions on dynamic platforms
is ineffective. First, performing policy inference on low-
computational platforms carried by small robots often results
in significant processing delays, causing the generated actions
to become outdated due to the robots motion, as shown in
Fig. 1. Second, current visuomotor policies [3, 4, 9] commonly
employ action chunking to promote temporal action consis-
inference for sequential execution. This approach further re-
duces the robots responsiveness to environmental changes.
on specific mobile robot struggle to generalize across different
robotic systems due to their distinct locomotion characteristics.
To address the aforementioned challenges, we develop
training to dynamic platforms with negligible cost, equipping
various mobile robots with robust capabilities inherited from
imitation learning. Our key insight is that, in addition to visual
and arm pose inputs, STDArm incorporates additional high-
frequency observations of the robots position and employs a
lightweight network to perform real-time action corrections
based on the robots movement. By increasing the control
in robots state. Specifically, an action manager module is
first designed to decouple the policy networks asynchronous
inference from the real-time control loop, while interpolating
the policy outputs to higher frequency. This establishes high-
frequency control as the foundation of the entire system.
Building on this, STDArm combines high-frequency ego-
motion tracking through a visual SLAM system  operating
at 10 the base policys observation rate  with a lightweight
neural network that bridges the perception-action latency gap.
This integrated solution enables real-time compensation for
platform motion disturbances while maintaining the original
policys manipulation capabilities, achieving centimeter-level
operational precision during mobile manipulation tasks. Fi-
execution goals, we implement an action-stabilizing warm-up
routine prior to mission execution, enabling online calibration
of system parameters through controlled motion sequences.
To validate STDArms effectiveness, we conduct experi-
ments across two distinct robotic arm configurations and four
mobile platforms, including drones. Two canonical visuomotor
the foundational policy networks. We design three manip-
ulation tasks with progressively stringent precision require-
ments to evaluate STDArms manipulation stability during
robots movement. The most challenging task demands end-
effector positioning accuracy within 4 cm. Experimental re-
sults demonstrate that while the native policy achieves near-
zero success rates during the robots movement, STDArm
attains success rates comparable to those of the native policy
under ideal static conditions. These experimental configura-
tions validate that STDArm enables cost-effective migration of
static-trained policy networks to diverse dynamic platforms.
We present an efficient system, STDArm, enabling direct
deployment of static-trained policy networks to mobile
robotic platforms with negligible cost, while preserving
the policys original performance.
The designed action manager module is incorporated to
enhance robotic decision-making and control frequencies,
forming the foundational architecture of STDArm.
By predicting the robots state and estimating system
latency online, we precisely maintain the stability of
manipulation during the robots movement.
To validate our approach, we conduct experiments using
two types of robotic arms across three different tasks.
world scenario to validate its effectiveness.
II. RELATED WORK
A. Imitation Learning
Imitation learning enables robots or agents to rapidly
acquire complex behaviors by leveraging expert-provided
demonstrations. In recent years, this approach has been widely
applied to tasks such as robot manipulation [3, 4, 10, 11, 12,
modeling capabilities of diffusion models to better address
the challenges posed by the diversity in robot manipulation
demonstrations. 0  integrates a pre-trained multimodal
language model with a flow-matching-based diffusion action
head to achieve efficient and generalized robotic policies.
CARP  aims to design an autoregressive generation policy
that extracts multi-scale representations of the entire robot ac-
tion sequence, generating action sequence predictions through
a coarse-to-fine autoregressive process. The efficient collection
of high-quality expert data is one of the crucial prerequisites
for the effectiveness of the aforementioned imitation learning
methods. However, unstable and dynamic robotic platforms,
like drones and legged robots, introduce significant challenges
to the data collection process. To address this, STDArm intro-
duces an efficient system that allows policies trained on data
collected from static platforms to be seamlessly transferred for
inference on unstable and dynamic robotic platforms.
The dynamically unstable platform also imposes require-
ments on the inference frequency of the policy, as a higher
inference speed can better adapt to the dynamic changes in
the task settings. In particular, the recently popular diffusion
policy faces challenges in improving inference frequency due
to its need for multiple iterative denoising steps. Consistency
Policy [18, 19] attempts to use consistency distillation to
enable a student network to learn a direct mapping over
longer step spans on the probability flow ODE. AdaFlow ,
based on flow-based generative models, adaptively adjusts
the computational cost during inference according to the
variance of the state, thereby significantly improving inference
efficiency while maintaining decision diversity. The above
methods all attempt to reduce the number of denoising steps
in diffusion policy to increase the overall policy inference
frequency. Meanwhile, BID  creates action chunks at
each observation stamp and uses two loss functions to select
chunks that try achieving both long-term consistency and
short-term reactivity. However, STDArm adopts a more direct
approach by using an action manager and interpolated action
on low-computational platforms.
a wide range of learning-based algorithms, ensuring flexibility
and broad applicability.
Observation
Action Sequence
Temporal
Ensemble
Low-frequency
Decision
Stabilizer
Estimation
Interpolation
Prediction
Action Buffer
High-frequency
Response
Inference
Pipeline of STDArm. Given images and joint state observations, our system first generates a low-frequency action sequence via a policy network.
These actions are then passed to action manager, which maintains an action buffer using temporal ensemble and generates high-frequency actions through
interpolation. Subsequently, a stabilizer refines the action based on estimated latency and real-time pose predictions to compensate for the motion of the robot
platform. Finally, the stabilized action is sent to the robotic arm for precise and stable manipulation.
B. Mobile Manipulation
Early methods for mobile manipulation have shown notable
success in tasks like grasping, including ground-based mobile
manipulation [22, 23, 24, 25, 26, 27, 28] and unmanned aerial
manipulation (UAM) [29, 30, 31]. However, these approaches
often depend on human engineered designs tailored to specific
scenarios or require extensive data collection for training,
which restricts their adaptability across diverse engineering ap-
plications. To mitigate these limitations, MobileALOHA
introduces a cost-effective wheeled dual-arm system capable
of remote operation to streamline data collection and relies
on ACT  for policy learning and deployment. Despite its
operations within similar environmental conditions, limiting
its broader applicability. Recognizing these constraints, UMI
on Legs  focuses on efficient data collection and Diffusion
Policy  deployment across a variety of dynamic legged
robots. Its performance is validated on platforms such as
when applied to highly dynamic platforms like drones, ma-
nipulation faces heightened challenges in achieving precise
and robust control due to the dynamic and unstable nature
of their bases. Addressing these challenges, our STDArm
builds on the strengths of previous methods, enabling efficient
data collection and stable deployment of imitation learning
for manipulation operating on more challenging platforms,
including drones. This advancement underscores the potential
for enhanced control and adaptability in complex and dynamic
environments.
III. STDARM
As shown in Fig. 2, we present the STDArm architecture
through its workflow, tracing the data flow from RGB image
inputs and joint state measurements to final action execu-
tion. We initially employ a foundational policy network to
generate action sequence for low-frequency decision-making,
as detailed in Sec. III-A. Then, an action manager module
enhances control frequency through temporal ensemble and
action interpolation strategy, as discussed in Sec. III-B. To
enable real-time compensation for platform motion distur-
the conversion of preliminary actions into the final executed
actions. Finally, we design an action-stabilizing warm-up
routine before mission execution in Sec. III-D, to perform
online parameter estimation essential for the stabilizer module.
It is worth mentioning that all these modules are deployed
on edge devices without the assistance of external platforms
for computation.
A. Policy Network
The policy network  takes mo steps of observations O,
including images I and end-effector poses S as input. Based
on these observations, the network predicts the action sequence
u for the next ma time steps. Both the observation steps
mo and prediction steps ma are predefined based on prior
experience. Denoting the action sequence as ut1:tma
{ut1, ..., utma}, this process can be formulated as:
where u is defined as the end-effectors position, which will be
used to control the robotic arm. In order to avoid prolonged
policy inference blocking the entire system, the policy net-
work  is assigned a dedicated subprocess. Following each
inference cycle, the policy network immediately retrieves the
latest observations to ensure a continuous generation of action
sequences. This approach enhances the decision-making fre-
quency of the policy network, which is beneficial for handling
dynamic scenarios.
B. Action Manager
To effectively select the appropriate actions for execution
from the continuously generated action sequences, we design
an action manager module to handle the output of the policy
network. First, the action manager maintains an action buffer
A  {At, t N}, which sequentially stores both recently
executed actions and predicted actions awaiting execution. We
define At to be in the same form as u. A temporal ensemble
algorithm is then employed to merge the newly generated
action sequences u with the contents of the action buffer A.
frequency of the robotic arm. The details are as follows:
1) Temporal Ensemble: As illustrated in Fig. 2,  reads
the most recent observation Ot1mo1:t1 at time t1, and after
an inference duration of tInf, generates an action sequence
merge ut11:t1ma with current action buffer A, where
indicates that t2 is the first execution moment after obtaining
the action sequence generated from the observation at t1. A
critical issue of merging lies in determining the correspon-
dence between actions in ut11:t1ma and those already in
A. Aligning them by timestamp is a straightforward approach,
treating the action at t2 in ut11:t1ma as corresponding to the
action at t2 in A. However, in practice, we observe that due to
inconsistencies in policy inference, actions predicted for the
same timestamp may not match, potentially leading to action
actions. To address this, we use timestamp alignment as an
initial solution and select the appropriate match by comparing
the similarity of actions as follows:
tu  arg min
kK d(utk, At2k)
t [t2 ms, t2  ms],
where d() denotes the Euclidean distance, K represents the
overlapping range between ut11:t1ma and A, K is the
cardinality of K, and ms is the predefined search range. The
resulting tu indicates that utu:tuk corresponds to At2:t2k.
After establishing the correspondence, inspired by ACT ,
we employ temporal ensemble to further enhance the smooth-
ness of the actions. Specifically, the action buffer A is updated
as follows:
At2k  wnew
0 k A t2,
k > A t2,
where the weights wnew
of ut11:t1ma are computed fol-
lowing an exponential distribution parameterized by the hyper
parameter . For the action manager, the weight of each time
step is initialized to 0. After performing temporal ensemble,
the weight of each time step is updated by adding the corre-
sponding weight of the predicted action, i.e.:
ti  exp(i), i [1, ma],
2) Action Interpolation: Additionally, considering the rel-
atively low frequency of the policy networks action sequence
output compared to the high frequency required for actual
action execution, we perform linear interpolation for actions
at finer time step . Specifically, for  located between t and
t  1, the interpolated action can be expressed as:
A  ( t)At  (t  1 )At1.
Input Relative Pose Sequence
Estimation
Prediction
Predicted Relative Pose Sequence
Estimation
Stabilizer
By utilizing the pose at action generation, the predicted pose, and
the extrinsic parameters between the robotic arm and the pose estimator
as inputs, the stabilizer compensates for each action output by the action
counteracting platform motion.
The continuous merging of action sequences into the action
buffer ensures that interpolation is always performed inter-
C. Prediction Compensation Based Stabilizer
After obtaining high-frequency actions in real time through
the action manager, we still need to address the impact of
the robots motion on manipulation, such as the robots active
movement or the wobble caused by its instability. The most
direct approach involves monitoring the robots motion and
using this data to refine its arm actions. However, consid-
ering computation, communication and control delays, this
method adjusts the robotic arms action using outdated robot
position data, which significantly reduces the success rate
of high-precision manipulation tasks. Therefore, we design a
prediction compensation based stabilizer, which predicts the
platforms motion over a short future period and modifies the
actions to counteract the motion effectively.
As shown in Fig. 3, we first implement a pose prediction
module incorporating two parts: a SLAM module for pose
estimation and a lightweight prediction network. In terms of
data flow, we employ a real-time, high-frequency visual SLAM
using stereo camera with an IMU1 to obtain a high-frequency
sequence of poses. Then, for each prediction in prediction
network M, we use the current pose p0 as the base coordinate
system and calculate the sequence of high-frequency relative
pose Pl0:1  {pl0, ..., p1} from the past l0 frames,
where pi  p1
pi and Pl0:1 serves as the input
to the prediction network. The prediction network directly
outputs the future l1 frames high-frequency relative pose
sequence P
l1}, which predicts the future
pose by p
i  p0  p
Considering the limitation of computational resources and
the requirements for system real-time performance, we im-
plement a lightweight prediction network M. Specifically, we
parallelize an LSTM  and a GRU , which process
the same input. We concatenate their corresponding temporal
outputs along the feature dimension and finally pass them
through a fully connected layer to produce the prediction
Realsense
Realsense
(a) MT-3DoF
Platform
Realsense
Follower Arm
Leader Arm
(b) MF-5DoF
Programmable
Leadscrew
(c) LS-3DoF
Matrice 300
(d) UAV-3DoF
Four experimental configurations we design. Each is equipped with two cameras for observation inputs and a T265 camera for pose estimation. The
robotic arms operate independently of the robot platforms, with MT-3DoF, LS-3DoF and UAV-3DoF sharing the same arm.
Pick and Place
Stack Cylinder
Get the Delivery
UAV-3DoF
Get the Delivery
We design three tasks of varying difficulty levels and conduct experiments across three experimental configurations.
M(x)  FC(concat(LSTM(x), GRU(x))).
target action A can be expressed as:
tf)1  E,
where E denotes the extrinsics between the visual SLAM
coordinate system and the arm coordinate system, manually
pre-calibrated; t represents the time interval between the
generation of A by policy network  and its execution; f
indicates the output frequency of the SLAM system. t, which
will be estimated online in Sec. III-D, is the total latency
spanning from pose estimation to action execution, incorporat-
ing processing time for pose estimation, pose prediction, data
ultimately executed by the robot becomes
D. Latency Estimation
To align STDArms optimization objectives with task exe-
cution goals, we implement a pre-mission warm-up routine to
perform online calibration of system latency, thereby enabling
the stabilizer module to accurately adjust the actions. To
achieve this, the stabilizer module is employed to help the
robot do the end-effector stabilization task, enabling quanti-
tative assessment of various latencies impact on operational
stability. The gripper maintains hold of a red marker while
a fixed monocular third-view camera tracks its positional
variance during robotic arm base moving. We conduct a
linear search for the latency starting from zero, identifying the
latency that minimizes the markers movement as the system
delay. To ensure that variations of the robots movement dont
affect the correctness of the latency estimation, we adopt the
velocity ratio between the markers pixel-plane motion and the
VSLAM-estimated platform motion as the evaluation metric.
IV. IMPLEMENTATION
A. Hardware
Our STDArm is a modular mobile manipulation system,
which consists of three core components: 1) a reconfigurable
mobile base platform, 2) a multi-DoF robot arm, and 3) a
computing platform. For imitation learning policies inputs, the
arm is equipped with two RGB cameras: one near the end
effector to monitor interaction dynamics and another at the arm
base to observe objects of interest. A forward-facing camera
is additionally integrated to support real-time pose estimation.
To validate system versatility, as illustrated in Fig. 4, we
implement four experimental configurations:
armroarm-m2-s.htm
All configurations utilize an NVIDIA Jetson AGX Xavier
Developer Kit 5 as the computing platform. Specially, all
components of UAV-3DoF except the drone are powered via
USB interfaces from the drone.
B. Tasks Design
As shown in the Fig. 5, we design three tasks with varying
requirements for operational precision as follows:
Pick and Place. We first conduct a classic task in
robot manipulation, where the robot needs to pick up
a block and throw it into a small container. The block
is approximately 3 cm wide, and the pen holder has a
diameter of about 8 cm.
Stack Cylinder. We attempt to stack two cylinders, each
with a diameter of 8 cm. This requires the robotic arm to
have a control precision within 4 cm, as any deviation
beyond 4 cm results in a failed stack. To facilitate
of 2.5 cm to one of the cylinders.
Get the Delivery. Finally, we attempt to complete a
real-world taskpicking up takeout. The robot needs to
extend its gripper near the handle of a bag or basket,
grasp the bag, and lift it. We successfully perform this
task using a drone as well.
These tasks are validated on MT-3DoF and MF-5DoF.
tested. This limitation is that the wind generated by the drone
can blow away the target objects of the other two tasks.
V. EXPERIMENTS
A. Experimental Setup
For each task described in Sec.IV-B, we gather 50 sequences
of stationary state data to train the policy network, with target
objects randomly placed within the robotic arms reachable
workspace. Additionally, for each configuration outlined in
Sec.IV-A, we collect approximately five minutes of motion
data to train the pose prediction network.
In the case of MT-3DoF and MF-5DoF, we simulate mobile
scenarios by manually shaking the platforms. To ensure fair-
shaking the platform is unaware of the specific algorithm being
tested and receives no visual feedback during the process,
focusing solely on generating random perturbations. This
methodology not only ensures impartiality but also inherently
demonstrates the robustness of our approach by leveraging the
natural variability of human motion.
For LS-3DoF, a programmable leadscrew moves the arm
base at 12 cms to provide perturbations. Since the motion
systemsjetson-agx-xavier
is precisely controllable, LS-3DoF provides reproducible ex-
perimental conditions, but its regular motion makes trajectory
prediction straightforward. For this reason, we only employ it
for ablation studies
Regarding UAV-3DoF, the aerial platforms inherent in-
stability arises from its dynamic manipulation environment,
including self-induced aerodynamic disturbances and limited
positional precision during hovering. We leverage these natu-
ral instability characteristics rather than introducing artificial
perturbations. Notably, to accommodate the manipulations
operational range constraints and the drones finite positional
holding capacity, an operator manually maintains the relative
positioning between the drone and the delivery target. This
operational paradigm is adopted because that fully autonomous
drone navigation exceeds the scope of this study.
As for the specific network architecture, we employ ResNet-
18  as vision backbone of the policy network, with input
images of shape 3480640. The observation and prediction
horizons are configured differently for each method: For DP,
we set the observation horizon to 2 and action prediction
horizon to 8 at 5Hz, utilizing a transformer  combined
with DDIM  for denoising with 20 steps during inference.
For ACT, we configure the observation horizon to 1 and action
prediction horizon to 100 at 30Hz. In the action manager,
actions are interpolated to 50Hz, and the temporal ensemble
parameter  is set to 1. The pose prediction network takes a 1s
pose sequence as input and generates 0.5s predictions, where
pose frequency is 200Hz. This configuration ensures optimal
real-time performance under constraints including inference
variability. The code framework is adapted from the lerobot
codebase .
B. Experiment Results
The experimental results presented in Tab. I demonstrate
the efficacy of STDArm in enabling policy networks trained
under static conditions to operate effectively on dynamically
unstable platforms. Under static configurations, baseline meth-
ods (DP and ACT) achieve satisfactory performance across
all tasks, MF-5DoF attaining perfect scores in all tasks with
ACT. However, dynamic scenarios reveal severe performance
degradation for baseline methods, particularly for precision-
demanding tasks like Pick and Place (up to 0 success rate)
and Stack Cylinder (up to 13.3 success rate).
The integration of STDArm yields significant improvements,
with 46.7-53.3 relative performance recovery in dynamic
Pick and Place and Stack Cylinder. Moreover, MT-3DoF
and MF-5DoF achieved improvements of 73.3 and 60
in Get the delivery, respectively. Notably, the aerial platform
(UAV-3DoF) exhibits distinct behavioral characteristics, where
baseline methods achieve only 13.3 success rate in dynamic
Get the Delivery, while STDArm-enhanced policies demon-
strate a 200 performance improvement (40 success rate).
This performance gap highlights the methods adaptability to
different instability sources, ranging from manual platform
perturbations (MT-3DoFMF-5DoF) to inherent aerodynamic
SUCCESS RATE OF OUR CONFIGURATIONS AND TASKS. THE BEST IN DYNAMIC IS IN BOLD.
Operation Mode
Configurations
Pick and Place
Stack Cylinder
Get the Delivery
DPSTDArm
ACTSTDArm
DPSTDArm
ACTSTDArm
UAV-3DoF
DPSTDArm
Pick and Place
Stack Cylinder
Get the Delivery
UAV-3DoF
Timeline
Temporal task progression during experiments. Beyond task completion metrics, the motion of the platforms also worth noting. Videos on the
project page visually demonstrate that the STDArm enables real-time compensation for platform motion disturbances while preserving the original policys
manipulation capabilities.
uncertainties (UAV-3DoF). Fig. 6 illustrates the robotic arms
task execution process during platform movement.
It should be noted that we use two different robotic arms
and two distinct policies to demonstrate STDArms plug-and-
play capability. Consequently, we did not conduct experiments
to compare different policies (e.g., DP vs. ACT) on th
