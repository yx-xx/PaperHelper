=== PDF文件: π₀ A Vision-Language-Action Flow Model for General Robot Control.pdf ===
=== 时间: 2025-07-22 15:47:47.919993 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：General Robot Control
Physical Intelligence
Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai,
Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine,
Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong,
Anna Walling, Haohuan Wang, Ury Zhilinsky
Z)0.GSO,SGL
mlhg('(G0)S,G
2.G.G,){z
2G)SO,SG)G
O()(.0G.G
GS2),.G.
p(p),GG.)
GS2)2.SG),.G.
O0p),)p.
.G20G)22G.)G
Fig. 1: Our generalist robot policy uses a pre-trained vision-language model (VLM) backbone, as well as a diverse cross-
embodiment dataset with a variety of dexterous manipulation tasks. The model is adapted to robot control by adding a separate
action expert that produces continuous actions via flow matching, enabling precise and fluent manipulation skills. The model
can then be used directly to perform tasks based on a prompt, or fine-tuned on high-quality data to enable complex multi-stage
AbstractRobot learning holds tremendous promise to unlock
the full potential of flexible, general, and dexterous robot systems,
as well as to address some of the deepest questions in artificial
intelligence. However, bringing robot learning to the level of
generality required for effective real-world systems faces major
obstacles in terms of data, generalization, and robustness. In
this paper, we discuss how generalist robot policies (i.e., robot
foundation models) can address these challenges, and how we can
design effective generalist robot policies for complex and highly
dexterous tasks. We propose a novel flow matching architecture
Physical Intelligence, San Francisco, California, USA. Correspondance to:
researchphysicalintelligence.company
built on top of a pre-trained vision-language model (VLM)
to inherit Internet-scale semantic knowledge. We then discuss
how this model can be trained on a large and diverse dataset
from multiple dexterous robot platforms, including single-arm
our model in terms of its ability to perform tasks via direct
high-level VLM policy, and its ability to acquire new skills via
fine-tuning. Our results cover a wide variety of tasks, such as
laundry folding, table cleaning, and assembling boxes.
Fig. 2: 0 controls a mobile manipulator to fold laundry. Our model is pre-trained on diverse data from 7 distinct robot
configurations and 68 tasks, and can then either be prompted directly or fine-tuned to complex downstream tasks, as in the
case of this laundry folding policy, which fetches laundry from a dryer, packs it into a hamper, brings the hamper to a folding
I. INTRODUCTION
A human being should be able to change a diaper, plan
an invasion, butcher a hog, conn a ship, design a
new problem, pitch manure, program a computer, cook
a tasty meal, fight efficiently, die gallantly.
Specialization is for insects.
Robert A. Heinlein, Time Enough for Love
Artificial intelligence systems come in all shapes and sizes,
from highly specialized systems that solve complex prob-
lems inaccessible to the human mind, such as predicting
the conformation of a protein , to systems that can
produce lifelike high-resolution images or videos based on
textual prompts . However, the axis along which human
intelligence most outpaces machine intelligence is versatility:
the ability to solve diverse tasks situated in varied physical
tions. Perhaps the most tangible progress toward this kind of
versatility in AI can be seen in large language- and vision-
language models [1, 48]: systems that are pre-trained on large
and very diverse corpora of images and text from the web,
and then fine-tuned (aligned) using more carefully curated
datasets meant to induce the desired pattern of behavior
and responsiveness. While such models have been shown
to exhibit broad instruction-following and problem-solving
abilities [53, 27], they are not truly situated in a physical world
the way that people are, and their understanding of physical
interaction is based entirely on abstract descriptions. If such
methods are to make tangible progress toward AI systems that
exhibit the kind of physically situated versatility that people
that is, data from embodied robot agents.
Flexible and general-purpose models that can be tasked
to perform a variety of robot behaviors have tremendous
practical ramifications, but they may also offer solutions to
some of the toughest challenges facing robot learning today,
such as availability of data, generalization, and robustness. In
natural language  and computer vision , general-purpose
foundation models that are pre-trained on diverse multi-task
data tend to outperform narrowly tailored and specialized
solutions. For example, if the goal is to recognize birds in
different image-language associations and then fine-tune or
prompt for the bird recognition task, than it is to train on only
bird recognition data. Similarly, we may find that for effective
specialized robot systems, it is more effective to first pre-train
on highly diverse robot data, and then fine-tune or prompt for
the desired task. This can resolve the data scarcity challenge,
because many more sources of data are available to a generalist
model  including data from other tasks, other robots, or
even non-robot sources  and it may resolve robustness and
generalization challenges, because the diverse data exhibits
a greater coverage of observations and actions, providing a
variety of scenes, corrections, and recovery behaviors that
might not be present in more narrow specialized data. Thus,
adopting a large-scale pre-training approach to robot learning
has the potential to address many of the fields challenges
and make practical learning-enabled robots a reality, while
at the same time furthering our understanding of the deepest
problems in artificial intelligence.
robot foundation models  involves a number of major
challenges. First, any such research must be done at a very
large scale, because the full benefits of large-scale pre-training
are often not present at smaller scales . Second, it requires
developing the right model architectures that can effectively
make use of diverse data sources, while at the same time being
able to represent the intricate and subtle behaviors necessary
to interact with complex physical scenes. Third, it requires
the right training recipe. This is perhaps the most important
in NLP and computer vision has relied heavily on delicate
strategies for curating pre-training and post-training data .
In this paper, we present a prototype model and learning
these three bottlenecks could be tackled. We illustrate our
model and system in Figure 1. To incorporate diverse data
model (VLM) to import Internet-scale experience. By basing
our model on a VLM, we inherit the general knowledge,
semantic reasoning, and problem-solving abilities of language-
and vision-language models. We then further train our model
to incorporate robot actions, turning it into a vision-language-
action (VLA) model . In order to make it feasible to utilize
a variety of diverse robot data sources, we employ cross-
embodiment training , where data from many robot types
is combined into the same model. These different robot types
have different configuration spaces and action representations,
including single and dual-arm systems, as well as mobile
manipulators. Additionally, in order to make it possible to
perform highly dexterous and intricate physical tasks, we use
an action chunking architecture  with flow matching (a
variant of diffusion) to represent complex continuous action
distributions [28, 32]. This enables our model to control robots
at frequencies of up to 50 Hz for dexterous tasks such as
laundry folding (see Figure 1). To combine flow matching
with VLMs, we use a novel action expert that augments the
standard VLM with flow-based outputs.
As with language models, the architecture of our model is
only part of our method. In order to flexibly and robustly
perform complex tasks, we need the right training recipe.
Our recipe mirrors the pre-trainingpost-training separation
commonly seen in exascale language- and image-language
models [1, 48], where the model is first pre-trained on a very
large and diverse corpus, and then fine-tuned on more narrow
and more carefully curated data to induce the desired pattern of
behavior  in our case, dexterity, efficiency, and robustness.
the model how to recover from mistakes, since mistakes are
rarely seen in such data. Training on only lower-quality pre-
training data does not teach the model to act efficiently and
robustly. Combining both provides the desired behavior: the
model attempts insofar as possible to act in a manner similar
to the high-quality data, but still has a repertoire of recoveries
and corrections that it can deploy in the case of a mistake.
The contributions of our work consist of a novel generalist
robot policy architecture based on VLM pre-training and flow
training recipes for such robot foundation models. We evaluate
our model out of the box with language commands, with
fine-tuning to downstream tasks, and in combination with a
high-level semantic policy that outputs intermediate language
commands to perform complex and temporally extended tasks.
While our model and system make use of a variety of ideas
presented in recent work, the combination of ingredients is
dexterity and generality that goes significantly beyond pre-
viously demonstrated robot foundation models. We evaluate
our approach by pre-training on over 10,000 hours of robot
laundry folding (see Figure 2), clearing a table, putting dishes
in a microwave, stacking eggs into a carton, assembling a box,
and bagging groceries.
II. RELATED WORK
Our work builds on recently proposed methods in large-
scale robot learning, as well as multimodal language models.
Our work is most closely related to recently proposed vision-
language action (VLA) models, which use pre-trained VLMs
that are fine-tuned for robot control [7, 24, 55]. Such models
employ autoregressive discretization to represent actions in a
manner analogous to text tokens. In contrast, our model em-
ploys a novel design that fine-tunes a VLM to produce actions
via flow matching [32, 28], a variant of diffusion [20, 46].
This allows us to handle high-frequency action chunks
(up to 50 Hz) and highly dexterous tasks, which we show
pose a major challenge for prior autoregressive VLAs . This
resembles a number of recent works on diffusion models for
action generation [9, 60]. In contrast to these works, our model
uses a pre-trained VLM backbone . Our contribution is also
fundamentally integrative, focusing on a framework for robot
foundation models, including not only the model architecture
itself but also a pre-training recipe, pre-training and post-
training phases, and a range of real-world experiments.
Outside of robot control, many models have been proposed
that combine pre-trained language models with diffusion [40,
and autoregressive large language models [19, 29, 59]. Such
models are typically concerned with image generation, but
our action generation model builds on a number of previously
proposed concepts. Like Zhou et al. , we train our model
via a diffusion-style (flow matching) loss applied on individual
sequence elements, in lieu of the standard cross-entropy loss
for decoder-only transformers. Like Liu et al. , we use
a separate set of weights for the tokens corresponding to
diffusion. Incorporating these concepts into a VLA model, we
introduce what to our knowledge is the first flow matching
VLA that produces high-frequency action chunks for dexterous
control.
Our work also builds on a rich history of prior works on
large-scale robot learning. Early work in this area often utilized
self-supervised or autonomous data collection [26, 22, 8],
providing a tractable data source for simple tasks such as
grasping [18, 37] or pushing , but without the complexity
of more dexterous behaviors. More recently, a number of high-
quality datasets have been collected for robot control that
enable broad generalization [23, 10, 52, 33, 34, 43, 13, 6], but
typically for simpler tasks that consist of object relocation and
rudimentary furniture manipulation (e.g., drawer opening) [31,
15]. More dexterous tasks have been studied at a smaller
equivalent to 10 or less hours. Since one of our aims is to
study complex and dexterous behaviors, we utilize a much
larger dataset, with about 10,000 hours of demonstrations,
complemented by the open-source OXE dataset . To our
experiment in terms of the amount of robot data. At this scale,
we show that a more sophisticated pre-trainingpost-training
recipe is highly effective  analogously to the recipes used
for large language models, a pre-training phase endows our
model with a broad base of knowledge, which is then refined
in a post-training phase with higher-quality curated data to
achieve the desired behavior.
The complexity of the tasks we illustrate goes significantly
beyond prior work. While recent work has illustrated a number
XZUYTSXQWXOVPRRN[
Fig. 3: Overview of our framework. We start with a pre-training mixture, which consists of both our own dexterous
manipulation datasets and open-source data. We use this mixture to train our flow matching VLA model, which consists
of a larger VLM backbone and a smaller action expert for processing robot states and actions. The VLM backbone weights
are initialized from PaliGemma , providing representations learned from large-scale Internet pre-training. The resulting 0
model can be used to control multiple robot embodiments with differing action spaces to accomplish a wide variety of tasks.
of more complex and dexterous behaviors, such as tying
shoelaces  or cooking shrimp , we show that our
framework can learn very long tasks, sometimes tens of
minutes in length, for behaviors that combine both physical
dexterity and combinatorial complexity. For example, our
laundry folding task requires the robot to manipulate a variety
of clothing items that can start in any configuration, and fold
multiple items in sequence. Our table bussing task requires
discerning the class of novel objects (trash or dishes). We
show that a single cross-embodiment model can be used as
the base model for these tasks. To our knowledge, our work
demonstrates the longest dexterous tasks in the end-to-end
robot learning literature.
III. OVERVIEW
We provide an outline of our model and training procedure
in Figure 3. In our training framework, we first assemble
a pre-training mixture consisting of a weighted combination
of our own dexterous manipulation datasets (Section V-C),
collected on 7 different robot configurations for 68 different
from 22 robots. The pre-training phase (Section V-A) also uses
diverse language labels, combining task names and segment
annotations (fine-grained labels for sub-trajectories, typically
about 2 seconds in length). The purpose of the pre-training
phase is to train a base model that exhibits broad capabilities
and generalization, but is not necessarily specialized for high
performance on any one task. This base model can follow
language commands and perform a variety of tasks at rudi-
mentary proficiency. For complex and dexterous tasks, we
then employ a post-training procedure (Section V-A), which
uses high-quality curated data to adapt the model to specific
downstream tasks. We study both efficient post-training with
small to moderate amounts of data, and high-quality post-
training with larger datasets for complex tasks such as laundry
folding and mobile manipulation.
Our model, which we describe in Section IV, is based on the
PaliGemma vision-language model , which we then further
train with our data mixture. To turn the base PaliGemma VLM
into 0, we add action outputs that use flow matching [32, 28]
to generate continuous action distributions. We describe this
design in detail in the following section. Note that we use
PaliGemma for convenience and because of its comparatively
small size (which is useful for real-time control), but our
framework is compatible with any base pre-trained VLM.
IV. THE 0 MODEL
The 0 model, illustrated in Figure 3, consists primarily
of a language model transformer backbone. Following the
standard late fusion VLM recipe [3, 11, 30], image encoders
embed the robots image observations into the same em-
bedding space as language tokens. We further augment this
backbone with robotics-specific inputs and outputs  namely,
proprioceptive state and robot actions. 0 uses conditional
flow matching [28, 32] to model the continuous distribution
of actions. Flow matching provides our model with high
precision and multimodal modeling capability, making it es-
pecially well suited to high-frequency dexterous tasks. Our
architecture is inspired by Transfusion , which trains a
single transformer using multiple objectives, with tokens1
corresponding to continuous outputs supervised via a flow
matching loss and tokens corresponding to discrete outputs
supervised via a cross-entropy loss. Building on Transfusion,
we additionally found that using a separate set of weights
for the robotics-specific (action and state) tokens led to an
improvement in performance. This design is analogous to a
mixture of experts [45, 25, 12, 16] with two mixture elements,
where the first element is used for image and text inputs, and
1In this paper, we use the word token to refer to an inputoutput slot along
the sequence dimension, whether the slot corresponds to a discrete variable
(e.g., a language token) or a continuous variable (e.g., an image patch or a
robot action).
the second is used for robotics-specific inputs and outputs. We
refer to the second set of weights as the action expert.
where At  [at, at1, ..., atH1] corresponds to an action
chunk of future actions (we use H  50 for our tasks), and ot
is an observation. The observation consists of multiple RGB
t is ith image
(with 2 or 3 images per robot), t is a sequence of language
and state qt are encoded via corresponding encoders and then
projected via a linear projection layer into the same embedding
space as the language tokens.
For each action at in the action chunk At, we have a
corresponding action token that we feed through the action
expert. During training, we supervise these action tokens using
a conditional flow matching loss [28, 32],
L()  Ep(Atot),q(A
t At)v(A
where subscripts denote robot timesteps and superscripts
denote flow matching timesteps, with
[0, 1]. Recent
work in high-resolution image  and video  synthe-
sis has shown that flow matching can achieve strong em-
pirical performance when combined with a simple linear-
Gaussian (or optimal transport) probability path , given
t At)  N(At, (1 )I). In practice, the network
is trained by sampling random noise  N(0, I), computing
the noisy actions A
t  At  (1 ), and then training
the network outputs v(A
field u(A
t At)  At . The action expert uses a full
bidirectional attention mask, so that all action tokens attend
to each other. During training, we sample the flow matching
timestep  from a beta distribution that emphasizes lower
(noisier) timesteps. See Appendix B for more details.
At inference time, we generate actions by integrating the
learned vector field from   0 to   1, starting with random
noise A0
t N(0, I). We use the forward Euler integration
where  is the integration step size. We use 10 integration
steps (corresponding to   0.1) in our experiments. Note
that inference can be implemented efficiently by caching
the attention keys and values for the prefix ot and only
recomputing the suffix corresponding to the action tokens for
each integration step. We provide more details regarding the
inference procedure, including the inference time for each part
of the model, in Appendix D.
While in principle our model can be initialized from scratch
or fine-tuned from any VLM backbone, in practice we use
PaliGemma  as our base model. PaliGemma is an open-
source 3 billion parameter VLM that offers a convenient trade-
off between size and performance. We add 300M parameters
for the action expert (which is initialized from scratch) for a
total of 3.3 billion parameters. We provide a full description
of the model architecture in Appendix B.
Non-VLM baseline model. In addition to our main VLA
use a VLM initialization for ablation experiments. This model,
which we refer to as 0-small, has 470M parameters, does not
use VLM initialization, and has a number of small differences
that we found to be helpful for training on our data without
VLM initialization, which are summarized in Appendix C.
This model is used in our comparisons to evaluate the benefits
of incorporating VLM pertaining.
V. DATA COLLECTION AND TRAINING RECIPE
Broadly capable robot foundation models require not only
an expressive and powerful architecture, but also the right
dataset and, more importantly, the right training recipe. In
the same way that LLM training is typically divided into
pre-training and post-training phases, we employ a multi-
stage training procedure for our model. The goal of the pre-
training phase is to expose the model to a diverse range of
tasks so that it can acquire broadly applicable and general
physical capabilities, while the goal of the post-training phase
is to provide the model with the ability to skillfully and
fluently execute the desired downstream task. Because of
datasets are distinct: the pre-training dataset should cover
as many tasks as possible, and within each of those tasks
should cover a diversity of behaviors. The post-training dataset
should instead cover behaviors that are conducive to effective
task execution, which should exhibit a consistent and fluent
strategy. Intuitively, the diverse (but lower quality) pre-training
data allows the model to recover from mistakes and handle
highly varied situations, which might not otherwise occur in
the high-quality post-training data, while the post-training data
teaches the model to perform the task well.
A. Pre-training and post-training
Fig. 4: Overview of our dataset: The pre-training mixture
consists of a subset of OXE  and the  dataset. We use
a subset of OXE, which we refer to as OXE Magic Soup
. The right figure illustrates the weight of the different
datasets in the pre-training mixture. The left figure illustrates
their relative sizes as measured by the number of steps.
We provide an overview of our pre-training mixture in Fig-
ure 4. Since each training example corresponds to a timestep
i.e., a tuple (ot, At),  we will quantify data in terms
of timesteps in this discussion. 9.1 of the training mixture
consists of open-source datasets, including OXE , Bridge
datasets typically have one or two cameras and use low-
frequency control, between 2 and 10 Hz. However, these
datasets cover a wide range of objects and environments. To
learn dexterous and more complex tasks, we also use 903M
timesteps of data from our own datasets, where 106M steps are
from single-arm robots and 797M are from dual-arm robots.
This data has 68 tasks, where each task is composed of
complex behaviors  e.g., the bussing task involves putting
a wide range of different dishes, cups, and utensils into a
bussing bin, and a wide array of trash items into the garbage.
Note that this definition of task is significantly different from
prior work, which typically uses any combination of noun
and verb (e.g., pick up the cup vs. pick up the plate)
to constitute a distinct task. Therefore, the actual range of
behaviors in our dataset is significantly broader than this
number of tasks would imply. We discuss the specific robots
and tasks in our dataset in more detail in Section V-C.
Since the datasets are somewhat imbalanced in size (e.g.,
the more difficult laundry folding tasks are overrepresented),
we weight each task-robot combination by n0.43, where n
is the number of samples for that combination, such that
over-represented combinations are down-weighted. The con-
figuration vector qt and action vectors at always have the
dimensionality of the largest robot in the dataset (18 in our
dimensional configuration and action spaces, we zero-pad the
configuration and action vectors. For robots with fewer than
three images, we also mask out the missing image slots.
In the post-training phase, we fine-tune our model with a
smaller task-specific dataset to specialize it to particular down-
stream applications. As mentioned previously, our definition
of task is fairly broad  e.g., the bussing task requires
manipulating a wide range of different objects. Different tasks
require very different datasets, with the simplest of the tasks
necessitating only 5 hours and the most complex tasks using
100 or more hours of data.
B. Language and high-level policies
More complex tasks that require semantic reasoning and
high-level strategy, such as table bussing, can also benefit from
a high-level policy that decomposes high-level tasks (such as
bus the table) into more immediate subtasks (such as pick
up the napkin or throw the napkin into the trash). Since
our model is trained to process language inputs, we can use a
high-level VLM to make these semantic inferences, a method
that is analogous to LLMVLM planning methods such as
SayCan . We use such a high-level policy to assist our
model with high-level strategy for several of our experimental
C. Robot system details
Our dexterous manipulation datasets include 7 different
robot configurations and 68 tasks. We summarize these plat-
forms in Figure 5, and discuss them below:
Bimanual Trossen
Bimanual ARX
Bimanual UR5e
Mobile Fibocom
Mobile Trossen
Fig. 5: The robots used in our experiments. These include
single and dual-arm manipulators with 6-DoF and 7-DoF arms,
as well as holonomic and nonholonomic mobile manipulators.
0 is trained jointly on all of these platforms.
UR5e. An arm with a parallel jaw gripper, with a wrist-
mounted and over-the-shoulder camera, for a total of two
camera images and a 7-dimensional configuration and action
Bimanual UR5e. Two UR5e setups, for a total of three camera
images and a 14-dimensional configuration and action space.
Franka. The Franka setup has two cameras and an 8-
dimensional configuration and action space.
Bimanual Trossen. This setup has two 6-DoF Trossen ViperX
arms in a configuration based on the ALOHA setup [4, 57],
with two wrist cameras and a base camera, and a 14-
dimensional configuration and action space.
Bimanual ARX  bimanual AgileX. This setup uses two
6-DoF arms, and supports either ARX or AgileX arms, with
three cameras (two wrist and one base) and a 14-dimensional
configuration and action space. This class encompasses two
distinct platforms, but we categorize them together because of
their similar kinematic properties.
Mobile Trossen  mobile ARX. This setup is based on the
Mobile ALOHA  platform, with two 6-DoF arms on a
mobile base, which are either ARX arms or Trossen ViperX
arms. The nonholonomic base adds two action dimensions,
for a 14-dimensional configuration and 16-dimensional action
space. There are two wrist cameras and a base camera. This
class encompasses two distinct platforms, but we categorize
them together because of their similar kinematic properties.
Mobile Fibocom. Two 6-DoF ARX arms on a holonomic base.
The base adds three action dimensions (two for translation and
one for orientation), for a 14-dimensional configuration and
17-dimensional action space.
We summarize the proportion of our dataset from each robot
in Figure 4.
VI. EXPERIMENTAL EVALUATION
Our experimental evaluation consists of out-of-box evalu-
ation experiments that compare our base (pre-trained) model
Fig. 6: Out-of-box evaluation tasks: To evaluate our base
bussing easy, bussing hard, grocery bagging, and toast
out of toaster. The tasks require a combination of dexterous
to alternative model designs with direct prompting, as well as
detailed fine-tuning experiments that evaluate our model on
challenging downstream tasks, comparing it to other methods
that have been proposed for dexterous manipulation. We study
the following research questions:
How well does 0 perform after pre-training on a variety
of tasks that are present in the pre-training data? We study
this question by directly evaluating 0, with comparisons to
other robot foundation models.
How well does 0 follow language commands? These
experiments compare 0 to 0-small, a smaller version of our
model without VLM initialization, to evaluate its performance
on following language commands. We evaluate with both
human-provided commands and commands specified by a
high-level VLM policy, as discussed in Section V-B.
How does 0 compare to methods that have been proposed
specifically for addressing dexterous manipulation tasks?
These experiments study downstream tasks for which we can
either fine-tune our model from the pre-trained initialization,
or train it from scratch on task-specific data, comparing to
prior methods that were proposed for dexterous manipulation.
We aim to evaluate both the benefits of our architecture and
our pre-training procedure.
Can 0 be adapted to complex, multi-stage tasks? In our
final set of experiments, we fine-tune 0 to a set of particularly
complex tasks, including folding laundry and bussing a table.
These tasks take between 5 and 20 minutes to complete. Some
require guidance from a high-level policy.
A. Evaluating the base model
In our first set of experiments, we evaluate the model after
pre-training on our full mixture, without any post-training,
to evaluate how well our base model can perform a variety
of tasks. We compare to other robot foundation models in
the literature: both VLAs and smaller models that are trained
from scratch on the same pre-training mixture. We evaluate
on the following tasks, visualized in Figure 6, with each task
commanded to the same base model via a language command.
Shirt folding: the robot must fold a t-shirt, which starts
flattened.
Bussing easy: the robot must clean a table, putting trash in the
trash bin and dishes into the dish bin. The score indicates the
number of objects that were placed in the correct receptacle.
Bussing hard: a harder version of the bussing task, with more
objects and more challenging configurations, such as utensils
intentionally placed on top of trash objects, objects obstructing
each other, and some objects that are not in the pre-training
dataset.
Grocery bagging: the robot must bag all grocery items, such
as potato chips, marshmallows, and cat food.
Toast out of toaster: the robot removes toast from a toaster.
Providing comparisons for these experiments is challenging
because very few prior models can operate at this scale. We
compare to OpenVLA , a 7B parameter VLA model that
was originally trained on the OXE dataset . We train
OpenVLA on our full mixture. This is a very difficult mixture
for OpenVLA, which does not support action chunking or
high-frequency control. We also compare to Octo , a
smaller 93M parameter model. While Octo is not a VLA, it
does use a diffusion process to generate actions, providing a
valuable point of comparison for our flow matching VLA. We
also train Octo on the same mixture as our model. Due to time
the same number of epochs as our full model. We therefore
also compare to a compute parity version of our model,
which is trained for only 160k steps (as opposed to 700k
steps for our main model), which is equal to or lower than the
number of steps provided to the baselines (160k for OpenVLA,
320k for Octo). We also include a version of the OpenVLA
model that we fine-tuned only on the UR5e data, without
cross-embodiment training, in the hopes of providing an even
stronger baseline on the UR5e tasks. Finally, we include a
comparison to the 0-small model described in Section IV,
which can be viewed as a scaled-down version of our model
without VLM pre-training.
The evaluation metric uses a normalized score averaged over
10 episodes per task and method, where an episode receives a
score of 1.0 for a full success, and a fractional score for partial
success. For example, the score for bussing is the fraction of
objects that are correctly placed in the proper receptacle. We
describe the scoring rubrics in Appendix E. The results, shown
in Figure 7, show that 0 attains by far the best results across
the board on all the out-of-box tasks, with near perfect success
rates on shirt folding and the easier bussing tasks, and large
improvements over all baselines. The parity version of 0,
which is trained for only 160k steps, still outperforms all the
OpenVLA struggles on these tasks because its autoregressive
discretization architecture does not support action chunks. The
UR5e-only OpenVLA model performs better, but is still far
Shirt Folding
(Bi-ARX)
Bussing Easy
Bussing Hard
Grocery Bagging
(Bi-Trossen)
Average Task Progress
Direct Prompting (Out-of-Box)
Performance Across Tasks
0 (parity)
OpenVLA (UR5e only)
Fig. 7: Out-of-box evaluation results: We evaluate 0 trained
for the full 700k steps, a version trained for 160k steps that
matches the number of updates for baseline models, 0-small,
and three baselines: OpenVLA and Octo trained on all of our
we found to work better on UR5e tasks). Across all tasks
and all comparisons, even the parity version of our model
outperforms all baselines, and the full version of our model
achieves the best results by a large margin.
below the performance of 0. Octo does support action chunks,
but has a comparatively limited representational capacity. This
comparison illustrates the importance of combining large,
expressive architectures with the ability to model complex
distributions via flow matching or diffusion. Additionally, the
comparison to 0-small illustrates the importance of incor-
porating VLM pre-training. Unfortunately, it is hard to make
this last comparison fair: 0-small uses fewer parameters, but
larger models are difficult to use without pre-training. Overall,
these experiments show that 0 provides a powerful pre-
trained model with the ability to effectively perform a variety
of tasks with a variety of robots, with much better performance
than prior models.
B. Following language commands
In the next set of experiments, we fine-tune the base 0
model to follow language commands in a set of evaluation
domains. We compare this fine-tuned 0 model with the 0-
small model described in Section IV, which we found to
be the strongest baseline in the previous section. Recall that
0-small does not use a VLM initialization. This experi-
ment therefore aims to measure how much VLM pre-training
boosts our models ability to follow language instructions.
Note that 0-small is also a significantly smaller model
VLM initialization serves both to make it practical to train
a much larger model without overfitting, and to improve
language instruction following. We nonetheless hope that this
experiment sheds light on the language capabilities of 0.
The language instructions for each task consist of objects to
pick up and locations to place those objects, with language-
labeled segments that are about 2 seconds in length. Each full
Fig. 8: The tasks in our language evaluation. We evaluate
our model on 3 different language-conditioned tasks, each of
which requires following a sequence of intermediate language
commands. The tasks involve bussing a table (top) to put
dishes in a bin and garbage in a trash bin, setting a table
(middle) by taking items out of a bin, and packing a shopping
bag (bottom).
task consists of numerous such segments. The tasks in this
evaluation consist of:
cutlery in a bin, and trash into a trash bin.
Table setting: the robot must take out items from a bin to set
a table, including a place mat, dishes, silverware, napkin, and
Grocery bagging: the robot must pack grocery items, such as
bags of coffee beans, barley, marshmallow, seaweed, almonds,
In Figure 8, we show the language-conditioned tasks in our
evaluation and present the evaluation results. We evaluate five
different conditions. 0-flat (and 0-small-flat) corresponds
to directly command the model with the task description
(e.g., bag the groceries), without intermediate language com-
mands. 0-human (and 0-small-human) provides intermediate
step commands (e.g., which object to pick and where to place
it) from an expert human user. These conditions evaluate each
models ability to follow more detailed language commands:
while these intermediate commands provide considerable in-
formation for how to perform the task, the model must be
able to understand and follow those commands to benefit from
them. Finally, 0-HL evaluates 0 with high-level commands
provided by a high-level VLM, as discussed in Section V-B.
This condition is also autonomous, without any human expert.
The results in Figure 9, averaging over 10 trials per
significantly better than that of 0-small. This suggests a
significant improvement from the larger pre-trained VLM
initialization. This capability translates to an improvement
in performance with expert human guidance (0-human) and
with high-level model guidance (0-HL). The results indicate
that 0s language following ability directly translates into
better autonomous performance on complex tasks with high-
level guidance.
C. Learning new dexterous tasks
In the next set of experiments, we evaluate our model on
new tasks that differ significantly from the pre-training data,
Fig. 9: Language evaluation. We compare flat versions of
our policies, flat, which receive only the overall task com-
mand (e.g., bag the groceries) with a method that receives
intermediate commands from a human expert, human, or a
high-level VLM policy, HL. We also compare our model to a
small non-VLM variant under the expert condition, 0 and
show a significant improvement with 0 from intermediate
language commands provided by a human expert and to a
lesser degree by an autonomous high-level policy. Notably,
due to 0-smalls limited language following ability, overall it
does not gain with the addition of a high-level expert.
requiring entirely new behaviors. For these evaluations, we
fine-tune the model using various amounts of data for each
new task. While each task is new, we partition the tasks into
tiers depending on how much they differ from tasks in the
pre-training data. The tasks, shown in Figure 10, are:
UR5e stack bowls. This task requires stacking bowls, with
four bowls of different sizes. Since this task requires grasping
and moving dishes like the bussing task in the pre-training
a variety of bowls, and the evaluations use a mix of seen and
unseen bowls.
Towel folding. This task requires folding a towel. Since this
is similar to shirt folding, which is present in pre-training, we
place it in the easy tier.
Tupperware in microwave. This task requires opening a
it. The containers come in different shapes and colors, and
the evaluations use a mix of seen and unseen containers. The
container manipulation resembles pre-training data, but the
microwave is not found in pre-training.
Paper towel replacement. This task requires removing an old
cardboard paper towel tube from a holder and replacing it with
a fresh paper towel roll. Because no such items are found in
Franka items in drawer. This task requires opening a drawer,
packing items into a drawer, and closing it. Because there is no
similar task with the Franka robot in pre-training, we consider
this hard.
We compare our model after fine-tuning both to Open-
VLA  and Octo , which also employ a pre-training
and fine-tuning recipe. Since our aim is to evaluate the specific
Fig. 10: Fine-tuning evaluation tasks: We fine-tune our
model to a variety of downstream tasks that are distinct from
the tasks seen in pre-training. Our tasks represent a range of
similarity from the pre-training tasks, with tasks that are most
similar to pre-training (stack bowls and towel folding), a task
that introduces an unseen new element (a microwave), and
tasks that require new motions and new object types (Franka
items in drawer and paper towel replacement).
models (rather than the architectures), we use the publicly
available pre-trained checkpoints for these models, which are
trained on OXE , and then fine-tune them to each task. We
also compare to ACT  and Diffusion Policy , which
are designed specifically for learning dexterous tasks from
smaller datasets. ACT and Diffusion Policy are trained only
on the fine-tuning datasets, which are of similar size to the
individual datasets used in the ACT and Diffusion Policy
experiments [9, 57]. We evaluate 0 by fine-tuning from our
pre-trained base model, as well as by training from scratch.
This comparison is meant to evaluate the individual benefits of
the 0 architecture and our pre-training procedure. We hypoth-
esize that the 0 architecture with VLM initialization should
already provide a stronger starting point for the individual
its performance, especially with smaller fine-tuning datasets.
Figure 11 shows the performance across all of the tasks for
a variety of methods, averaging over 10 trials per task, with
different amounts of fine-tuning data on each task. We include
all of the baselines on the stack bowls and Tupperware in mi-
crowave tasks. Since OpenVLA and Octo attain significantly
worse performance, we only run these for one of the dataset
the real world. The results show that 0 generally outperforms
other methods. Interestingly, the strongest prior models are
the ones that are trained entirely from scratch on the target
presents a major challenge for prior approaches. While the 5-
hour policy for 0 on the Tupperware task performs similarly
to the baselines, the 1-hour version is significantly better. As
Fine-Tuning Data (Hours)
Average Task Progress
Average Across All Tasks
Fine-Tuning Data (Hours)
Average Task Progress
Paper Towel Replacement
(Bi-UR5e)
Fine-Tuning Data (Hours)
Average Task Progress
Items in Drawer
(Franka)
Fine-Tuning Data (Hours)
Average Task Progress
Towel Folding
(Bi-ARX)
Fine-Tuning Data (Hours)
Average Task Progress
Stack Bowls
Fine-Tuning Data (Hours)
Average Task Progress
Tupperware in Microwave
(Bi-ARX)
0 (scratch)
Fig. 11: Fine-tuning with varying amounts of data. 0 can learn some easier tasks even with smaller amounts of data, and
the pre-trained model often attains a larger improvement over the model trained from scratch.
that are more similar to the pre-training data, though the pre-
trained model is frequently better than the non-pre-trained
D. Mastering complex multi-stage tasks
In our final set of experiments, we tackle a range of
challenging multi-stage tasks via a combination of fine-tuning
and language. For some of these tasks, data is present in pre-
no data is present in pre-training. The tasks in this evaluation,
shown in Figure 12, are:
Laundry folding: This task requires a static (non-mobile) bi-
manual system to fold articles of clothing. The clothing items
start in a randomized crumpled state in a bin, and the goal is
to take out the item, fold it, and place it on top of a stack of
previously folded items. The randomized initial configuration
of the crumpled laundry presents a major challenge, since the
policy needs to generalize to any configuration. This task is
present in pre-training.
Mobile laundry: Here, the Fibocom mobile robot in Figure 5
has to fold laundry, facing many of the same challenges while
controlling orientation and translation. This task is present in
pre-training.
Dryer unloading: Here, the Fibocom mobile robot has to take
laundry out of a dryer and place it into a hamper. This task is
present in pre-training.
Table bussing: This task requires bussing a table with a
diverse array of novel objects in a clutter scene, presenting
a much greater challenge than the benchmark in our out-of-
box evaluation: the policy must generalize to unseen objects
of varying shapes and sizes, and perform complex dexterous
and carefully grasping thin, delicate items such as glasses.
The robot must handle dense clutter and intelligently sequence
various behaviors  for example, to clean off a plate with
into the garbage, and then place the plate in the bin. This task
is not present in pre-training.
Box building: The robot has to assemble a cardboard box
that starts in a flattened state. This task presents a number of
major challenges: the box needs to bent in the right way, and
the robot needs to hold down parts of the box while folding
brace during folding motions. The robot might need to retry
some folds, requiring a reactive and intelligent strategy. This
task is not present in pre-training.
To-go box: This task requires moving several food items from
a plate into a to-go box, requiring packing the items into the
box so that they do not stick out, and then closing the box
with both arms. This task is not present in pre-training.
Packing eggs: The robot needs to take six eggs out of a
bowl and pack them into an egg carton, and then close the
carton. The eggs need to be grasped in a manner appropriate
to their pose inside the bowl, and then placed into open slots
in the carton. This presents challenges due to the egg shape,
box requires the use of both arms. This task is not present in
pre-training.
The results, showing average scores per task over 10 trials,
Fig. 12: We evaluate a range of complex and temporally
extended tasks. This includes: folding laundry from a bin
with a stationary (a) or mobile (b) robot, bussing a real
lunch table (c), assembling a box (d), packing eggs into a
carton (e), and packing food into a to-go box (f). These tasks
require combining dozens of individual behaviors, such as
a huge variety of object configurations, and complex physical
are presented in Figure 13. The scoring rubrics are in Ap-
pendix E. A score of 1.0 represents a perfect execution, while
partial scores correspond to partially completed tasks (e.g., 0.5
indicates that half the objects were bussed correctly). These
tasks are very difficult, and we were not able to solve them
with other methods. We therefore use these tasks to compare
to ablations of our approach, evaluating 0 after pre-training
and fine-tuning, out of the box after pre-training only (out-
of-box), and training on the fine-tuning data without any
pre-training (scratch). The results show that 0 can solve
many of these tasks, with our full pre-training and fine-tuning
recipe performing best across the board. Note that many of
these more difficult tasks show a very large improvement
from using the pre-trained model, indicating that pre-training is
especially useful with harder tasks. The absolute performance
of 0 varies across the tasks, likely due to differences in task
difficulty and the degree to which the tasks are represented
in pre-training. We recommend that readers watch the task
videos on the accompanying website for a more complete
impression of these tasks and their complexity. We believe
that this level of autonomous performance on such challenging
tasks represents a new state of the art in dexterous robot
manipulation with learned policies.
VII. DISCUSSION, LIMITATIONS, AND FUTURE WORK
We presented a framework for training a robot founda-
tion model, which we refer to as 0, that consists of pre-
training on highly diverse data, followed by either out-of-
box evaluation or fine-tuning to complex downstream tasks.
Fig. 13: Post-training results on complex tasks in terms of
average scores over 10 trials. The full pre-trained 0 model
attains more than 50 of the maximum score across all of the
significant improvements on the hardest tasks.
Our empirical evaluation studies tasks that combine dexterity,
Our model incorporates Internet-scale vision-language model
(VLM) pre-training with flow matching for representing com-
plex high-frequency action chunks. Our pre-training mixture
consists of 10,000 hours of dexterous manipulation data from
7 different robot configurations and 68 tasks, in addition
to large amounts of previously collected robot manipulation
data from OXE , DROID , and Bridge . To our
ever used for a robot manipulation model. Our fine-tuning
experiments include over 20 tasks, where we show that our
model outperforms a variety of baselines, including prior VLA
models  and models designed specifically for dexterous
manipulation [57, 9]. We also examine how our post-training
recipe can enable highly complex tasks, such as folding mul-
tiple articles of clothing from arbitrary initial configurations
or assembling boxes.
Our framework broadly resembles the training procedures
employed for large language models, which typically consist
of pre-training a base model on very large datasets scraped
from the web, followed by a post-training procedure that aims
to align the model to enable it to follow instructions and
perform user commands. It is generally recognized that most
of the knowledge in such models is acquired in the pre-
training phase, while the post-training phase serves to tell
the model how it should leverage that knowledge to fulfill
user commands. Our experiments imply that an analogous
phenomenon might take place with robot foundation models,
where pre-trained models have some zero-shot capabilities,
but complex tasks like laundry following require fine-tuning
with high-quality data. Training on only this high-quality data
results in a brittle model that does not reliably recover from
does not always exhibit the fluent strategies demonstrated in
the post-training data.
We hope that our results will serve as a stepping stone to-
ward general and broadly applicable robot foundation models.
Our experiments suggest that such models may soon be a
for future work. First, our experiments do not yet provide a
comprehensive understanding of how the pre-training datasets
should be composed: we combined all data available to us, but
understanding what type of data is more helpful to add and
how it should be weighted remains an open problem. Not all
tasks in our evaluation work reliably, and it remains unclear
how to predict how much and what kind of data is needed
to attain near-perfect performance. Finally, it remains to be
seen how much positive transfer there is in combining highly
diverse data, particularly from different tasks and different
robot foundation models might become a reality, it is left for
future work to understand whether this universality extends
to much more distinct domains, such as autonomous driving,
ACKNOWLEDGEMENTS
We thank Laura Smith and Dibya Ghosh for feedback on
the paper and assistance with figures and videos, Philip Clark,
Kelly Sims, and Saunaz Moradi for feedback on writing, and
Evan Pokrandt, Joakim Keussen, Dan Philibin, Eitan Penner,
Adam Lisagor, and Greg Miller for help with illustrations,
discussion. We are tremendously grateful to all of the robot
operators for tirelessly collecting robot manipulation data. For
a full contribution statement, see Appendix A.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen
Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-
in robotic affordances. arXiv preprint arXiv:2204.01691,
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Advances in neural information processing systems, 35:
bidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer
Aloha 2: An enhanced low-cost
hardware for bimanual teleoperation.
arXiv preprint
Lucas Beyer, Andreas Steiner, Andre Susano Pinto,
Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim
Emanuele Bugliarello, et al. Paligemma: A versatile 3b
vlm for transfer. arXiv preprint arXiv:2407.07726, 2024.
Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Ab-
hinav Gupta, Shubham Tulsiani, and Vikash Kumar.
nipulation via semantic augmentations and action chunk-
ing. In 2024 IEEE International Conference on Robotics
and Automation (ICRA), pages 47884795. IEEE, 2024.
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Danny Driess, Avinava Dubey, Chelsea Finn, Pete Flo-
der Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil
Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey
Karl Pertsch, Kanishka Rao, Krista Reymann, Michael
Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran,
Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan
Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.
edge to robotic control. arXiv preprint arXiv:2307.15818,
Serkan Cabi, Sergio Gomez Colmenarejo, Alexander
Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik,
et al. Scaling data-driven robotics with reward sketch-
ing and batch reinforcement learning.
arXiv preprint
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran
Song. Diffusion policy: Visuomotor policy learning via
action diffusion. The Inte
