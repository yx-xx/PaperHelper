=== PDF文件: Map Space Belief Prediction for Manipulation-Enhanced Mapping.pdf ===
=== 时间: 2025-07-22 09:42:35.149491 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Map Space Belief Prediction for
Manipulation-Enhanced Mapping
Joao Marcos Correia Marques1
Nils Dengler2,3,4
Tobias Zaenker2,4
Jesper Mucke2
Shenlong Wang1
Maren Bennewitz2,3,4
Kris Hauser1
These authors contributed equally to this work
1. University of Illinois at Urbana-Champaign, IL, USA
2. Humanoid Robots Lab, University of Bonn, Germany
3. The Lamarr Institute, Bonn, Germany
4. The Center for Robotics, University of Bonn, Germany
Robots Belief
Robots Belief
Robots Belief
Scene after manipulation
New instances discovered
uncertainty reduced
New instance uncovered
uncertainty reduced
Time Step
Fig. 1: Example scenario with occlusions in a confined shelf environment. Given a current partial map of the environment (belief t), our planner decides
whether gathering another observation or manipulating the scene would be best to reduce map uncertainty. In this example, first a observation action would
increase environmental knowledge, followed by a push to unveil the hidden can behind the two boxes at time t  2. The predicted belief map is visualized
as a top-down projection of the shelf, ignoring the occluding top shelf board.
AbstractSearching for objects in cluttered environments
requires selecting efficient viewpoints and manipulation actions
to remove occlusions and reduce uncertainty in object locations,
of manipulation-enhanced semantic mapping, where a robot has
to efficiently identify all objects in a cluttered shelf. Although
Partially Observable Markov Decision Processes (POMDPs) are
standard for decision-making under uncertainty, representing
unstructured interactive worlds remains challenging in this
formalism. To tackle this, we define a POMDP whose belief is
summarized by a metric-semantic grid map and propose a novel
framework that uses neural networks to perform map-space be-
lief updates to reason efficiently and simultaneously about object
physics. Further, to enable accurate information gain analysis, the
learned belief updates should maintain calibrated estimates of un-
certainty. Therefore, we propose Calibrated Neural-Accelerated
Belief Updates (CNABUs) to learn a belief propagation model
that generalizes to novel scenarios and provides confidence-
calibrated predictions for unknown areas. Our experiments show
that our novel POMDP planner improves map completeness and
accuracy over existing methods in challenging simulations and
successfully transfers to real-world cluttered shelves in zero-shot
fashion.
I. INTRODUCTION
Active sensing has long been studied in robotics for tasks
such as exploring an unknown environment , complete 3D
object model acquisition , and searching for an unobserved
target object [26, 12, 47]. To build complete maps as efficiently
as possible, Next Best View (NBV) planning  is often
employed to reduce the uncertainty about the map as quickly
as possible. Although NBV planning offers an approach for
static scenes in which the robot simply moves the camera
passively through free space, there are many applications, such
as household and warehouse robotics, in which robots may
need to manipulate the environment in order to gain better
viewpoints [11, 33]. We refer to this problem as Manipulation-
Enhanced Mapping (MEM). MEM offers two significant new
challenges beyond standard NBV problems. First, in order to
decide when and where to manipulate objects, the robot should
reason about how object movement may affect previously
occluded regions. Second, it must anticipate the impact of
manipulations on observed objects and possibly partially-
observed or unobserved objects. For example, pushing boxes
in a grocery shelf backward will move all objects simultane-
ously until the furthest, occluded one, hits a wall.
MEM is related to the mechanical search problem
in which the robot manipulates clutter to reveal a target
object. Prior approaches in mechanical search tend to rely
on restrictive assumptions, such as a static viewpoint, which
ignores a robots ability to look around obstacles [18, 39].
Other studies assume full observability of object dynamics
and poses  or are limited to a fixed set of predefined
objects . These assumptions are too limiting for complex
cluttered scenes like shelves. The most closely-related work to
ours is Dengler et al. , who address these limitations by
training a reinforcement learning policy for viewpoint planning
and learn a push scoring network from human annotations to
derive manipulation actions, switching between manipulation
and manipulation when the information gain from obtaining
novel images of the environment saturates. However, their
approach to switching between action modes is inefficient,
since waiting for information gain saturation to perform a
push results in the agent sampling the environment several
times to reveal details that could have been revealed easily
through manipulation. Furthermore, the proposed method does
not update its environmental map after a push, leaving the
viewpoint planner, conditioned solely on this outdated map,
less capable of exploiting the newly revealed space.
This paper formulates the MEM problem as a Partially
Observable Markov Decision Process (POMDP) in the belief
space of semantic maps. By maintaining map-space beliefs,
our approach is applicable to unstructured cluttered environ-
ments with an arbitrary numbers of objects. The POMDP
computes the next best viewpoint or manipulation action that
maximizes the agents expected information gain over a short
horizon (Fig. 1) in a receding-horizon fashion. Our approach
leverages neural network methods for map-space belief prop-
literature to drastically improve map completion rates and offer
better guidance for object search [12, 47]. The key challenge in
belief propagation with manipulation actions is that they often
reduce certainty when the objects dynamics are unknown
or the robot interacts with unobserved objects. To address
this challenge, we introduce the Calibrated Neural-Accelerated
Belief Update (CNABU) technique to learn belief propagation
models for both observation (obtaining new images from the
environment) and manipulation actions. Confidence calibration
is especially important for belief propagation because over-
confidence in either object dynamics or map prediction can
result in ineffective exploration andor early termination. We
employ evidential deep learning to obtain better off-the-shelf
model calibration .
Our experiments in simulation environments demonstrate
that our proposed MEM planner outperforms prior work
and CNABU-enhanced baselines in terms of metric-semantic
accuracy. Furthermore, we perform hardware experiments with
a UR5 robot equipped with a gripper and an in-hand camera,
demonstrating zero-shot transfer of the learned models, and
showing the efficacy of our method in mapping of cluttered
shelves. An implementation of our method can be found on
Github1.
II. RELATED WORK
A. Next Best Viewpoint Planning
NBV planning is a well-researched approach in the area of
active vision that has been applied to both object reconstruc-
tion and large-scale scene mapping. Generally, NBV consists
of two steps: First sampling view candidates, then evaluating
which candidate is the best. For object reconstruction tasks
1 enhanced map prediction
the object. For large-scale scenes, sampling is more challeng-
ing. Monica and Aleotti  sample at the contour of the
explored scene. Other approaches sample at either predefined
or dynamically detected regions of interest. For the evaluation,
most approaches compute an estimated information gain to
determine the utility of a view. The information gain is often
based on the expected entropy reduction, e.g. by counting
unknown voxels in the field of view. Other approaches like
Hepp et al.  rely on a learned utility to predict the
best view. In this work, we build upon existing concepts of
NBV planing, but enhance them by incorporating manipulation
actions to interactively shape and explore the environment,
allowing the robot to gather richer information and adapt its
strategy based on both observation and interaction.
B. Mechanical Search in Shelves and Piles
Mechanical search algorithms [9, 18, 39] locate and extract
one or multiple target objects from a given scene, while
dealing with confined spaces, occlusion and object occurence
correlations. The task consists of multiple steps, i.e., visual
execution. For visual reasoning, current research demonstrates
that the scene can be effectively explored by interacting with
objects [3, 11, 26, 33] to actively reduce or overcome occlu-
Kim et al.  propose a method for locating and retrieving
a target object using both pushing and pick-and-place actions.
long-term map, and rebuilds environmental knowledge from
scratch with each observation. Therefore, the approach can
lead to unnecessary manipulation actions, as the target may
already be visible from other viewpoints. In the context of
planning for Manipulation Among Movable Objects (MAMO)
[36, 40], Saxena and Likhachev  introduced a method for
object retrieval in cluttered, confined spaces. Despite achieving
strong retrieval performance, their approach depends entirely
on prior knowledge of object shapes and dynamics. Pajarinen
et al.  propose a POMDP formulation for manipulating
objects under uncertain segmentation using a particle belief
prehensile manipulation to enable efficient belief propagation.
Muguira-Iturralde et al.  propose a planner based on two-
level hierarchical search to enable visibility-aware navigation
with movable objects. Their algorithm, Look and Manipulate
Backchaining (LAMB), however, relies on the assumptions
of deterministic action outcomes and on extremely simplified
environment dynamics (grasps always succeed, pushed objects
always slide along axis without rotation), having limited
applicability in the real world.
In this work, we do not focus on retrieving individual
environment. With our long-term occupancy and semantic map
on perfect model knowledge or single-shot scene 
