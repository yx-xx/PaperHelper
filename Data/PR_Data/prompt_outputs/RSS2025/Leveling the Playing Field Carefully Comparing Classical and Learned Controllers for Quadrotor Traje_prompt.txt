=== PDF文件: Leveling the Playing Field Carefully Comparing Classical and Learned Controllers for Quadrotor Traje.pdf ===
=== 时间: 2025-07-22 09:42:07.772551 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Leveling the Playing Field: Carefully Comparing
Classical and Learned Controllers for Quadrotor
Trajectory Tracking
Pratik Kunapuli
University of Pennsylvania
pratikkseas.upenn.edu
Jake Welde
University of Pennsylvania
jweldeseas.upenn.edu
Dinesh Jayaraman
University of Pennsylvania
dineshjseas.upenn.edu
Vijay Kumar
University of Pennsylvania
kumarseas.upenn.edu
AbstractLearning-based control approaches like reinforce-
ment learning (RL) have recently produced a slew of impressive
results for tasks like quadrotor trajectory tracking and drone
racing. Naturally, it is common to demonstrate the advantages of
these new controllers against established methods like analytical
controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more
complicated than might appear at first sight. As a case study, we
take up the problem of agile tracking of an end-effector for a
quadrotor with a fixed arm. We develop a set of best practices
for synthesizing the best-in-class RL and geometric controllers
(GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric
access to: (1) the task definition, in the form of an objective
and (3) feedforward information, describing the desired future
trajectory. The resulting contributions are the following: our im-
provements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above
asymmetries can yield misleading conclusions. Prior works have
implied that RL outperforms GC, but we find the gaps between
the two controller classes are much smaller than previously pub-
lished when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL
has better transient performance, resulting in GC performing
better in relatively slow or less agile tasks, but RL performing
better when greater agility is required. Finally, we open-source
implementations of geometric and RL controllers for these aerial
I. INTRODUCTION
The capabilities of aerial robots have seen explosive growth
in recent years, with many exciting results in fast quadrotor
advances have involved data-driven techniques, and this has
spawned careful studies of the design choices in learning-
based controller synthesis, such as policy architectures, train-
ing procedures, and modeling choices. It is now largely
accepted wisdom that data-driven controller synthesis ap-
proaches outperform more classical model-based methods for
aerial robot control tasks.
pared these two very different classes of controllers. This
may be partly be attributed to the small overlap in research
Fig. 1: Trajectory Tracking for a Quadrotor. Rollouts of tra-
jectory tracking from an initial perturbation of a reinforcement
learning controller (blue) and a geometric controller (orange)
on a quadrotor. Robots are visualized at t  1 and t  5 to
highlight transient and steady-state performance relative to the
reference trajectory (grey). RL Controller has better transient
performance (t  1), but worse steady-state error (t  5)
compared to GC.
communities engaged in the development and application of
model-based versus learned controllers. In any case, existing
empirical studies largely focus on careful comparisons within
each broad model class: such as Sun et al.  who compare
model-based controllers, or Dionigi et al. , Kaufmann et al.
who benchmark learning-based controllers.
As we will discuss in detail in Sec II and IV, the few prior
studies that do compare learned and model-based controllers
suffer from some shortcomings. While they aim to reproduce
in good faith the respective common practices of the model-
based and learned control communities, this inadvertently
confounds the comparison, because these standard practices
are very different. It is standard in data-driven controller
synthesis research to optimize parameters with a carefully
designed objective function on a dataset of experiences that
exactly match the target task. On the other hand, the tuning
of a model-based controller is often a much more heuristic
manual procedure to achieve a good enough configura-
tion. Inheriting such practices when comparing learned and
model-based controllers can produce misleading conclusions:
the model-based controller may be sub-optimally tuned and
perform worse only on that account. In other words, correct
comparisons across learned and model-based controllers is
more complicated than might appear at first sight.
In this paper, our first contribution is to identify and fix
three key asymmetries that can produce misleading gains
for learning versus model-based methods on standard aerial
vehicle tasks like trajectory tracking: objective functions, task-
relevant data, and feedforward inputs specifying upcoming
trajectory waypoints. Our second contribution is to apply
these proposed improvements in experimental protocol to
throughly benchmark reinforcement learned controllers (RL)
and geometric controllers (GC) for agile trajectory tracking
in quadrotors. Having leveled the playing field, our findings
substantially erode the gains of RL versus GC controllers:
the two controller classes perform about on par in most
of our evaluations, with small gains for GC in steady-state
(Fig 1). These findings add nuance to todays prevailing
wisdom about the relative merits of these controllers. We show
further how our improved recipe for empirical comparison
can be applied towards validating various hypotheses on new
aerial robot classes and tasks. Finally, we provide re-useable
implementations of best practices for both RL and geometric
controller synthesis for aerial vehicle trajectory tracking tasks
including task implementations in a highly parallel RL-ready
future practitioners and researchers.
II. RELATED WORKS
In this section, we elaborate on the deficiencies highlighted
in Sec I in the state of current scientific evidence regarding the
efficacy of learned and model-based controllers for quadrotor
control.
Comparative studies for quadrotor control have been per-
formed in the past, but a majority of them consider compar-
isons within the same class of controllers. Among learning-
based methods in quadrotor trajectory tracking tasks, Kauf-
mann et al.  evaluate the effect of control abstraction on
tracking performance in hardware experiments, Dionigi et al.
evaluate the effect of observation on policy performance,
and Welde et al.  evaluate the sample efficiency gains
afforded by a symmetry-informed approach to tracking control
via RL. While comparisons across model-based analytical con-
trollers are fairly straightforward [24, 25], comparisons involv-
ing learning-based methods must contend with a data-driven
approach and compare to a non-data-driven approach, making
it difficult to perform a correct experimental comparison. As
the communities are largely disjoint, it is reasonable that these
papers are preoccupied comparing against other works of the
same controller class, and thus there are few works that even
consider the comparison between RL and analytical controllers
like geometric controllers (GC) for trajectory tracking tasks.
RL-based control is the state-of-the-art for agile trajectory
tracking in quadrotors, owing to better computational tractabil-
ity than Model Predictive Control (MPC) and outperforming
GC in tracking error. To evaluate such claims, we summarize
the few most relevant prior comparison studies in Table I.
Based on our close reading of these papers, supplementary
presents the access to task objective functions, task-aligned
experiences or datasets, and feedforward waypoint tracking
specifications made available to each method considered in
these works. Significant asymmetries are apparent in the access
granted to RL and GC methods respectively, which we believe
(and show evidence in Sec V) might confound these prior
findings. Among the recent works presenting a model-free
RL method, there are few that train directly on the tracking
static task like hovering [7, 8, 19], making the upper bound
of tracking performance unclear. Even those few that directly
optimize the tracking task either omit a comparison against
an analytical controller [5, 12, 28] or compare against an im-
plementation lacking essential components proposed in prior
work on analytical control , resulting in an asymmetrical
comparison. Often this is a result of inheriting a baseline
controller which may not be optimized for the task at hand,
such as an existing module of the firmware of a commercially-
available platform [8, 19]. These observations motivate the
key contribution of our work: to identify and fix oversights
in these prior experimental comparisons (Sec IV), ultimately
enabling us to arrive at more reliable and nuanced conclusions
regarding the efficacy of RL and GC controllers for various
aerial vehicles and tasks (Sec V).
III. STATE-OF-THE-ART CONTROLLERS
In this section we overview two popular classes of con-
(RL), that have widely been used for trajectory tracking in
quadrotors. Our empirical evaluations and comparisons in later
sections will focus on these two classes of controllers.
A. Geometric Control
Seminal works in the quadrotor control literature [14, 17]
showed that quadrotors are differentially flat systems and
analytically developed a hierarchical controller for this system.
This control paradigm uses an explicit control law, constructed
from separate position and attitude control loops that are
connected via a backstepping-like approach to yield a cascade-
like control structure. In this paper, we refer to this controller
simply as the Geometric Controller (GC) for brevity (but
it is sometimes also referred to as SE(3) Control  or
Differential-Flatness Based Control (DFBC) ).
The controller exploits the coupled dynamics between the
attitude and position of the system, and controls them with two
geometric PD control loops separately. Firs
