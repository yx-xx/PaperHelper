=== PDF文件: FACTR Force-Attending Curriculum Training for Contact-Rich Policy Learning.pdf ===
=== 时间: 2025-07-22 15:45:07.228705 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Contact-Rich Policy Learning
Jason Jingzhou Liu, Yulong Li, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak
Carnegie Mellon University
Equal contribution
Fig. 1: FACTR. We present Force-Attending Curriculum Training (FACTR)  a system that leverages robot external joint torques for both
teleoperation and improving policy generalization for complex contact-rich tasks. [Left] Our low-cost leader-follower setup employs actuated
servo motors to enable force feedback in both the leader arm and gripper, improving teleoperation success rate, completion time, and ease of
use. [Right] FACTRs behavior cloning policy utilizes robot force information to enhance performance and generalization across objects with
diverse geometries and textures in contact-rich tasks. Video results, codebases, and instructions at
AbstractMany contact-rich tasks humans perform, such as
box pickup or rolling dough, rely on force feedback for reliable
execution. However, this force information, which is readily avail-
able in most robot arms, is not commonly used in teleoperation
and policy learning. Consequently, robot behavior is often limited
to quasi-static kinematic tasks that do not require intricate force-
feedback. In this paper, we first present a low-cost, intuitive,
bilateral teleoperation setup that relays external forces of the
follower arm back to the teacher arm, facilitating data collection
for complex, contact-rich tasks. We then introduce FACTR, a
policy learning method that employs a curriculum which corrupts
the visual input with decreasing intensity throughout training.
The curriculum prevents our transformer-based policy from over-
fitting to the visual input and guides the policy to properly
attend to the force modality. We demonstrate that by fully
utilizing the force information, our method significantly improves
generalization to unseen objects by 43 compared to baseline
approaches without a curriculum. Video results, codebases, and
instructions at
I. INTRODUCTION
Contact-rich tasks are an integral part of daily life, from
lifting a box and rolling dough to cracking an egg or opening a
door. These tasks, while seemingly simple, involve a complex
interplay of forces and require precise adjustments based on
force feedback. Humans rely heavily on this force feedback
to generalize across tasks and objects, adapting seamlessly to
variations in visual appearances and geometries. However, in
robot learning, force information remains underutilized, even
though it is readily available on many modern robotic arms,
such as the Franka Panda and the KUKA LBR iiwa. Instead,
most data-driven methods, including those using Behavior
Cloning (BC), focus primarily on visual feedback for both data
collection and policy learning, overlooking the critical role of
force. This limited use of force information hinders the vision-
only policies ability to generalize to novel objects. For instance,
in tasks like lifting up a box with two arms, the primary factor
influencing the action is the objects geometry, while attributes
such as color or texture are irrelevant. In such cases, force
feedback provides a clear signal for mode switching, such
as detecting when contact is established, which can facilitate
object generalization compared to relying solely on vision.
One of the main reasons for the under-utilization of force
feedback in robot learning is the lack of an intuitive and low-
cost teleoperation system that can capture force feedback during
data collection itself. Recently, low-cost leader-follower systems
have become popular for teleoperation, offering intuitive control
of robot arms by mirroring the joint movements of the leader
arm controlled by a teleoperator to the follower arm [32, 29].
joints are not actuated) and unilateral (the leader arm does
not receive information from the follower arm). This makes
teleoperation difficult for dynamic, contact-rich tasks where
precise force adjustments are necessary . To overcome this
that provides force feedback by actuating motors in the leader
arm joints based on external joint torques transmitted from the
follower arm (Fig. 1 Left). By actuating the motors, we also
provide active gravity compensation and resolve the kinematic
redundancy due to the redundant degrees of freedom of the
arm. These enhancements improve the teleoperation experience,
leading to a 64.7 increase in the task completion rate, a 37.4
reduction in completion time, and an 83.3 improvement in
subjective ease of use across four evaluated contact-rich tasks.
The second challenge lies in effectively incorporating robot
force information into policy learning. Although recent methods
such as diffusion policy  and action chunking transformers
achieve impressive results for fine-grained manipulation,
they often fail to generalize to unseen objects with variations
in object visual appearances and geometries. Humans, on the
other hand, can disregard irrelevant visual details once contact
is established and rely solely on force feedback to perform
tasks such as lifting a box or rolling dough. Therefore, to
improve generalization, we seek to incorporate force input
into autonomous robot policies. However, making effective
use of force information in policy learning is challenging, as
policies often overfit to using visual modality , effectively
disregarding force data. This issue arises because contact force
signals are typically less discriminative, often remaining near
zero for extended periods when the arm is not in contact with
the environment during an episode. Hence, without proper care
during training, policies tend to ignore force input and rely
primarily on visual information. We empirically analyze this
effect in Sec. V-C and Fig. 9.
To mitigate this imbalance, we propose Force-Attending
Curriculum Training (FACTR), a curriculum training strategy
designed to improve the policys ability to effectively leverage
force information. FACTR systematically reduces the reliance
on visual information during training by applying operators
such as Gaussian blurring or downsampling with varying scales
to visual inputs. A scheduler gradually decreases the blurring
scale and increases the fidelity of the visual inputs. Intuitively,
this approach encourages the policy to focus more on force
input during initial training phases and gradually balances
force with visual inputs as training progresses. We ground this
intuition with a theoretical analysis on a simplified scenario
through the framework of Neural Tangent Kernels . We
explore FACTR in both the pixel space and latent space, testing
various operators and scheduling strategies. Our experiments
show that FACTR improves the success rate for unseen objects
by an average of 40.0 in four challenging contact-rich tasks
Fig. 2: Our low-cost bimanual teleoperation system with force-
feedback. The system features two actuated leader arms, two follower
arms with external joint torque sensors (such as the Franka Panda
and the KUKA LBR iiwa), a front camera and two wrist cameras.
(Fig. 1 Right)  box lifting, prehensile pivoting, fruit pick-
force-attending curriculum training.
In summary, our contributions are as follows.
Low-Cost Teleoperation with Force Feedback: We
design a low-cost bilateral leader-follower teleoperation
system with force feedback, gravity compensation, and
redundancy resolution, demonstrating a 64.7 improve-
ment in task completion rate and an 83.3 enhancement
in ease of use for teleoperation through a user study.
Force-Attending Curriculum Training: We propose
to use force feedback in policy learning and achieves better
generalization capability to object visual appearances and
geometries. Evaluated on four challenging contact-rich
policies by 40.0 compared to policies with force
information as part of input but trained without FACTR.
II. RELATED WORKS
A. Imitation Learning with Force
Imitation learning has recently experienced significant ad-
algorithms that leverage demonstrations to train robotic poli-
cies [17, 4]. Although traditional approaches primarily rely on
visual and joint position inputs, many real-world tasks require
explicit force feedback to improve stability, adaptability, and
safety [18, 9]. Recent work has applied learning methods to
train with demonstrations that incorporate gripper force or
tactile signals, resulting in policies capable of handling small,
fragile objects and performing contact-intensive tasks such as
vegetable peeling [15, 31, 14]. However, utilizing force data
from the robot arms, such as joint torques, remains under-
explored. One approach involves using an end-effector force
sensor to estimate compliance parameters or virtual position
targets through kinesthetic teaching and force tensors [10, 3].
Another method infers a 6D wrench for low-level control by
integrating torque sensing into a diffusion policy .
learning can lead to overfitting to visual information, causing
the policy to disregard force input. FoAR  explicitly predicts
contact and non-contact phases to regulate the fusion of vision
and force modalities, which requires additional data labeling.
We propose FACTR to effectively incorporate force and vision
input into policy through a curriculum, enabling policies to
leverage force for improved object generalization.
B. Low-Cost Teleoperation Systems with Force Feedback
Parallel to advances in imitation learning, significant efforts
have been made to collect low-cost and high-quality data with
hand-held grippers [27, 5] or leader-follower systems [32, 29,
25]. Hand-held grippers naturally provide force feedback to
the operator, but they do not directly record force data. Recent
work has added force sensors to hand-held grippers to address
this limitation . However, hand-held grippers are in general
limited by the kinematic differences between humans and
the robots. Although the leader-follower systems are not prone
to this limitation, they often lack force feedback, impairing
their effectiveness in contact-rich tasks. Recently, Kobayashi et
al.  implemented a bilateral leader-follower teleoperation
system where in addition to the follower following the joint
positions of the leader, the leader also gets an additional torque
if there is a difference in its joint position from that of the
follower. However, when the follower arm is in motion without
the ease of use and precision of the system . Our approach
introduces an alternative bilateral teleoperation method by
relaying only external joint torques from the follower arm back
to the leader arm, providing force feedback without impairing
operational precision.
III. FACTR LOW-COST BILATERAL TELEOPERATION
Leader-follower
teleoperation in manipulation tasks. These systems feature
kinematically equivalent leader and follower arms, allowing
intuitive control through joint space mapping, where the leaders
joint positions are mirrored as targets for the follower. This
setup lets users naturally feel the follower arms kinematic
constraints. However, most implementations lack force feed-
of the environment, which is crucial for teleoperating contact-
rich tasks . Instead, those leader arms are mostly passive,
lacking active motor torque actuation, despite being equipped
with servo motors capable of actuation. Furthermore, the
lack of active torque means the leader arms require external
structural frames and rubber bands or strings to achieve gravity
In this paper, we aim to fully leverage the servo motors in the
leader arm and gripper to achieve force-feedback enabled teleop-
eration with affordable hardware. Similarly to GELLO , our
leader arms use off-the-shelf servos and 3D-printed components,
forming a scaled-down but kinematically equivalent version of
the follower arms, as shown in Fig. 2. By actuating the servo
resolution through nullspace projection, gravity and friction
augment the teleoperation experience while still using low-cost
hardware to provide functions that are usually only available
with much more expensive teleoperation devices. Please see
Appendix VIII for a detailed Bill of Materials.
A. Force Feedback
Force feedback provides the operator with a tangible sense of
interaction with the environment, allowing more intuitive and
delicate manipulation, especially in contact-rich tasks or tasks
with limited visual feedback . We implement a control law
that relays external joint torques sensed by the follower arm
to the leader arm, allowing the operator to feel the physical
constraints experienced by the follower arm:
feedback  fKf,pext Kf,d q
where f is a scalar constant, ext is the external joint torque
sensed by the follower arm, Kf,p and Kf,d are the PD gains
for the force feedback, respectively. Here, Kf,p is calculated as
the ratio between the maximum torque of the leader and that of
the follower, and Kf,d q helps reduce oscillations in the leader
arm when the follower arm is in contact. We note that ext is
a readily available measurement in various collaborative robot
iiwa. In particular, we implement mediated force feedback by
scaling down ext with f, which has been shown to improve
the accuracy of the operation while reducing the cognitive
load of the operator . Furthermore, we highlight that our
implementation only transmits external forces from the follower
to the leader; as a result, the operator does not experience the
internal friction and inertia of the follower arm during motion,
providing a clearer perception of the environment .
In addition, we implement force feedback for the parallel-
jaw gripper. Since our servo-based gripper does not contain
an external force sensor, we utilize the present current reading
of the gripper servo to provide force feedback as follows:
where h,t is the force feedback torque sent to the gripper leader
system sets   0.1 which provides a good user experience.
B. Customizable Redundancy Resolution
For kinematic redundant manipulators, without regulating
the joint space, the manipulator tends to drift into unde-
sirable configurations under the influence of gravity during
teleoperation. Approaches like Gello  rely on mechan-
ical components, such as springs, to regularize the joint
space. However, these components introduce non-uniform,
configuration-dependent wrenches at the end-effector, resulting
in an unintuitive teleoperation experience. In addition, using
mechanical joint regularization effectively prevents the user
from setting custom joint regularization targets for redundancy
resolution. In confined-space manipulation settings, the inability
to control the joint regularization target can impair the arms
In contrast, our proposed method leverages the following
null-space projection control law to regulate joint positions ,
which stabilizes the joint-space at any user-defined desirable
posture without imposing additional end-effector wrenches,
regardless of the arms configuration:
(Kn,p (q qrest) Kn,d q)
where J is the manipulator Jacobian matrix, qrest is a user-
defined resting posture configuration, Kn,p and Kn,d are the
PD gains for the null space projection. Note that
the null-space projector.
C. Gravity Compensation
To ensure the leader arms remain stationary, allowing the
user to easily pause teleoperation, we implement gravity
compensation. This is achieved by modeling the dynamics
of the leader arm and computing the joint torques required to
counteract dynamic forces using the recursive Newton-Euler
algorithm (RNEA) for real-time inverse dynamics .
grav  M(q)q  C(q, q)q  g(q)  RNEA(q, q, q)
where M(q) is the mass (or inertia) matrix, C(q, q) is the
Coriolis and centrifugal matrix, and g(q) is the gravity vector.
D. Additional Compensation and Controls
To reduce the perceived friction in the leader arm during tele-
prevent users from exceeding joint limits of the follower arm in
order to respect the workspace of the follower arm. Finally, for
bi-manual follower arms, the system uses Riemannian Motion
Policies  for dynamic obstacle avoidance between the two
follower arms. Please refer to Appendix IX for more details.
E. Overall Control Law for the Leader Arm
In summary, the control torques are defined as follows:
feedback relays external forces from the follower arm
back to the leader arm, allowing the operator to sense the
geometric constraints of the environment.
null resolves kinematic redundancy by regulating the
joints at a user-defined rest posture in the null-space.
grav provides gravity compensation for the leader arm.
friction compensates for the leader arm joint frictions to
enable smoother teleoperation.
limit prevents the joints of the leader arm from violating
the joint position limits of the follower arm.
The resulting combined torque applied to the servo motors
of the leader arm is defined as follows:
feedback  null  grav  friction  limit
Fig. 3: Customizable Joint Regularization [Left] Without the
flexibility to define the resting joint configuration qrest, the arms
reachability is restricted, leading to collisions in confined spaces.
[Right] Our leader arm allows the user to define custom resting joint
IV. FACTR: FORCE-ATTENDING CURRICULUM TRAINING
Naively incorporating robot force data into policy learning
does not necessarily ensure policy improvement. Contact force
signals often provide limited discriminative information for
the policy, as it remains close to zero for significant periods
when the arm is not interacting with the environment during an
episode. As a result, the policy tends to disregard force input
and rely predominantly on visual information, as empirically
analyzed in Sec. V-C and Fig. 9.
To fully leverage the robot force data collected from our
teleoperation system, we introduce Force-Attending Curriculum
Training (FACTR), a training strategy designed to effectively
integrate force information into policy learning. FACTR applies
operators like Gaussian blur or downsampling to corrupt visual
throughout training. The curriculum intuitively encourages
contribution from the force modality at the start of training. We
ground this intuition with a theoretical analysis on a simplified
scenario through the framework of Neural Tangent Kernels .
In this section, we first present the base policy model used for
learning from teleoperated demonstrations, and then motivate
and describe FACTR, our curriculum training approach. Our
overall method is summarized in Algorithm 1 and Fig. 4.
A. Problem Statement and Base Model
We consider a policy (  ) that produces a chunk of
future actions of length k qt:tk (joint positions) given (i) a
visual observation It (image at time t), and (ii) an external
joint torque reading t. Our goal is to learn  via behavior
cloning (BC) from a dataset of expert trajectories D. Each
trajectory in D comprises tuples
, where qt is the
ground-truth (expert) joint position target at time t. We let
next k time steps. The loss is defined by:
where qt:tk are the experts future joint position targets and
Transformer
Vision Encoder
External
Joint Torque
Force Encoder
Pixel Space Curriculum
Latent Space Curriculum
n  Max Iter
n  Max Iter
Fig. 4: FACTR allows our policy to better integrate force information without overfitting to visual information, resulting in better generalization
to objects with unseen visual appearances and geometries. Our policy takes as inputs RGB images I and external joint torque , which are
then tokenized by a vision encoder and a force encoder before fed into an action transformer to regress joint position targets qt:tk. FACTR
applies a blurring operator of scale n in either pixel or latent space, initialized at a large value then gradually decreased through the training.
Our policy  is based on an encoder-decoder transformer
that integrates vision and force modalities. Visual observations
and force readings are converted into tokens, fed to the encoder,
then decoded into action tokens through cross attention.
A pre-trained vision transformer (ViT) [7, 6] is used to
encode an input image It into a sequence of vision tokens
t RMvd for some number of tokens Mv and embedding
dimension d. An MLP-based force encoder is applied to the
joint torque t, resulting in a single force token: zF
The tokens are concatenated to form the model input:
R(Mv1)d.
self-attention and feed-forward layers:
R(Mv1)d.
This yields the encoded vision and force tokens.
For the decoder, we introduce k action tokens, A Rkd.
A transformer decoder Dec refines these tokens through self-
attention and cross-attention to HE
During cross attention, each action token attends to both vision
and force representations. If we split HE
t into its vision (V)
and force (F) parts, the cross-attention weights for each layer
l can be decomposed as follows. For simplicity of notation,
assume these weights are already averaged over multiple heads:
For the vision part:
(A(l)WQ(l)) (HE(l)
For the force part:
(A(l)WQ(l)) (HE(l)
These (l)
V and (l)
F measure how strongly each action token
attends to vision vs. force tokens at layer l, and will be the
main source of analysis in Sec. V-C.
t to action space,
which represents joint position targets for the follower arm:
where da is the dimension of the action space. Substituting
Appendix X for the detailed policy architecture and training
hyperparameters.
B. Force-Attending Curriculum
Through experiments, as shown in Sec. V-C and Fig. 9,
we found that naively concatenating force data to the policy
observation during training often results in policies that neglect
force input, failing to leverage force input to the fullest extent.
To address this, we employ a curriculum that gradually unveils
detailed visual information, encouraging the model to learn
to utilize force first. Specifically, we define two operators:
P (I, n) for the pixel space, and L(z, n) for the latent
of a Gaussian kernel or the kernel size of a max pooling
operator) that is updated over the course of training for N
total gradient steps. During training, we apply the pixel-space
operator P to image It or L to visual latent tokens zV
close in the metric space, thus encouraging more contribution
from the force modality, particularly at the start of the training.
Consider the limit  , each visual input converges to
Algorithm 1 Force-Attending Curriculum Training (FACTR)
steps N; Pixel-space operator P (I, ); latent-space operator
L(z, ); Scheduler defining n for n  1 . . . N
chunking transformer
n Scheduler(n)
get current scale
if pixel-space curriculum then
vision tokens
if latent-space curriculum then
force token
approximately the same tensor. Hence, the model can only
learn a single global output for all visual inputs. Thus, at the
early stage of the curriculum, the gradient updates focus more
on using the force information and updating the force encoder
to maximally differentiate between inputs.
C. Curriculum Operators
We consider two types of operators: Gaussian blur and
downsampling.
For the Gaussian blur, we define the 2D kernel G as:
The operator P (I, ) applies this kernel using the 2D
convolution operator :
P (I, )  I G
For the 1D Gaussian blur, the kernel g is defined as:
The operator L(zV , ) similarily applies this kernel using 1D
L(zV , )  zV g
For downsampling, we use MaxPool followed by nearest
interpolation. In 2D, the pixel-space operator P (I) is:
P (I)  NearestInterp(MaxPool2D(I))
In 1D, the latent-space operator I(zV ) is the same except
that a MaxPool1D is used.
By gradually reducing n, the curriculum ensures that the
model focuses first on force tokens, and then incorporates visual
information in the later stage of the training. This produces a
policy  that more robustly fuses force and vision for control,
alleviating the issue of overfitting to the vision modality.
Theoretical Analysis:
We analyze the effects of Gaussian blur
as an example of a curriculum operator through the framework
of Neural Tangent Kernels (NTK) . Although we consider
a simple two-layer model here, the intuition applies to more
complex architectures like vision transformers (ViTs), which
have a more sophisticated NTK . The formal theoretical
analysis is presented in Appendix VII.
D. Curriculum Schedulers
Over the course of training (indexed by n  1, . . . , N), we
adjust n via a scheduler to control the information released
from the visual branch. Given a initial scale 0, we consider
the following schedulers:
Decay Type
Scheduler Equation
Constant
Exponential
dsteps > 1
to 0 for certain gradient steps, and adjust the decay formula
to account for this duration. The rationale behind this step is to
warm-up the randomly initialized force encoder with relatively
low visual information.
V. EVALUATION
A. Experimental Setup
We setup four contact-rich tasks, which are illustrated in
Fig. 5 along with the training and testing objects of various
shapes and visual appearances. For all tasks, we use Franka
Panda arm(s) with OpenManipulator-X gripper(s). Each task
uses either a front ZED2 camera or wrist cameras mounted
near the grippers with RGB observations. We describe the tasks
details and the success criteria below.
Box Lift: A bi-manual task where two arms lift a box and
balance in the air for at least two seconds, using the front
camera and external joint torque from the arms.
Non-Prehensile Pivot: The robot flips an item by pivoting
it against the corner of a fixture until the item is rotated
by 90and can stand stably, using the front camera and
external joint torque from the arm.
Fruit Pick and Place: The robot grasps a soft and delicate
fruit and places it in a bowl, using the wrist camera and
the external joint torque of the gripper.
Rolling Dough: The robot continuously rolls the dough to
shape it into a cylinder for at least 8 seconds, using the
front camera and the external joint torque of the arm.
B. Teleoperation Evaluation
We compare our leader-follower teleoperation system, which
includes our leader arm with force feedback, gravity compen-
Fig. 5: Tasks. We evaluate our leader-follower teleoperation system
and autonomous policies trained with FACTR on four contact-rich
tasks. These tasks are challenging as they require the robot to perceive
and respond to the force feedback as it manipulates objects with unseen
visual appearances and geometries.
follower baseline system with mechanical joint regulation,
similar to . We summarize our results in Fig. 7.
Our experiments show that our system allows users to
complete tasks with 64.7 higher task completion rate, 37.4
reduced completion time, and 83.3 improvement in the
subjective ease of use metrics.
We observe that for tasks that require continuous contact
between the arm and an object, such as non-prehensile pivoting
and bimanual box lifting, the un-actuated teleoperation system
often causes the follower arm to lose contact with the object.
This occurs because of the absence of force feedback, which
prevents the user from perceiving the environments geometric
constraints through the leader arms. As a result, maintaining
continuous contact with the object becomes challenging.
For the un-actuated system, the follower arm frequently
exceeds its joint velocity limits when moving under continuous
contact. This occurs because the operator can easily maneuver
the leader arms in ways that cause significant deviations
between the leader and follower joint positions, especially
when the follower arm is in contact with the environment.
When contact is lost, the resulting large joint-space error causes
the PID controller to generate large torques, causing abrupt
movements that exceed the velocity limits. On the other hand,
our systems force feedback renders geometric constraints of
the environment for the operator through the leader arms,
ACT (VisionForce)
ACT (Vision-Only)
Train Objects
Test Objects
Success Rate
Box Lift
Fruit Pick-Place
Rolling Dough
Box Lift
Fruit Pick-Place
Rolling Dough
Success Rate
Fig. 6: FACTR leads to better object generalization.
preventing the operator from moving the leader arms too far
away from the follower arms during environment contacts.
C. Policy Evaluation
Questions. In our real-world evaluation, we seek to address
the following research questions regarding FACTR:
How does FACTR perform compared to baseline ap-
proaches that do not use force feedback and ones that use
force feedback without FACTR?
How do different curriculum parameters affect policy
performance?
Training and Evaluation Protocol. We collected 50 demon-
strations with our teleoperation system. We trained each method
with the same hyperparameters, where details can be found in
the Appendix X. We compare the following methods:
ACT (Vision-Only) : Action Chunking Transformer
which only takes in visual observation.
ACT (VisionForce) [13, 14]: Action Chunking Trans-
former which takes in both visual and force observation,
but trained without a curriculum.
FACTR (Ours): Action Chunking Transformer trained with
Force-Attending Curriculum. For each task, we train a
latent space curriculum with the Guassian Blur operator
and linear scheduler. We discuss more detailed ablations
on the curriculum in Sec. V-D.
For each object in each task, we evaluated 5-10 trials. We
present the average success rate for training and testing objects,
respectively. Detailed evaluation results for each object can be
found in the Appendix XII.
FACTR leads to better generalization. We present our
main quantitative results in Fig. 6. All the policies perform
similarly on the train objects for most tasks, except for the
rolling dough task, where the vision-only policy smashes the
dough without any rolling actions and fails completely. Note
that the visual observations are hard to distinguish during
the oscillatory rolling motions, while the force signals form
a) Completion Rate
b) Completion Time (sec)
c) Ease of Use
Un-Actuated Leader-Follower
Box Lift
Pick-Place Rolling
Box Lift
Pick-Place Rolling
Box Lift
Pick-Place Rolling
Fig. 7: User study. FACTR teleoperation system allows users to complete tasks with significantly higher success rate, using less time, and
they subjectively found our system to be easier to use.
Fig. 8: We visualize the external joint torque norm of the Franka arm
for a collected trajectory. Blue highlights indicate pre-contact phases,
while purple and green mark torque peaks and troughs at the doughs
left and right ends, respectively. The oscillatory torque pattern helps
the policy distinguish observations despite similar visual inputs.
a corresponding oscillatory pattern, as shown in Fig. 8; this
distinctive torque pattern helps policies with force input to
complete the task.
For the test objects, the vision-only policy achieves a success
rate of 21.3 on average, which is significantly worse than
policies incorporating force. Without a curriculum, policies
naively incorporating force achieve a success rate of 61.2,
while FACTR achieves a success rate of 87.5, which shows
that FACTR leads to significantly better generalization to novel
objects. We hypothesize that the force information provides
important signals for mode switching at moments such as when
the robots get into contact with the box in the lifting task and
when the object is grasped in the fruit pickup task.
Policies with FACTR learns to identify mode switching.
To better understand the policies trained with FACTR. We
visualize the attention behavior during policy training and
inference. Specifically, we visualize the cross attention of the
action tokens to the memory tokens denoted as (1)
for the first layer of the decoder, where (1)
defined in Sec. IV.
During policy rollout, we visualize the average cross attention
of the action tokens to the force or vision tokens of the first
decoder layer as shown in Fig. 9. FACTR learns to attend to
force more during task execution. For example, in the box
lifting task, attention to force outweighs that of vision as the
arms contact the box, signaling a mode switch. While without
the curriculum, the policy does not pay enough attention to
FACTR leads to better recovery behavior. Another notable
observation is that FACTR also facilitates recovery behavior.
per object. A trial begins when the policy successfully lifts
the box for the first time; we then knock the box down and
assess the second attempt. As shown in Table I, all policies
maintain nearly 100 recovery success on training objects.
drops significantly from 31.7 on the first attempt to 13.3
on the second. In contrast, force-attending policies maintain
similar success rates across both attempts.
We observe that vision-only policies often remain static after
the box is knocked down, failing to retry. We hypothesize that
this occurs because the vision-only policy overfits to training
training distribution. In contrast, FACTR policies detect loss of
contact through external joint torque readings, which revert to
pre-lift values when the object is dropped. Since our FACTR
policies effectively attend to force input, they successfully
recover to a pre-lift state and attempt the task again.
Train Objects
Test Objects
ACT (Vision-Only)
ACT (VisionForce)
TABLE I: Evaluation of recovery behaviors for box lifting.
D. Ablations on Curriculum
To further validate the significance of a curriculum, we
trained models with fixed n across training. Moreover, to
ablate on pixel space and latent space curriculum, and different
scheduler and operator choices, we train policies on different
FACTR (Ours)
Attention
No FACTR (Baseline)
Attention
Time Steps
Time Steps
Fig. 9: Policies trained with FACTR learns to identify mode switching. We visualize the average cross attention of the action tokens to
the force or vision tokens of the first decoder layer during policy rollout. [Left] Without the curriculum, the policy does not pay enough
attention to force, and either fails to lift or balance the novel boxes. [Right] FACTR learns to attend to force more to complete the task. For
combination of these parameters. We choose the task of
the ablations. We evaluate only on the five test objects for five
trials each, since they are more indicative of policy performance
than train objects. The results are presented in TABLE II.
Pixel Space
Latent Space
Downsample
Downsample
Constant
TABLE II: Curriculum ablation.
Fixed-Scale Operator vs. Curriculum. We found that
performance with a curriculum of decaying smoothing performs
better than a fixed curriculum across all tasks. We hypothesize
that to enable better performance, the final policy needs to
take in the fully unblurred vision information. Through the
On the other hand, with fixed smoothing, even though policy
may not overfit to visual information, it cannot extract the
necessary details from unblurred vision to complete the tasks.
Comparisons with other scheduler parameters. We further
compare policies trained with either pixel space or latent space,
two operators (Gaussian blur and downsample) as defined in
Sec. IV-C, and four schedulers (linear, cosine, exponential,
and step) as defined in Sec. IV-D. However, we do not find a
uniform advantage or disadvantage for any set of parameters.
The results suggest that FACTR is relatively robust to different
sets of curriculum parameters.
VI. CONCLUSION AND LIMITATIONS
We introduced FACTR, a curriculum approach to train force-
based policies to improve performance and object general-
ization in contact-rich tasks. FACTR leverages a blurring
operator with decreasing scales on the visual information
throughout training. This encourages the policy to leverage
force input at the beginning stages of training, preventing
the problem where the policy overfits to visual input and
thus neglects force input. This approach was demonstrated
through a series of experiments on the following tasks: box
task completion rates and generalization to unseen object
appearances and geometries. Additionally, our teleoperation
feedback and gravity compensation, was shown to provide
a more intuitive user experience, as evidenced by higher task
completion rates and user satisfaction in our studies.
While FACTR demonstrates significant improvements in
force-based policy learning for contact-rich tasks, it has limita-
tions. First, the precision of the external joint torque sensors
in our follower arm is limited. This limitation can particularly
affect tasks that involve subtle force adjustments during fine-
grained manipulation since the torque readings can be too noisy
to be used effectively. Future work could explore integrating
high-resolution tactile sensors or haptic gloves to enhance
feedback precision and improve overall system performance.
torque sensors in the follower arms. Future work can explore
adapting our system for an arm mounted with an end-effector
force-torque sensor. Third, the effectiveness of our curriculum
learning approach can be influenced by several hyperparameters,
such as the choice of the blurring operator and scheduling
strategies. These parameters can be highly task-dependent,
requiring extensive tuning for different applications. Developing
adaptive or self-tuning curriculum strategies could help mitigate
this issue by dynamically adjusting hyperparameters based on
task-specific requirements. Addressing these limitations could
further enhance FACTRs applicability and robustness across
a broader range of contact-rich manipulation tasks.
ACKNOWLEDGMENTS
We thank Arthur Allshire, Andrew Wang, Mohan Kumar
thank Tiffany Tse, Ray Liu, Sri Anumakonda, Sheqi Zhang
with teleoperation. This work is supported in part by ONR
and AFOSR FA9550-23-1-0747.
REFERENCES
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R
with an infinitely wide neural net. Advances in neural
information processing systems, 32, 2019.
Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy
transformers reason with abstract symbols? International
Conference on Learning Representations, 2023.
Claire Chen, Zhongchun Yu, Hojung Choi, Mark
informed actions from kinesthetic demonstrations for
dexterous manipulation. arXiv preprint arXiv:2501.10356,
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
fusion policy: Visuomotor policy learning via action
diffusion. In Robotics: Science and Systems, 2023.
Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau,
Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and
Shuran Song. Universal manipulation interface: In-the-
wild robot teaching without in-the-wild robots. Robot
