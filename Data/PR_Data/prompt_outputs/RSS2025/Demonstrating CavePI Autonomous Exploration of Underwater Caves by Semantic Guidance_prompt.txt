=== PDF文件: Demonstrating CavePI Autonomous Exploration of Underwater Caves by Semantic Guidance.pdf ===
=== 时间: 2025-07-21 14:24:56.444728 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating CavePI: Autonomous Exploration of
Underwater Caves by Semantic Guidance
Alankrit Gupta, Adnan Abdullah, Xianyao Li, Vaishnav Ramesh, Ioannis Rekleitis and Md Jahidul Islam
RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA,
Department of ME, University of Delaware, Newark, DE 19716, USA, Email: yiannisrudel.edu.
These two authors have contributed equally.
AbstractEnabling autonomous robots to safely and efficiently
importance to water resource management, hydrogeology, ar-
system design and algorithmic integration of a visual servoing
framework for semantically guided autonomous underwater cave
exploration. We present the hardware and edge-AI design consid-
erations to deploy this framework on a novel AUV (Autonomous
Underwater Vehicle) named CavePI. The guided navigation is
driven by a computationally light yet robust deep visual per-
ception module, delivering a rich semantic understanding of the
environment. Subsequently, a robust control mechanism enables
CavePI to track the semantic guides and navigate within complex
cave structures. We evaluate the CavePI system through field
experiments in natural underwater caves and spring-water sites,
and further validate its ROS (Robot Operating System)-based
digital twin in a simulation environment. Our results highlight
how these integrated design choices facilitate reliable navigation
under feature-deprived, GPS-denied, and low-visibility condi-
tions. The system design, code, and data are available on the
project website:  AUV.
I. INTRODUCTION
Autonomous underwater cave exploration is an important
area of research, providing valuable insights into some of the
planets least explored ecosystems to understand and manage
marine resources. Underwater caves, in particular, hold im-
mense potential for advancing our knowledge of archaeology,
within these caves serve as critical records of historical climate
and geology events while playing a key role in monitoring
groundwater flow within Karst topographies, which supply
freshwater to nearly a quarter of the global population .
hazardous for human divers due to their confined spaces,
complex geometries, and lack of natural light [40, 5]. To
this end, Autonomous Underwater Vehicles (AUVs) present
a promising solution, offering safe and efficient exploration
while minimizing risks and improving precision .
caves presents significant challenges: navigation is difficult
without GPS, while poor visibility, backscattering, and silt
disturbances hinder perception. Thus, uninformed exploration,
as often done by AUVs in open-water ecological surveying
and mapping tasks , is quite challenging. For autonomous
navigation inside underwater caves [47, 66], it is important
Fig. 1: The CavePI AUV navigates by leveraging the semantic
guidance of a caveline from its down-facing camera. A deep visual
perception module extracts the semantic cues, which are processed
by an onboard planner to make visual servoing decisions.
to be aware of the entry and exit directions as well as a
semantic understanding of existing navigational markers, when
available. More specifically, underwater caves explored by
humans are augmented by a line (string), referred to as the
the major sections of the cave. Other, navigational markers
such as arrows and cookies  indicate the closest path to
the exit (arrows directions) and the presence of landmarks
andor divers (cookies); real-life examples and demonstrations
are provided in the the supplementary video. The caveline, to-
gether with the navigational markers, provides a 1D retraction
of the 3D environment, indicating the orientation and distance
from the entrance of the primary passages.
In this paper, we demonstrate the system design and al-
gorithmic integration of CavePI, a novel AUV specifically
developed for navigating underwater caves using semantic
guidance from cavelines and other navigational markers; see
Fig. 1. Designed for single-person deployments, CavePIs
compact design and 4-degree-of-freedom (4-DOF) motion
model enables safe traversal through narrow cave passages
with minimal silt disturbance. It features a forward-facing
camera for visual servoing and diver-robot interaction, and a
downward-facing camera for caveline detection and tracking,
effectively minimizing blind spots as low as 30below the
vides altitude data to maintain a safe distance from the ground
plane. Additionally, two onboard lights enhance visibility in
low-light conditions, complemented by on-device software
visual filters  for improved perception. The computational
framework of CavePI includes two single-board computers
(SBCs): a Jetson Nano for perception, and a Raspberry Pi-
5 for planning and control.
For visual servoing, we integrate a caveline detector that
performs real-time pixel-level segmentation of the caveline
using the robots down-facing camera stream. While prior
studies have proposed vision transformer (ViT)-based frame-
works such as CL-ViT  and CaveSeg  for caveline
unsuitable for the resource-constrained SBCs. To address
bileNetV3  and ResNet101   combined with a
lightweight DeeplabV3  as the segmentation head. By
leveraging GPU-accelerated computation on a Jetson Nano
18.2 frames per second (FPS) while ensuring robust seg-
mentation performance. The resulting segmentation map is
post-processed to extract caveline contours, which are used
to guide a proportional-integral-derivative (PID)-based Pure
Pursuit controller  for precise heading adjustments for
smooth tracking-by-detection . Note that CavePI leverages
the caveline position and directions for semantic guidance,
while the ground plane is ranged by sonar. Other markers:
arrows (exit direction) and cookies (divers presence) trigger
momentary decisions such as taking u-turns, invoking logging
strating robust caveline following performances of CavePI.
In addition, we develop a digital twin (DT) of CavePI to
support pre-mission planning and testing, providing a cost-
effective platform for validating mission concepts. Built using
the Robot Operating System (ROS) and simulated in an un-
derwater environment via Gazebo, it addresses the challenges
posed by expensive and delicate underwater technologies that
require stringent safety standards for testing. Even verified
systems can exhibit unforeseen behavior due to new tech-
nological components; for example, a novel caveline detec-
tion algorithm might erroneously identify unrelated objects
as cavelines, leading CavePI to follow incorrect paths and
potentially creating hazardous conditions. The DT allows for
ter technologies, algorithms, or system modifications before
physical deployment, ensuring safer and more economical
evaluations. Moreover, as marine exploration missions require
substantial logistical planning and time investment, initial
mission planning and performance assessments conducted on
the DT facilitate rapid feedback and iterative improvements,
enhancing overall mission efficiency.
CavePIs guided navigation capabilities are initially evalu-
ated by line-following experiments conducted in a controlled
of 1.5 m. Cavelines are arranged in both regular geometric
patterns (e.g., circles, rectangles) and irregular configurations
such as curves, spirals, and vertical slopes. The line-following
accuracy is quantified by measuring the tracking error, defined
as the distance between the line and the optical center of the
down-facing camera on the plane of the line. Similar evalu-
ations are performed in the Gazebo simulation environment
using the DT. Additionally, the delay in decision-making and
its impact on an overshoot at sharp corners are analyzed.
controller are fine-tuned via exhaustive search, and overshoots
are minimized for subsequent runs.
Following extensive in-house testing and refinement, CavePI
is deployed in diverse real-world environments, including 1.5-
6 m deep spring waters and 12-30 m deep natural underwater
grottos and caves. These outdoor settings pose unique percep-
tion and navigation challenges, such as strong currents and
turbidity in open waters, as well as noisy, low-light conditions
within overhead environments. These deployments validate the
robustness of CavePI, demonstrating its capability to perform
long-term autonomous missions in complex underwater envi-
ronments. Our long-term goal is to make it more generalizable
for exploring any overhead structures (e.g., inside ship hulls,
Need for a new AUV. The existing AUVs, such as SUN-
capabilities for marine exploration tasks. We designed CavePI
specifically with (i) a small footprint: one person can carry
and deploy it; (ii) a down camera dedicated for tracking
navigation markers; (iii) a single-enclosure, low-cost, and low-
power design with 4 hours of endurance, as cave missions
take longer than open-water tasks. The size, weight, cost, and
power statistics of existing robots are not the ideal fit. For
8.6 kg with 40, 432 cm3 volume (maximum width: 38 cm).
of dollars; in comparison, the overall cost of CavePI is less
than 3,500. On the other hand, UX1 requires on-land logistic
support and relies on a slow pendulum system for pitch
control  making it unsuitable for turbulent cave environments.
BlueROV2 or other ROVs are not autonomous, and thus re-
quire significant modifications (additional cameras, autopilot,
etc.) to be deployed autonomously.
II. BACKGROUND  RELATED WORK
A. Autonomous Underwater Cave Exploration
While SOTA AUVs are proficient in executing fully au-
tonomous missions, their deployments have primarily been
Fig. 2: The proposed CavePI system design is shown; (a) isometric 3D view of the robot; (b) side-view and top-view displaying the outer
enclosure; (d) the fully assembled system. CavePI is one-person deployable, weighs 8.8 kg, and has a depth rating of 65 meters (213 ft).
constrained to open water environments , where obsta-
cles are minimal. Underwater caves, however, present unique
and formidable navigation challenges, including overhead
and surrounding obstacles, narrow passageways, and complex
topologies. Early efforts to address these challenges include
the work of Mallios et al. , where manually operated
AUVs collected acoustic data for offline mapping. Similarly,
Weidner et al. [63, 62] utilized stereo camera systems to
generate high-resolution 3D reconstructions of cave walls,
Vision-based state estimation and navigation in underwater
caves pose significant challenges due to factors such as lighting
To overcome these obstacles, Rahman et al. [42, 44] proposed
a data fusion framework that integrates visual, acoustic, iner-
estimation and sparse representations of underwater cave envi-
ronments. Advances such as shadow-based mapping , con-
tour extraction , and real-time stereo reconstruction
have further enhanced the density and fidelity of cave mapping
techniques. More recently, visual learning-based approaches
have shown promise for autonomous visual servoing within
underwater caves [1, 66]. Other contemporary works with the
SUNFISH AUV  have demonstrated effective navigation
capabilities in these environments by employing Doppler
Velocity Log (DVL)-based dead reckoning and sonar-based
SLAM algorithms. However, it is not one-person portable, and
requires considerable logistics for successful deployment.
B. Robot Navigation by Semantic Guidance
Terrestrial and aerial robotic systems benefit from their
feature-rich surroundings, leveraging semantic knowledge for
navigation; examples include detecting road lanes , traf-
fic signs , power lines [7, 2] riverbanks , and sea
horizon lines . Traditional approaches for extracting such
features include edge detection  and line segment analy-
sis . Advanced learning-based methods employ conditional
random fields (CRFs) , convolutional neural networks
(CNNs) , and Vision Transformers (ViTs) [13, 37] for
robust semantic scene parsing. More recently, vision-language
utilized to generate semantically meaningful embeddings for
high-level scene comprehension and robot navigation [21, 12].
semantic features . Underwater robots rely on multi-modal
sensing e.g., scanning sonars for additional cues and naviga-
tion guidance [6, 59, 67]. To this end, semantic knowledge
representation of targets  has been proven effective for path
planning and real-time decision-making of AUVs in partially-
known dynamic environments . Additionally, semantic
mapping techniques using laser scanners are employed for
subsea pipeline following, inspection, and intervention .
Recent studies also utilize 3D lasersonar point clouds for
underwater landmark recognition  and eventually integrate
them in semantic SLAM pipelines .
For navigating underwater caves safely, learning-based
frameworks have been developed for detecting and follow-
ing cavelines  and other navigation markers . How-
optimization to achieve real-time deployments . Addi-
and decision-making capabilities remain underexplored. We
address this gap by conducting comprehensive real-world
evaluations using our CavePI robotic platform, demonstrat-
ing its effectiveness for real-time semantic segmentation and
navigation in complex underwater cave environments.
III. CAVEPI SYSTEM DESIGN
The CavePI AUV design includes three major subsystems:
sensory bay, computational bay, and locomotion bay. Our
proposed system and its components are shown in Fig. 2.
A. Sensor Bay: Acoustic-Optic Perception Subsystem
The CavePI platform includes visual and acoustic sensors:
a front-facing fisheye camera, a downward-facing low-light
within a transparent dome at the head of the AUV, captures
forward-facing visuals with a 160field-of-view (FOV) and
outputs a video feed at 1920  1080 resolution. This camera
detects navigation markers and visual commands (QR codes)
Fig. 3: Major electronics and sensor-actuator connections of CavePI.
from divers, and will assist in obstacle avoidance in the next it-
eration. It is worth noting that the cylindrical enclosure is a 6
tube while the dome has a 4 diameter. A custom interface is
built to connect the two; see section V-A for more details. The
low-light camera, mounted inside the computational enclosure,
captures downward-facing visuals with an 8064FOV, also
at the same resolution and frame rate. Additionally, a Ping2
sonar altimeter-echosounder from Blue Roboticsis mounted
on the underside of the robot; the sonar has a range of 100
of the range, allowing it to detect obstacles directly beneath
CavePI at an output frequency of 10 Hz. These sensory com-
ponents collectively provide robust environmental awareness
for autonomous navigation in challenging underwater caves.
B. Computational Bay
As illustrated in Fig. 2, the computational and electronic
components of CavePI are housed within an acrylic cylindrical
enclosure. This enclosure, with a thickness of 6.35 mm and a
depth rating of 65 meters, forms the main body of the robot,
providing mechanical stability, buoyancy, and waterproof pro-
tection for the electronics. The computational elements include
a Raspberry Pi-5, a NvidiaJetson Nano, and a Pixhawk
flight controller. The Jetson Nano is dedicated to processing
visual data from the cameras, performing image processing
tasks critical for scene perception and state estimation. The
Raspberry Pi-5 manages planning and control modules, en-
suring real-time underwater navigation. The Pixhawk flight
controller acts as a bridge between hardware and software,
receiving actuation commands from the Raspberry Pi-5 and
transmitting them to the thrusters and lights via the MAVLink
communication protocol. Additionally, the Pixhawk integrates
a 9-DOF IMU, offering 3-axis gyroscope, accelerometer, and
magnetometer measurements, which are used to calculate the
attitude of CavePI during underwater operations.
The enclosure also contains the battery compartment, volt-
age regulators, electronic speed controllers (ESCs), and a Bar-
30 pressure sensor. The battery compartment holds a 14.8 V
(18 Ah) battery pack, regulated to power internal components
Fig. 4: Data flow among major computational modules of CavePI is
shown in the form of ROS Topics: red and blue arrows represent
subscribed and published topics in the ROS graph, respectively.
(e.g., cameras, computers) and external components (e.g.,
sufficient power to sustain over 6 hours of operation of CavePI.
Each thruster is controlled by an ESC, which drives the three-
phase brushless motor using PWM signals from the Pixhawk.
The Bar-30 sensor provides high-precision pressure readings
with a resolution of 0.2 mbar and an accuracy of 2 mm, with
a working depth of up to 300 meters. This pressure data is
processed to determine underwater depth, ensuring reliable and
accurate interoceptive perception during operations.
C. Locomotion Bay: Middleware Integration
The end-to-end integration of CavePI ensures that each
computational component operates in sync, tied to a ROS2
Humble-based middleware backbone. The modular design also
allows for future upgrades, ensuring that the CavePI can
be tailored to meet evolving research in marine ecosystem
exploration and monitoring. The sensor-actuator signal com-
munication graph is illustrated in Fig. 3.
The CavePI AUV is designed for low-power operation and
integrates a modular ROS2 framework to support application-
specific perception, planning, and control methods. As de-
picted in Fig. 4, the detector node acquires visual data from
the two cameras to identify the caveline for navigation. The
mission planner node then integrates the caveline informa-
tion with the estimated position data to generate subsequent
waypoints for the mission. Finally, the autopilot controller
node utilizes these waypoints, along with the detected caveline,
positional data, and depth readings from the Bar-30 sensor, to
generate precise actuation signals for the thrusters, enabling
accurate movements and depth control. Additionally, CavePI
can function as an ROV through an optional tether-based
teleoperation module. This module transmits user commands
from a joystick to the onboard Raspberry Pi-5, which processes
the inputs and relays them to the thrusters for manual control.
Power footprint and synchronization. As shown in Table I,
CavePI offers an endurance of 6 hours at maximum capacity.
While it can operate much longer, the battery profile
recommends running it above 20 power; the remaining
capacity tends to deplete rapidly, which is a safety issue for
recovery. While running in full capacity, the integrated system
ensures an overall system throughput of 3.6 Hz. Specifically,
both the cameras run at 30 Hz; the IMU and sonar frequencies
are 100 Hz and 10 Hz, respectively. In our implementation,
software synchronization is achieved via a joint ROS2 pub-
lisher that shares a clock across all sensory topics.
TABLE I: The battery power consumption characteristics of CavePI
(at maximum capacity) over time, showing 6 hours of endurance.
Time (hours)
D. CavePI Digital Twin
We develop a digital twin (DT) model of CavePI by using
the Unified Robot Description Format (URDF), with links
and joints carefully assigned to represent the various CAD
components designed in SolidWorks. To replicate the sensor
suite of the physical CavePI, Gazebo plugins are integrated to
simulate the front-facing camera, down-facing camera, IMU,
pressure sensor, and sonar. Additional plugins are employed to
simulate environmental forces, including buoyancy, thrust, and
hydrodynamic drag, thereby enhancing the physical realism.
A controlled open-water scenario is created in Gazebo to
simulate realistic missions, featuring a thin line arranged in
a rectangular loop to mimic a caveline. Since the simulated
environment lacks real-world perception challenges such as
low light or turbid water conditions, the perception subsys-
tem remains simplified. Instead of deploying computationally
intensive deep visual learning models, simpler edge detection
and contour extraction techniques  are used to identify the
caveline from the down-facing camera feed. The remaining
navigation and control subsystems mirror the real-world im-
plementation and operate via two ROS nodes. The first node
processes the extracted contours to make navigation decisions
and publishes high-level control commands (e.g., yaw angle).
The second node subscribes to these commands, computes the
required thrust and hydrodynamic drag forces, and publishes
them as ROS topics to control the simulated robot model.
Beyond replicating caveline following experiments, we uti-
lize the DT system for preliminary testing and fine-tuning
of new control algorithms. It also enables the simulation
of complex cave environments, such as narrow passages,
dead ends, and sharp turns. Conducting repeated real-world
experiments in such scenarios to improve the control system
can be logistically demanding where the simulation offers an
efficient alternative for extensive evaluation and fine-tuning.
IV. CAVEPI NAVIGATION PIPELINE
The CavePI AUV is designed to autonomously navigate
underwater by following a caveline and other navigation
markers. However, a caveline appears a few pixels wide in the
bottom-facing cameras FOV and is significantly challenging
to detect in noisy conditions with almost no ambient light.
Fig. 5: Simplified model architecture for caveline segmentation is
shown; we use a DeepLabV3  head with two choices for backbone
pipeline for reliable operation.
A. Semantic Guidance: Tracking by Detection
The model selection process plays a crucial role in opti-
mizing accuracy and efficiency on resource-constrained edge
devices. While object detection and semantic segmentation are
two established techniques in visual learning, they produce
different interpretations of the target RoI. Object detection
models generate bounding boxes around the target, which,
given the cavelines irregular shape and orientation, encom-
pass significant background regions, complicating the heading
angle estimation. To address this, we opt for semantic seg-
as a more precise marker for tracking-by-detection.
TABLE II: Edge performances (on a Jetson Nano device) for the two
model configurations (Fig. 5) available in CavePI are compared.
Backbone
Inference rate
GPU usage
MobileNetV3
18.2 fps
ResNet101
Model architecture and design choices. Onboard resource
constraints of CavePI significantly influence the choice of
model architecture. The Jetson Nano, with its shared mem-
ory architecture, allocates GPU memory dynamically from
a limited 2 GB pool, which must also accommodate the
dual camera feeds and Nano-Pi control signal communica-
tion. Considering these limitations, we initially selected the
MobileNetV3 (Large) backbone  for feature extraction.
This lightweight architecture feeds extracted features into a
DeepLabV3  head to generate binary segmentation maps.
While the MobileNetV3-DeepLabV3 combination is computa-
tionally efficient, requiring only about 314 MB GPU memory
for 11 million parameters, it proved insufficient for accu-
rately segmenting the thin caveline in turbid water conditions.
To address this limitation, we adopted a heavier backbone,
model achieved slightly better performance, as highlighted in
Table II, albeit at the cost of reduced inference speed. Both
architectures were initialized with pre-trained weights and
fine-tuned on a custom dataset to enhance caveline detection
accuracy in challenging underwater environments.
Algorithm 1 Visual Servoing Controller of CavePI
Extract set of contours {C} from I
if C  then
State lost no caveline detected
Rotate 360small loops for recovery
if C > 1 then
Sort {C}: {C0, C1, ..., Cn, ...} toward heading
(un, vn) Centroid(Cn)
Cnext arg maxCn{C}
(un ui)2  (vn vi)2
farthest contour from (ui, vi)
(uc, vc) Centroid(Cnext) contour center
slope((ui, vi), (uc, vc)) next heading angle
v f() thruster commands
Send v to thrusters
Dataset and model training. For this study, we utilized
the open-source CL-ViT dataset, which includes pixel-level
annotations of cavelines . It comprises 3, 150 RGB images
collected from three distinct underwater cave systems. To
enhance the datasets diversity and variance, we manually
annotated an additional 150 images from our laboratory
The dataset was divided into a training set of 3, 200 images
and a validation set of 100 images. Both models were fine-
tuned on this dataset over a maximum of 50 epochs using the
following training configurations: the Adam optimizer with an
initial learning rate of 0.001, a cosine learning rate scheduler
with Tmax  20, and a cross-entropy loss function. This
training configuration was designed to optimize these models
performances in diverse underwater environments.
Benchmark comparison. The fine-tuned models are evaluated
on the open-source CL-Challenge dataset  that contains
caveline annotations for 200 noisy low-light images from dif-
ferent cave systems. Using the lighter MobileNetV3 backbone,
our model achieves a mean intersection-over-union (mIoU) of
48.95, whereas the considerably heavier ResNet101 model
demonstrates only a marginal improvement with a mIoU of
50.48. For reference, the highest mIoU score reported on
this dataset is 58.3 . Since smooth real-time operation
requires a minimum control loop of 3.6 Hz, we opt for the
MobileNetV3 backbone that exceeds this requirement with an
inference speed of 18.2 fps (frames per second).
Model compression and integration. The two aforemen-
tioned models are optimized to deploy on the Jetson Nano
device of CavePI. First, the models are converted into Open
Fig. 6: A few visual servoing test cases are shown; (a) AUV heading
is estimated based on its farthest caveline contour; (b,c) scenarios
for straight and turn decisions, respectively; and (d) an overshoot
scenario occurred due to delayed decision-making.
Neural Network Exchange (ONNX)  format that uses com-
putational graphs (nodes and edges) to represent operations
and data flow within the pipeline. Subsequently, a serialized
data structure is created by NVIDIAs TensorRT SDK ,
which stores highly optimized deep learning models ready
for fast inference on NVIDIA GPUs. Table II compares the
onboard performance of the two models used in this work.
B. Autonomous Control
Algorithm 1 outlines the visual servoing control algorithm
employed by CavePI. The process begins by extracting cave-
line contours, C, from the segmentation map I, for semantic
guidance. As the caveline is often detected as fragmented
contours (see Fig. 6), the center of the farthest contour (uc, vc)
is identified and selected as the next waypoint in CavePIs
path planning. The heading angle  of this waypoint is then
calculated with respect to the image center (ui, vi), about the
x-axis of the image frame, following the right-hand rule. This
heading angle serves as a high-level navigation command,
which is transmitted to the Raspberry Pi-5 for execution within
the computational subsystem.
The centroid of all detected contours in the set C is
(un, vn). The farthest contour is determined by:
(uc, vc)  arg max
(un ui)2  (vn vi)2
The instantaneous heading vector is defined as the vector
from the image center (ui, vi) to the centroid of the farthest
detected contour (uc, vc), represented as
The equation of the instantaneous line of motion in the image
frame is given by:
where (u, v) represents the free variable on the line.
the robots heading direction, enabling the computed heading
angle to function as the primary error signal for a PID con-
troller on the Raspberry Pi-5. The PID controller is calibrated
to minimize this error, maintaining CavePIs heading toward
the designated waypoint. The controller-generated signals are
transmitted via the MAVLink communication protocol to the
electronic speed controllers (ESCs), which regulate the speed
and rotational direction of the thrusters. Upon failure to detect
any caveline, CavePI executes a circular search pattern to re-
acquire tracking. If a line is not detected, it signals flashing
lights (helps if divers are nearby); subsequently, it starts
navigating towards the (last seen) arrow direction (toward the
cave exit) to rediscover the caveline andor exit the cave.
utilizing a PID controller, which processes the error between
the current and desired depths and generates appropriate
control signals to achieve stable depth regulation.
V. SYSTEM EVALUATION
A. Mechanical Stability under Hydrostatic Pressure
The CavePI system is evaluated for structural robustness
and functional durability to support long-term autonomous
operation at depths. Critical components of AUVs generally
include the interfacing joints of the actuators (thrusters),
sensors (e.g., cameras and sonar), and enclosure doors . As
shown earlier in Fig. 2, CavePIs head section is connected to
the main body via a custom-designed aluminum plate, referred
to as the dome connector, which ensures a watertight interface
for the forward-facing camera. Similarly, the clamps used in
the thruster and sonar subassemblies, as well as the plates
for electronics mounting within the computational enclosure,
are custom-designed to enhance mechanical efficiency and
dynamic stability. Components exposed directly to the under-
water environment, including thrusters, the Ping2 sonar, the
dome connector, and external cables, are identified as sensitive
elements requiring careful design and validation to ensure
system reliability under challenging conditions.
The thrusters and Ping2 sonar have depth ratings of more
than 500 meters and 300 meters, respectively, along with
their cables. To evaluate the structural strength of the dome
connector of CavePI, we use finite element analysis (FEA)
methods . Tetrahedral elements are utilized to generate the
finite element mesh from the 3D model of the dome connector,
while the Von Mises yield criterion is applied to assess the
maximum stresses, total deformation, and factor of safety
(FoS). The analysis considered hydrostatic pressures [29, 28]
at the maximum water depth of 65 m.
The number of nodes and finite elements in the mesh
indicates the mathematical models quality: a higher count
typically reflects a more accurate model. In the analysis of
Fig. 7: (a) FEA mesh of the dome connector; (b) its total deformation
due to hydrostatic forces at 65 m water depth; (c) equivalent stresses
as per Von-Mises yield criterion; and (d) maximum stresses at the
zoomed-in area (best viewed digitally at 2 zoom).
the dome connector, the mesh contained 4, 636, 236 nodes
and 3, 300, 641 elements; see Fig. 7. Based on the Von-Mises
15.3 MPa. The maximum deformation, observed at the inner
most circumference of the dome connector, was 0.005 mm.
The minimum FoS is calculated using the yield strength (y)
of the material (Aluminium 6061-T6), and maximum stress
(max) as follows:
Note that a FoS greater than 2 for hydrostatic applications and
4 for hydrodynamics applications is considered sufficient [25,
58]. Thus, an FoS over 15 indicates that the dome connector
is sufficiently stable for the intended operation pressure.
B. Caveline Following Performance
Setup. The line following experiments are initially conducted
in a 2 m3 m laboratory water tank with a maximum depth
of 1.5 m. As a first setup, a line is laid on the tank floor in
a rectangular loop, without any depth variation; see Fig. 8.
simulate the rough underwater terrains; see Fig. 15 (a). Upon
detecting the line via the down-facing camera, CavePI au-
tonomously follows the loop while maintaining a specified
depth. The systems line-following accuracy, depth-holding
tuned in preparation for outdoor deployments.
Evaluation process. CavePIs caveline-following performance
is evaluated by measuring the tracking error, defined as
the perpendicular distance between the optical center of the
Fig. 8: The laboratory setup used for tracking accuracy evaluation is shown. CavePI detects and follows the line laid on the tank floor,
completing a loop around the 6 meter perimeter in approximately 2 minutes.
(a) Depth Control Accuracy
(b) Tracking Accuracy
Fig. 9: Line-following and depth-holding accuracy are reported for a
laboratory experiment: (a) The target depth was set to 0.35 m; (b) the
tracking error (), i.e., the offset of the optical center of the camera
from the detected line is plotted for a rectangular loop following task.
downward-facing camera and the nearest point on the caveline
in the plane of caveline; see Fig. 9. This measurement lever-
ages pose information from CavePIs IMU, depth readings
from the sonar, and visual information from the segmentation
mask of the caveline captured by the down-facing camera.
To formalize the calculation, we define several reference
frames; see Fig. 10. The initial pose of the robot assumes
its front axis is aligned with the caveline and its downward
axis perpendicular to the caveline plane. This reference frame,
denoted as {N}, has its origin at the camera. A corresponding
frame {S}, with the same orientation as {N} but with its origin
Fig. 10: Configurations of the coordinate frames are shown at: (a)
initial time t0, and (b) an arbitrary time t. Note that the {C} is a
(rigid) body-fixed frame attached to the robot while {N} and {S}
are non-rotating (fixed orientation) frames, even though their origins
translate along with the robot.
at the sonar, is also defined. Additionally, we define the global
reference frame {G}, and the CavePI body frame {C} with its
origin at the down-facing camera. The transformation matrix
from any frame a to b is defined as b
The roll, pitch, and yaw angle measurements from IMU are
expressed in the global frame {G} at any time instance.
For a given image I, we extract the caveline contour closest
to the camera center and calculate the distance between the
camera center and the pixel IP on the contour edge, then
convert it to a 3D point CP using:
CP   K1(IP);
where K is the camera intrinsic matrix and  is the depth
scale factor. The rotation of the CavePIs frame with respect
to the sonar frame is calculated as follows:
GR and N
GR are computed from instantaneous and
initial IMU measurements, respectively. Subsequently, CP is
transformed to the sonar frame using:
where the scalars a, b, and c are obtained from the above
equation. The raw depth measurement from sonar
converted to the sonar frame Sd and  is calculated as:
S R(3,3)  Cd, and
TABLE III: Tracking errors from the line following experiments are
compared for various choices of Kp and Kd values of the heading
controller. Here, : mean tracking error (cm), : standard deviation.
TABLE IV: Depth errors from the line following experiments are
compared for various choices of Kp and Kd of the depth controller.
The 3D coordinates of NP is calculated using:
S t  SP .
C. PID Controller Tuning
To ensure stable operation in turbulent water conditions,
CavePIs PID-based Pure Pursuit controller [45, 50] is de-
signed and fine-tuned through extensive experimentation. Pure
Pursuit is a widely used geometric tracking algorithm that
guides a vehicle toward a target point on a path , calculated
at a fixed lookahead distance. By steering the vehicle to
continuously align with this moving target point, the controller
ensures smooth convergence to and following of the intended
Our initial trials reveal challenges, including high overshoot,
difficulty maintaining depth, and instability at sharp corners.
These issues are iteratively calibrated to achieve robust and
reliable motion control. A grid search technique is employed
to optimize the proportional gain (Kp) and differential gain
(Kd) for the two onboard controllers  depth and heading
controllers. For each tested parameter pair, the robot traverses a
complete 6-meter loop in the testbed at a depth of 0.35 meters.
Tracking () and depth errors for each trial are recorded
at 0.6 Hz. Table III and IV summarize the observations of
the tuning parameters combinations for heading and depth
with Kp  3.4 and Kd  0.9 minimized the tracking error,
and therefore employed these values as the heading controller
gains in all subsequent experiments. Similarly, Kp  600 and
Kd  50 are chosen for the depth controller.
D. ROS Prototyping and Gazebo Simulation
The digital twin of CavePI is developed using ROS and
simulated within Gazebos underwater environment to evaluate
its line-tracking performance; see Fig. 11. As a simplified
representation of the physical platform, it enables extensive
pre-deployment testing of mission-critical functionalities. The
digital twin is designed to be positively buoyant, consistent
with the actual CavePI, and receives depth-control inputs to
maintain a certain depth.
Simulating the digital twin under
Fig. 11: The digital twin (DT) of CavePI, modeled in ROS, is used for
virtual testing in Gazebo underwater environment. (a) An isometric
view of the DT in Gazebo; (b,c) Line following experiments in a
hexagonal loop and a lawn-mower pattern, respectively.
realistic underwater conditions allows for detailed analysis of
hydrostatic and hydrodynamic forces, informing component
placement for stable buoyancy and achieving near-zero roll
and pitch angles when stationary. A complete sensor suite is
integrated to confirm optimal camera placement, particularly
for the downward-facing camera essential for caveline detec-
tion. Additionally, it incorporates a PID controller to simulate
CavePIs line-tracking functionality, serving as a platform for
preliminary control algorithm design and validation. Never-
system through experimental trials due to factors such as non-
linear drag forces, caveline detection dependencies, and design
parameter variations between the CAD model and the actual
developed a VR interface for live interactive simulations and
immersive experiments. In this interface, users wear a head-
mounted display (HMD) to engage in an immersive virtual
mission experience; they experience an egocentric view (i.e.,
robots perspective) as well as an external global perspective
in real-time while the DT model of the robot conducts a
mission. Additionally, they can switch from autonomous mode
to teleoperation mode and directly control the CavePI in
Fig. 12: Interactive demonstration setup for CavePIs virtual missions
is shown. A user wears an HMD to experience various camera
perspectives while using a handheld controller.
different virtual worlds; see Fig. 12.
VI. FIELD EXPERIMENTAL DEMONSTRATION
A. Field Trials and Experimental Setups
The guided navigation capability of CavePI was demon-
strated in two distinct real-world environments: (1) shallow
riverine areas (2 m - 6 m depth) near springs outlets; and (2)
deep natural underwater grottos and caves (15 m - 30 m depth).
The experiments included 15 open-water trials in spring areas
and 10 trials inside underwater grottos. In the open-water
patterns and linear configurations along the uneven riverbed.
In contrast, cave trials used an actual caveline, which varied in
challenges. Strong currents near the spring outlets caused
significant drift, particularly when navigating across or against
the flow. Additionally, low-light conditions inside underwater
caves hindered the accurate detection of the semantic markers.
Field deployments were conducted under the supervision of
two support divers responsible for initiating and concluding
the missions. Divers carried QR-code tags to issue visual
commands to CavePI via its front-facing camera. Upon re-
ceiving the start command, the robot activated depth-hold
mode and began line-following while maintaining the specified
depth. A surface station operator monitored each mission
remotely. For cave deployments, a tether connected the robot
to the surface station to enable remote emergency intervention
if necessary (see Fig. 13). The trials highlighted both the
strengths and potential areas for improvement in CavePIs
Fig. 13: (a) A support diver is placing the robot on a caveline to
initiate tracking; (b) two divers are following the robot once tracking
is initiated. Note that the tether is used only for remotely aborting
the mission in case of emergency.
B. Results and Observations
Laboratory tank tests. Sec. V-B details the setup and evalu-
ation procedures for assessing CavePIs line-following perfor-
mance in laboratory tests. During these tests, CavePI operated
at a depth of 0.35 meters below the water surface. The depth
controller demonstrated robust performance, maintaining the
desired depth with a mean error of approximately 2 cm. At the
beginning of each trial, heading control signals were disabled
for 10 seconds to allow CavePI to stabilize at the target depth
after its descent. Consequently, higher tracking errors were
observed during this initial period, as shown earlier in Fig. 9.
(a) Depth Control Accuracy
(b) Tracking Accuracy
Fig. 14: Line-following and depth-holding accuracy are reported for
an open-water experiment: (a) the target depth was set to 0.525 m;
(b) the higher tracking error is caused by strong currents, leading to
loss of tracking on one occasion.
where the tank edges were misidentified as caveline segments
(see Fig. 16 a), leading to overshooting. Despite these transient
demonstrating the robustness of its control strategy.
Open-water tests: Spring riverine environments. These
trials were evaluated using the same methodology described in
Sec. V-B, focusing on both line-tracking and depth-regulation
accuracy. Fig. 14 presents a representative result from a 10-
minute open-water experiment. As expected, the field results
exhibit greater variation compared to the controlled laboratory
experiments due to environmental disturbances. Despite these
trial-specific parameter tuning.
In contrast, the heading controller proved less resilient under
strong currents, resulting in much higher tracking errors. As
illustrated in Fig. 14 b, the system experienced significant
lateral drift, occasionally causing CavePI to lose the caveline.
This limitation became particularly pronounced when traveling
Fig. 15: A few snapshots from our field trials for caveline tracking and following experiments with CavePI are shown. The setups include:
(a) rectangular loops and slopes in a water tank; (b) irregular shapes in Spring riverine water, and (c) low-light underwater cave scenarios
(note that the tether is only for safety and easy recovery). Images are best viewed digitally at 2 zoom; full video demonstration is submitted
in the supplementary files; also available online at:
perpendicular to the current (see Fig. 17 b), as there was
no active lateral control to counter crossflow. Additionally,
the vehicles slightly back-heavy design induced a pitch-up
motion during upstream travel, as shown in Fig. 17 c, further
compromising stability and navigation accuracy.
Low-light tests: nighttime cave environments. These trials
utilized an earlier (ablation) iteration of CavePI equipped with
a three-thruster configuration, in contrast to the newly pro-
posed four-thruster design. The three-thruster setup required
precise orientation of the downward thruster to achieve proper
heave motion; otherwise, thrust forces could inadvertently in-
duce roll. Additionally, the absence of a dedicated roll-control
mechanism posed challenges for stabilization. In contrast,
the four-thruster configuration incorporates two thrusters for
heave motion, enabling dedicated roll control and significantly
improving overall stability during navigation.
Despite being equipped with two onboard lights, CavePI
faced considerable challenges in low-light environments due
to glare and backscattering effects, which impeded accu-
rate semantic perception. Nighttime trials conducted in an
underwater cavegrotto system revealed that the lightweight
segmentation models struggled to detect the caveline from
camera images. During two one-hour dives, support divers
reported instances where submerged tree roots, mosses, and
other thin structures were mistakenly identified as the caveline.
minute. These findings underscore the need for integrating
a more powerful computational platform capable of running
robust segmentation models to enhance CavePIs perception
capabilities in challenging environments.
VII. LIMITATIONS  CHALLENGES
A. Design Aspects
CavePIs onboard components are packed within a single
pressure-sealed tube, leaving minimal space for additional
hardware. Notably, major housing volume is occupied by the
LiPo battery, which can be replaced with a more compact
alternative to create space for future add-ons. We also plan to
upgrade the perception SBC from the existing Jetson Nano
to a Jetson Orin Nano that offers 4 memory with more
Fig. 16: A few perception failure modes are shown. The down-facing
camera falsely detects (a) the edge of the laboratory tank; (b) tree
roots as the caveline, resulting in incorrect tracking.
advanced GPU resources. The redesigned system will also
incorporate a magnetic switching mechanism for seamless
power control. Outside of the enclosure, the 4 thrusters are
arranged to allow control over surge, heave, roll, and yaw
turbulence and reduces thrust efficiency. To address this, we
are investigating alternative thruster placements that will en-
hance motion dynamics without affecting the robots buoyancy
properties and dynamic stability.
B. Perception Challenges
The current onboard sensor suite offers a limited under-
standing of the surrounding 3D environment. For instance,
the Ping2 sonar provides only 1D depth measurements, which
we will replace with an advanced 360scanning sonar system
for enhanced spatial awareness. The scanning sonar, combined
with other state estimation sensors, will map the environment
and effectively avoid obstacles during navigation. Moreover,
the two cameras currently operate independently for different
purposes without synchronization. In the next iteration, we
propose incorporating a 45slanted camera and combining
all the visual feeds into a mosaic vision. This advanced vision
system will offer wider FOV with more peripheral information
and improve visual servoing performance. Additionally, the
current caveline detection model occasionally misidentifies
objects such as submerged tree roots as part of the caveline
as shown in Fig. 16 b, and it sometimes fails to detect the
line under low-light conditions. These issues lead to signif-
icant tracking inaccuracies. Although more computationally
intensive models might address these shortcomings, they are
currently infeasible due to hardware limitations. With the
proposed design modifications for the next iteration, we plan
to adopt a more robust detection model to improve CavePIs
tracking performance. Furthermore, hand gesture recogni-
tion  will be integrated to enable seamless cooperation
between divers and CavePI during underwater cave operations.
With this advanced sensor setup, we will deploy advanced
SLAM algorithms, such as SVIn2  for robust navigation
in GPS-denied underwater cave environments.
C. Smooth 6-DOF Control
Although CavePI is a 6-DOF AUV capable of maneuvering
in a 3D environment, it currently offers active control over
Fig. 17: A few tracking failure modes are shown: (a) CavePI is
correctly following the line; (b) lateral drift under strong currents; (c)
pitching upward due to currents, attributed to its back-heavy design;
(d) overshooting its intended trajectory while moving downstream.
four DOFs: surge, heave, roll, and yaw. In future iterations,
we intend to reposition its thrusters to enable control over
the remaining two DOFs  pitch and sway  thereby en-
hancing the robots maneuverability in complex underwater
environments. Furthermore, CavePIs autonomous control pri-
marily utilizes a proportional-derivative (PD) controller, which
performs effectively under conditions with minimal environ-
mental disturbances. However, this straightforward control
strategy becomes unstable in more complex scenarios, such
as when water currents are present. In such conditions, lower
proportional gains are insufficient for adjusting CavePIs yaw
to align with the cave line, while higher proportional gains
cause overshooting in the robots trajectory during sharp turns
(see Fig. 17 d), attributed to both perception latency and
the limitations of the current control method. Additionally,
nonlinear (primarily quadratic) drag forces significantly impact
the robots stability and must be incorporated into the control
system design. To overcome these challenges, we are develop-
ing a more robust nonlinear adaptive control system to enhance
stability. Concurrently, we are optimizing the communication
between the perception and control modules to reduce latency
and improve overall responsiveness.
VIII. DISCUSSION
Our investigation identifies four primary challenges imped-
ing effective visual servoing and safe navigation of AUVs
within underwater cave environments: (1) accurate scene pars-
ing under noisy sensing conditions, with very little ambient
light; (2) robust state estimation in feature-scarce environments
lacking GPS or external localization aids; (3) reliable tracking
and control mechanisms to maintain visibility of navigation
markers during maneuvering; and (4) the inherent logistical
complexity and risks involved in field deployments for in-
situ experimental validation. These challenges are further
exacerbated in our experiments by the compact form factor,
low inertia, and energy-efficient design of the CavePI platform,
which collectively reduce motion stability in the presence
of unpredictable hydrodynamic disturbances, particularly near
cave entrance and spring outlets. Furthermore, the performance
of our semantic-guided navigation framework hinges on the
accuracy of the underlying semantic segmentation model,
which is challenged by variations in waterbody types, turbid-
cluttered cave systems. These findings highlight the pressing
need for more robust perception and control strategies tailored
to the unique operational demands of underwater caves.
To this end, the broader research community stands to
benefit from our open documentation of system performance
under real-world conditions for both successful outcomes and
failure cases. These findings emphasize the critical need to
balance perception accuracy with computational efficiency to
enhance mission reliability in natural underwater environments
an aspect often underrepresented in controlled laboratory
settings. Another key insight from our field deployments is
the influence of external disturbances on caveline-following
performance. Our results indicate that effective sway control is
essential for enabling compact marine robotic systems to main-
tain stability and trajectory in the presence of strong currents.
This observation underscores the need for further research
into adaptive control strategies that can dynamically respond
to environmental disturbances. Moreover, our integration of
visual tags as a mechanism for human-robot interaction offers
a scalable and practical alternative to tethered communication
for short-range underwater tasks, advancing the potential for
more flexible and intuitive operations.
Building upon this foundational system, our ongoing work
focuses on advancing toward fully autonomous underwater
cave exploration with greater resilience and mission safety.
Recent upgrades include the integration of more powerful
compute (NvidiaJetson AGX Orin instead of Orin Nano),
an adaptive control framework to dynamically compensate for
external disturbances, and multi-modal sensor fusion  to
enhance perception robustness and state estimation accuracy.
Ongoing work is also integrating a more dense segmentation
pipeline (using CaveSeg ) for detecting overhead structures
and companion divers from sparse pointcloud (by SVIn2 )
to improve autonomy. Furthermore, the ability to operate in
partially mapped or entirely unknown caves introduces op-
portunities for innovation in semantic SLAM, risk-aware path
be particularly valuable in human-robot collaborative missions,
where the robot must support shared situational awareness and
dynamic decision-making in real-time.
IX. CONCLUSION  FUTURE WORK
Underwater cave exploration is important in hydrology for
freshwater resource management, as well as in geological and
archaeological studies. Since cave surveys are labor-intensive
and hazardous, fully autonomous platforms can help signifi-
cantly. This work demonstrates CavePI, a portable and cost-
effective AUV tailored for semantic-guided navigation in GPS-
denied underwater environments. The platforms key strength
lies in extracting sparse semantic cues from noisy, low-light
conditions through a fast and precise segmentation pipeline,
enabling intelligent and adaptive navigation. The planning and
control systems have been meticulously designed, rigorously
ensure robust and reliable performance. Comprehensive field
deployments in diverse, challenging underwater environments,
along with extensive digital twin simulations, have demon-
strated CavePIs reliability, adaptability, and operational effi-
ciency. These results underscore its potential as a versatile and
dependable tool for safe, long-term autonomous exploration
of underwater caves. Future iterations will optimize system
performance and capabilities to support a wider range of
applications for exploring any overhead structures (ship hulls,
pipelines) without predefined markers. To achieve this, future
work will focus on enabling zero-shot semantic understanding
and active planning capabilities.
ACKNOWLEDGMENT
This research is supported in part by the U.S. National
Science Foundation (NSF) grants 2330416, 2024741, and
1943205; and the University of Florida (UF) Research grant
132763. We are thankful to Dr. Nare Karapetyan, Ruo Chen,
and David Blow for facilitating our field trials at Ginnie open-
water springs and Blue Grotto. Furthermore, we acknowledge
the help from Woodville Karst Plain Project (WKPP), El Cen-
tro Investigador del Sistema Acufero de Quintana Roo A.C.
(CINDAQ), Global Underwater Explorers (GUE), Ricardo
access to challenging underwater caves and mentoring us in
underwater cave mapping. The authors are also grateful for
equipment support by Halcyon Dive Systems, Teledyne FLIR
REFERENCES
Adnan Abdullah, Titon Barua, Reagan Tibbetts, Zijie Chen,
Md Jahidul Islam, and Ioannis Rekleitis.
Semantic Segmentation and Scene Parsing for Autonomous Un-
derwater Cave Exploration. In IEEE International Conference
on Robotics and Automation (ICRA). IEEE, 2024.
Dimitrios Alexiou, Georgios Zampokas, Evangelos Skartados,
Kosmas Tsiakas, Ioannis Kostavelis, Dimitrios Giakoumis, An-
tonios Gasteratos, and Dimitrios Tzovaras. Visual Navigation
Based on Deep Semantic Cues for Real-Time Autonomous
Power Line Inspection.
In International Conference on Un-
manned Aircraft Systems (ICUAS), pages 12621269, 2023.
Frederick
Alexandre
Duriez. Finite Element Method-Based Kinematics and Closed-
Loop Control of Soft, Continuum Manipulators. Soft robotics,
Diego Renan Bruno and Fernando Santos Osorio.
Classification System Based on Deep Learning Applied to The
Recognition of Traffic Signs for Intelligent Robotic Vehicle
Navigation Purposes. In Latin American Robotics Symposium
(LARS) and Brazilian Symposium on Robotics (SBR), 2017.
Peter L Buzzacott, Erin Zeigler, Petar Denoble, and Richard
Vann. American Cave Diving Fatalities 1969-2007. Interna-
tional Journal of Aquatic Research and Education, 3(2):7, 2009.
Massimo Caccia, Gabriele Bruzzone, and Gianmarco Veruggio.
Sonar-Based Guidance of Unmanned Underwater Vehicles. Ad-
vanced robotics, 15(5):551573, 2001.
Alexander Ceron, Ivan Mondragon, and Flavio Prieto. Onboard
Visual-Based Navigation System for Power Line Following with
UAV. Int. Journal of Advanced Robotic Systems, 15(2), 2018.
Liang-Chieh Chen.
Rethinking Atrous Convolution for Se-
mantic Image Segmentation. arXiv preprint arXiv:1706.05587,
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. DeepLab: Semantic Image
Segmentation with Deep Convolutional Nets, Atrous Convolu-
Analysis and Machine Intelligence, 40(4):834848, 2017.
Ling Ding, Huyin Zhang, Jinsheng Xiao, Cheng Shu, and
Shejie Lu.
A Lane Detection Method Based on Semantic
Segmentation. Computer Modeling in Engineering  Sciences,
Samsung Energy Business Division. Introduction of INR18650-
INR18650-30Q-Data-Sheet.pdf?x70095, 2014. Accessed: 04-
Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Pira-
Using CLIP for Zero-Shot Vision-and-Language Navigation.
arXiv preprint arXiv:2211.16649, 2022.
Heming Du, Xin Yu, and Liang Zheng. VTNet: Visual Trans-
former Network for Object Goal Navigation.
arXiv preprint
Sheck Exley. Basic Cave Diving: A Blueprint for Survival. Cave
Diving Section of the National Speleological Society, 1986.
Derek Ford and Paul Williams. Introduction to Karst. John
Wiley  Sons, Ltd, 2007.
Evgeny Gershikov, Tzvika Libe, and Samuel Kosolapov. Hori-
zon Line Detection in Marine Images: Which Method to
International Journal on Advances in Intelligent
Yogesh Girdhar, Nathan McGuire, Levi Cai, Stewart Jamieson,
Seth McCammon, Brian Claus, John E San Soucie, Jessica E
robot for ecosystem exploration. In 2023 IEEE International
Conference on Robotics and Automation (ICRA), pages 11411
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
Khadidja Himri, Pere Ridao, Nuno Gracias, Albert Palomer,
Narcs Palomeras, and Roger Pi.
Semantic SLAM for an
AUV Using Object Recognition from Point Clouds.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen,
Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming
Searching for MobileNetV3.
In IEEECVF International Conference on Computer Vision
(ICCV), pages 13141324, 2019.
Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Bur-
gard. Visual Language Maps for Robot Navigation. In IEEE
International Conference on Robotics and Automation (ICRA),
Md Jahidul Islam, Youya Xia, and Junaed Sattar. Fast Underwa-
ter Image Enhancement for Improved Visual Perception. IEEE
Robotics and Automation Letters (RA-L), 5(2):32273234, 2020.
Md Jahidul Islam, Alberto Quattrini Li, Yogesh A Girdhar, and
Ioannis Rekleitis. Computer Vision Applications in Underwater
Robotics and Oceanography.
Computer Vision: Challenges,
Bharat Joshi, Sharmin Rahman, Michail Kalaitzakis, Brennan
nis Rekleitis. Experimental Comparison of Open Source Visual-
Inertial-Based State Estimation Algorithms in the Underwater
Domain. In IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 72217227, 2019.
Masoud R Kazemi.
Reliability Based Analysis and Design
of Anchor Retrofitted Concrete Gravity Dams.
In World
Conference on Earthquake Engineering, 2004.
Michael J Lace and John E Mylroie. The Biological and Ar-
chaeological Significance of Coastal Caves and Karst Features.
In Coastal Karst Landforms, pages 111126. Springer, 2013.
Jin Han Lee, Sehyung Lee, Guoxuan Zhang, Jongwoo Lim,
Wan Kyun Chung, and Il Hong Suh. Outdoor Place Recog-
nition in Urban Environments Using Straight Lines. In IEEE
International Conference on Robotics and Automation (ICRA),
Yi Li, Andres Villada, Shao-Hao Lu, He Sun, Jianliang Xiao,
and Xueju Wang. Soft, Flexible Pressure Sensors for Pressure
Monitoring Under Large Hydrostatic Pressure and Harsh Ocean
Environments. Soft Matter, 19(30):57725780, 2023.
Marianela Machuca Macas, Jose Hermenegildo Garca-Ortiz,
Taygoara Felamingo Oliveira, and Antonio Cesar Pinho Brasil
Junior. Numerical Investigation of Dimensionless Parameters in
Carangiform Fish Swimming Hydrodynamics. Biomimetics, 9
Angelos Mallios, Pere Ridao, David Ribas, Marc Carreras, and
Richard Camilli. Toward Autonomous Exploration in Confined
Underwater Environments.
Journal of Field Robotics, 33(7):
Travis Manderson, Juan Camilo Gamboa Higuera, Stefan Wap-
and Gregory Dudek. Vision-Based Goal-Conditioned Policies
for Underwater Navigation in the Presence of Obstacles, 2020.
Alfredo Martins, Jose Almeida, Carlos Almeida, Andre Dias,
Nuno Dias, Jussi Aaltonen, Arttu Heininen, Kari T Koskinen,
Claudio Rossi, Sergio Dominguez, et al. Ux 1 system design-
a robotic system for underwater mining exploration. In 2018
IEEERSJ International Conference on Intelligent Robots and
Systems (IROS), pages 14941500. IEEE, 2018.
Quentin Massone, Sebastien Druon, Yohan Breux, and Jean Tri-
boulet. Contour-based Approach for 3D Mapping of Underwater
Galleries. In Global Oceans: SingaporeUS Gulf Coast, 2020.
Mohammadreza
Aishneet
Edge-Centric Real-Time Segmentation for Autonomous Under-
water Cave Exploration. In IEEE International Conference on
Machine Learning and Applications (ICMLA), 2024.
TensorRT.
ONNX. Open Neural Network Exchange.  2017.
Shivam K Panda, Yongkyu Lee, and M Khalid Jawed. Agronav:
Autonomous Navigation Framework for Agricultural Robots
and Vehicles using Semantic Segmentation and Semantic Line
Detection. In IEEECVF Conference on Computer Vision and
Pattern Recognition, pages 62726281, 2023.
Pedro Patron, Emilio Miguelanez, Joel Cartwright, and Yvan R
Petillot.
Semantic Knowledge-Based Representation for Im-
proving Situation Awareness in Service Oriented Agents of
Autonomous Underwater Vehicles. In OCEANS, 2008.
Pedro Patron et al. Semantic-based Adaptive Mission Planning
for Unmanned Underwater Vehicles. PhD thesis, Citeseer, 2010.
Leah Potts, Peter Buzzacott, and Petar Denoble. Thirty Years of
American Cave Diving Fatalities. Diving Hyperbaric Medicine,
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual
Models from Natural Language Supervision. In International
conference on machine learning, pages 87488763, 2021.
Sharmin Rahman, Alberto Quattrini Li, and Ioannis Rekleitis.
Sonar Visual Inertial SLAM of Underwater Structures. In IEEE
International Conference on Robotics and Automation, pages
Sharmin Rahman, Alberto Quattrini Li, and Ioannis Rekleitis.
Contour based Reconstruction of Underwater Structures Using
In IEEERSJ
International Conference on Intelligent Robots and Systems
(IROS), pages 80488053, 2019.
Sharmin Rahman, Alberto Quattrini Li, and Ioannis Rekleitis.
International Journal of Robotics Research, 41(11-12):
Arturo L Rankin, Carl D Crane III, and David G Armstrong II.
Evaluating a pid, pure pursuit, and weighted steering controller
for an autonomous land vehicle. In Mobile Robots XII, volume
Kristof Richmond, Chris Flesher, Laura Lindzey, Neal Tanner,
and William C Stone. Sunfish: A human-portable exploration
auv for complex 3d environments. In OCEANS 2018 MTSIEEE
Kristof Richmond, Chris Flesher, Neal Tanner, Vickie Siegel,
and William C Stone.
Autonomous Exploration and 3-D
Mapping of Underwater Caves with the Human-portable SUN-
FISH AUV. In Oceans: SingaporeUS Gulf Coast, 2020.
Avilash Sahoo, Santosha K Dwivedy, and PS Robi. Advance-
ments in The Field of Autonomous Underwater Vehicle. Ocean
Moveh Samuel, Mohamed Hussein, and Maziah Binti Mo-
A review of some pure-pursuit based path tracking
techniques for control of autonomous vehicle.
International
Journal of Computer Applications, 135(1):3538, 2016.
Louis L Scharf, William P Harthill, and Paul H Moose.
comparison of expected flight times for intercept and pure pur-
suit missiles. IEEE Transactions on Aerospace and Electronic
Dhruv Shah, Bazej Osinski, Sergey Levine, et al.
Robotic Navigation with Large Pre-trained Models of Language,
In Conference on robot learning, pages
Florian Shkurti, Wei-Di Chang, Peter Henderson, Md Jahidul
Multi-Robot Convoying using Visual Tracking by Detection. In
IEEERSJ International Conference on Intelligent Robots and
Systems (IROS), pages 41894196, 2017.
Thomas Jeongho Song. Experimental Evaluation of Underwater
Semantic SLAM.
Masters thesis, Massachusetts Institute of
Sumedh Sontakke, Jesse Zhang, Seb Arnold, Karl Pertsch,
Erdem Byk, Dorsa Sadigh, Chelsea Finn, and Laurent Itti.
cies. Adv. in Neural Information Processing Systems, 36, 2024.
Lorenzo Steccanella, Domenico Bloisi, Jason Blum, and
Alessandro Farinelli. Deep Learning Waterline Detection for
Low-Cost Autonomous Boats.
In Proceedings of the 15th
International Intelligent Autonomous Systems Conference (IAS-
15), pages 613625. Springer, 2019.
Satoshi Suzuki and KeiichiA be. Topological Structural Analy-
sis of Digitized Binary Images by Border Following. Computer
Mehron Talebi, Sultan Mahmud, Adam Khalifa, and Md Jahidul
munication Using Compact Magnetoelectric Antennas. ArXiv
preprint arXiv:2411.09241, 2025.
SafetyCulture Content Team.
Factor of Safety: Ratio for
Safety in Design and Use.
Pedro V Teixeira, Dehann Fourie, Michael Kaess, and John J
Leonard.
Scenes. In International Conference on Intelligent Robots and
Systems (IROS), pages 80608066, 2019.
Guillem Vallicrosa, Khadidja Himri, Pere Ridao, and Nuno Gra-
cias. Semantic Mapping for Autonomous Subsea Intervention.
Weihan Wang, Bharat Joshi, Nathaniel Burgdorfer, Konstantinos
Rekleitis. Real-Time Dense 3D Mapping of Underwater Envi-
ronments. In IEEE International Conference on Robotics and
Automation (ICRA), 2023.
Nicholas Weidner. Underwater Cave Mapping and Reconstruc-
tion Using Stereo Vision. Masters thesis, Computer Science and
Engineering Department, University of South Carolina, 2017.
Nicholas Weidner, Sharmin Rahman, Alberto Quattrini Li, and
Ioannis Rekleitis.
Underwater Cave Mapping using Stereo
In IEEE International Conference on Robotics and
Automation (ICRA), pages 5709  5715, 2017.
Anqi Xu, Gregory Dudek, and Junaed Sattar. A Natural Gesture
Interface for Operating Robotic Systems. In IEEE Int. Conf. on
Robotics and Automation, pages 35573563, 2008.
Ping Yang, Changhui Song, Linke Chen, and Weicheng Cui.
Image Based River Navigation System of Catamaran USV
with Image Semantic Segmentation.
In WRC Symposium on
Advanced Robotics and Automation, pages 147151, 2022.
Boxiao Yu, Reagan Tibbetts, Titon Barna, Ailani Morales,
Ioannis Rekleitis, and Md Jahidul Islam. Weakly Supervised
Caveline Detection For AUV Navigation Inside Underwater
In IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 99339940. IEEE, 2023.
Fei Yu, Yuemei Zhu, Qi Wang, Kaige Li, Meihan Wu, Guan-
gliang Li, Tianhong Yan, and Bo He. Segmentation of Side
Scan Sonar Images on AUV. In IEEE Underwater Technology
Wenqiang Zhan, Changshi Xiao, Yuanqiao Wen, Chunhui Zhou,
Haiwen Yuan, Supu Xiu, Xiong Zou, Cheng Xie, and Qiliang
Adaptive Semantic Segmentation for Unmanned Surface
Vehicle Navigation. Electronics, 9(2):213, 2020.
