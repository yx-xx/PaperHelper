=== PDF文件: Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization.pdf ===
=== 时间: 2025-07-21 14:03:28.548000 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Physics-Driven Data Generation for Contact-Rich
Manipulation via Trajectory Optimization
Lujie Yang1,2, H.J. Terry Suh1, Tong Zhao2, Bernhard Paus Grsdal1, Tarik Kelestemur2,
Jiuguang Wang2, Tao Pang2, and Russ Tedrake1
1Massachusetts Institute of Technology
2Robotics and AI Institute
Fig. 1: Physics-driven data generation overview. Leveraging trajectory optimization, our framework automatically generates thousands of
dynamically feasible contact-rich trajectories across a range of embodiments and physical parameters from only 24 human demonstrations.
The policy trained with imitation learning from the generated dataset is more robust and performant.
AbstractWe present a low-cost data generation pipeline
that integrates physics-based simulation, human demonstra-
tasks. Starting with a small number of embodiment-flexible
human demonstrations collected in a virtual reality simulation
optimization-based kinematic retargeting and trajectory opti-
mization to adapt them across various robot embodiments and
physical parameters. This process yields a diverse, physically
data transfer, and offers the potential to reuse legacy datasets
collected under different hardware configurations or physical
parameters. We validate the pipelines effectiveness by training
diffusion policies from the generated datasets for challenging
long-horizon contact-rich manipulation tasks across multiple
robot embodiments, including a floating Allegro hand and bi-
manual robot arms. The trained policies are deployed zero-
shot on hardware for bimanual iiwa arms, achieving high
success rates with minimal human input. Project website:
I. INTRODUCTION
The emergence of foundation models has transformed fields
such as natural language processing and computer vision,
where models trained on massive, internet-scale datasets
demonstrate remarkable generalization across diverse reason-
ing tasks [1, 2, 3]. Motivated by this success, the robotics com-
munity is currently pursuing foundation models for generalist
robot policies capable of flexible and robust decision-making
across a wide range of tasks [4, 5, 6], leading to significant in-
dustrial investments in large-scale robot learning . However,
the pursuit for generalist robot policies remains constrained
by the limited availability of high-quality datasets, espe-
cially for contact-rich robotic manipulation. Existing datasets
[5, 8, 9, 10] are orders of magnitude smaller than those used
to train foundation models in other domains, such as Large
Equal contribution. Correspondence to <lujiemit.edu>.
Language Models (LLMs). The scarcity of diverse, high-
fidelity manipulation data limits policy generalization across
different embodiments, task contexts, and physical conditions.
To address data scarcity, robot learning researchers often
rely on a spectrum of data sources varying in cost, quality, and
transferability. The most informative data typically consists of
high-quality demonstrations specific to the task, environment,
and embodiment [5, 8], but such data is costly and time-
consuming to collect, as it requires human teleoperation with
specialized hardware. At the opposite end of the spectrum,
there is a wealth of lower-quality data in the form of internet
videos showing humans and robots performing manipulation
tasks [11, 12, 13, 14]. However, the significant embodiment
gap and limited action labeling make this data difficult to
transfer effectively to robot policies. Simulation data offers
a middle ground, providing the potential to generate large,
[15, 16, 17]. In practice, effective policy learning can be
achieved by co-training on a mixture of data from different
points along this spectrum, reducing data collection costs
while improving generalization [18, 19].
A key insight in this work is that human demonstrations and
model-based planners complement each other in critical ways
for generating high-quality robot data. Human demonstrations,
though costly to collect, offer valuable global information
for solving complex tasks. However, collecting real-world,
contact-rich
manipulation
teleoperation
challenging
multi-contact
to hardware latency, embodiment mismatches between the
human and robot, and the fine-grained control required .
In contrast, trajectory optimization has demonstrated success
in generating locally-optimal trajectories for contact-rich
tasks [21, 22, 23], but often relies on global guidance in the
form of good initial guesses.
In this work, we propose a data generation framework
that leverages the strengths of both approaches: human
demonstrations can provide global guidance, while trajectory
optimization can locally refine these demonstrations to ensure
dynamic feasibility. Starting with a small number of human
demonstrations collected in a virtual reality (VR) environment,
our method uses model-based trajectory optimization to
generate large datasets of dynamically feasible, contact-rich
trajectories in simulation. The demonstrations guide the plan-
ner through complex search spaces, while the planner ensures
physical consistency and robustness across varying physical
parameters and robot embodiments. Our pipeline, visualized
in Fig. 1, enables efficient cross-embodiment data transfer,
where demonstrations collected with one robot configuration
can be adapted to another, and supports domain randomization
for improved generalization and robustness. Additionally, it
provides the potential to revive and adapt legacy datasets
collected with different hardware or configurations, making
old datasets valuable for new robot systems.
Our key contributions include:
1) We present an intuitive, embodiment-flexible demon-
stration interface based on virtual reality and physics
contact-rich manipulation.
2) We propose a scalable framework that leverages trajec-
tory optimization to transform a small number of human
demonstrations into large-scale, physically consistent
initial conditions, and physical parameters.
3) We validate our approach by training policies on the
generated dataset for challenging contact-rich manipu-
lation tasks across multiple robot platforms, including
bimanual robot arms and a floating base Allegro hand.
4) We achieve high success rates in zero-shot hardware
deployment on bimanual iiwa arms, highlighting the
utility of augmented datasets in real-world scenarios.
II. RELATED WORKS
In this section, we review the most relevant approaches
for generating diverse robot data for contact-rich tasks. We
categorize the methods into data collection, data augmentation,
model-based planning, demonstration-guided reinforcement
learning and cross-embodiment transfer.
A. Data Collection for Imitation Learning
Behavior Cloning , which trains robot policies to mimic
expert behavior, has shown impressive empirical results in a
wide range of dexterous manipulation tasks . Collecting
high-quality robot data has been an essential component of
imitation learning (IL). Many such methods rely on human
experts teleoperating a robot to accomplish specific tasks.
Researchers have adopted interfaces such as 3D spacemouse
[25, 26], and puppeteering platforms [27, 28] for end-effector
and whole-body control [29, 30].
Virtual and augmented reality (VRAR) interfaces have
recently gained traction as effective alternatives for robot
data collection [31, 32], reducing cognitive load, physical
like kinesthetic teaching or 3D mouse control . These
technologies offer a more intuitive data collection paradigm
for complex tasks, especially in dexterous manipulation.
AR2-D2  enables data collection without a physical robot
by projecting a virtual robot into the physical workspace,
but lacks real-time feedback necessary for precise control.
DART  supports data collection entirely in simulation,
visualized through a VR headset, but faces challenges
bridging the sim-to-real gap for physical robot deployment.
ARCap  integrates real-time AR feedback, but requires
specialized hardware, including an RGBD camera, motion
capture gloves, and VR controllers, in addition to the AR
headset. ARMADA  enables real-world manipulation
data collection with bare hands through real-time virtual
robot feedback, achieving high success rates when replayed
on physical hardware. In contrast to these existing systems,
our work focuses on scalable data generation from a small
number of human demonstrations by leveraging trajectory
B. Data Augmentation
Despite many research efforts, collecting large datasets re-
mains time-consuming and costly, requiring a large amount of
human effort and resources. To address these challenges, sig-
nificant effort has been devoted to automating the data gener-
ation process through data augmentation techniques. Existing
approaches have leveraged state-of-the-art generative models
for visual [38, 39, 40] and semantic [41, 42, 43] augmenta-
tions. MimicGen  and its bimanual extension DexMim-
icGen  automatically synthesize large-scale datasets from
a small number of human demonstrations. These works de-
compose long-horizon tasks into object-centric subtasks and
replay transformed demonstrations open loop in simulation.
SkillMimicGen  extends this paradigm by segmenting
tasks into motion and skill components, augmenting local
manipulation skills with MimicGen-style replay and using
motion planning to connect these skill segments. RoboCasa
leverages generative models to create diverse kitchen
scenes with abundant 3D assets and utilizes MimicGen for
automated trajectory generation. While these approaches have
shown success in automating data generation, they primarily
rely on kinematic replay of demonstrations, which is often
inadequate for contact-rich manipulation tasks. Our work
can be viewed as an important extension to MimicGen line
of works to support dynamically feasible contact-rich data
and continuous reasoning about making and breaking contacts
with the environment.
C. Trajectory Optimization for Contact-Rich Tasks
Planning and control through contact remains a significant
challenge for both learning-based and model-based methods
due to the explosion of contact modes and the nonsmooth
nature of contact dynamics. To tackle these challenges, re-
searchers have explored various trajectory optimization for-
mulations for multi-contact interactions.
Contact-Implicit Trajectory Optimization Existing works
based on contact-implicit trajectory optimization (CITO) [22,
21] have sought to formulate the combinatorial problem into
a smooth optimization problem by using complementarity
constraints. CITO has been applied in various domains, in-
cluding planar manipulation [48, 49], dynamic pushing ,
and locomotion tasks [51, 52, 53]. Recent efforts have ex-
tended CITO for real-time applications as model predictive
control (MPC) [54, 55], with successful hardware deployment
on quadrupeds using tailored solvers [56, 57]. Aydinoglu et
al.  parallelize the solution of linear complementarity
problems using alternating direction method of multipliers
(ADMM) and validate the method on hardware for multi-
contact manipulation tasks. While CITO shows promising
scalability for handling contact modes, it suffers from poor
global exploration and relies on good initial guesses . A
new line of work tries to address these issues with efficient
global optimization , but does not yet scale to the tasks
we consider here.
Sampling-Based Planning Sampling-based methods have
also shown great promise for solving trajectory optimiza-
tion for contact-rich tasks. Hamalainen et al.  employ
sampling-based belief propagation for humanoid balancing,
juggling and locomotion. Carius et al.  extend the path in-
tegral formulation to handle state-input constraints and validate
the approach on quadruped stabilization on hardware. More
control (SPC) for simpler contact tasks like pushing, while
Howell et al.  and Li et al.  extended SPC to more
Pang et al.  use smoothed contact dynamics with global
sampling to generate contact-rich plans in under a minute, with
performance comparable to reinforcement learning. Cheng et
al. introduce HiDex , a hierarchical planner that combines
Monte-Carlo Tree Search with integrated contact projection,
achieving rapid planning for dexterous manipulation tasks.
contact-rich data generation in behavior cloning can be prob-
degrades downstream policy performance [68, 69].
In this work, we leverage low-entropy human demonstra-
tions to guide the global planning for multi-contact inter-
actions and utilize trajectory optimization to locally refine
the trajectories for specific physical parameters and robot
embodiments. From a small number of demonstrations, the
model-based planner can efficiently generate abundant, high-
D. Demonstration-Guided Reinforcement Learning
While IL often demands a large number of expert demon-
strations to achieve robust and high-performing policies, re-
inforcement learning (RL) aims to solve tasks autonomously
through reward-driven exploration. However, pure RL can
suffer from inefficient exploration and the need for extensive
reward shaping, especially in complex manipulation tasks
[70, 71]. To address these challenges, researchers have ex-
plored using demonstrations to guide RL, improving both
sample efficiency and exploration quality.
Demonstrations have been integrated into RL pipelines in
various ways, including adding them directly to the replay
buffer [72, 73], using behavior cloning for policy pretraining
[74, 75, 76], and augmenting task rewards with information
extracted from demonstrations [77, 78, 79]. Sleiman et al.
guide RL with demonstrations generated from a model-based
trajectory optimizer for multi-contact loco-manipulation tasks,
and validate their method on hardware with a quadrupedal
mobile manipulator. While these approaches search over the
parameters of a neural network policy and potentially optimize
a more global objective, we leverage trajectory optimization
as a complementary tool to locally refine and expand demon-
stration trajectories. This enables the efficient generation of
contact-rich data while avoiding the computational overhead,
approximation errors, and unnecessary exploration associated
with RLs high-dimensional search space.
E. Cross-Embodiment Generalization
Reusing datasets and policies across different embodiments
unlocks the potential for large-scale robot learning. One line
of work learns latent plans from videos of humans interacting
with the environment and transfers this knowledge for robotic
manipulation [81, 82]. Another approach involves portable
data collection tools, such as hand-held grippers [20, 83],
for in-the-wild human demonstrations. While these methods
enable policy deployment on multiple robot platforms, they
are often constrained to robots with the same end-effector
used during data collection, limiting generalization across
platforms. On the other hand, to leverage large-scale datasets,
recent works pull data from a heterogeneous set of robots
ranging from navigation to manipulation, and train a robotic
foundation model capable of accomplishing a diverse range
of tasks [84, 85]. Our proposed framework enables reusing
the same set of easy-to-collect demonstrations for multiple
for contact-rich tasks.
III. DATA COLLECTION
We present a Virtual Reality (VR)-based data collection
pipeline designed for intuitive and efficient collection of hu-
man demonstrations across multiple robot embodiments. The
pipeline emphasizes simplicity and cross-embodiment general-
ization while minimizing the reliance on physical robot hard-
ware. While we consider the data collection pipeline to be one
of our contributions, we emphasize that the simulation-based
large-scale data generation method presented in the next sec-
tion is independent of this particular data collection approach.
Our data collection pipeline (Fig. 3) is a human-hand
demonstration interface in VR. We use an Apple Vision Pro to
track the poses of the human demonstrators hands and stream
the poses to the Drake physics simulator , which simulates
Fig. 3: VR-based human-hand demonstration framework.
the contact interaction between the object and the hands. The
updated object pose is then sent back to Apple Vision Pro for
real-time visualization in VR using Vuer .
Our demonstration interface is fast and cost-effective. Since
the system operates entirely in simulation, it removes the
dependency on robot hardware, significantly reducing the cost
and complexity of data collection. In practice, it takes approx-
imately 7 minutes to collect 24 long-horizon demos for each
considered system. The setup is also intuitive to use, as the
human demonstrator does not have to mentally close the em-
bodiment gap between the human body and the specific robot.
We demonstrate our pipeline on two different classes of
robot embodiments: a dexterous hand and a bimanual manip-
ulation setup.
Floating Allegro Hand For the dexterous hand, we con-
sider a 22-DOF free-floating Allegro hand manipulating a cube
on a table as shown in Fig. 4. Since the Allegro hand only
has four fingers, we restrict the VR-based demonstrations to
using four fingers on the right hand to interact with the object
in simulation.
Bimanual Robot Arms For the bimanual manipulation
Franka Emika Panda arms. Each pair of arms collaboratively
manipulates a big box (Fig. 4). During the VR demonstrations,
the human demonstrator uses both index fingers to manipulate
a small cube in VR and constrains their wrist movement to
mimic the fixed base. During kinematic motion retargeting
(detailed in Sec. IV-A), the small cube and fingers are scaled
to match the size of the larger box and the robot manipulators.
This design facilitates two forms of cross-embodiment
generalization. First, it utilizes easy-to-collect human finger
demonstrations to guide planning for harder and higher-
dimensional tasks, such as the dual-arm manipulators. Second,
it supports the reuse of the same set of demonstrations across
multiple robot platforms, as both the iiwa and Panda arms can
leverage the same data to accomplish the manipulation task,
eliminating the need for embodiment-specific demonstrations.
IV. AUTOMATED DATA GENERATION
In this section, we present our method for automatically
generating large quantities of physically feasible trajectories
for contact-rich manipulation tasks across a range of objects,
initial conditions, and embodiments from only a handful of
demonstrations. The presented method also offers the potential
to adapt legacy datasets collected using outdated configura-
tions to new robot settings, reducing the cost of collecting
large amounts of data on the new robot setups from scratch.
Our method starts out by retargeting kinematic motions
from the original embodiment-flexible human demonstrations
collected
specific
embodiment
producing
kinematically
feasible
trajectories.
These trajectories are then refined and augmented through
the use of local trajectory optimization to obtain dynamically
feasible trajectories for a range of physical parameters. The
following subsections provide a detailed breakdown of each
step in the pipeline.
A. Kinematic Motion Retargeting
Given a sequence of demonstrations xdemo
with horizon T,
we aim to find the robot configurations qretarget
that match the
positioning of the demonstrator while avoiding penetration and
obeying joint limits. At each time step, we solve the following
nonconvex program:
qretarget
qretarget
wii(qretarget
) i(xdemo
s.t. j(qretarget
qmin qretarget
where wi > 0 are weight parameters, and i and i represent
the i-th mappings from the robot configuration and demon-
strator state to corresponding points on the embodiments. The
corresponding points of interest for each robotdemonstrator
pair are manually defined. For example, on the bimanual
robot arm system, 0 is the forward kinematics from the
robot joint angles to the left robot arms end effector position,
while 0 is a map from the hand pose to the fingertip of
the left index finger. We find the resulting plans generated by
trajectory optimization relatively robust to the correspondence
and weight parameter selection. j denotes the signed distance
function between the j-th collision pair and (1b) enforces non-
penetration constraints. qmin and qmax are the lower and upper
bounds on the joint angles. Notice that qretarget and xdemo can
have different dimensions as long as both i and i map them
to vectors in the same space (e.g., Apple Vision Pro captures
5 landmarks on the index finger while each robot arm has 7
DOF in the bimanual robot arm system). We solve (1) using
a Sequential Quadratic Programming (SQP)-style algorithm:
during each iteration, the nonpenetration constraint (1b) is
linearized and the matching objective (1a) is quadratically
approximated around the solution to the previous iteration.
We warmstart the solution of the nonlinear program at time t
with the optimal solution from the previous timestep qretarget
to encourage faster convergence and temporal consistency.
B. Demonstration-Guided Trajectory Optimization
The kinematically consistent robot trajectories qretarget
generally not dynamically feasible due to the embodiment
gap and differences in physical parameters. However, they
Fig. 4: Human hand demo in VR and kinematic retargeting for different embodiments. The blue spheres illustrate the demo hand landmarks
scaled to the specific system.
can provide good guidance on generating dynamically feasible
trajectories with complex multi-contact interactions. In partic-
when and where to make contact with the object, which model-
based planning can then locally refine. We define the retargeted
system state xretarget
to incorporate both the object state xobject
which is a subset of xdemo
, and the robot state as a function
of qretarget
. The trajectory xretarget
is then locally refined by
solving the following nonconvex optimization program:
t  arg min
xT xretarget
(xt xretarget
s.t. xt1  f(xt, ut)
j(xt) 0, j
xmin xt xmax
umin ut umax.
the dynamics engine, xminxmax (uminumax) are the lower and
upper bounds on the state (input), Qt, Rt are the cost matrices
for the state and input, respectively, and QT is the cost matrix
for the terminal state. To encourage precise tracking of the
object trajectory, we assign higher weights to the entries of
Qt which correspond to xobject
. The detailed parameters can
be found in Appendix X-A.
In general, model-based planners can struggle to discover
high-quality long-horizon contact-rich trajectories without
demonstrations. CITO requires good initial guesses and can
easily get stuck in local optima without making progress.
Human demonstrations offer valuable global guidance that
helps overcome these challenges, and xretarget
can naturally
serve as the initial guess to CITO-based methods where local
adjustments are made to obey dynamical constraints (2b).
Thanks to access to the system dynamics f in simulation,
we can locally perturb the physical parameters as well as
robot and object states around a nominal demonstration. From
the single demonstration, we can solve (2) for a distribution
of tasks with different dynamics f(xt, ut, t), where t
represents all the perturbations. We assume the kinematically
retargeted trajectory xretarget
still provides good guidance on
achieving the task in the vicinity of the nominal demonstration.
This way, a large number of physically consistent trajectories
with various physical properties and initial conditions can be
generated from a single human demonstration. We outline our
data generation pipeline in Algorithm 1.
Algorithm 1: Automated Data Generation
1 Input: Probability distribution , augmentation number
2 Output: N dynamically consistent trajectories on
target embodiments {(x
3 qretarget
Solve (1) for xdemo
Sample 0:T ;
, 0:T and
xt1  f(xt, ut, t);
V. TRAJECTORY OPTIMIZATION EXPERIMENTS
While kinematic retargeting of demonstrations might suffice
to generate data for simpler manipulation tasks such as pick
and place, it often falls short for the more challenging contact-
rich tasks requiring frequent contact mode switches and fine-
grained actions. In this section, we demonstrate that trajectory
optimization is crucial for generating diverse, dynamically
feasible contact-rich trajectories on three high-dimensional
dexterous manipulation systems: a floating Allegro hand, bi-
manual iiwa arms, and bimanual Panda arms.
Our data generation framework is agnostic to the choice
of the trajectory optimizer. We implement the cross-entropy
method (CEM)  to solve (2) over a distribution of physical
parameters and initial conditions, as specified in Table I.
Task Manipulating the object to a target pose on the table
(Fig. 7). The object is initially placed randomly on the table
with an arbitrary face upward. Task success is defined as the
object reaching within 3 cm and 0.2 rad of the target pose
for the Allegro hand, and within 10 cm and 0.2 rad for the
Parameter
Floating Allegro Hand
Bimanual Robot Arms
Init. obj. trans. pert. (cm)
Init. obj. rot. pert. (rad)
Object side length (cm)
Object mass (kg)
Friction coefficients
Task horizon (s)
50  260 (Panda  iiwa)
TABLE I: Ranges of different physical parameters . The initial
object pose is only perturbed in yaw, x, and y to ensure the object
sits stably on the table.
Fig. 5: Trajectory optimization is crucial for generating dynamically feasible trajectories. (Top) Before trajectory optimization, the
kinematically retargeted demos easily lose contact and drive the object out of reach with different physical parameters or slight deviations in
object states. (Bottom) Trajectory optimization encourages robots to establish contact with and maintain good manipulability of the object.
The tricolor axis indicates the object orientation.
Perturbation
Allegro Hand
iiwa Arms
Panda Arms
Original demo
Object size
Initial object translation
Initial object orientation
Trajectory optimization
TABLE II: Success rates of replaying kinematically retargeted trajec-
tories of the 24 original human demos, and trajectory optimization
under random perturbations in physical parameters and object initial
conditions.
bimanual robot arms. This task requires long-horizon reason-
ing of complex multi-contact interactions between the robot
and the object. The necessary frequent contact mode switches
and high-dimensional action space pose great challenges for
traditional model-based planners, while the precise contact
interactions require fine-grained control actions.
Dynamic Feasibility While kinematic motion retargeting
can generate visually plausible robot and object trajectories,
these trajectories often lack dynamical consistency due to the
differences in physical parameters and embodiment between
the human demonstrator and the target robot. To illustrate
original 24 human demos and record the success rates for
each system in Table II. Furthermore, we randomly sample
object sizes and perturbations of initial object poses according
to Table I and roll out the nominal kinematically retargeted
trajectories. Some trajectories still succeed under certain per-
turbations thanks to caging grasps or other strategies that
encourage robustness during the human demonstration. For all
the systems, the successful rollouts are relatively short, manip-
ulating the object to the goal pose within only 1 or 2 rotations.
The low success rate of purely kinematically retargeted tra-
jectories highlights the importance of trajectory optimization
for locally refining the demos for the particular embodiments
and physical parameters. Before trajectory optimization, the
floating Allegro hand lightly touches the cube and easily loses
contact when rotating it clockwise (demonstrated in Fig. 5a).
After trajectory optimization, the hand increases the contact
behavior that encourages contact can be observed for the
bimanual iiwa arms: the demo trajectory tries to rotate the box
clockwise only using a single arm, while trajectory optimiza-
tion encourages the other arm to help hold the box and reorient
the box more stably. These refinements that encourage contact
are particularly helpful when the object is heavier or smaller,
or when the friction coefficients are lower than expected.
In addition, replaying the kinematically retargeted trajectory
often fails when the object pose deviates slightly from the
Fig. 5c). In contrast, trajectory optimization accounts for the
systems true dynamics and can adjust the robots actions
accordingly. The success rates of trajectory optimization under
random perturbations in physical parameters and object initial
conditions for each system are recorded in Table II.
We compare our method to MimicGen , an automatic
data generation pipeline that adapts demonstrations to diverse
spatial configurations through geometric transformations and
replay. To ensure a fair comparison, we collect 24 demon-
strations directly with the floating Allegro hand in simula-
tioneliminating embodiment mismatch between the source
demonstration and target taskand apply MimicGen-style
replay under varying physical parameters (Table I). Pure replay
achieves a success rate of 51, while trajectory optimization
increases this to 86, and smoothes jitters and imperfec-
tions in human demonstrations. Notably, our tasks are highly
switches. In contrast, the contact behaviors in MimicGen
(e.g., pick-and-place, insertion) involve fewer contact switches.
Fig. 6: Distribution and snapshots of trajectories generated from a single demonstration. (a) The original demonstration (orange)
is locally perturbed and augmented to about 100 dynamically feasible contact-rich trajectories (blue) for each system. The density map
represents the object pose distribution of the generated trajectories in the specific 2-dimensional slices. (b) Snapshots of 30 dynamically
feasible trajectories under random physical parameters and object initial poses for bimanual iiwa arms are visualized.
quiring the entire robot to interact with the object. Simply
transforming the end-effector pose in an object-centric manner
as in MimicGen disregards the contact between the rest of the
robot and the object, and can easily result in loss of contact
and task failure.
Cross-Embodiment Generalization We demonstrate that
a single set of human demonstrations can be effectively
repurposed to generate dynamically consistent, contact-rich
trajectories across different robotic embodiments with varying
task horizons. Specifically, human demonstrations involving
two index fingers manipulating a small cube are retargeted
to fixed-base bimanual Kuka LBR iiwa and Franka Emika
Panda arms manipulating a larger box (visualized in Fig. 4).
This approach addresses key challenges in data collection for
contact-rich tasks: directly teleoperating two real robot arms to
flip a large box would be both physically demanding and cost-
prohibitive due to hardware latency, limited feedback, and the
embodiment gapdifferences in kinematic structure, degrees
of freedom, and workspace between human and robotic arms.
In contrast, performing the same task on a smaller scale using
human fingers is more intuitive, reduces physical effort, and
enables faster, more consistent demonstration collection.
The iiwa and Panda arms differ in contact geometry, velocity
within the trajectory optimization framework described in
(2). For safe hardware deployment, we enforce conservative
velocity limits on the iiwa arms, while only applying soft
velocity regularization on the Panda arms in simulation to
allow for more aggressive motions.
Data Diversity Trajectory optimization efficiently aug-
ments a single demonstration to a wide distribution of trajec-
tories with locally perturbed physical parameters and initial
conditions as visualized in Fig. 6. The diverse states in
the generated dataset cover a larger training distribution and
encourage smoother learned policies, as will be discussed in
the next section.
VI. BEHAVIOR CLONING EXPERIMENTS
We illustrate our frameworks capability to efficiently pro-
duce diverse, high-quality contact-rich datasets for training
behavior cloning policies across multiple robotic platforms,
including the floating Allegro hand and the bimanual Panda
arms in simulation as well as bimanual iiwa arms on hardware.
We show that policies trained on the generated data generalize
to a wide distribution of physical parameters and initial
the ones trained only on the original demonstrations.
A. Policy Evaluation in Simulation
From only 24 human demonstrations, our data generation
pipeline can efficiently generate thousands of dynamically
feasible contact-rich trajectories using trajectory optimization.
We train state-based diffusion policies  on the 24 original
demo trajectories, as well as 500 and 1000 generated trajec-
tories. While our method is compatible with any Behavior
Cloning algorithm, we adopt diffusion policies due to its recent
success in contact-rich tasks [20, 69, 89]. Fig. 7 visualizes the
policy rollouts. We evaluate the performance by conducting 48
policy rollouts for each embodiment in simulation and record
the success rates in Fig. 9. The success criteria are the same
as specified in the trajectory optimization experiments.
1) Floating Allegro Hand: While the human demonstrator
completes the task in approximately 5 seconds on average in
the virtual reality environment, the demonstration trajectories
are temporally scaled by a factor of 2.5 to ensure smoother,
dynamically feasible motions on the floating Allegro hand,
which is subject to velocity limits. We define the task horizon
as 25 seconds to allow the policy sufficient time to recover
from missed contacts and other errors during the execution.
The task complexity arises from the 22-dimensional action
Fig. 7: Policy rollouts for different embodiments. The object manipulation task requires the robots to frequently make and break contact
with the object. It also requires precise control of the robot since small deviations in positions can result in missing contact interactions and
lead to task failure.
Fig. 8: Failure cases of baselines. (a) The baseline policy trained on the original 24 demonstrations for the floating Allegro hand frequently
misses contact or gets stuck on the cube. (b-c) The baseline policies for the bimanual robot arms often exhibit jittery motion, resulting in
loss of contact, the box being kicked out of reach, or the robot arms running into and getting stuck on the box surface.
Fig. 9: Success rates of policy evaluation in simulation and hardware.
space of the Allegro hand and the long-horizon nature of
the task, which requires a sequence of coordinated rolling,
position. These factors together present significant challenges
for traditional model-based planners without guidance.
The baseline behavior cloning policy trained on the original
set of 24 demonstrations achieves a success rate of 1048
21 and exhibits significant jittery behavior when encoun-
tering out-of-distribution states. The workspace, characterized
by diverse object orientations and translations, is sufficiently
large that minor deviations during policy rollouts often drive
the trajectory out of the demonstrated distribution. Common
failure modes include the Allegro hand repeatedly missing
contact with the cube or becoming stuck on its surface while
attempting reorientation (visualized in Fig. 8a), which often re-
sult in the object being trapped in intermediate orientations. In
our pipeline demonstrate a higher likelihood of re-establishing
contact with the object after initial misses, resulting in signif-
icantly improved success rates up to 3948  81.
2) Bimanual Robot Arms: The baseline policy trained on
the original set of 24 human demonstrations achieves a success
rate of 2748  56 on the bimanual iiwa system. We hypoth-
esize that the restrictive velocity limits encourage more quasi-
static behavior, leading to longer trajectories with a higher
density of state-action pairs in the training data. In contrast, the
Fig. 10: Policy rollouts on hardware. The fixed-base bimanual iiwa arms perform a sequence of coordinated rolling, pitching, and yawing
actions to reorient the box to the goal pose.
Fig. 11: Policy failure and recovery on hardware. The baseline policy frequently (a) gets stuck on the box surface when small deviations
from the demonstration trajectories occur, and (b) struggles to recover from out-of-distribution states, where the object is never intentionally
lifted for accomplishing the task in the generated dataset. Policies trained on augmented datasets (c) sometimes fail due to unmodeled
collision geometry, but (d) can recover from undesired sliding by employing firmer grasps found by trajectory optimization.
baseline policy yields a success rate of 1448  29 on the
bimanual Panda system, likely due to the more dynamic nature
of the learned behavior under its looser velocity constraints.
Both baseline policies exhibit remarkably jittery motion, fre-
quently kicking the box out of reach, losing contact, or running
into and getting stuck on the box surface during reorientation
(visualized in Fig. 8b and c). Policies trained on the augmented
and are capable of re-establishing contact with the object
after initial misses, resulting in as high as 4448  92
success rates for bimanual iiwa arms and 4248  87.5
for bimanual Panda arms. Additionally, the learned policies
capture multimodal behaviors observed in the original human
counterclockwise for similar object poses.
B. Policy Evaluation on Hardware
We zero-shot deploy the trained policies on hardware for
bimanual iiwa arms to flip a 30 cm cubic box on a table
(Fig. 10). An OptiTrack motion capture system is employed
to estimate the object pose. The baseline behavior cloning
policy only achieves 623  26 success rate, with most
successful rollouts being relatively short-horizon, involving
only 1 or 2 rotations. Common failure modes of the baseline
policy include: 1) deviation from the demonstration trajectory,
causing the arms to collide with the box surface (Fig. 11a), and
2) significant box sliding during rolling, resulting in the policy
encountering out-of-distribution states and failing to recover
(Fig. 11b). In contrast, as shown in Fig. 9b, the policy trained
on 500 generated trajectories achieves 1723  74 success
achieves 1623  70 success rate. Despite occasional box
sliding during rolling, these policies demonstrate an improved
ability to stabilize the box by using one arm to hold the
opposite side more firmly to prevent further sliding (Fig 11d).
augmented datasets exhibit failure modes originating from un-
modeled collision geometries on iiwa arms, which lead to sig-
nificant undesired yaw motions of the box during pitch actions.
VII. LIMITATIONS AND FUTURE WORK
While our method efficiently generates abundant contact-
rich trajectories, several limitations remain. First, although our
human-hand demonstration framework is fast and intuitive, it
may not fully exploit the kinematic capabilities of the target
terous maneuvers. Future work could explore the application
of our automated data generation framework to embodiment-
aware legacy datasets, better capturing the unique motion
capabilities of different robotic systems.
mance in the vicinity of the demonstration due to trajectory op-
far outside the demonstrated regions, such as those resulting
from catastrophic failure. Future work could explore more ad-
vanced planning techniques to iteratively improve the learned
policies robustness in unvisited regions of the state space.
pipeline primarily for training robust state-based policies. Ex-
tending the framework to train visuomotor policies by incorpo-
rating high-quality synthetic rendering from simulation could
further improve policy transferability to real-world scenarios.
VIII. CONCLUSION
In this work, we present a novel, cost-effective pipeline that
combines physics-based simulations, human demonstrations,
and model-based planning to address data scarcity in contact-
rich robotic manipulation tasks. A key insight of our approach
is that human demonstrationseven when collected on a
different morphologyoffer valuable global task information
that model-based planners often struggle to discover indepen-
dently due to the high-dimensional search space and complex
contact dynamics. By leveraging these demonstrations as a
global prior, our method refines and augments them through
kinematic retargeting and trajectory optimization, resulting
in large datasets of dynamically feasible trajectories across
a range of physical parameters, initial conditions, and em-
bodiments. Our framework significantly reduces the reliance
on costly, hardware-specific data collection while offering
the potential to reuse legacy datasets collected with outdated
hardware or configurations. We demonstrate its effectiveness
across multiple robotic systems in simulation and successfully
zero-shot deploy policies trained on the augmented dataset to
a bimanual iiwa hardware setup.
IX. ACKNOWLEDGEMENT
The authors would like to thank Huaijiang Zhu, Haonan
Chen and Jiayuan Mao for valuable discussions and insight-
ful feedback on the paper. This work was supported by
Robotics and AI Institute Agmd Dtd 812023, Amazon PO
REFERENCES
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
F. L. Aleman, D. Almeida, J. Altenschmidt, S. Alt-
preprint arXiv:2303.08774, 2023.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
F. Azhar, et al., Llama: Open and efficient founda-
tion language models, arXiv preprint arXiv:2302.13971,
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-
Z. Chen, et al., Palm 2 technical report, arXiv preprint
M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Bal-
Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh,
S. Levine, P. Liang, and C. Finn, Openvla: An open-
source vision-language-action model, in Proceedings
of The 8th Conference on Robot Learning, vol. 270.
A. ONeill, A. Rehman, A. Gupta, A. Maddukuri,
A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta,
A. Mandlekar, et al., Open x-embodiment: Robotic
learning datasets and rt-x models : Open x-embodiment
on Robotics and Automation (ICRA), 2024, pp. 6892
Octo Model Team, D. Ghosh, H. Walke, K. Pertsch,
K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo,
T. Kreiman, Y. Tan, L. Y. Chen, P. Sanketi, Q. Vuong,
T. Xiao, D. Sadigh, C. Finn, and S. Levine, Octo: An
open-source generalist robot policy, in Proceedings of
from microsoft, nvidia, and other big tech firms, Reuters,
A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna,
S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama,
L. Y. Chen, K. Ellis, et al., Droid: A large-scale
in-the-wild robot manipulation dataset, arXiv preprint
H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng,
P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim,
M. Du, et al., Bridgedata v2: A dataset for robot
learning at scale, in Conference on Robot Learning.
S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher,
K. Schmeckpeper, S. Singh, S. Levine, and C. Finn,
ings of the Conference on Robot Learning, ser. Proceed-
ings of Machine Learning Research, vol. 100.
D. Damen, H. Doughty, G. M. Farinella, S. Fidler,
A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Per-
epic-kitchens dataset, in Proceedings of the European
conference on computer vision (ECCV), 2018, pp. 720
I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik,
and T. Darrell, Real-world robot learning with masked
visual pre-training, in Conference on Robot Learning.
S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta,
Robot Learning, ser. Proceedings of Machine Learning
S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn,
D. Sadigh, and P. Liang, Language-driven representation
learning for robotics, arXiv preprint arXiv:2302.12766,
F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu,
H. Jiang, Y. Yuan, H. Wang, et al., Sapien: A simulated
part-based interactive environment, in Proceedings of
the IEEECVF conference on computer vision and pat-
tern recognition, 2020, pp. 11 09711 107.
preprint
S. James, Z. Ma, D. R. Arrojo, and A. J. Davison,
L. Wang, X. Chen, J. Zhao, and K. He, Scaling
proprioceptive-visual learning with heterogeneous pre-
trained transformers, in Advances in Neural Information
Processing Systems, vol. 37, 2024, pp. 124 420124 450.
A. Wei, A. Agarwal, B. Chen, R. Bosworth, N. Pfaff,
and R. Tedrake, Empirical analysis of sim-and-real
cotraining of diffusion policies for planar pushing from
C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel,
S. Feng, R. Tedrake, and S. Song, Universal manipula-
tion interface: In-the-wild robot teaching without in-the-
wild robots, in Proceedings of Robotics: Science and
I. Mordatch, E. Todorov, and Z. Popovic, Discovery of
complex behaviors through contact-invariant optimiza-
M. Posa, C. Cantu, and R. Tedrake, A direct method for
trajectory optimization of rigid bodies through contact,
The International Journal of Robotics Research, vol. 33,
T. A. Howell, K. Tracy, S. Le Cleach, and Z. Manch-
timization with conic and complementarity constraints,
in The International Symposium of Robotics Research.
D. A. Pomerleau, Alvinn: An autonomous land vehicle
in a neural network, Advances in neural information
processing systems, vol. 1, 1988.
B. Burchfiel, R. Tedrake, and S. Song, Diffusion pol-
The International Journal of Robotics Research, p.
Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, Viola: Imitation
learning for vision-based manipulation with object pro-
posal priors, in Conference on Robot Learning. PMLR,
Z. Fu, T. Z. Zhao, and C. Finn, Mobile aloha: Learning
bimanual mobile manipulation using low-cost whole-
body teleoperation, in Proceedings of The 8th Confer-
ence on Robot Learning, ser. Proceedings of Machine
Learning Research, vol. 270.
T. Z. Zhao, J. Tompson, D. Driess, P. Florence, S. K. S.
A simple recipe for robot dexterity, in Proceedings of
The 8th Conference on Robot Learning, ser. Proceedings
of Machine Learning Research, vol. 270. PMLR, 0609
Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn,
Robot Learning, ser. Proceedings of Machine Learning
T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang,
K. M. Kitani, C. Liu, and G. Shi, Omnih2o: Universal
and dexterous human-to-humanoid whole-body teleoper-
ation and learning, in Proceedings of The 8th Confer-
ence on Robot Learning, ser. Proceedings of Machine
Learning Research, vol. 270.
F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Geor-
domain datasets, in Proceedings of Robotics: Science
and Systems, 2022.
T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen,
K. Goldberg, and P. Abbeel, Deep imitation learning for
complex manipulation tasks from virtual reality teleoper-
and automation (ICRA).
A. Smith and M. Kennedy III, An augmented reality
interface for teleoperating robot manipulators: Reducing
demonstrator task load through digital twin control,
arXiv preprint arXiv:2409.18394, 2024.
J. Duan, Y. R. Wang, M. Shridhar, D. Fox, and R. Kr-
Proceedings of The 7th Conference on Robot Learning,
ser. Proceedings of Machine Learning Research, vol. 229.
Y. Park, J. S. Bhatia, L. Ankile, and P. Agrawal, Dexhub
and dart: Towards internet scale robot data collection,
arXiv preprint arXiv:2411.02214, 2024.
S. Chen, C. Wang, K. Nguyen, L. Fei-Fei, and C. K. Liu,
for robot learning with augmented reality feedback,
arXiv preprint arXiv:2410.08464, 2024.
N. Nechyporenko, R. Hoque, C. Webb, M. Sivapurapu,
and J. Zhang, Armada: Augmented reality for robot ma-
nipulation and robot-free data acquisition, arXiv preprint
X. Zhang, M. Chang, P. Kumar, and S. Gupta, Diffusion
meets dagger: Supercharging eye-in-hand imitation learn-
S. Tian, B. Wulfe, K. Sargent, K. Liu, S. Zakharov, V. C.
zero-shot novel view synthesis, in Proceedings of The
8th Conference on Robot Learning, ser. Proceedings of
Machine Learning Research, vol. 270.
L. Y. Chen, C. Xu, K. Dharmarajan, Z. Irshad, R. Cheng,
K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg,
embodiment robot learning, in Conference on Robot
Learning (CoRL), 2024.
Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Ra-
able multi-task multi-scene visual imitation learning,
arXiv preprint arXiv:2212.05711, 2022.
Z. Chen, S. Kiami, A. Gupta, and V. Kumar, Genaug:
Retargeting behaviors to unseen situations via generative
T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan,
S. Wang, J. Singh, C. Tan, J. Peralta, B. Ichter, et al.,
Scaling robot learning with semantically imagined ex-
Y. Narang, L. Fan, Y. Zhu, and D. Fox, Mimicgen:
A data generation system for scalable robot learning
using human demonstrations, in Conference on Robot
Learning.
Z. Jiang, Y. Xie, K. Lin, Z. Xu, W. Wan, A. Mandlekar,
L. Fan, and Y. Zhu, Dexmimicgen: Automated data gen-
eration for bimanual dexterous manipulation via imitation
C. Garrett, A. Mandlekar, B. Wen, and D. Fox,
for efficient skill learning and deployment, in 8th Annual
Conference on Robot Learning, 2024.
S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo,
A. Joshi, A. Mandlekar, and Y. Zhu, Robocasa: Large-
scale simulation of everyday tasks for generalist robots,
arXiv preprint arXiv:2406.02523, 2024.
A. O. Onol, P. Long, and T. Padr, Contact-implicit tra-
jectory optimization based on a variable smooth contact
model and successive convexification, in 2019 Interna-
tional Conference on Robotics and Automation (ICRA).
J. Moura, T. Stouraitis, and S. Vijayakumar, Non-
prehensile planar manipulation via trajectory optimiza-
tion with complementarity constraints, in 2022 Interna-
tional Conference on Robotics and Automation (ICRA).
J.-P. Sleiman, J. Carius, R. Grandia, M. Wermelinger,
and M. Hutter, Contact-implicit trajectory optimization
for dynamic object manipulation, in 2019 IEEERSJ in-
ternational conference on intelligent robots and systems
Y. Tassa, T. Erez, and E. Todorov, Synthesis and stabi-
lization of complex behaviors through online trajectory
ence on Intelligent Robots and Systems.
M. Neunert, F. Farshidian, and J. Buchli, Efficient
whole-body trajectory optimization using contact con-
straint relaxation, in 2016 IEEE-RAS 16th International
Conference on Humanoid Robots (Humanoids).
A. W. Winkler, C. D. Bellicoso, M. Hutter, and
J. Buchli, Gait and trajectory optimization for legged
systems through phase-based end-effector parameteriza-
M. Wang, A. O. Onol, P. Long, and T. Padr, Contact-
implicit planning and control for non-prehensile manip-
ulation using state-triggered constraints, in The Interna-
tional Symposium of Robotics Research. Springer, 2022,
V. Kurtz, A. Castro, A. O. Onol, and H. Lin, Inverse dy-
namics trajectory optimization for contact-implicit model
predictive control, arXiv preprint arXiv:2309.01813,
M. Neunert, M. Stauble, M. Giftthaler, C. D. Bellicoso,
J. Carius, C. Gehring, M. Hutter, and J. Buchli, Whole-
body nonlinear model predictive control through contacts
for quadrupeds, IEEE Robotics and Automation Letters,
S. Le Cleach, T. A. Howell, S. Yang, C.-Y. Lee,
J. Zhang, A. Bishop, M. Schwager, and Z. Manchester,
Fast contact-implicit model predictive control, IEEE
Transactions on Robotics, 2024.
A. Aydinoglu, A. Wei, W.-C. Huang, and M. Posa,
Consensus complementarity control for multi-contact
H. T. Suh, T. Pang, T. Zhao, and R. Tedrake, Dexterous
contact-rich manipulation via the contact trust region,
B. P. Graesdal, S. Y. C. Chia, T. Marcucci, S. Morozov,
A. Amice, P. A. Parrilo, and R. Tedrake, Towards tight
convex relaxations for contact-rich manipulation, arXiv
preprint arXiv:2402.10312, 2024.
P. Hamalainen, J. Rajamaki, and C. K. Liu, Online
control of simulated humanoids using particle belief
J. Carius, R. Ranftl, F. Farshidian, and M. Hutter, Con-
strained stochastic optimal control with learned impor-
tance sampling: A path integral approach, The Interna-
tional Journal of Robotics Research, vol. 41, no. 2, pp.
C. Pezzato, C. Salmi, E. Trevisan, M. Spahn, J. Alonso-
dictive control leveraging parallelizable physics simula-
T. Howell, N. Gileadi, S. Tunyasuvunakool, K. Za-
time behaviour synthesis with mujoco, arXiv preprint
A. H. Li, P. Culbertson, V. Kurtz, and A. D. Ames,
arXiv preprint arXiv:2409.14562, 2024.
T. Pang, H. J. T. Suh, L. Yang, and R. Tedrake, Global
planning for contact-rich manipulation via local smooth-
ing of quasi-dynamic contact models, IEEE Transac-
tions on Robotics, vol. 39, no. 6, pp. 46914711, 2023.
X. Cheng, S. Patil, Z. Temel, O. Kroemer, and M. T.
via hierarchical contact exploration, IEEE Robotics and
Automation Letters, vol. 9, no. 1, pp. 390397, 2023.
S. Belkhale, Y. Cui, and D. Sadigh, Data quality in
imitation learning, Advances in Neural Information Pro-
cessing Systems, vol. 36, 2024.
H. Zhu, T. Zhao, X. Ni, J. Wang, K. Fang, L. Righetti,
and T. Pang, Should we learn contact-rich manipulation
policies from sampling-based planners? arXiv preprint
T. Chen, J. Xu, and P. Agrawal, A system for general
in-hand object re-orientation, in Conference on Robot
Learning.
H. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik,
In-hand object rotation via rapid motor adaptation, in
Conference on Robot Learning. PMLR, 2023, pp. 1722
M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin,
B. Piot, N. Heess, T. Rothorl, T. Lampe, and M. Ried-
ment learning on robotics problems with sparse rewards,
arXiv preprint arXiv:1707.08817, 2017.
A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba,
and P. Abbeel, Overcoming exploration in reinforcement
learning with demonstrations, in 2018 IEEE interna-
tional conference on robotics and automation (ICRA).
A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani,
J. Schulman, E. Todorov, and S. Levine, Learning
complex dexterous manipulation with deep reinforcement
learning and demonstrations, in Robotics: Science and
Systems XIV.
H. Hu, S. Mirchandani, and D. Sadigh, Imitation
bootstrapped reinforcement learning, in Proceedings of
N. Hansen, Y. Lin, H. Su, X. Wang, V. Kumar, and
A. Rajeswaran, Modem: Accelerating visual model-
based reinforcement learning with demonstrations, in
The Eleventh International Conference on Learning Rep-
resentations.
Y. Zhu, Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi,
S. Tunyasuvunakool, J. Kramar, R. Hadsell, N. de Freitas,
et al., Reinforcement and imitation learning for diverse
visuomotor skills, arXiv preprint arXiv:1802.09564,
A. Kanazawa, Amp: Adversarial motion priors for styl-
ized physics-based character control, ACM Transactions
on Graphics (ToG), vol. 40, no. 4, pp. 120, 2021.
X. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne,
ing of physics-based character skills, ACM Transactions
On Graphics (TOG), vol. 37, no. 4, pp. 114, 2018.
J.-P. Sleiman, M. Mittal, and M. Hutter, Guided
reinforcement learning for robust multi-contact loco-
Learning (CoRL 2024), 2024.
J. Li, Y. Zhu, Y. Xie, Z. Jiang, M. Seo, G. Pavlakos, and
Y. Zhu, Okami: Teaching humanoid robots manipulation
skills through single video imitation, in 8th Annual
Conference on Robot Learning, 2024.
C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu,
Y. Zhu, and A. Anandkumar, Mimicplay: Long-horizon
imitation learning by watching human play, in 7th
Annual Conference on Robot Learning.
M. Seo, H. A. Park, S. Yuan, Y. Zhu, and L. Sentis,
J. Yang, C. Glossop, A. Bhorkar, D. Shah, Q. Vuong,
C. Finn, D. Sadigh, and S. Levine, Pushing the lim-
its of cross-embodiment learning for manipulation and
R. Doshi, H. R. Walke, O. Mees, S. Dasari, and
S. Levine, Scaling cross-embodied learning: One policy
for manipulation, navigation, locomotion and aviation,
in 8th Annual Conference on Robot Learning.
R. Tedrake and the Drake Development Team, Drake:
Model-based design and verification for robotics, 2019.
[Online]. Available:
visualization
collection
environment
[Online]. Available:
P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y.
Annals of operations research, vol. 134, pp. 1967, 2005.
X. Li, T. Zhao, X. Zhu, J. Wang, T. Pang, and K. Fang,
Planning-guided diffusion policy learning for generaliz-
able contact-rich bimanual manipulation, arXiv preprint
Parameter
Plan Duration
Floating Allegro Hand
Bimanual iiwa Arms
Bimanual Panda Arms
TABLE III: Parameters for CEM. T: planning horizon. qo: scalar
weight for tracking object trajectories. qr: scalar weight for tracking
robot trajectories. ru: scalar weight for control input.
Parameter
Obs. Dim.
Act. Dim.
Floating Allegro Hand
Bimanual iiwa Arms
Bimanual Panda Arms
TABLE IV: Parameters for diffusion policies.
horizon. Ta: action horizon. Freq: environment frequency (Hz, both
observations and actions).
X. APPENDIX
In appendix, we present the implementation details of CEM
and policy training.
A. CEM Implementation Details
We provide detailed parameters for the CEM implemen-
tation in Table III. We optimize over the action knot points
mands sent to Drake. Drake simulates the contact dynamics f
at 200 Hz. The state cost matrix Qt  diag(qo 1no, qr 1nr),
where no and nr denote the object and robot state dimensions,
and 1 is a vector or all 1s. The terminal state cost matrix
QT  10  Qt. The input cost matrix Rt  diag(ru  1nu),
where nu represents the control input dimension. All of
the systems adopt 50 samples, 5 elites and initial standard
deviation   0.05  1nu for action sampling.
B. Policy Implementation Details
We train UNet-based diffusion policies  for all tasks.
The action space is the robot configuration (joint angles, and
additional floating base coordinates for the Allegro hand),
while the observation space is the robot configuration and ob-
ject pose (with orientations represented by rotation matrices).
Detailed parameters are listed in Table IV.
