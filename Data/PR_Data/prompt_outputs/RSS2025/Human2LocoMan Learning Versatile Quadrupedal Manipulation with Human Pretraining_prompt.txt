=== PDF文件: Human2LocoMan Learning Versatile Quadrupedal Manipulation with Human Pretraining.pdf ===
=== 时间: 2025-07-22 15:57:33.670800 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Manipulation with Human Pretraining
Yaru Niu1, Yunzhe Zhang1,
Mingyang Yu1, Changyi Lin1, Chenhao Li1, Yikai Wang1, Yuxiang Yang2, Wenhao Yu2,
Tingnan Zhang2, Zhenzhen Li3, Jonathan Francis1,3, Bingqing Chen3, Jie Tan2, and Ding Zhao1
1Carnegie Mellon University
2Google DeepMind
3Bosch Center for AI
Robot Data Finetuning
Human Data Pretraining
Policy Rollout
Fig. 1: Human2LocoMan provides a unified framework for collecting human demonstrations and teleoperated robot whole-
body motions, enabling flexible and scalable data collection. Human data is used for cross-embodiment model pretraining,
while robot data is leveraged for policy finetuning. Human2LocoMan achieves positive transfer from human to quadrupedal
AbstractQuadrupedal robots have demonstrated impressive
locomotion capabilities in complex environments, but equipping
them with autonomous versatile manipulation skills in a scalable
way remains a significant challenge. In this work, we introduce a
system that integrates data collection and imitation learning from
both humans and LocoMan, a quadrupedal robot with multiple
manipulation modes. Specifically, we introduce a teleoperation
and data collection pipeline, supported by dedicated hardware,
which unifies and modularizes the observation and action spaces
of the human and the robot. To effectively leverage the collected
co-training and pretraining with multimodal data across different
embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks
in both unimanual and bimanual modes, supplemented by a
corresponding human dataset. Experimental results demonstrate
that our data collection and training framework significantly
improves the efficiency and effectiveness of imitation learn-
Authors contributed equally to this work.
bilities. Our hardware, data, and code are open-sourced at:
I. INTRODUCTION
While quadrupedal robots have demonstrated impressive
locomotion capabilities in complex environments [1, 2, 3,
ities to manipulation tasks [8, 9, 10, 11, 12, 13, 14], en-
abling autonomous and versatile quadrupedal manipulation
at scale remains a major challenge. In this work, we take
inspiration from the open-source LocoMan platform ,
a quadrupedal robot equipped with two leg-mounted loco-
manipulation skills across multiple operating modes. Imitation
learning has long been a fundamental approach for teaching
robots complex skills through demonstrations , with the
acquisition of high-quality data being critical for achieving
efficient and effective learning. Prior works have explored
various strategies for collecting in-domain robot data, pri-
marily focusing on robot arms [16, 17, 18, 19], humanoid
robots [20, 21, 22], and quadrupeds equipped with top-
mounted arms [10, 11, 23]. However, collecting egocentric
manipulation data on a quadrupedal platform like LocoMan
remains underexplored. To scale up data collection for im-
itation learning, recent works propose leveraging simulation
data [24, 25, 26] or human data [17, 27, 28, 29, 30, 31]. Human
guidance [17, 28], improve visual encoders , simulate in-
domain robot data [27, 30], or serve as additional training
data by treating humans as an alternative embodiment with
similar kinematic structures . However, transferring skills
from humans to quadrupedal robots remains challenging due to
the substantial embodiment gap, which complicates both data
collection and policy transfer. To address these challenges, we
propose Human2LocoMan, a unified framework that bridges
the human-to-quadruped gap. Human2LocoMan introduces
a novel teleoperation and data collection system that aligns
human and robot data, coupled with a modular transformer-
based architecture for robust cross-embodiment learning. To-
manipulation skills on quadrupedal robots.
leverages an extended reality (XR) headset to capture human
motions while streaming a first-person or first-robot (during
teleoperation) view to the operator. For human data collection,
the operator simply wears the XR headset and performs
tasks naturally. During teleoperation, we align the human
and quadruped into a unified coordinate frame to bridge the
embodiment gap. In addition to mapping human hand motions
to the robots grippers, we map human head motions to the
robots torso, expanding the robots workspace and enhancing
active sensing capabilities. Target poses are then passed to a
whole-body controller to generate coordinated robot motions.
In contrast to works that use egocentric human data to
pretrain vision encoders  or learn high-level intent , we
treat the human as another embodiment and use human data
for cross-embodiment learning. Despite mapping human and
robot data to a unified frame, there exist obvious gaps ranging
from differences in dynamics to extra wrist cameras on the
robot. Thus, we design a modular transformer architecture,
Modularized Cross-embodiment Transformer (MXT), which
shares the transformer trunk, but has embodiment-specific
tokenizers  detokenizers. To enable positive transfer, the MXT
policy is first pretrained on human data and subsequently
finetuned with a small amount of robot data. We evaluate our
approach on six household tasks, across both unimanual and
bimanual manipulation modes. Our results demonstrate strong
task performance by MXT compared to competitive baselines,
effective positive transfer from human demonstrations to robot
and out-of-distribution (OOD) scenarios.
In summary, our paper provides the following contributions:
We propose Human2LocoMan, a framework that en-
ables flexible and scalable data collection of human
demonstrations and teleoperated robot trajectories for
learning versatile quadrupedal manipulation skills.
We design MXT, a modular transformer architecture that
facilitates effective cross-embodiment learning despite
large embodiment gaps between humans and quadrupedal
We introduce the first XR-based teleoperation system and
manipulation dataset for the open-source LocoMan
hardware platform.
We demonstrate positive human-to-robot transfer, high
success rates, and strong robustness across six challeng-
ing household tasks, in both unimanual and bimanual
manipulation modes.
II. RELATED WORK
Embodiments for Diverse Loco-Manipulation Skills: Learn-
ing manipulation skills on quadrupedal robots has shown
promise and popularity in recent years, due to the versatility
and mobility of the platforms. Many manipulator configu-
rations and capabilities have been proposed for quadrupeds,
including non-prehensile manipulation using the quadrupeds
legs or body (e.g., dribbling a soccer ball, pressing buttons,
closing appliance doors, etc.) [32, 33, 34, 35, 36, 37, 38, 39],
using a back-mounted arm for tabletop tasks [8, 40], or
using leg-mounted manipulators for spatially-constrained (e.g.,
reaching toys underneath furniture) or bi-manual manipula-
tion tasks . In this work, we take inspiration from the
open-source LocoMan hardware platform , with two leg-
mounted manipulators, which enable the training of policies
across challenging tasks and multiple operating modes.
Learning Versatile Quadrupedal Manipulation: Reinforce-
ment learning (RL) has been used for training individual
non-prehensile manipulation skills [32, 33, 35, 36, 37, 38,
controllers to track end-effector poses for uni-manual grasping
[8, 9, 10, 47, 48, 49, 50]; here, policies are trained in simu-
lation then transferred to the real robot platform, often with
high cost in training complexity and training time. To mitigate
some of these issues, imitation learning (IL) allows robots to
directly learn from expert demonstrations [15, 51, 52, 53] and
thus provides an alternative approach to efficiently acquiring
more general manipulation skills [26, 54, 55, 56, 57]. How-
for stable whole-body controllers. Prior works have trained
non-prehensile quadrupedal manipulation policies by learning
from demonstrations collected in simulation , or grasping
policies for a top-mounted arm using data collected from real-
world demonstrations [10, 11, 13]. Our work introduces a
scalable way of achieving more versatile manipulation skills
on quadrupedal platforms encompassing both unimanual and
bimanual manipulation tasks, using a small amount of robot
data combined with human demonstrations collected via our
novel teleoperation and data collection system.
(a) Teleoperation and
Data Collection
(c) Training
(d) Deployment
Observations
(b) Dataset
Human Proprioception
Robot Proprioception
Pretraining
Finetuning
Fig. 2: Human2LocoMan framework. (a) The data collection system leverages an XR headset to collect egocentric human data
and teleoperated robot data. Human and robot data are mapped to a unified coordinate frame. (b) The dataset consists of aligned
to-collect human data, and then finetuned on a small amount of robot data. (d) We evaluate the autonomous Human2LocoMan
policies on six household tasks in unimanual and bimanual modes.
Data Collection for Imitation Learning: Various methods
have been utilized to collect data for imitation learning.
Joysticks and spacemouses [16, 58, 59] are commonly used
to directly teleoperate the robot for data collection. Cameras
are employed to capture human motions and map them to the
robot [17, 20, 60, 61, 62]. VR controllers provide a more
intuitive way for the human to teleoperate the robot with
visual or haptic feedback for dexterous manipulation tasks
on robot arms, quadrupeds, and humanoid robots [13, 21,
the robot in task space, other works employ ex-skeleton
or leader-follower systems to collect robot demonstrations
by mapping the joint positions of the leader system to the
robot [18, 19, 23, 31, 66]. To ease the burdens of teleoperating
real robots and to scale up data collection, recent works have
achieved success by collecting human demonstrations in the
wild with AR-assisted  or hand-held grippers [11, 67],
though these are limited to a specific robot or end-effector
type. Other works enable more ergonomic data collection
with body-worn cameras [27, 68] or VR glasses . We
introduce a unified framework to collect cross-embodiment
data including both robot and human demonstrations, where
the teleoperation system considers the whole-body motions of
the embodiments to extend its workspace and actively sense
the environment. The different manipulation modes of both
the robot and human are regarded as different embodiments
and the collected data can be used for model pretraining.
Cross-Embodiment Learning: Drawing from the success of
foundation models in computer vision and natural language,
there have been many endeavors to replicate the success in
robotics by training generalist policies on large-scale data from
different embodiments [69, 70, 71, 72, 73, 74]. However,
this remains an open challenge due to the heterogeneity
of robot embodiments, and gaps in kinematics, vision, and
proprioception.
Different architectures have been proposed to handle the
heterogeneity. CrossFormer  formulates policy learning
as a sequence-to-sequence problem, so that any number of
camera views or proprioceptive sensors can be handled as
sequences of tokens, and adds special readout tokens as part
of the input sequence. In comparison, HPT  features a
modularized structure and maps the variable observations to
a fixed number of number tokens. In our work, we propose
Modularized Cross-embodiment Transformer (MXT) that also
employs a modularized design, but further enhances the modu-
larity by identifying fine-granular alignment of data modalities
between embodiments.
ment and demonstrates positive transfer by co-training on both
human and robot data. To enable such transfer, EgoMimic min-
imizes the kinematic gap by selecting a human-like robot em-
aligning action distributions, and addresses the appearance gap
through visual masking. In comparison, Human2LocoMan
offers greater flexibility and scalability, achieving positive
transfer from humans to multiple quadrupedal embodiments
without requiring explicit domain alignment.
III. METHODOLOGY
In this section, we describe the design and implementation
of our system Human2LocoMan, which integrates teleop-
embodied learning.
A. Human2LocoMan System Overview
We utilize the Apple Vision Pro headset and the Open-
Television system  to capture human motions and stream
first-person or first-robot video to the human operator. A
lightweight stereo camera with a 120-degree horizontal field
of view is mounted on both the VR headset and the LocoMan
robot to provide egocentric views, while additional cameras,
such as RGB wrist cameras, can be optionally attached to the
robot. Through the Human2LocoMan teleoperation system
(Section III-B), the human operator can control the LocoMan
robot to perform versatile manipulation tasks in both uniman-
ual and bimanual modes. In the unimanual mode, we also map
human head motions to the robots torso movements to expand
the teleoperation workspace and enhance active sensing. The
Human2LocoMan system enables the collection of both hu-
man and robot data, transforming them into a shared space.
Masks are applied to distinguish across different embodiments
and manipulation modes. The collected human data are used
to pretrain an action model called the Modularized Cross-
embodiment Transformer (MXT). The in-domain robotic data
collected via teleoperation are used to finetune the pretrained
model to learn a manipulation policy that predicts the 6D
poses of LocoMans end effectors and torso, as well as gripper
actions.
B. Human2LocoMan Teleoperation and Data Collection
A unified frame for both human and LocoMan. To map
human motions to LocoMans various operation modes via
VR-based teleoperationand to enhance the transferability
of motion data across different embodimentswe establish
a unified reference frame, Fu, to align motions across embod-
iments. As shown in Figure 2(a), this unified frame is attached
to the rigid body where the main camera is mounted. At the
embodiments reset pose, the x-axis points forward, aligned
with the workspace and parallel to the ground; the y-axis
points leftward; and the z-axis points upward, perpendicular
to the ground.
Motion mapping. We map the human wrist motions to Lo-
coMans end-effector motions, map the human head motions
to LocoMans torso motions, and hand poses to LocoMans
gripper actions. The 6D poses of the human hand, head,
and wrist poses in SE(3) in the VR-defined world frame
are streamed from the VR set to the Human2LocoMan
teleoperation server. The human head pose is represented as
vr ), and the wrist poses are (xr-wrist
, Rr-wrist
(xl-wrist
, Rl-wrist
), where x
vr denotes the translation and R
denotes the rotation in the VR-defined world frame. Then,
the 6D poses can be transformed into the unified frame Fu
uni)  (Rvr
vr), where Rvr
uni is the rotation
matrix of the VR-defined frame relative to the unified frame
initialize
teleoperation
manipulation
transferred
randomly
initialized
including
teleoperate
initializing
posture. The target pose for the robot at time step t,
(xtorso,t
can be expressed as follows.
t  gripper
humans motions to robots torso, right end effector, and left
end effector, respectively. xgripper
and xgripper
are the maximum
and minimum gripper angles, respectively. dtip
represents the
distances between the reference finger tips of both human
hands at time step t, and dtip
max is the maximum finger tip
distance for the human operator.
Whole-body controller. The robot target pose at time t,
the whole-body controller of the LocoMan robot, which is
adapted from the one introduced in , a unified whole-
body controller designed to track the desired poses of the
We employ null-space projection for kinematic tracking and
quadratic programming for dynamic optimization to compute
the desired joint positions, velocities, and torques.
To handle the large embodiment gap between the human and
the LocoMan robots, and to facilitate smooth teleoperation of
a dynamic quadrupedal platform with whole-body motions, we
consider the handling and recovery from robots joint limits,
the manipulability index as:
to assess the proximity of the target pose to singularity, where
J represents the Jacobian of the robots manipulator at the
target pose. If Imani falls below a predefined threshold mani,
the target pose is considered near singularity. To detect self-
collision pairs among the robots body parts. If any of the
following conditions are metjoint limit violation, singular-
instead of pt
t. To mitigate rapid movements, we apply linear
interpolation between xtorso,t
and gripper,t
. Addition-
and Rr-eef,t
and Rl-eef,t
smooth large action variations.
Data Collection. We record the robot data {DR
t1 during
t } is the robot data at time
step t including the robot observations oR
t and robot actions aR
and T is the episode length. We define the IR
are images obtained from the robots main stereo camera and
Transformer Encoder
Transformer Decoder
Head Image Tokens
Other Image
Fixed Positional Embeddings
Head Image Tokenizer
Tokenizer
Tokenizer
Shared by all
embodiments
Current Head Image
Wrist Images
Current Pose of
Rigid Body with the
Head Camera
Tokenizer
Head Cam.
Pose Tokens
EEF Pose
EEF Pose
Proprio.
Tokenizer
Other Proprio.
Other Proprio.
EEF Pose Detokenizer
EEF 6D Pose
BodyHead Pose
Detokenizer
Gripper Angles
Angle Detokenizer
BodyHead 6D Pose
Fig. 3: Modularized Cross-embodiment Transformer (MXT) architecture. The inputs are organized as a list of modalities
and encoded each by a separate tokenizer into a fixed number of tokens. The transformer trunk handles decision making by
consuming the concatenated encoded tokens and producing a fixed number of raw output tokens. Each of the detokenizers at
the end decodes a fixed subset of the output tokens into a modality of the final actions.
the wrist camera, respectively. Then, we can formulate oR
t in the dataset as follows.
t [main image] : Imain,t,
t [wrist image] : Iwrist,t,
t [body pose] : [xtorso
t [EEF pose] : [xr-eef
t [EEF to body pose] : [xr-eef
t [gripper angles] : gripper
t [body pose] : [xtorso, t
t [EEF pose] : [xr-eef, t
t [gripper angles] : gripper, t
We record the human data {DH
t1 in real time during
humans manipulation. Similarly, the human data at time step
t } can be defined by human observations oH
and human actions aH
t as follows.
t [main image] : IH
t [body pose] : [xhead
t [EEF pose] : [xr-wrist
t [EEF to body pose] : [xr-wrist
xl-wrist
t [grasping states] : gripper
t [body pose] : [xhead, t
t [EEF pose] : [xr-wrist, t
, Rr-wrist, t
, Rl-wrist, t
t [grasping actions] : gripper, t
In this way, we ensure that the human and robot data are
unified in terms of both format and spatial interpretation,
and can be used to train our proposed Modularized Cross-
Embodiment Transformer introduced in Section III-C.
C. Modularized Cross-embodiment Transformer
To train a policy on LocoMan that benefits from heteroge-
neous human data, we opt for task-space control in this work,
where the actions predicted by the policy are represented as
key pose parameters of the physical embodiment, such as the
end effector 6D pose and the body 6D pose. While previous
works on learning robot skills [20, 22, 31] often choose joint-
space action representations for the policy, the fundamental
embodiment gap between the human and quadrupedal robots
like LocoMan means that the joint spaces for the human and
the robot are largely distinct, which will likely hinder the trans-
fer of action prediction capabilities between the embodiments.
data collection pipeline with the unified pose frame into the
learning framework.
multi-embodiment
collection
the overall structure and the majority of parameters are
transferrable. To this end, we propose a modularized design
called Modularized Cross-embodiment Transformer (MXT).
MXT consists mainly of three groups of modules: tokenizers,
transformer trunk, and detokenizers. The tokenizers act as
encoders and map embodiment-specific observations to tokens
in the latent space, and the detokenizers translate the output
tokens from the trunk to actions in the action space of each
embodiment. The tokenizers and detokenizers are specific to
one embodiment and are reinitialized for each new embodi-
reused for transferring the policy among embodiments. Figure
3 illustrates the architecture of our network.
Tokenizers. The tokenizers T transform raw observations into
tokens for the transformer trunk. Drawing from the design
in previous works , we use a cross attention layer to
format observational features into a fixed number of tokens.
For image inputs, the features are obtained from a pretrained
ResNet encoder that can be finetuned during training; for
proprioceptive or state-like inputs, the features are computed
by passing the raw input through a trainable MLP network.
Detokenizers. The detokenizers D serve as action decoder
heads and map output tokens from the trunk to actions in
each embodiments action space. We adopt the action chunking
technique . At each inference step, the detokenizers predict
an action sequence of h steps and temporal ensemble is applied
to the outputs, following . Within each detokenizer, we use
a cross attention layer to transform the latent action tokens
output by the trunk to a sequence of actions with length h
and appropriate action dimensions.
Trunk. The trunk is an encoder-decoder transformer, where
the input sequence length and the output sequence length
are both fixed, as the number of tokens for each input or
output modality is fixed by design. By sharing the trunk
weights across the human and robot embodiments, the trunk
is trained to capture the common decision making patterns
across different embodiments.
Modality Decomposition in Tokenizers  Detokenizers. Due
to the aligned data format and the unified observation and
action spaces across embodiments, we are able to separately
transform each semantically distinct component of the ob-
servational input and the action output, which we refer to
as modality, and specify the compositional structure at the
interface of the transformer trunk and the tokenizers  detok-
enizers. This design provides another layer of modularization
to training and is core to the effectiveness of our method.
code the input observation ot with multiple tokenizers {Te,mi}
at the finer granularity of modalities denoted by ot[mi]. For
through the vision tokenizer, we use separate tokenizers for
each camera view. All the encoded modalities are concatenated
to compose the input tokens to the transformer trunk.
the transformer output tokens corresponding to each action
and decode the selected tokens to yield each modality with
separate detokenizers {De,mi}. For convenience, we use the
set of observation and action modalities as defined by the data
collection formats in (3) and (4).
By explicitly decomposing the input and output modalities
and encoding them separately, we are leveraging the innate
structure of observations and actions and imposing such a
structure on the token sequences processed by the transformer.
modalities learned during training can be shared across em-
Although we employ a consistent data format and aligned
inputoutput representations across embodiments, some modal-
ities are not present or available for all embodiments. For
tasks to improve manipulation accuracy. In this case, we use
masks defined during data collection to signify redundant
dimensions in the observations as well as in the action labels.
We refer the reader to Appendix Section VI-A for more
implementation details.
In general, the highly modularized design of our learning
framework offers great flexibility in handling all types of ma-
nipulation tasks across different embodiments, and effectively
enhances the learning performance by capturing the common
patterns in manipulation problems.
Algorithm 1 Pretraining MXT on human data and finetuning
on LocoMan data
Initialize the MXT policy network  with parameters .
Set pretraining learning rate pretrain
for step  1, 2, ... do
Pretraining Stage
Sample a batch B from Dhuman
Compute Lhuman(B)  P
i Lhuman,mi(B) with Eq.6
Optimize the policy weights  with backpropagation
Reinitialize the tokenizers and detokenizers of . Preserve
the trunk weights trunk learned from pretraining.
Set finetuning learning rate finetune
for step  1, 2, ... do
Finetuning Stage
Sample a batch B from DLocoMan
Compute LLocoMan(B)  P
i LLocoMan,mi(B) with Eq.6
Optimize the policy weights  with backpropagation
D. Training Paradigm
We leverage the human data to pretrain the network for
versatile manipulation policies. Specifically, for a given task,
we first pretrain our network with the human dataset, and
then finetune it with the LocoMan dataset (Algorithm 1).
Only the transformer trunk weights are loaded from the
pretrained checkpoint for finetuning. For certain tasks that are
similar in nature but with different manipulation modes, we
also collectively pretrain the model on the human datasets
from these tasks, and then finetune on each task with the
corresponding LocoMan dataset.
Learning Objective. We use the behavioral cloning objec-
tive for both pretraining and finetuning. In general, given a
dataset De on an embodiment e and aligned action modalities
where Le,mi is the 1 loss of the action modality mi with
respect to the dataset of embodiment e. In practice, we
optimize the following batched loss for each training batch
Be  {(oj, Aj)}n
j1 as a proxy of Le,mi():
1 (aj,l [mi] , aj,l [mi])
where aj,l [mi]  (Aj)l [mi] is the l-th step action of modality
mi in the action label sequence sample Aj  {aj,l}h
mi at step l, and h is the chunk size or the action horizon.
Higher tap
Transfer
Lower tap
Left push
Left tap
Right push
Right tap
OOD objects for unimanual
OOD objects for bimanual
OOD objects
OOD objects
Reach and grasp
Right grasp
Right release
Left grasp
Left release
OOD objects for unimanual
OOD objects for bimanual
Unimanual
Toy Collection
Bimanual
Toy Collection
Unimanual Shoe
Organization
Bimanual Shoe
Organization
Unimanual
Scooping
Bimanual
Fig. 4: Rollouts of the MXT policy and the objects used across manipulation tasks in our experiments. Green arrows indicate
end-effector motions, red arrows denote torso movements, and pink arrows represent gripper actions. Both unimanual and
bimanual toy collection tasks assess the robots ability to grasp objects of varying shapes, colors, and positions. The unimanual
variant emphasizes coordination between the torso and end-effector, while the bimanual variant highlights synchronized control
of two loco-manipulators. Unimanual and bimanual shoe rack organization tasks evaluate non-prehensile manipulation skills
such as pushing and tapping. The unimanual variant additionally requires torso articulation to reach shoes placed at different
heights. Scooping is a complex task involving tool use, deformable object manipulation, and wide-range torso motion. Pouring
is a long-horizon task that demands precise coordination of both loco-manipulators.
IV. EXPERIMENTS
In this section, we aim to answer the following re-
search questions: (1) Does the Human2LocoMan system
enable versatile quadrupedal manipulation capabilities? (2)
How does MXT compare to state-of-the-art imitation learn-
ing architectures? (3) How does human data collected by
Human2LocoMan contribute to imitation learning perfor-
mance? (4) Do the design choices in MXT facilitate positive
transfer from Human to LocoMan?
A. Experimental Setup
1) Tasks: We evaluate MXT on six household tasks of
varying difficulty, across unimanual and bimanual manipula-
tion modes of the LocoMan robot, with data collected by the
Human2LocoMan system:
Unimanual Toy Collection (TC-Uni). In this task, the
robot must pick up a toy randomly positioned within
a rectangular area and place it into a designated basket
on the ground. Completing this task requires the robot
to coordinate its whole-body motions to efficiently and
accurately reach various locations on the ground and
above the basket. As shown in Figure 4, we use 10
objects for robot finetuning and all objects for human
pretraining and real-robot evaluation. The substeps of this
task include: grasp the toy, and release the toy.
Bimanual Toy Collection (TC-Bi). Similar to Unimanual
TABLE I: Human2LocoMan embodiments (RRight, LLeft).
Embodiments
Human-Unimanual (R)
Human-Unimanual (L)
Human-Bimanual
LocoMan-Unimanual (R)
LocoMan-Unimanual (L)
LocoMan-Bimanual
Toy Collection, this task requires the robot to pick up a toy
randomly placed within two rectangular areas on either
side of a basket. We use 10 objects for robot finetuning,
while all objects are included in human pretraining and
real-robot evaluation. The substeps of this task include:
grasp the toy, and release the toy.
Unimanual Shoe Rack Organization (SO-Uni). This
longer-horizon task involves organizing two shoes placed
on different levels of a shoe rack. The robot must coordi-
nate whole-body motions to reach various rack levels and
utilize both prehensile and non-prehensile manipulation
skills. As shown in Figure 4, this task involves three pairs
of shoes, with one pair being out-of-distribution (OOD).
The substeps of this task include: push the shoe on the
higher rack, tap the shoe on the higher rack, transfer the
gripper to the lower level, and tap the shoe on the lower
Bimanual Shoe Rack Organization (SO-Bi). One pair of
shoes is randomly placed at the edge of the third level of
the shoe rack. The robot must push one shoe inward and
align it with the other. The substeps of this task include:
push the shoe, and tap the shoe.
Unimanual Scooping (Scoop-Uni). The robot performs
unimanual manipulation using a litter shovel to scoop
a 3D-printed cat litter from varying locations and poses
within a litter box, and then dump it into a trash bin. This
long-horizon task involves both tool use and deformable
object manipulation. The task is decomposed into the
following substeps: grasp the shovel, scoop the litter, tilt
the shovel, dump the litter, and place the shovel back.
Bimanual Pouring (Pour-Bi). The robot performs biman-
ual manipulation to pour a Ping Pong ball from one cup
to another. This longer-horizon task requires the robot to
accurately reach both cups, which are randomly placed
within a rectangular area on a table, lift one cup, pour the
ball into the other, and then place both cups back on the
table. This task evaluates the coordination and precision
of the robots bimanual manipulation. The substeps of
this task include: pick up both cups, pour the ball, and
place both cups.
2) Human2LocoMan Embodiments: As shown in Table I,
the unimanual and bimanual modes of Human2LocoMan
represent distinct embodiments, each differing in morphology,
utilize wrist cameras on the LocoMan robot for the three
unimanual manipulation tasks.
ous numbers of human and robot trajectories with the
Human2LocoMan system. The details of the collected data
are demonstrated in Table II. About 10 data of each task is
used for validation.
TABLE II: Records of data collection for different tasks.
human traj.
human time (min)
robot traj.
robot time (min)
Scoop-Uni
4) Training details.: For Toy Collection and Shoe Rack
of both the unimanual and bimanual versions of the task, then
we finetune the model on each unimanual or bimanual task
with the corresponding robot data. For each task, we choose
a set of training hyperparameters (e.g. batch size, chunk size)
that are kept the same for all methods. (See Appendix Section
VI-C.) We also list the model hyperparameters we use for our
method and the baselines in the Appendix Section VI-A and
5) Baselines: We compare Human2LocoMan to the fol-
lowing SOTA imitation learning baselines:
Humanoid Imitation Transformer (HIT): HIT  is an
imitation learning framework designed for humanoid skill
learning that also extends to any robot embodiment.
It builds upon ACT  and employs a decoder-only
architecture that simultaneously predict the future action
sequence and future image features. It discourages the
vision-based policy to ignore the visual input and over-
fit on proprioceptive states by introducing a L2 image
feature loss to the original behavioral cloning policy.
HIT itself is not capable of handling data from different
domains and embodiments, and we position HIT as a
reference implementation that efficiently learns from in-
domain robot demonstrations.
Heterogeneous Pretrained Transformer (HPT): HPT
is a framework for learning from vast amounts of data
collected from humans, teleoperation, simulation, and
real-life robots. HPT also has a modularized design and
consists of the stems, the trunk, and the head, where the
stems and heads are similar to our tokenizers and deto-
kenizers. The trunk is designed to capture the complex
mapping between the input and output in a unified latent
space through large-scale pretraining. The implementa-
tion of HPT differs from our framework in several key
aspects. Firstly, we leverage the unified observation and
TABLE III: Result Summary. We report success rate (SR) in  and task score (TS) for each task.
Toy Collection
Shoe Rack Organization
Scooping
Unimanual
Bimanual
Unimanual
Bimanual
Unimanual
Bimanual
Pretrained
Number of trajectories: TC-Uni smaller20, larger40; TC-Bi smaller30, larger60; SO-Uni smaller40, larger80; SO-Bi smaller40, larger80;
Scoop-Uni smaller30, larger60; Pour-Bi smaller30, larger60.
TC-Uni-ID-SR
TC-Uni-ID-TS
TC-Uni-OOD-SR
TC-Uni-OOD-TS
MXT-Pretrained-L
MXT-Pretrained-S
MXT-Scratch-L
MXT-Scratch-S
MXT-Agg-Pretrained-L
MXT-Agg-Pretrained-S
MXT-Agg-Scratch-L
MXT-Agg-Scratch-S
HPT-Pretrained-L
HPT-Pretrained-S
HPT-Scratch-L
HPT-Scratch-S
TC-Bi-ID-SR
TC-Bi-ID-TS
TC-Bi-OOD-SR
TC-Bi-OOD-TS
Success Rate or Normalized Task Score
Fig. 5: Ablation study on unimanual and bimanual toy collection. We compare MXT, its ablation MXT-Agg, and baseline
HPT on SR and TS. Here, L denotes the larger training set (40 trajectories for TC-Uni, 60 trajectories for TC-Bi), while
S denotes the smaller training set (20 trajectories for TC-Uni, 30 trajectories for TC-Bi).
action frames to align data from different embodiments
on the modality level, while HPT can only construct
tokenizers for all image or proprioceptive data, and one
detokenizer for all action dimensions. The ResNet image
encoder in HPT is also frozen to achieve efficient learning
with large models, while we opt to finetune the ResNet
encoder along with the whole network end-to-end to
better account for the visual gap between embodiments.
More implementation details of these baselines can be found in
Appendix Section VI-B. For the HPT baseline, we train with
several different settings: training with only LocoMan data,
pretraining with our human data and finetuning on LocoMan
with LocoMan data. For the HIT baseline, we only train on
LocoMan data, as it is unable to incorporate human data.
6) Evaluation Metrics: We present the evaluation results
using three metrics: i) success rate (SR), ii) task score (TS),
and iii) validation loss. To calculate the success rate and task
the evaluated method for one task. The policy is rolled out for
24 times with in-distribution (ID) objects and 12 times with
out-of-distribution (OOD) objects.
For each task, we define a set of critical substeps necessary
to fully complete the task. When calculating the task score,
successfully completing each intermediate substep earns one
taskearns an additional point. The final task score is the
sum of points across all rollouts for that task. The success
rate of a method on a given task, under either the ID or OOD
rollouts where all substeps are completed) to the total number
of rollouts performed.
In addition, we report the best validation loss as another
metric for training performance. For all the included methods,
we align how the loss is computed so that these losses can be
meaningfully compared. Note that the validation loss is not a
faithful indicator of the policy performance, but it does reflect
how well the model is optimized, especially when there is a
significant difference in the validation loss of different policies
in the same setting. We mainly use this metric to analyze
the training process of different architectures (MXT, HIT and
HPT) and to provide a separate dimension to our evaluation.
B. Results and Analysis
(1) Does the Human2LocoMan system enable versatile
quadrupedal manipulation capabilities?
Data collection. As shown in Table II, Human2LocoMan
teleoperation enables the collection of a substantial amount
of robot data (over 50 trajectories) within 30 minutes across
all tasks. Using the Human2LocoMan human data collection
time frame. Even for the most challenging task, a human
can collect over 300 trajectories within one and a half hours.
in many tasks approaches, that of a human. These results
highlight the data collection efficiency of our system.
Task versatility. As depicted in Figure 4, Human2LocoMans
policy can perform tasks across a wide range of scenarios,
including unimanual and bimanual manipulation, prehensile
and non-prehensile manipulation, deformable object manipu-
transfer
Substep Name
Success Rate ()
Unimanual Shoe Org (ID)
transfer
Substep Name
Success Rate ()
Unimanual Shoe Org (OOD)
Substep Name
Success Rate ()
Pour (ID)
Substep Name
Success Rate ()
Pour (OOD)
dump place
Substep Name
Success Rate ()
Scoop (ID)
dump place
Substep Name
Success Rate ()
Scoop (OOD)
MXT-Pretrained-L
MXT-Pretrained-S
MXT-Scratch-L
MXT-Scratch-S
Fig. 6: Substep success rate. The success rate for some substep
is calcuated as the percentage of trials where the robot success-
fully completed the substep. For each task, we calculate this
with 24 ID rollouts and 12 OOD rollouts. MXT-Pretrained:
MXT pretrained on human dataset (including unimanual and
bimanual if applicable), then finetuned on the LocoMan data.
denotes the larger training set (80 trajectories for SO-Uni, 60
trajectories for Pour and Scoop), while S denotes the smaller
training set (40 trajectories for SO-Uni, 30 trajectories for Pour
and Scoop).
and conditions.
Task performance. We summarize the success rates and task
scores of our method and HIT across all tasks in Table III.
Human2LocoMans MXT achieves strong performance on all
tasks using a relatively small dataset. The baseline method also
attains decent performance on most tasks. These results high-
light the high quality of our collected data and demonstrate
the effectiveness of Human2LocoMans data collection and
training pipeline.
(2) How does MXT compare to state-of-the-art imitation
learning architectures?
Compared to HIT. As shown in Table III, in most evaluated
across both ID and OOD inference scenarios, MXT without
pretraining achieves comparable or superior performance rel-
ative to HIT. Moreover, pretrained MXT consistently outper-
forms the HIT baseline in terms of both success rate and task
score. From Figure7, we find that MXT demonstrates lower
validation loss compared to HIT on most tasks, indicating
superior training convergence. The performance improvement
is particularly evident in tasks with larger datasets, sug-
gesting that MXT scales more effectively with increasing
data availability. Notably, HIT achieves a significantly lower
validation loss compared to the MXT variants, while attaining
comparable performance in SR and TS metrics under both ID
and OOD settings relative to the best MXT model. As shown
in the substep success analysis in Figure 6, the primary failures
of the lower-performing MXT models occur during the first
two substeps, push and tap1. One potential reason for this
is that the unimanual shoe organization task exhibits relatively
less variation in object locations and types compared to other
and pretraining.
Compared to HPT. From Figure 5, we observe that MXT
consistently outperforms HPT in both SR and TS metrics
across all combinations of pretraining and data sizes on the
toy collection tasks. Validation loss results, shown in Figure 8,
reveal a similar trend in the unimanual toy collection task
across a broader range of dataset sizes. Notably, we observe
severe overfitting in HPT experiments when training on our
suggests that the modular design of the MXT architecture
facilitates better generalization.
(3) How does human data collected by Human2LocoMan
contribute to imitation learning performance?
Table III, pretraining on human data has a substantial positive
impact on LocoMan manipulation performance. The policy
maintains strong performance even when robot data is limited,
highlighting both its efficiency and robustness. We hypothe-
size that MXT benefits from learning useful complementar-
itiesi.e., positive transfer effectsbetween human demon-
strations and LocoMan robot data. Specifically, comparing
MXT-Pretrained to MXT-Scratch in Table III, we observe
that pretraining improves performance on TC-Uni, TC-Bi,
and Scooping tasks under ID settings, where objects exhibit
diverse locations. MXT-Pretrained tends to produce smoother
and more robust motions, enabling more accurate localiza-
tion of target objects. For instance, as shown in Figure 6,
MXT-Pretrained achieves substantially better scooping perfor-
mancewhich requires precise localizationcompared to all
other methods. Moreover, Table III reveals large performance
gaps on OOD objects in tasks such as TC-Bi, SO-Uni, and
objects in shape, texture, and color. These results suggest
that MXT, by leveraging human demonstrations during the
pretraining stage, is able to generalize effectively to novel
scenarios unseen during robot training.
Long-horizon performance. For a more detailed analysis on
long-horizon tasks that require multiple manipulation steps,
we present in Figure 6 how the success rate decays with each
substep in tasks including SO-Uni, Pour-Bi and Scoop-Uni.
MXT-Pretrained is shown to maintain a decent success rate as
the long-horizon task progresses, while MXT-Scratch and HIT
tend to fail more after the first substep, especially in Pouring
and Scooping tasks. We note that the second substep in these
tasks commonly involves moving and localizing an object
with precision, and pretraining with human data appears to
help with completing such challenging substeps. This suggests
that human data incorporated during pretraining can promote
manipulation accuracy, which is key to completing a sequential
long-horizon task.
(4) Do the design choices in MXT facilitate positive transfer
from Human to LocoMan?
Our framework presents positive cross-embodiment transfer
despite substantial embodiment gaps. From Figure 8, we see
the gap in validation loss between HPT-Pretrained and HPT-
Scratch is not as much as for MXT. The HPT-Small and HPT-
Base models also do not generalize as well as MXT-Pretrained.
This highlights the ability of MXT to consume human data
which has a large embodiment gap from the LocoMan data.
For more concrete comparisons, we present SR and TS
results based on 36 trials, comprising 24 OOD and 12 ID
than MXT, both when finetuned and trained from scratch.
We attribute part of this performance gap to HPT using
frozen image encoders by default. We also provide additional
ablations of MXT where we aggregate the input modalities,
i.e. tokenize them with a single tokenizer, and decode actions
with a single detokenizer; this baseline (marked with Agg
in Figure 5(b)) incorporates the key HPT designs including
cross attention tokenization of visual and proprioceptive inputs
and trunk weight sharing, while finetuning the vision encoders
and remaining architecturally comparable to MXT. MXT
consistently benefits from pretraining and outperforms this
baseline when both are finetuned, highlighting the advantage
of modularized tokenization for leveraging human data.
respect to HPT, as evidenced by little to no improvement
when finetuning the pretrained model compared with training
from scratch. This is likely due to increased representation
power in the tokenizer, which permits more overfitting in
the transformer trunk and could negatively impact the trunk
transferability. However, with the incorporation of our modular
hibits improved transferability. The modular design effectively
aids in the trade-off between more network representation
power and better transferability in our framework, and allows
attaining both qualities.
V. LIMITATIONS
While our system introduces a novel approach to cross-
embodiment manipulation and efficient data collection for
quadrupedal robots, it has several limitations. First, the teleop-
eration system still requires some practice for human operators
to achieve precise manipulations and may feel unintuitive in
certain aspects, such as controlling torso movements via head
motions. Second, although we envision our system enabling
large-scale cross-embodiment learning, in this work we have
not yet scaled it to other robotic platforms or incorporated
additional robotic datasets. As future work, we plan to validate
its scalability and robustness across different robot types,
including robotic arms and humanoids.
VI. CONCLUSION
In this paper, we introduce Human2LocoMan, a unified
framework for flexible data collection and cross-embodiment
TC-Uni-140
TC-Uni-80
TC-Uni-40
TC-Uni-20
TC-Bi-60
TC-Bi-30
Scoop-Uni-60 Scoop-Uni-30
Best Validation Loss
MXT-Pretrained
MXT-Scratch
SO-Uni-80
SO-Uni-40
SO-Uni-10
SO-Bi-80
SO-Bi-40
Pour-Bi-60
Pour-Bi-30
Best Validation Loss
Fig. 7: Best validation loss of our method and HIT on
all our tasks. MXT-Pretrained: MXT pretrained on human
dataset (including unimanual and bimanual if applicable), then
finetuned on the LocoMan data. MXT-Scratch: MXT trained
only on the LocoMan data. The number suffix denotes the
number of demonstrations in the LocoMan training set.
TC-Uni-140
TC-Uni-80
TC-Uni-40
TC-Uni-20
Best Validation Loss
MXT-Pretrained
MXT-Scratch
HPT-Pretrained
HPT-Scratch
HPT-Base
HPT-Small
Fig. 8: Best validation loss of our method and HPT on the
unimanual Toy Collection task. MXT-Pretrained: MXT pre-
traine
