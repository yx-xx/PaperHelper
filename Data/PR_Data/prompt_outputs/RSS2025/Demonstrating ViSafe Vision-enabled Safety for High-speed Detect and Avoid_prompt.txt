=== PDF文件: Demonstrating ViSafe Vision-enabled Safety for High-speed Detect and Avoid.pdf ===
=== 时间: 2025-07-22 09:41:43.457564 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating ViSafe: Vision-enabled Safety for
High-speed Detect and Avoid
theairlab.orgvisafe
Parv Kapoor, Ian Higgins, Nikhil Keetha, Jay Patrikar, Brady Moon, Zelin Ye, Yao He,
Ivan Cisneros, Yaoyu Hu, Changliu Liu, Eunsuk Kang, Sebastian Scherer
Carnegie Mellon University
Equal Contribution
Intruder
Avoidance triggered
Intruder detected
Avoidance triggered
Intruder platforms used in the real-world flight tests
Exploded view of the ViSafe payload
ViSafe payload mounted on the X6 Ownship
We demonstrate ViSafe, a high-speed vision-only airborne collision avoidance system. Top: Rendering of a real-world flight test log where the
ViSafe system detects an incoming intruder with a 144 kmh closure rate and performs an avoidance maneuver to ensure safe separation. The annotations
showcase the detections and multi-camera tracks from the vision-based aircraft detection and tracking system, while the trajectory shows the log of the
performed real-world avoidance maneuver. The numbered annotations showcase the different stages of the flight test, from intruder detection to avoidance
completion. Bottom: ViSafe payload, ownship, and intruder platforms used in the real-world flight tests.
AbstractAssured safe-separation is essential for achieving
seamless high-density operation of airborne vehicles in a shared
airspace. To equip resource-constrained aerial systems with this
safety-critical capability, we present ViSafe, a high-speed vision-
only airborne collision avoidance system. ViSafe offers a full-
stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with
a custom multi-camera hardware prototype designed under
SWaP-C constraints. By leveraging perceptual input-focused
control barrier functions (CBF) to design, encode, and enforce
safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations.
We evaluate ViSafes performance through an extensive test
campaign involving both simulated digital twins and real-world
flight scenarios. By independently varying agent types, closure
weather and lighting), we demonstrate that ViSafe consistently
ensures self-separation across diverse scenarios. In first-of-its-
kind real-world high-speed collision avoidance tests with closure
rates reaching 144 kmh, ViSafe sets a new benchmark for vision-
only autonomous collision avoidance, establishing a new standard
for safety in high-speed aerial navigation.
I. INTRODUCTION
Collision avoidance systems are critical to enabling safe
operations in shared airspace. The integration of Uncrewed
Aerial Systems (UASs) into an already congested National
Airspace System raises pressing concerns about ensuring the
safe separation of airborne vehicles. Existing solutions, such
as Autonomous Collision Avoidance Systems (ACAS)
and Unmanned Traffic Management (UTM)  frameworks,
have demonstrated effectiveness. However, these systems of-
ten depend on multiple active sensor modalitiessuch as
small UASs due to stringent size, weight, power, and cost
(SWaP-C) constraints. Addressing threats posed by coopera-
tive as well as non-cooperative aerial entities like balloons and
rogue drones while respecting SWaP-C resource constraints
remains an ongoing challenge. As a result, current regulations
impose strict line-of-sight requirements on human operators,
significantly restricting the utility and scalability of UASs.
Cameras offer a lightweight, cost-effective alternative for
enabling safety-critical Detect and Avoid (DAA) capabilities in
sUAS. With the growth of data-driven methods, vision-based
object detection offers a promising direction for tracking small
aircraft in images with low signal-to-noise ratios . How-
systems with downstream provably safe collision avoidance
methods remain challenging.
We present ViSafe, a vision-only airborne collision avoid-
ance system to impart see-and-avoid capabilities to sUAS.
ViSafe builds on our prior work AirTrack , which uses
high-resolution detection and tracking networks to detect
aerial objects. We extend AirTrack to the multi-camera setting
by using multi-view fusion to track detected intruder posi-
tions across multiple cameras. Additionally, we formulate the
downstream vision-based collision avoidance problem from a
control theoretic perspective using Control Barrier Functions
(CBFs) , which provide provable guarantees of safety and
are suited for runtime monitoring and response.
CBFs have been successfully applied in various domains, in-
cluding safe navigation [19, 6], robotic manipulation [55, 39],
and industrial automation systems . Their deployment for
safe collision avoidance in airspaces has also been investigated
[16, 38, 42]. While existing work assumes global availability
of information and uses proprioceptive information, ViSafe
removes this assumption in the formulation. We identify key
challenges in deploying CBFs in the wild and provide insights
for deploying these techniques for high-speed collision avoid-
ance. Moreover, we offer an edge compute-focused solution
for real-time deployment on resource-constrained platforms.
designing our formulations, where we leverage the empirical
performance statistics of our vision inference system (Air-
Track ) to account for uncertainty in state estimation. This
is achieved using a multi-view Kalman Filter and the ASTM
F3442F3442M standard (satisfied by AirTrack) to determine
the range profile in which intruder detections are reliable.
Bridging the gap between theoretical formulations and real-
world safe behavior requires extensive testing. To address
operating environment in simulation. Using Nvidia Isaac Sim
, we render realistic camera feeds for use in our visual
detection algorithm, providing large-scale and diverse scenario
benchmarking abilities. These tests were run using the same
hardware as the real-world payload, thereby minimizing our
sim-to-real gap for testing.
test our avoidance algorithms at multiple real-world testing
facilities. We created representative configurations in the field
to analyze our systems collision avoidance capabilities. We
conducted approximately 80 hours of flight testing across
two outdoor testing locations to validate our hypotheses.
We observe similar collision avoidance performance in both
simulation and field testing, which further lends credibility to
our hardware-in-the-loop simulation fidelity.
The main contributions of this work are as follows:
1) Multi-view vision-only aircraft detection  tracking and
CBF-based collision avoidance system that assumes no
global availability of information or communication.
2) Custom-built
hardware
simultaneously
streams multiple camera inputs, provides state estimation,
performs deep learning model edge inference, and com-
putes avoidance maneuvers on board in real time.
3) Digital twin and hardware-in-the-loop simulation to per-
form DAA benchmarking and performance analysis under
different agent types, closure rates, interaction geome-
4) First-of-its-kind real-world flight tests demonstrating that
ViSafe ensures safe aerial separation in encounter scenar-
ios with a closure rate of up to 144 kmh.
II. RELATED WORK
A. Collision Avoidance logics
One of the seminal works in airborne collision avoidance
was the Traffic Collision and Avoidance System (TCAS) ,
which functions on a cooperative surveillance mechanism. The
agents communicate their position and intent using Mode S
developed. There are variants of this algorithm for different
agent types in different airspaces (ACAS Xa, Xu), etc. The
key factor driving the development of ACAS algorithms is
the availability of extended surveillance data using ADS-B,
which enables aircraft to assess potential collision risks and
coordinate maneuvers collaboratively. These logics involve
generating cost tables for agent states and possible actions
through simulation and optimization . These tables are then
used for Resolution Advisories (RAs) during deployment.
sors and information to provide RAs. Deploying these sensors
on small UAS is challenging, and there is a large push toward
sensor miniaturization for this purpose. The investigation of
ACASXu with visual inference information is a relatively
unexplored area . Additionally, most avoidance logics are
designed for large UAS, and hence, resolution advisories tend
to be relatively simple. However, small UAS can be in dense
airspaces with other UAS where a finer discretization of action
space is needed. This can lead to state space explosion, where
cost tables can be prohibitively large.
Our proposed technique works with vision inference data
and does not depend on radar and ADS-B data. Additionally,
our RAs are generated at runtime using efficient mixed integer
linear programming (MILP) solvers, which are suited for more
fine-grained control of UAS. Moreover, our technique can
factor in nominal control inputs, which incorporate the liveness
requirement of reaching a goal. It helps our agent stray not
too far from the originally planned trajectory.
B. Control Barrier Functions for Aerial Collision Avoidance
There has been a recent surge in using CBFs to ensure
the safe separation of UAS. Squires et al. identify key
challenges with designing CBFs for collision avoidance and
propose a construction technique. However, they assume in-
formation availability at all times during an episode and
test their formulations in simulation without sensor noise.
of CBFs in a decentralized setup with message passing to
communicate control outputs. However, they do not consider
non-cooperative setups in which all agents do not provide in-
formation. Our proposed technique assumes a non-cooperative
setup in which no information about other agents is explicitly
communicated to the controlled agent. Moreover, we deal
with information unavailability, where the CBF is only active
when intruder information from our vision-based detection and
tracking module is available.
C. Aircraft Detection and T
