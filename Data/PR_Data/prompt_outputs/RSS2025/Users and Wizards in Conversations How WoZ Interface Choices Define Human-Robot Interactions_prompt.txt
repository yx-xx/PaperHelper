=== PDF文件: Users and Wizards in Conversations How WoZ Interface Choices Define Human-Robot Interactions.pdf ===
=== 时间: 2025-07-21 15:44:10.747853 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Users and Wizards in Conversations:
How WoZ Interface Choices Define Human-Robot
Interactions
Ekaterina Torubarova
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
ekatorkth.se
Jura Miniota
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
jurakth.se
Andre Pereira
Division of Speech, Music and Hearing
KTH Royal Institute of Technology
atapkth.se
AbstractIn this paper, we investigated how the choice of
a Wizard-of-Oz (WoZ) interface affects communication with a
robot from both the users and the wizards perspective. In
a conversational setting, we used three WoZ interfaces with
varying levels of dialogue input and output restrictions: a) a
restricted perception GUI that showed fixed-view video and ASR
transcripts and let the wizard trigger pre-scripted utterances and
gestures; b) an unrestricted perception GUI that added real-time
audio from the participant and the robot c) a VR telepresence
interface that streamed immersive stereo video and audio to the
wizard and forwarded the wizards spontaneous speech, gaze and
facial expressions to the robot. We found that the interaction
mediated by the VR interface was preferred by users in terms of
robot features and perceived social presence. For the wizards, the
VR condition turned out to be the most demanding but elicited a
higher social connection with the users. VR interface also induced
the most connected interaction in terms of inter-speaker gaps and
and the largest silences. Given these results, we argue for more
WoZ studies using telepresence interfaces. These studies better
reflect the robots of tomorrow and offer a promising path to
automation based on naturalistic contextualized verbal and non-
verbal behavioral data.
Index TermsVR, Wizard-of-Oz, teleoperation, social robotics
I. INTRODUCTION
In the field of Human-Robot Interaction (HRI), Wizard-of-
Oz (WoZ) interfaces where a human operator replaces part
or the entire decision process of a system, are commonplace
, especially in scenarios involving complex social decision-
making or language exchange. Many HRI studies employ
WoZ settings, with humans controlling one or more robot
functions . Yet, the wizards data from these studies, rich
in potential, is often discarded post-study, however, this data
can be invaluable for different applications. Immersive WoZ
interfaces offer a unique opportunity in this regard as they
allow for the collection of rich, context-specific data that can
be used in refining behaviors and decision-making processes.
In this paper, we delve into a comparative analysis of
three different WoZ interfaces, each offering varying degrees
of input and output capabilities that will result in varying
levels of flexibility and control: a) a restricted perception
An example of the Meta Quest Pro tracking the operators facial
expression and displaying them on the Furhat robot in real-time.
graphical user interface (GUI) receiving a fixed-view video
stream and automatic speech recognition (ASR) feedback
from the participants, with pre-written utterances and gestures
delivered via synthetic robot voice; b) an unrestricted per-
ception GUI receiving fixed-view video, and real-time audio
from the participant, offering the same output; and c) a VR
interface receiving immersive video and real-time audio, with
unrestricted spontaneous natural speech and gestures. Our
experiment involved 25 users and 5 novice wizards engaging in
5-minute human-robot conversations centered around ethical
dilemmas. Each wizard, controlling the robot via each of the
three interfaces, engaged in three conversations with 5 unique
users in a within-subjects design, resulting in 15 interactions
per wizard (75 in total).
We hypothesize that these interface variations will influence
both the communication efficiency and users preferences
for the robot. In particular, we expect that the interaction
mediated by the immersive VR interface will foster deeper
engagement and perceived social connection, enabling more
natural conversations due to enhanced turn-taking and non-
verbal expression capabilities. Additionally, by assessing the
workload and habituation process across the three interfaces of
the wizards who do not have prior experience operating robots,
we hypothesize the VR interface to provide the most intuitive
control resembling real-life human-human interaction.
The unique contributions of this paper include 1) testing and
sharing a novel VR WoZ interface compared to more common
GUI interfaces; 2) analysis of both users and wizards data,
lacking in prior studies on WoZ methodologies; 3) in-depth
qualitative assessment of novice wizards experience of ac-
quiring different interfaces. Our findings aim to guide future
decisions on WoZ interface design, whether for targeted HRI
studies or paving the path for automation.
II. RELATED WORK
A. WoZ in Conversational Systems and HRI
Multiple studies investigated the intricacies of the WoZ
approach in tasks requiring verbal communication with the
user due to its complexity and sensitivity . A foundational
paper delved into the potential of the WoZ technique to
emulate interactive technologies, with a special emphasis on
speech input and output systems . More than 30 years later,
implementing believable verbal communication still remains a
Scarecrows - quick and capable, but requiring cautiousness
and transparency .
In verbal communication, there is a great demand for the
wizard to rapidly select and input open-domain content. The
study by  tackled the problem of a single wizard load
by proposing a cooperative multi-wizard platform Wizundry,
where the wizards could split speech-based tasks in real-
by  proposed to address this challenge through voice
puppetry. Their system leverages a neural TTS system in
near real-time, allowing natural speech input to determine
the output for a specific synthetic target voice. With a high
demand on wizard, in an extended conversation, turn-taking
timing when producing a behavior is particularly important:
response latency in everyday communication is reported to be
universally as short as 200 ms . While a significant line of
research is dedicated to modeling turn-taking in conversational
study by  compared their prototypical WoZ dialogue corpus
to face-to-face (FTF) dialogue. They found longer speaker
switch time in WoZ dialogues and more overlapped speech
in FTF dialogues, suggesting tighter turn coordination, but no
difference in speech duration between speakers. Delayed turn-
taking has also been consistently linked to lower engagement
. Thus, we consider it valuable to investigate both the
turn-taking dynamics of WoZ dialogues and the subsequent
impressions of end users and wizards.
Another early study argued that computer dialogue inter-
faces shouldnt merely replicate human-human interactions.
inherent to human-computer interactions . Building on
this foundation, a more recent exploration into the dynamics
between humans and various communicative agents, such as
unique challenges each agent presents . This underscores
the significance of robust WoZ simulations specialized for
HRI. A preliminary study with a humanoid robot  observed
that wizards experience difficulties in four different sub-
exploration into WoZ dialogues  further scrutinized the
challenges of emulating natural human dialogues using a
Graphical User Interface (GUI) WoZ approach. While the
interactions were enjoyable for the users, this study highlights
the difficulties of fully incorporating an operators intricate
interaction tactics into dialogue systems with limited dialogue
options. A recent study  examined how the balance
between robotic features and wizards features affects the
perception of teleoperated robots in a supermarket setting.
This study examined degrees of social presence by varying
the robots interaction modes: with or without a wizards
face and with or without conversion to a robotic voice. The
in-between condition, with the wizards face and converted
robotic voice, was perceived as more trustworthy and was
able to maintain longer interactions with customers. This study
highlights the delicate balance between anthropomorphizing
robots and maintaining their inherent robotic nature for opti-
mal user engagement in social interaction scenarios.
B. Virtual Reality (VR) WoZ for Immersive Robot Telepresence
In the domain of Immersive Robot Telepresence (IRT),
the design and efficiency of remote control interfaces play a
crucial role in determining operator performance for remotely
controlled robots. Multiple studies showed that VR-mediated
interfaces can allow high immersivity and efficient, intuitive
task execution. An insightful study compared three remote
control interfaces for mobile inspection robots, particularly
emphasizing the potential of VR devices . The primary
control interface incorporated a VR headset, data gloves for
enhanced gripper control, joystick-based movement controls,
and a motion tracking system to gauge head orientation and
hand positions. The results showed that VR-enhanced inter-
faces significantly boosted operator productivity, heightened
spatial presence, and improved distance evaluation. Notably,
such VR interfaces also reduced the adaptation time for oper-
and comfort levels. In , an operator wore a VR headset and
used handheld controllers to control a Pepper robot by simply
moving their body, allowing for intuitive embodied acting
and efficient robot control with first-person view. The authors
claimed their system could reduce wizard response times
and produce more contextually apt robot behaviors. However,
the system did face challenges such as motion sickness and
lagging movements caused by the slow servos present in
the Pepper robot. They also underscored the need for future
integration of eye-tracking technology, which would elevate
the realism and responsiveness of the robot interaction in social
contexts. Similarly,  utilized a VR headset connected to
the robots camera perspective, coupled with a Leap Motion
Controller that offered the operator a hands-free interaction
medium to control the Pepper robot. An augmented reality
Interface
Input Type
Output Type
Restricted Perception GUI
fixed-view video
ASR transcriptions
pre-written speech
(synthesized voice)
built-in gestures
Unrestricted Perception GUI  fixed-view video
real-time audio
pre-written speech
(synthesized voice)
built-in gestures
VR Telepresence Interface
immersive video
real-time audio
unrestricted speech
(natural voice)
spontaneous gestures
COMPARISON OF THE INPUT AND OUTPUT OF EACH INTERFACE.
interface was introduced to facilitate the control of a robots
verbal and nonverbal behaviors in . The results showed
that when a single operator controlled both types of behavior
that require tight coordination, it yielded better task results and
provided a heightened sense of presence compared to when
the task was split between two operators (one for verbal and
another for nonverbal behavior). The study also suggested that
this interface, which harnesses full-body motion control for
the robot while also enabling verbal interactions, presented a
powerful tool for WoZ interactions in human-robot scenarios,
as well as for data collection to inform the development of
autonomous multimodal dialogue systems.
C. WoZ for Data Collection or Automation
WoZ interfaces provide valuable data for learning models of
social behavior and have been used to automate robot actions.
where robots learn a behavior policy from WoZ demonstra-
tions. This study showed that robots that learned from prior
interactions successfully engaged children in play. In ,
the authors used WoZ interactions with constrained wizard
laborative robot tutor. Supervised Progressively Autonomous
Robot Competencies was proposed in , , enabling
robots to learn online from wizard guidance and become
more autonomous. Another study  used an LSTM model
on WoZ data to identify conversation shift moments, finding
that model performance varied across contexts, highlighting
the importance of tailoring WoZ data collection. The authors
of  explored human-to-robot conversation transition for
language learning, showing high accuracy in wizard response
automation. While our paper doesnt focus on automation, the
proposed telepresence system can facilitate the collection of
more naturalistic data for future automation endeavors.
III. SYSTEM
A. Social Robot
In this study, we used the humanoid robotic head Furhat
2.7.0 , capable of advanced speech synthesis and speech
recognition (Fig. 2). The robots face is back-projected on
the plastic mask, allowing for different appearances, be-
lievable eye-gaze and realistic facial movements (for lip-
synchronization and timely expressions that reveal emotional
Fig. 2. Furhat robot: 1) built-in static camera; 2) built-in speaker; 3) stereo
camera; 4) external speaker.
states such as sadness and excitement). The motors in the
robots neck allow for head movements such as nods or
headshakes. Additionally, it is equipped with a large voice
B. Graphical User Interfaces
For the GUI-based interfaces, we used the built-in web
interface for wizarding the Furhat robot (see Fig. 3). By
the fixed-view robots camera (Fig. 3.1) and a dialogue history
component that displays the utterances of both interlocutors,
with the user speech recognized by the built-in ASR (Fig.
3.2). We employed this default interface as the Restricted GUI
condition. For the Unrestricted GUI condition, we used the
same system but added a real-time audio stream from the
We also created two sections of buttons corresponding to
the robots utterances: dynamic (Fig. 3.4) and static (Fig. 3.5).
The dynamic section, in which the options changed based
on the conversation state, had a maximum of four buttons
displayed at any given time. In contrast, the static buttons
remained consistently visible on the screen, with ten buttons in
their statement, 3) ask the participant to repeat, 4) mention
another viewpoint, 5) agree, 6) disagree, 7) give an uncertain
Each static button had multiple utterance options to ensure
variety. This design ensured that pressing the same button
repeatedly would not produce repetitive utterances. However,
we maintained a sequential order for the different utterances
linked to each static button to ensure a more consistent
experience across users. In addition to the dialogue buttons,
by default, the Furhat interface includes a typing box for
producing a custom utterance (Fig. 3.3). The wizards were
informed that they could use this option during the interaction
but they should avoid long delays caused by typing.
C. VR Telepresence Wizard Interface
To implement the VR Interface condition, we created a
telepresence platform for the Furhat robot using the Meta
Quest Pro1, a VR headset that has built-in eye and face
Fig. 3. WoZ GUI in the middle of the conversation: 1) robots camera view;
2) dialogue history; 3) typing box; 4) dynamic buttons; 5) static buttons.
tracking. The facial expressions, head, and eye movements of
the wizard wearing the headset were collected from the headset
and displayed on the Furhat robots mask in real-time (Fig.
1). We used Meta platforms Movement SDK for Unity2 and
a Furhat CSharp interface inside the Unity3 editor to integrate
the VR headset with the Furhat robot to create a VR-mediated
WoZ interface. The full open-source version of our code can
be found in Github repository4.
While Furhat is equipped with a built-in camera at its base,
it was not suitable for the VR setup. It was the fixed-view
camera used in the initial two conditions, thus not allowing
for a full immersion for the wizard. Instead, we used a stereo
camera inserted into the robots fur hat (Fig. 2.3). This camera
was located slightly above the eye level of the robot but
oriented in a way that the wizard could keep the head straight
and maintain eye contact with the user. It also allowed the
wizard to move their head freely, maintaining a first-person
view. The camera was directly connected to the computer to
reduce lag and displayed the video to the operator in real time.
(Fig. 2.4) instead of the built-in speaker (Fig. 2.2) which can
only be used for Furhats speech synthesis.
D. Ethical Dilemmas Conversation Scenario
As a conversational scenario, we chose three ethical dilem-
mas that we expected to provide an engaging 5-minute con-
versation. The dilemmas chosen for this study were shared by
the Stockholm Museum of Technology holding an exhibition
on AI and society prior to running this study. These dilemmas
were chosen for their relevance and ease of scripting, enabling
participants to quickly memorize and navigate dialogues. The
dilemmas were the following: 1) Should we have robot judges
that use machine learning to make decisions? 2) If we invent a
pill that stops aging but only 5 of people can use it, should
it be allowed? 3) Would you take a DNA test before a date if
the other person asks you?
We developed a dialogue script for each of the topics which
was used to generate the dynamic buttons in the GUIs (Fig.
4). In the VR Interface condition, the wizards were instructed
to follow a similar structure of the dialogue but were not
restricted in what they could say. The scenario for each topic
included three issues (e.g. for the dilemma about robot judges:
bias and fairness; accountability; and emotions). Each issue
contained three pros and three cons. Apart from that, the
wizards had the option to introduce the next issue or to
reintroduce the current one again. To script the text of the
1) Imagine an ethical dilemma: [Dilemma Text]. Produce
five main issues of [Question of the Dilemma].
2) Now provide three pros and three cons of [Issue] in this
context.
3) Now write them in a more conversational way in one
short paragraph.
4) Reintroduce [Issue] in three different ways as if the other
person forgot what we were talking about, and we want
to get back on track.
Each paragraph was further tuned to follow the topic more
closely or provide more concrete examples and detailed de-
scriptions. While refining the script, we selected three out of
five issues raised about each topic to keep the conversation
within the intended 5 minutes. To create a fairer comparison
between our conditions we also added facial expressions
available in Furhats gesture library that were synchronized
with the robots speech whenever a button was pressed in
both of our GUIs. For this, we prompted Gemini6 with the
There is a script for what a social robot should say during
a dialogue: [Script]. There is a list of facial expressions and
gestures the robot can perform: BigSmile, Blink, BrowFrown,
the gestures across the script where appropriate.
The result was refined in cases if gestures were too frequent
or too sparse, or if they significantly interrupted the speech
flow. We selected the best-performing language model for each
of these tasks. Although the results were largely satisfactory,
both the text of the script and the gestures were manually
revised.
IV. METHOD
A. Participants
We aimed at a sample size of 5 wizards, each interacting
with 5 unique users across three conditions, resulting in a
total of 75 interactions. The data of one wizard and all
corresponding users was excluded from the analysis due to
technical issues with the VR headset that occurred with 4
users. To reach our intended sample size, we recruited another
wizard and 5 new users. All were recruited through university
channels and a local experiment recruitment portal.
Fig. 4. Dialogue script flow.
In the final analysis, 5 wizards were included (1 male, 4
with robots was medium to low (M  2.2 out of 5, SD  1.09),
as well as average familiarity with VR (M  2.6 out of 5, SD
1.14). 4 out of 5 wizards reported no prior experience with
social robots (one wizard has seen or interacted with Furhat
and Pepper). All wizards had experience in performing arts
(dance, stand-up comedy, theater, public speaking) and were
native or fluent in English. The performing experience criterion
was added to make sure the wizards were comfortable in the
task of performing the robots role. Prior to the experiment,
they were interviewed and informed about the tasks they would
have to do during the experiment. The wizards were rewarded
with a gift card equal to 91 USD for their participation.
As for the users, we analyzed data from 25 participants (11
28.5, SD  11.27). They had a low average familiarity with
robots (M  1.92 out of 5, SD  0.79), and 65.3 reported no
prior experience with social robots. All users were fluent or
native in English. They were rewarded with either a gift card
equal to 9 USD or a cinema ticket for their participation. All
of the participants gave written consent to participate in the
study. According to the Swedish national regulations, ethical
approval for the study was not required because we did not
collect sensitive personal data or conduct physical intervention.
B. Measures
To evaluate the users perception of the robot, we used the
Godspeed questionnaire . To evaluate the social aspects
of the interaction, we used the social presence questionnaire
described in  evaluated in other social HRI settings. The
questionnaire consists of 18 questions assessing six dimen-
fect interdependence. To evaluate the wizards proficiency and
progression experience, we used the NASA-TLX questionnaire
, a common tool for assessing task load; and the System
Usability Scale (SUS) . In the SUS questionnaire, we
omitted questions 1 and 4 because they were not relevant for
the current experiment, since they refer to future recurrent use
of the systems, which we did not intend.
C. Experimental Setup
The experiment was conducted in two adjacent rooms: the
control room and the experimental room. The experimental
room was soundproof with no visual connection to the control
room. The robot was installed in the experimental room, and
users sat in front of the robot at a comfortable distance and at
the same eye level. In the the Restricted GUI and Unrestricted
GUI conditions, the robot used its built-in static camera (Fig.
2.1) and provided tracking software to autonomously attend
to the user with eye and head movements, and had micro-
expressions enabled (e.g. blinking, small head movements). In
the VR Interface condition, both attention and facial expres-
sions were captured from the wizards natural behavior that
was displayed on the robot. The robots appearance and voice
were changed between conditions. Six robot mask textures
were selected, three male and three female, and two male
and female Amazon Polly voices from the built-in voice
library. The order of the topics, interfaces, and robot features
(corresponding to the wizards gender) was randomized for
each user. Video and audio data from the interaction were
recorded with two cameras: one facing the user and another
facing the robot.
D. Procedure
Before starting the experiment, the wizards filled in a
questionnaire that reported on their familiarity with social
robots and VR. Then they had a 1-hour practice to get used
to the interfaces and the topics. They were trained on the
functionalities of each interface and practiced using them with
the experimenter. They were informed that the user would not
know about them controlling the robot and that their task was
to make the robot have a naturalistic and smooth conversation
with the user. They were also asked to get familiar with the
script for each topic.
The wizard then left the room while a user entered and re-
entered when the user was sitting in the experimental room.
This procedure ensured that the wizard and the user did
not see each other before the experiment. Before starting
the interaction, the users also answered how familiar they
were with social robots. Then they could read the topics
they were supposed to discuss and think about their opinion.
the robot about the topic that the robot would suggest for 5
minutes. After 5 minutes, the robot informed the participants
that it was the time to wrap up the discussion. After the
Presence questionnaires regarding that interaction. After that,
they were instructed to have another conversation with the
robot. They were told that the robot would be powered by
a different system which would not have any recollection of
their previous conversation. The same procedure was repeated
for each of the three topics. Before and after each interaction,
the robots face projector was turned off and the user was
told that the robot system was being changed. At the end of
their experiment, the users were asked to answer two open-
ended questions: (1) Did you feel any differences between the
systems? If so, what kind of differences? (2) Did you feel that
any of the systems could be controlled by a human? If so,
which aspects of which system?
For each interaction, the wizards were reminded about the
topic and the interface they would use. They had time to check
the script if they needed to before the interaction. They were
instructed to have a conversation for 5 minutes and finalize
it by saying goodbye to the user when appropriate once the
experimenter signaled to them by a gentle tap on the back that
the time had run out. After each interaction, the wizards filled
in the NASA-TLX questionnaire.
After discussing all three topics and filling out the ques-
fact controlled by a human operator and they could meet the
wizard. At the end of the experiment day, the wizards filled in
a System Usability Scale (SUS) for each of the systems, and
left their open-ended comments about their experience. After
they were done filling in all questionnaires they were asked
to sum up their experience in a semi-structured manner.
V. RESULTS
A. User Questionnaires
1) Godspeed: To examine whether Godspeed scores were
affected by experimental condition, we first calculated a mean
score for each section of the questionnaire as well as the mean
total score for each participant. Then, we fitted a linear mixed-
effects model (LMM) with condition as a fixed effect and
wizard ID and participant ID as random effects to account for
potential variability between wizards and individual users. For
each section as well as the total score, the VR Interface condi-
tion achieved the largest mean score followed by Unrestricted
GUI followed by Restricted GUI. Each comparison showed
significant differences between conditions (p < 0.05), except
for the Safety score comparison between Restricted GUI and
Unrestricted GUI (p > 0.05). Full results for each comparison
see in Table II.
2) Social Presence: For social presence, similarly to ,
we compared both the total mean score and a mean score for
each section. Similarly to the analysis of Godspeed, we fitted
an LMM with condition as a fixed effect and wizard ID and
participant ID as random effects. For each section as well as
the total score, the VR Interface condition achieved the largest
mean score followed by Unrestricted GUI followed by Re-
stricted GUI. Each comparison showed significant differences
between conditions (p < 0.05), except for the Attention Allo-
cation and Affect Interdependence score comparisons between
Unrestricted GUI and VR Interface (p > 0.05). Full results
for each comparison see in Table III.
3) Perceived Autonomy: In the open-ended responses re-
garding perceived autonomy, 38 of users reported no differ-
ences between the conditions in the level of human control.
42 suggested that the VR Interface condition was con-
trolled by a human. When specified why, 54 of these users
mentioned that the voice of the robot was coming from a
human. This result contradicted both our pilot study with an
expert wizard where no user reported awareness of human
existing perceptions about the robots autonomy were resistant
to change .
4) Perceived Differences: In the open-ended responses by
the users a common trend was either a gradient in preference
between the conditions from Restricted GUI as the least
preferred to VR Interface as the most preferred (e.g. They
were increasing in being humanlike and interactive.; I felt
a very huge change in between the first interaction [Restricted
GUI] and the last [VR Interface]. Felt it more organic in the
third discussion.); or a mention of the most preferred or the
least preferred interaction (e.g. The last system [VR Interface]
was more humanlike and enabled an organic conversation
compared to the other two.; I preferred the last one [VR
Interface]. I think this was due to the way the robot talked.;
the last robot [Restricted GUI] was very machine-like. The
second [VR Interface] was much human-like.).
characteristics
mentioned
responses
human-likeorganic
machine-likeartificial
interactiv-
ityresponsiveness (5), little vs. a lot of interruptions (3), high
vs. low attention (1), high vs. low emotion understanding (1),
and best vs. worst voice (1).
B. Wizard Questionnaires
Given the limited sample size of the wizards, we focused
on a qualitative data analysis rather than statistical inference.
1) NASA-TLX: To analyze the wizards workload with
different systems, we took an average of raw NASA-TLX
scores for each wizard with each system per interaction. The
average NASA-TLX score per system suggest that for our
4.04, SD  1.22), followed by the Restricted GUI (M  3.18,
SD  1.03), and Unrestricted GUI was the least demanding
2) System Usability Score: To analyze the SUS for each
a 0-100 scale . The results suggest that the Unrestricted
GUI was the most usable (M  62, SD  11.2), followed by
the VR interface (M  43, SD  19.6), and the Restricted GUI
was the least usable (M  38, SD  14.3).
3) Interviews: In their open-ended reflections, W1 and W5
mentioned the Unrestricted GUI to be the easiest to use.
to use due to difficulty in timing their responses and incorrect
speech transcriptions. W2 and W4 mentioned that they felt
rude when they interrupted the users with the Restricted GUI.
W2 mentioned that using both GUIs ... was not so flexible
to react to subtle changes in [the users] flow.
Regarding the VR interface, W2 and W5 mentioned that it
was difficult for them to remember the scenario for the topics.
W3 and W5 mentioned that they would prefer the Unrestricted
GUI if they were not familiar with the topic and the VR
interface if they were familiar with the topic. W2 wizard
mentioned that they felt overwhelmed from concentrating
using the VR interface because I really tried to perform.
Godspeed Section
Condition
Estimate
Comparison
Estimate
Anthropomorphism
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Likeability
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Intelligence
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Total Score
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
TABLE II
LINEAR MIXED MODEL RESULTS FOR EACH SECTION OF THE GODSPEED QUESTIONNAIRE ACROSS THE THREE INTERFACE CONDITIONS. N.S. STANDS
FOR P-VALUE > 0.05.
Social Presence Section
Condition
Estimate
Comparison
Estimate
Co-Presence
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Attention Allocation
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Message Understanding
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Behavioral Interdependence
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Affect Understanding
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Affect Interdependence
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Total Score
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
TABLE III
LINEAR MIXED MODEL RESULTS FOR EACH SECTION OF THE SOCIAL PRESENCE QUESTIONNAIRE ACROSS THE THREE INTERFACE CONDITIONS. N.S.
STANDS FOR P-VALUE > 0.05.
W4 mentioned that it was inconvenient to use due to the echo
they heard in their headphones. W1 mentioned that the ability
of the VR to translate the facial emotions made it the coolest
In terms of the social aspect, W5 mentioned that the VR
interface was the most enjoyable: ... VR was more enjoyable
for me personally. I had a larger degree of freedom and
I was having a closer view of the participant which made
the conversation run smoother after the second instance of
VR use. W6 mentioned that ...with the VR, that I felt
higher degree of responsibility. W4 and W5 highlighted the
differences between the users they interacted with. W5: If the
participant showed reluctant and withdrawn, or defensive to
the topics, I found the VR more challenging because it was in a
greater degree up to me to build up rapport. W4 mentioned
that with the Restricted GUI they could withdraw from the
social awkwardness they felt with the audio and that it felt
easier socially.
In addition, W2 and W3 mentioned that they felt a dif-
ference between the condition most preferred by them and
by users. W3: I felt more comfortable when I couldnt hear
... I didnt interrupt them ... But I had to socially engage more
and it felt more demanding.
Speech Parameter
Condition
Estimate
Comparison
Estimate
Speech Duration (participant)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Speech Duration (robot)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Turn-Yielding Gaps Duration (between
participants and robots utterance)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Turn-Processing Gaps Duration (between
robots and participants utterance)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Cooperative Overlaps Proportion (robots
backchannels overlap with participant)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Destructive Overlaps Proportion
(participant interrupts robot)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
Destructive Overlaps Proportion
(robot interrupts participant)
Restricted GUI
Restricted GUI vs Unrestricted GUI
Unrestricted GUI
Restricted GUI vs VR
Unrestricted GUI vs VR
TABLE IV
LINEAR MIXED MODEL RESULTS FOR COMPARING SPEECH PARAMETERS ACROSS THE THREE INTERFACES. ROWS 1-4 SHOW ESTIMATE IN SECONDS,
ROWS 5-7 SHOW LOG-ODDS, N.S. STANDS FOR P-VALUE > 0.05.
C. Speech Dynamics
To analyze differences in speech dynamics across condi-
scribed using WhisperX large-v2 model . The transcrip-
tions underwent a manual check, where speakers were also
manually tagged. Since the model occasionally merged utter-
ances divided by a long within-speaker pause into a single
200 ms minimum silence threshold to obtain a more precise
total speaker time. Since we were interested in conversational
dynamics between the interlocutors, i.e. their involvement in
the dialogue and efficiency of turn taking, we extracted the
following speech parameters: speech duration of both speak-
ers; duration of turn-yielding gaps (from participant to robot);
duration of turn-processing gaps (from robot to participant);
proportion of overlaps.
1) Speech Duration: To examine whether speech duration
varied by condition and the speaker (participant or robot), we
calculated the total speaker time for each interaction. We then
fitted a linear mixed-effects model (LMM) separately for the
participants speech and for the robots speech with condition
as a fixed effect and wizard ID and participant ID as random
effects to account for potential variability between wizards and
individual users. The results of each following models see in
Table IV.
The participants spoke on average the longest in the VR
Interface condition, followed by the Unrestricted GUI, and
the least in the Restricted GUI condition. All comparisons
were significant (p < 0.05). The robot, on the contrary, spoke
on average the longest in the Unrestricted GUI condition,
and significantly reduced in both the Restricted GUI and the
VR Interface conditions (p < 0.001), although there was
no significant difference between the latter two conditions
2) Turn-Yielding Gaps Duration: For turn-yielding gap
duration (the gaps between the participants and the robots
utterances), we fitted an LMM to investigate the effect of the
condition on the gap duration. The longest gaps were found
in the Restricted GUI condition, which significantly decreased
in both Unrestricted GUI and the VR Interface condition
(p < 0.001). The comparison between the Unrestricted GUI
and the VR Interface condition was not significant (p > 0.05).
3) Turn-Processing Gaps Duration: For turn-processing
gaps duration (the gaps between the robots and the partic-
ipants utterances), we fitted another LMM with the same
parameters. The longest gaps were found in the Restricted GUI
GUI and the VR Interface condition (p < 0.05). The com-
parison between the Unrestricted GUI and the VR Interface
condition was not significant (p > 0.05).
4) Overlaps Proportion: We aimed to differentiate between
cooperative and disruptive overlaps. Several studies ,
found that cooperative overlaps, i.e. those indicating agree-
engagement compared to disruptive overlaps, i.e. disagree-
dialogue scenario, we focused on backchannels as instances
of cooperative overlaps. Out of all overlaps, we manually
tagged backchannels as cooperative overlaps and the other
instances as disruptive. We investigated whether the condition
affected the likelihood of overlaps occurring in an interaction,
by calculating the proportion of each type of overlap from the
total number of between-speaker transitions per interaction.
a) Cooperative Overlaps: There were no instances of
the participants backchannels overlapping with the robots
utterance. For the opposite, i.e., the robots backchannels over-
lapping with the participants utterances, we fitted a binomial
logistic regression model to investigate whether cooperative
overlap proportion was affected by the condition. The model
showed no significant effects of the condition (p > 0.05).
b) Destructive Overlaps: We found no significant effect
of the condition on the proportion of participants interrupting
the robot (p > 0.05). In the opposite case, we found a
significant effect of the condition on the proportion of the robot
interrupting the participant. The Restricted GUI condition had
the highest log-odds of destructive overlaps, corresponding to
the probability of 17.8. In the Unrestricted GUI condition,
the log-odds reduced slightly, but this change was not signif-
icant (p > 0.05). In the VR Interface condition, the log-odds
decreased significantly compared to both the Restricted GUI
and the Unresticted GUI (p < 0.001).
VI. DISCUSSION
We conducted a mixed-design study where participants
interacted with the Furhat robot as wizards controlling it or
as users conversing with it. We investigated the differences in
perceiving the interactions powered by three WoZ interfaces:
Restricted GUI, Unrestricted GUI, and VR Interface, each
offering a varying level of input and output capacity, from
both the users and the wizards perspectives.
Regarding the users, we found a clear trend of VR Interface
being the most preferred condition and Restricted GUI being
the least preferred, as was shown by virtually every dimension
of the Godspeed and Social Presence questionnaires and users
open-ended reflections. The Unrestricted GUI was also clearly
in the middle between the two conditions. Analyzing the
speech parameters of the conversations, we found that in the
Restricted GUI condition, the robot was taking the longest
pauses before producing an answer and was more likely to
interrupt the participant. These parameters might have been
the driving force of the subjective rating, as reflected in
the open-ended responses. It also contained the least speech
produced by the participant and relatively low speech from the
condition contained a lot of silence, leading to disengagement
and alienation. Notably, in the VR Interface condition, the
robot produced the least speech, while participants in this
condition spoke the most. Given that the VR Interface con-
dition was most preferred by the participants, we suggest that
timely and context-appropriate robots responses encouraged
the participants to be more engaged, while also enjoying the
social qualities of the robot, despite limited verbal output.
Turn-yielding and turn-processing gaps duration gradually
increased from VR Interface condition to Restricted GUI. This
not only suggests that the VR Interface condition induced the
most naturalistic conversational flow, but also might suggest
that participants adapted to the robots pace in all conditions,
indicating prosodic synchrony .
As for the wizards perspective, descriptive statistics suggest
that the task load appears to be higher using the VR interface,
both from NASA-TLX and SUS results. The wizards open-
ended reflections generally support this trend. This trend can
be due to the selected scenario, as several wizards mentioned
increased demand of remembering points of discussion. While
none of the wizards mentioned experiencing motion sickness
using the VR interface, the not-perfect latency and possible
discomfort of wearing the VR headset could have also affected
the wizards preferences. Since the operators only had a 1-
hour practice prior to the experiment, which is unusual for
WoZ setups, most of the downsides identified in the telep-
resence condition can probably be faced by having a better-
trained operator who is more experienced with the dialogue
scenario and is more comfortable with socially interacting
in VR. Despite that, several wizards reported feeling signif-
icantly more socially engaged in the VR Interface condition,
however for some it resulted in feeling more responsibility
or concentration while performing their role. Notably, the
wizards preferences appear to vary, with some wizards having
comfortable load levels and preferring the VR interface to the
GUIs. The downsides that were pointed to the prototypical
WoZ GUIs are harder to overcome and were already clearly
identified in the related work section (hard to time turn-taking
and limited flexibility in dialogue). Difficulty in timing and
restriction in spontaneous reactions to the user was perceived
as frustrating or socially awkward, with some wizards feeling
rude to accidentally interrupt the user. The Restricted GUI
was clearly the least preferred both by the wizards and the
users as it exacerbates these issues whereas having non-
restricted perception but restricted output appears to be the
least demanding task for wizards.
VII. LIMITATIONS
The small sample size, particularly of wizards, and their
limited training time may have influenced the results. Future
research with a larger and more diverse sample, as well as
more extensive training, is needed to further validate these
findings. Also, other techniques and interfaces for achieving
unrestricted output could be tested.
VIII. CONCLUSION
In this study, we compared the perception of interactions
with a robot powered by three different WoZ interfaces by both
users and wizards. Our VR-mediated interface demonstrated
superior performance, was the most preferred, and provided
a higher sense of social presence for users. It also fostered
a greater sense of social connection for wizards, as reflected
in their statements, such as I really tried to perform and
acknowledgment of a higher degree of responsibility when
operating the robot. Notably, turn-yielding gaps improved
significantly using both VR interface and the Unrestricted
smoother turn-taking dynamics.
The VR telepresence interface offers a promising path
toward creating more natural and engaging social human-
robot interactions. This is particularly important with the rapid
advancements in generative AI, which are expected to enable
robots to provide more responsive and adaptive feedback. As
speech technology continues to evolve, testing interaction dy-
namics in ecologically valid ways is crucial. VR telepresence
systems facilitate the collection of rich, contextualized data,
paving the way for automating social behaviors in robots.
While restricted input or output may still be valuable for
controlled WoZ experiments, our findings highlight the trade-
offs between different interface approaches.
ACKNOWLEDGMENTS
This work was supported by KTH Digital Futures (Sweden)
and the Swedish Research Council project 2021-05803.
REFERENCES
L. D. Riek, Wizard of oz studies in hri: a systematic review and new
reporting guidelines, Journal of Human-Robot Interaction, vol. 1, no. 1,
P. Baxter, J. Kennedy, E. Senft, S. Lemaignan, and T. Belpaeme,
From characterising three years of hri to methodology and reporting
human-robot interaction (hri), pp. 391398, IEEE, 2016.
N. M. Fraser and G. N. Gilbert, Simulating speech systems, Computer
Speech  Language, vol. 5, no. 1, pp. 8199, 1991.
T. Williams, C. Matuszek, R. Mead, and N. Depalma, Scarecrows in
Human-Robot Interaction, vol. 13, no. 1, pp. 111, 2024.
S. Hu, H. C. Yen, Z. Yu, M. Zhao, K. Seaborn, and C. Liu, Wizundry:
A cooperative wizard of oz platform for simulating future speech-based
interfaces with multiple wizards, Proceedings of the ACM on Human-
Computer Interaction, vol. 7, no. CSCW1, pp. 134, 2023.
M. P. Aylett and Y. Vazquez-Alvarez, Voice puppetry: Towards con-
versational hri woz experiments with synthesised voices, IEEEACM
International Conference on Human-Robot Interaction, 2020.
T. Stivers, N. J. Enfield, P. Brown, C. Englert, M. Hayashi, T. Heine-
Universals and cultural variation in turn-taking in conversation, Pro-
ceedings of the National Academy of Sciences, vol. 106, no. 26,
D. DeVault, J. Mell, and J. Gratch, Toward natural turn-taking in a
virtual human negotiation agent., in AAAI Spring Symposia, 2015.
A. Pellet-Rostaing, R. Bertrand, A. Boudin, S. Rauzy, and P. Blache,
A multimodal approach for modeling engagement in conversation,
Frontiers in Computer Science, vol. 5, p. 1062342, 2023.
N. Dahlback, A. Jonsson, and L. Ahrenberg, Wizard of oz studies:
why and how, in Proceedings of the 1st international conference on
Intelligent user interfaces, pp. 193200, 1993.
E. Greussing, F. Gaiser, S. H. Klein, C. Stramann, C. Ischen, S. Eimler,
K. Frehmann, M. Gieselmann, C. Knorr, A. Lermann Henestrosa, et al.,
Researching interactions between humans and machines: Methodolog-
ical challenges, Publizistik, vol. 67, no. 4, pp. 531554, 2022.
A. Shin, J. Oh, and J. Lee, Apprentice of oz: Human in the loop
system for conversational robot wizard of oz, in 2019 14th ACMIEEE
International Conference on Human-Robot Interaction (HRI), pp. 516
K. Funakoshi and M. Komuro, Toward mindful machines: A wizard-
of-oz dialogue study, in 2019 IEEE International Conference on
Humanized Computing and Communication (HCC), pp. 9499, IEEE,
S. Song, J. Baba, J. Nakanishi, Y. Yoshikawa, and H. Ishiguro, Costume
vs. wizard of oz vs. telepresence: how social presence forms of tele-
operated robots influence customer behavior, in 2022 17th ACMIEEE
International Conference on Human-Robot Interaction (HRI), pp. 521
J. Jankowski and A. Grabowski, Usability evaluation of vr interface for
mobile robot teleoperation, International Journal of Human-Computer
S. Thellman, J. Lundberg, M. Arvola, and T. Ziemke, What is it like to
be a bot? toward more immediate wizard-of-oz control in social human-
robot interaction, in Proceedings of the 5th International Conference
on Human Agent Interaction, pp. 435438, 2017.
N. Tran, J. Rands, and T. Williams, A hands-free virtual-reality tele-
operation interface for wizard-of-oz control, in Proceedings of the 1st
International Workshop on Virtual, Augmented, and Mixed Reality for
HRI (VAM-HRI), 2018.
A. Pereira, E. J. Carter, I. Leite, J. Mars, and J. F. Lehman, Aug-
mented reality dialog interface for multimodal teleoperation, in 2017
26th IEEE International Symposium on Robot and Human Interactive
Communication (RO-MAN), pp. 764771, IEEE, 2017.
W. B. Knox, S. Spaulding, and C. Breazeal, Learning from the wizard:
programming social interaction through teleoperated demonstrations,
in Proceedings of the 2016 International Conference on Autonomous
Agents  Multiagent Systems, pp. 13091310, 2016.
P. Sequeira, P. Alves-Oliveira, T. Ribeiro, E. Di Tullio, S. Petisca,
F. S. Melo, G. Castellano, and A. Paiva, Discovering social interaction
strategies for robots from restricted-perception wizard-of-oz studies,
in 2016 11th ACMIEEE International Conference on Human-Robot
Interaction (HRI), pp. 197204, IEEE, 2016.
E. Senft, P. Baxter, J. Kennedy, and T. Belpaeme, Sparc: Supervised
progressively autonomous robot competencies, in Social Robotics: 7th
International Conference, ICSR 2015, Paris, France, October 26-30,
E. Senft, S. Lemaignan, P. E. Baxter, M. Bartlett, and T. Belpaeme,
Teaching robots social autonomy from in situ human guidance, Science
D. Lala, K. Inoue, and T. Kawahara, Evaluation of real-time deep learn-
ing turn-taking models for multiple dialogue scenarios, in Proceedings
of the 20th ACM International Conference on Multimodal Interaction,
O. Engwall, J. Lopes, and R. Cumbal, Is a wizard-of-oz required for
robot-led conversation practice in a second language?, International
Journal of Social Robotics, vol. 14, no. 4, pp. 10671085, 2022.
S. Al Moubayed, J. Beskow, G. Skantze, and B. Granstrom, Furhat: a
back-projected human-like robot head for multiparty human-machine in-
Training School, Dresden, Germany, February 21-26, 2011, Revised
Selected Papers, pp. 114130, Springer, 2012.
C. Bartneck, D. Kulic, E. Croft, and S. Zoghbi, Measurement in-
struments for the anthropomorphism, animacy, likeability, perceived
social robotics, vol. 1, pp. 7181, 2009.
A. Pereira, C. Oertel, L. Fermoselle, J. Mendelson, and J. Gustafson,
Effects of different interaction contexts when evaluating gaze models
in hri, in Proceedings of the 2020 ACMIEEE International Conference
on Human-Robot Interaction, pp. 131139, 2020.
S. G. Hart, Nasa-task load index (nasa-tlx); 20 years later, in Pro-
ceedings of the human factors and ergonomics society annual meeting,
vol. 50, pp. 904908, Sage publications Sage CA: Los Angeles, CA,
J. Brooke, Sus: a quick and dirtyusability, Usability evaluation in
J. Miniotaite, E. Torubarova, and A. Pereira, Comparing dashboard and
virtual reality wizard-of-oz setups in a human-robot conversational task,
J. Nasir, P. Oppliger, B. Bruno, and P. Dillenbourg, Questioning wizard
of oz: effects of revealing the wizard behind the robot, in 2022
31st IEEE international conference on robot and human interactive
communication (RO-MAN), pp. 13851392, IEEE, 2022.
M. Bain, J. Huh, T. Han, and A. Zisserman, Whisperx: Time-accurate
speech transcription of long-form audio, INTERSPEECH 2023, 2023.
Silero Team, Silero vad: pre-trained enterprise-grade voice activity
detector (vad), number detector and language classifier.
A. Cafaro, N. Glas, and C. Pelachaud, The effects of interrupting be-
havior on interpersonal attitude and engagement in dyadic interactions,
in Proceedings of the 2016 International Conference on Autonomous
Agents  Multiagent Systems, pp. 911920, 2016.
K. Hilton, The perception of overlapping speech: Effects of speaker
prosody and listener attitudes., in Interspeech, pp. 12601264, 2016.
N. Sadoughi, A. Pereira, R. Jain, I. Leite, and J. F. Lehman, Creating
prosodic synchrony for a robot co-player in a speech-controlled game
for children, in Proceedings of the 2017 ACMIEEE International
Conference on Human-Robot Interaction, pp. 9199, 2017.
