=== PDF文件: Boxi Design Decisions in the Context of Algorithmic Performance for Robotics.pdf ===
=== 时间: 2025-07-22 15:45:41.886104 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Algorithmic Performance for Robotics
Jonas Frey,1,3
Turcan Tuna,1
Lanke Frank Tarimo Fu,2
Cedric Weibel1
Katharine Patterson1
Benjamin Krummenacher1
Matthias Mller1
Julian Nubert1,3
Maurice Fallon2
Cesar Cadena1
Marco Hutter1
1 ETH Zurich
2 University of Oxford
3 Max Planck Institute for Intelligent Systems
Equal contribution. Contact: jonfreyethz.ch
Fig. 1: Boxi can be deployed handheld, on a legged robot, or a wheeled robot across a variety of environments.
AbstractAchieving robust autonomy in mobile robots op-
erating in complex and unstructured environments requires a
multimodal sensor suite capable of capturing diverse and com-
plementary information. However, designing such a sensor suite
involves multiple critical design decisions, such as sensor selection,
component placement, thermal and power limitations, compute
the importance of these key aspects is widely recognized, they
are often overlooked in academia or retained as proprietary
knowledge within large corporations. To improve this situation,
we present Boxi, a tightly integrated sensor payload that enables
robust autonomy of robots in the wild. This paper discusses the
impact of payload design decisions made to optimize algorithmic
performance for downstream tasks, specically focusing on state
estimation and mapping. Boxi is equipped with a variety of sensors:
two LiDARs, 10 RGB cameras including high-dynamic range,
global shutter, and rolling shutter models, an RGB-D camera, 7
inertial measurement units (IMUs) of varying precision, and a
dual antenna RTK GNSS system . Our analysis shows that time
impact on the state estimation performance. We frame this analysis
in the context of cost considerations and environment-specic
challenges. We also present a mobile sensor suite cookbook
to serve as a comprehensive guideline, highlighting generaliz-
able key design considerations and lessons learned during the
development of Boxi. Finally, we demonstrate the versatility
of Boxi being used in a variety of applications in real-world
I. INTRODUCTION
Designing a capable perception payload is fundamental to
the success of mobile robotics systems. The performance of
perception algorithms is inherently upper bounded by the
quality of the input sensor data, which might be corrupted by
aleatoric sensor noise and epistemic design issues, regardless of
the applied algorithm. Therefore, it is key to select the appro-
priate sensors while considering time synchronization, sensor
hardware design decisions to enable the next generation of
mobile robots to operate reliably in complex environments.
Previous studies have emphasized the critical role of time
synchronization [75, 70, 7], rolling shutter correction ,
SLAM performance under various conditions [45, 76], and
accurate intrinsic and extrinsic calibration [56, 26, 16, 30, 44]
in determining downstream task performance.
To this end, we introduce Boxi, a compact, standalone
sensing payload designed to enable the development of
autonomy solutions for mobile robots, with a particular focus
on quadrupedal legged robots. Boxi integrates best practices in
system design, and was co-developed in collaboration with
Leica Geosystems. We leverage Boxi to establish a direct link
between design decisions and algorithmic performance on
downstream tasks such as robot localization.
System Name
Other Sensors
GNSS Solution
Reference
SubT-MRS
FLIR Boson
Versatile
CatPack[33, 54]
FLIR Lepton 3.5 thermal
VersaVIS
Versatile
Shepards Stick
Handheld
FusionPortable
2x Event Camera
RTKMOCAPScanner
Versitale
Scanner  RTK
HW (STM32 MCU)
Scout V1.0
Versatile
(Industrial) BLK2Go
Handheld
WHU-Helmet
EverySync
Wheeled Robot
INS  MOCAP
Wheeled Robot
TartanDrive [68, 61]
Offroad Car
Handheld  Wheeled
Boxi (Ours)
1x Barometer
Legged Robot
TABLE I: Overview of Existing Sensor Payloads. Symbols:  present,  absent.
The design of Boxi is driven by four primary objectives:
(1) developing a state-of-the-art mobile robot sensor suite,
(2) fostering a better understanding between payload design de-
cisions and algorithmic performance, (3) allowing the collection
of large scale multi-sensor datasets with high-quality calibration
and time synchronization and (4) enabling autonomous mobile
navigation. Based on these objectives, we equipped Boxi with
accessible state-of-the-art sensors featuring two spinning Li-
DARs (repetitive and non-repetitive), 10 cameras (high-dynamic
based depth camera, 7 inertial measurement units (IMUs) of
varying precision and cost (5 USD to 12 000 USD), as well as
a dual antenna RTK-GNSS system  and tight integration
to Leica Geosystems MS60 Total Station (Reector Prism
Receiver Link). Compared to autonomous driving payloads,
Boxi is more compact and lightweight, but shares similar design
requirements provided in . It is based on the principle that
payload design must consider algorithmic performance .
To support and extend this notion to general robotics, we
present a sensory payload cookbook that summarizes design
decisions and lessons learned, enabling the development of
future perception systems beyond Boxi.
onboard state estimation, mapping, and navigation. At the core
of this work, we use Boxi to conduct a rigorous study to
investigate the state estimation performance across multiple
dimensions (e.g., accuracy, robustness to time, and extrinsic
calibration) with respect to hardware and software design
decisions in 7 different environments.
We begin by analyzing the performance of different sensor
modalities using popular and state-of-the-art approaches for
each modality (LiDAR, Camera, Proprioception, and GNSS).
We then compare different camera types (high dynamic range,
global shutter, and rolling shutter) in terms of image quality
and Visual Interial Odometry (VIO) performance. In addition,
we compare the performance impact of IMUs (5 USD to
12 000 USD) on dead-reckoning and LiDAR Interial Odometry
(LIO) performance. Lastly, we simulate time synchronization
and extrinsic calibration errors to investigate the effect on
state estimation. Through the steps described above, we will
address the following research question: Which sensors should
be purchased given the mapping accuracy requirements, budget
are extrinsic and intrinsic calibration, as well as time syn-
further research, the full hardware and software design is made
available open-source.
In summary, the main contributions of this work are:
multi-modal sensor payload optimized for data collection
on mobile robotic platforms.
Ablation studies on the effect of modalities, time syn-
chronization accuracy, and extrinsic calibration accuracy
across multiple real-world environments.
Compiling a comprehensive cookbook, featuring insights,
lessons learned, and best practices for designing payloads
for robotic platforms.
II. RELATED WORKS
A. Payloads for Autonomous Driving
The development of mobile mapping systems initially
focused on integration into wheeled platforms due to their
capacity to accommodate heavy sensors and their ability to
drive long distances . These systems often incorporated a
combination of cameras, LiDAR sensors, IMU, and GNSSa
sensor suite that continues to form the foundation of our work.
One of the milestones in the development of autonomous driv-
ing was marked by the introduction of the KITTI dataset ,
which featured comprehensive data from cameras, LiDAR,
the eld today. Subsequent data collection platforms focused
on semantic understanding  and temporal changes .
While the previous works were primarily driven by academia,
multiple industry-driven sensory payloads were used to release
high-quality datasets, such as NuScenes , Waymo , and
Argoverse . These datasets often feature similar sensor con-
gurations; however, detailed information on calibration, time
able. More recent efforts have introduced modern sensors such
as 360Field of View (FoV) radar  or event cameras .
B. Compact Vision Payloads
Within the last decade, a variety of more mobile and
lightweight payloads based on multi-camera setups were intro-
Co-Design
Deployment
Calibration
3 x HDR Camera
1 x Stereo Camera
5 x Global Shutter
1 x Hesai XT32
1 x Livox Mid-360
1 x Mini-Prism GRZ101
1 x GNSS Receiver
7 x IMUs 5USD - 10kUSD
Mechanical
Electrical
Software
Boxi Components
Fig. 2: Co-design of hardware, electrical systems, software, and calibration is essential for developing a robust sensor payload.
duced. The authors in  introduced the DLR 7, a stereo cam-
era system with FPGA-based time synchronization, enabling
handheld operation at just 800 g. Similarly, Nikolic et al.
developed an ultra-lightweight unit integrating stereo cameras
and an IMU with FPGA-based synchronization and exposure
time compensation, tailored for online visual feature extraction.
Zhang et al.  presented the PIRVS system, which directly
integrated the SLAM algorithms on-board, while considering
the latency between the image exposure period and CPU
processing. Expanding on these designs, Tschopp et al.
introduced VersaVIS, an open, versatile multi-camera and IMU
sensor suite with precise hardware synchronization, which later
evolved into the commercial Sevensense CoreResearch .
Although all previous work highlights the importance of calibra-
tion and time synchronization and includes FPGAs or ISPs for
processing or triggering, no systematic experimental evaluation
of their impact on state estimation performance is provided.
1) Compact Multi-Modal Sensor Payloads: The develop-
ment of more compact multi-modal sensor suites has been
of interest in industry and research
[64, 75, 54] driven by
their promising potential to enhance precision and resilience
in challenging environments. An industrial example is the
Leica Geosystems BLK2Go , a tightly integrated handheld
scanning device, which is equipped with three global shutter
that provides measurements up to 3 mm accuracy.
EverySync  is a multi-sensor payload focusing on time
synchronization accuracy. EverySync relies on the methods
developed in VersaVIS  to perform triggering and time
synchronization. Moreover, the authors of EverySync provided
an extensive analysis of accurate time synchronization and
achieved a time synchronization accuracy of 1 ms between
and Inertial payloads [31, 54, 35] showed the importance of
time synchronization and extrinsic calibration between modali-
ties for SLAM and state estimation purposes. Liu et al.
integrated and demonstrated a multi-LiDAR, multi-camera,
and multi-IMU setup with precise time synchronization up to
1 s using Precision Time Protocol (PTP) and a self-designed
STM32 MCU triggering board. Recently, Nguyen et al.
integrated a sensor payload and proposed using a continuous
time B-spline trajectory representation for reference pose esti-
mation. They showed that this formulation mitigates the impact
of time synchronization errors for SLAM and state estimation
evaluation.
III. OVERVIEW
A. How to read the paper
This paper is structured as follows. Section III-B details
Boxis key specications, providing the foundation for the
algorithmic performance analysis provided in Section IV, which
is self-contained and highlights the most relevant experiments
and ndings. Finally, Section V serves as a comprehensive
lessons learned as well as generalizable ndings for the design
of future sensor payloads.
B. Boxi Overview
Boxi is designed to enable autonomy and provide a
comprehensive understanding of the payload design decisions
on downstream task performance. This requires the integration
of different sensing modalities and sensor types. Therefore,
Boxi includes exteroceptive sensors such as two LiDARs and
a stereo camera setup for geometric perception. In addition,
the payload is equipped with high-dynamic range, global
and various tiers of IMUs for inertial sensing. Boxi provides
time synchronization accuracy ranging between 5 ms to 10 s
(sensor-dependent Section V-D2), and accurate extrinsic
calibrations (cam-to-cam 0.3 mm1, cam-to-LiDAR 1.0 mm
corner detection accuracy) between all cameras and LiDARs.
Precise extrinsic calibration is achieved by CNC machining
Boxi out of an aluminum mono-block with a manufacturing
precision of 0.05 mm in combination with state-of-the-art
calibration algorithms (customized version of Kalibr [56, 26],
IMU intrinsic , LiDAR extrinsic ).
The cameras and LiDARs on Boxi are carefully positioned to
capture the near-eld surroundings of the robot, which is crucial
for locomotion and navigation as opposed to a reality capture
setup. The complete sensor payload weighs 7.1 kg, making it
Description
est. Price
Livox Mid-360
6.5 W (avg)
Hesai XT-32
10 W (avg)
3000 USD
(CoreResearch)
Sevensense CoreResearch
12 W (max)
2000 USD
TierIV C1
1.7 W (max)
Stereolabs ZED2i
2 W (max)
Reference
Modied Leica AP20
MS60 Total Station Bluetooth Communication  Synchronization
5000 USD
Mini-Prism GRZ101
Static Accuracy 2 mm (3),
hFoV 360,
1000 USD
Novatel Span CPT7
Dual RTK GPS, Intertial Explorer, TerraStar
18 W(max)
40 000 USD
3GNSSA-XT-1
Multiband GPS Antenna [BeiDou, L-Band, GLONASS, GPS, Galileo, Navic]
1 W (each)
Honeywell HG4930
20 ggrav, GBI 0.25 h1,
ABI 0.025 mg
2 W (max)
12 000 USD
(STIM320)
Safran STIM320
5 ggrav,
GBI 0.3 h1,
2 W (max)
8000 USD
(AP20-IMU)
Proprietary IMU (AP20)
Mid-Range
Analog Devices ADIS16475-2
8 ggrav,
GBI 2.5 h1,
ABI 0.0036 mg
Bosch BMI085 (CoreResearch)
16 ggrav
TDK ICM40609 (Livox)
32 ggrav
(ZED2i-IMU)
Unknown (ZED2i)
TABLE II: Boxi Components: GSGlobalShutter, RSRollingShutter. Used camera settings are provided under their respective sections.
ABIAccelerometer Bias Instability, GBIGyro Bias Instability. The (AP20) requires a Leica MS60 Total Station at a cost of 50 000 USD.
suitable for deployment with a harness or with adaptable mount-
ing options that can be deployed on different robotic platforms.
The sensor arrangement is optimized for near-eld perception
(stereo camera baseline of 12 cm) and long-range using a high-
accuracy repetitive LiDAR. Throughout this work, each sensor
will be referred to by its abbreviation outlined in Table II.
IV. ALGORITHMIC PERFORMANCE
A. Real-World Datasets
We evaluated our hardware and software decisions on
data from seven real-world environments collected in the
wild (Figure 3), which are part of the GrandTour dataset.
All data were gathered by the legged robot ANYmal
equipped with Boxi and teleoperated by an expert human. The
selected scenarios are diverse in lighting and include indoor
and outdoor settings, conned spaces, wide open spaces, and
both structured and unstructured scenes. The deployments
take between 3 min-7 min and cover distances around 300 m
and more details are available in Appendix A.
B. Algorithms
As a representative of multi-camera visual-inertial state esti-
loop closures. Moreover, a customized version of the state-
of-the-art LiDAR-inertial geometric-observer based odometry
pipeline Direct LiDAR Inertial Odometry (DLIO)  is
used to represent tightly coupled LiDAR-inertial methods. In
represent the performance of proprioceptive state-estimation
methods. Alongside these modules, the post-processed GNSS-
IMU solution of the Inertial Explorer  is provided as an
industrial positioning solution for outdoor datasets. The sum-
mary of the algorithms is provided in Table III.
Algorithm
Modality
Description
Visual-Inertial
Multi-Camera VIO with Loop Closure
LiDAR-Inertial
Tightly coupled Filter-based LIO
Kinematic-Inertial
Proprioceptive Filter-based
Inertial Explorer
GNSS-Inertial
Industrial ofine INS
TABLE III: Summary of the State Estimation Algorithms
C. State Estimation - Effect of Modality
Different sensors provide complementary information that
allows a robot to operate in various environments that are
challenging for one or more sensor modalities. The goal of
this analysis is to present a rough estimate of how different
modalities perform across diverse environments, helping to
select the most suitable sensor for specic environments and
state estimation requirements. Before interpreting the results, it
is crucial to understand that the performance of each modality
is highly dependent on the motion pattern (e.g., motion blur,
saturation effects) and the environment (e.g., LiDAR degrada-
It is not possible to provide fully generalized claims that apply
to all possible scenarios, but we can deduce guiding principles.
We compare DLIO  (fuses data from Hesai and
HG4930), OKVIS2  (fuses data from CoreResearch-
Stereo pair and HG4930), GNSS  (IE-TC more details in
Appendix B) and TSIF . Table IV and Table V presents
Mission Name
Kinematic
Research Station
Mountain Ascent
Excavation Site
Demolished Building
Warehouse
TABLE IV: Modality Comparison in RTE per dataset.
Excavation Site
Research Station
Demolished
Mountain Ascent
Fig. 3: Subset of the GrandTour Datasets Overview: (top-row) left, front, and right HDR camera images; (bottom-row) satellite image with
projected GNSS path and camera image viewpoint; The warehouse dataset is not displayed.
Mission Name
Kinematic
Research Station
Mountain Ascent
Excavation Site
Demolished Building
Warehouse
TABLE V: Modality Comparison in ATE per dataset.
Relative Trajectory Error (RTE) and Absolute Trajectory Error
(ATE) results respectively across all seven tested environments
compared against the total station (TPS) measurements. DLIO,
consistently outperforms OKVIS2 and kinematic-based state
estimation in terms of ATE. With sufcient satellite coverage,
the ofine-optimized GNSS solution provides highly accurate
terms of RTE, kinematic-based state estimation performs well,
verifying its common use for control.
D. State Estimation - LiDAR Comparison
In the following, all Absolute Position Error (APE) and
Relative Position Error (RPE) error metric computations are
done with respect to our 6-Degrees of Freedom (DoF) ground
truth pose estimate, which is obtained by fusing TPS, IMU,
and Inertial Explorer post-processed poses (see Appendix B).
We test how the Hesai, an accurate long-range LiDAR,
compares against the lower-cost and lightweight Livox. For
Camera Comparision
Alphasense
LiDAR Comparision
Fig. 4: Camera and LiDAR Comparison using OKVIS2 (HG4930)
and DLIO (HG4930). (A-2, A-3, A-1, Z-2, H-3) indicates respective
camera conguration (see Section IV-E). The Camera is evaluated
on the Mountain Ascend dataset while the LiDAR comparison is
provided on the Hike (Open) and Warehouse (Conded) datasets.
this comparison, we selected an open (Hike) and conned
environment (Warehouse) and reported the results in Figure 4.
The Hike dataset contains limited geometric features compared
to the Warehouse dataset. Across both datasets, the Hesai
outperforms the Livox. However, in the dataset with a large
featureless space, the gap is more signicant.
E. State Estimation - Camera Comparison
In this study, we compare the accuracy of different cameras
for VIO using OKVIS2. We evaluated multiple different
camera congurations:
ADIS16475
AP20-IMU
Mean  0.5x Std Dev
ADIS16475
AP20-IMU
IMU type
Mean  0.5x Std Dev
Fig. 5: Effect of different IMUs on the performance of DLIO module
on the Mountain Ascend dataset. The generated APE and RPE errors
are visualized.
with the number representing the camera conguration. Detailed
mounting and FoV specications are available in Section V-C1.
We evaluated the performance in the Mountain Ascent
dataset (see Figure 4). Both the H-1 and Z-1 congurations
The CoreResearch unit, utilizing global shutter cameras,
consistently delivers the best accuracy across all environments.
For the HDR image, we provide OKVIS2 with the timestamp
of the center line in the middle of the exposure period; however,
the rst line is captured approximately 15 ms earlier, while the
last line is captured 15 ms later. Our results indicated that not
accounting for the rolling shutter effect signicantly degrades
performance. The ZED2i rolling shutter camera is connected
via USB (unlike the HDR camera, which uses GMSL2).
limited to the accuracy of the USB serial buffer readout. The
three HDR cameras have limited overlapping FoV compared
to the ZED2i stereo-pair. This explains the worse performance
of the three HDR cameras in comparison to the two ZED2i
cameras. Our results highlight the need to account for the
rolling shutter effect, and our paired multi-camera dataset
enables rigorous future studies and algorithmic development.
F. State Estimation - Effect of IMUs
IMUs are fundamental to state estimation due to their high-
rate measurements and their ability to perceive angular velocity
alongside linear acceleration. Numerous IMUs with different
noise characteristics and costs are available to researchers.
While the general relation between noise characteristics and
algorithmic performance is well-understood, how this relation
can manifest itself in real-world scenarios has received limited
attention [20, 49]. To bridge this gap, we tested six different
IMUs using DLIO  with Hesai LiDAR again on the Moun-
tain Ascend dataset. Figure 5 shows only a marginal difference
in APE for different IMUs. We hypothesize that this is the
case due to the tightly coupled LiDAR point cloud registration.
The point cloud registration corrects the inaccuracies in the
Time [s]
TPS Measurements
Time [s]
TPS Measurements
Time [s]
TPS Measurements
Fig. 6: IMU Dead-Reckoning during TPS measurement dropout (4.5 s-
9 s and 17 s-22 s).
IMU state propagation and ensures the correct bias estimation.
To analyze the impact of IMU noise characteristics indepen-
dent of the tightly-coupled LiDAR processing, we conducted a
dead-reckoning experiment. The TPS measurements are fused
with IMU measurements using Holistic Fusion (HF) , simi-
lar to our ground truth pose estimate explained in Appendix B.
During a TPS measurement dropout, IMU dead-reckoning
provides insight into the performance difference based on the
noise characteristics. The results are shown in Figure 6. TPS
measurements are unavailable between 4.5 s to 9 s as well as
from 17 s to 22 s. As seen in Figure 6, the tactile-grade HG4930
performs the best, resulting in a smooth alignment with the
TPS measurement after the dropout period. On the other hand,
consumer-grade IMUs diverge after a short period (around 5 s)
of dead-reckoning time. For a fair comparison, Holistic Fusion
was provided with the noise characteristics specied in the
IMU datasheet.
G. State Estimation - Ablations
The importance of accurate time synchronization and extrin-
sic sensor calibration has been highlighted by the community
as discussed in Section I and Section II. Despite this common
calibration accuracy are often vaguely stated. Therefore, we
provide an ablation study on how different time and extrinsic
calibration offsets affect performance.
1) Perturbation on Time Synchronization: Similar to Sec-
tion IV-F, we deployed DLIO fusing Hesai and HG4930
measurements. We added different constant time delays to
the IMU measurements and provided APE and RPE metrics
in Figure 7. A time offset less than 5 ms negligibly impacts
performance. We attribute this to the fact that the HG4930
Mean  0.5x Std Dev
Time Oset Value [ms]
Mean  0.5x Std Dev
Fig. 7: Time Offset - Performance DLIO (Hesai  HG4930) in APE
and RPE with offset applied to the IMU measurements.
Mean  0.5x Std Dev
Translation Oset Value [mm]
Mean  0.5x Std Dev
Fig. 8: Extrinsic Translation Offset - Performance DLIO (Hesai
HG4930) in APE and RPE with an offset applied to the positive X-
axis of the extrinsic calibration.
IMU operates at 100 Hz, with a respective measurement period
of 10 ms, which is larger than the time offset. We validated
the accuracy of the IMU time synchronization in Section V-D2
and further found that different IMUs have varying offsets,
which can be potentially attributed to internal ltering.
2) Perturbation on Extrinsic Calibration: Another funda-
mental property of a well-designed sensor payload is the avail-
ability of reliable and accurate extrinsic calibration between
sensors. Similar to Section IV-G1, we use DLIO and perturb
the extrinsic calibration in translation and rotation and provide
the results in Figure 8 and Figure 9, respectively.
Below 5 mm translation offset, DLIO performance is
minimally impacted (Figure 8). Moreover, as the offset
from this systematic error, and the LiDAR registration cannot
converge. DLIO does not perform online extrinsic calibration.
The translation offset might be negated by the point cloud
registration process employed after the IMU state propagation.
The rotational component of the extrinsic calibration is more
sensitive to perturbations, and errors as small as 2cause the
estimates to diverge (Figure 9). This observation aligns with
the fact that the Hesai is a long-range LiDAR, and the far-
away measurements are affected more by the rotational offset.
propagate the state; this geometric observer has convergence
characteristics in rotation and subsequently in translation,
which explains the robustness against the perturbations.
Mean  0.5x Std Dev
Rotation Oset Value [deg]
Mean  0.5x Std Dev
Fig. 9: Extrinsic Rotational Offset - Performance DLIO (Hesai
HG4930) in APE and RPE with an offset applied to the positive pitch
angle of the extrinsic calibration.
Alphasense
Fig. 10: Image Quality Comparison. The HDR camera excels in dark
and low-light conditions. The ZED2i misses details in challenging
scenarios (Row 1: Tree, Row 2: Sky). The CoreResearch struggles
with overexposure and misses crucial details that are essential for
localization.
3) Summary: Using Boxi, we demonstrated that performance
across modalities varies among the tested environments. Naively
replacing images recorded from a global shutter camera with
rolling shutter images results in a signicant performance drop.
to a lower-accuracy short-range LiDAR leads to a signicant
decrease in performance. While using a tactile-grade IMU does
not directly lead to superior state estimation for DLIO, in the
dead reckoning scenarios, the lower noise tactile-grade IMU
can temporarily mitigate drift effectively. Our ndings show
that DLIO is robust to small time-delay errors (up to 2 ms) and
large translation offsets. However, accurate rotational extrinsic
calibration remains critical to ensure precise state estimation.
H. Perception - Mapping
To showcase that Boxi is not only a SLAM-ready payload
but also an autonomy-ready payload, we integrated on-board
mapping and traversability analysis. Namely, we integrated a
previously established elevation mapping  framework that
combines visual and geometric information (see Figure 11-
d). This framework relies on accurate timestamps and precise
extrinsic calibration to associate camera images with the map.
We choose HDR camera images to be projected onto the
Fig. 11: Example Deployment of Boxi: (a) TPS setup used for the ground truth pose generation. (b) Accurate intrinsic and extrinsic camera
calibration enables precise projection of RGB data onto the colorized elevation map. (c) Accumulated point cloud and trajectory visualization
based on the registration by the DLIO module. (d) The 2.5D elevation mapping shows the smoothness of the local geometry perception. (e)
Traversability analysis derived from the elevation map. (f) 3D Volumetric map generated using the Wavemap  module.
map for their superior image quality. While various works
have previously shown that different cameras yield varying
performance for state estimation, in Figure 10, we show the
inuence of different lighting conditions on image quality. It
is clear that the HDR camera consistently provides the best
visual image quality across all environments, which is crucial
for effective mapping and scene understanding. Additionally,
we demonstrate how the elevation map can be utilized to assess
traversability risks for a legged robot (Figure 11-e).
We also integrated the 3D volumetric mapping framework,
sentation of the scene and is capable of handling overhanging
obstacles (Figure 11-f). The 3D representations and elevation
map can be used by the robot for downstream path planning.
I. Limitations - Algorithmic Performance
Even when evaluating methods over a large number of
hold across environments, motion characteristics, and different
systems. However, the above represents our best attempt to
quantitatively assess the sensitivity of different hardware design
decisions on state estimation performance.
We have tuned the parameters of DLIO to the best of
our knowledge to achieve competitive performance across all
datasets. However, for OKVIS2, we did not ne-tune the param-
eters for each camera setting. In addition, using a framework
that directly incorporates rolling shutter compensation could
offer deeper insights, as performance is signicantly affected
when rolling shutter effects are not accounted for. Additionally,
for many experiments, we do not measure effects in isolation
e.g., changing the shutter type alone is not feasible, as the
underlying imaging sensor differs between cameras. While
we ensured that resolution and FoV were comparable (e.g.,
between HDR and CoreResearch), other factors, such as dy-
namic range and sensor characteristics, still vary. While we
can provide accurate extrinsic and intrinsic calibration for the
camera and LiDARs, we found preliminary evidence for a time
synchronization offset between IMUs and other sensors within
the IMU periods. However, since our evaluation across multiple
datasets did not reveal a strong dependence of DLIO on time
are valid. Further time synchronization validations are available
in Section V-D2.
V. COOKBOOK TO A SENSOR PAYLOAD
In this section, we provide a generalizable cookbook to
design a sensor payload, covering the most important require-
ments. We explain at rst general requirements (Section V-A),
followed by the component selection (Section V-B), mechanical
design (Section V-C), electronics and communication (Sec-
tion V-D), calibration (Section V-E), software (Section V-F),
and our lessons learned (Section VI).
A. Boxi Payload Requirements
We required Boxi to rely only on an external power source
and provide a communication line for simple and seamless
integration into different platforms without requiring substantial
adaptations or modications. In addition to being compact, the
system should be exible and expandable, allowing for the
retrotting of additional sensing modalities. A rigid and stiff
chassis is essential for protecting all components as well as
reliable extrinsic calibration, which must remain consistent over
time. For Boxi, the observability of the kinematic chain between
the robot and the sensor payload is required, therefore rendering
hardware low-pass ltering, such as vibration dampeners, not
suitable.
The payload should incorporate mature, proven, and
production-ready sensors, which have already been tested in
real-world scenarios. To enable on-board autonomy, the Boxi
has to offer sufcient computational power. This includes
adequate CPU and GPU capabilities as well as enough band-
hour datasets. In accordance with the sensor and compute
should not exceed 300 W to guarantee long-term deployment
with limited battery capacity. Similarly, the weight of the
payload is not allowed to exceed 10 kg to allow for hand-held
and general-purpose legged robot operation.
All components must be synchronized and intrinsically cali-
brated up to industry standard accuracy. Each sensor must have
an appropriate FoV to enable autonomous navigation, as this
is the primary task of the payload, and the FoV between the
sensors should overlap adequately. Finally, the system should
be designed to be rugged. It must handle occasional falls and
impacts while operating in all-weather conditions, and must
be dust-proof and waterproof to the IP65 standard.
B. Component Selection
1) Cameras: To select the best vision suite, we considered
Sensor  Lens, Interfacing  Control, and Software. Despite
many important attributes such as resolution, frame rate,
and FoV being typically taken into consideration, two
equally important aspects are often neglected: lens distortion
characteristics and depth of eld. The former leads to a
distorted version of an image compared to a pin-hole camera
is in focus. The desired focus range depends on the application
and can range from centimeter-scale for inspection tasks to
10 m-100 m for autonomous driving.
The camera sensor and shutter mechanism (global or rolling)
determine the image quality, as well as the sensors sensitivity,
sensed spectrum, and dynamic range. Although a rolling shutter
is more cost-effective, it can introduce additional complexity on
the software processing side for certain types of environments
and motion characteristics.
The camera interface determines how the captured images
are transmitted to the host PC, with common options being
and Ethernet. Each connection type has unique benets or
or increased compute requirements on the host PC. Among
the available interfaces, GMSL2 is currently the emerging
industry standard for automotive driving cameras. However,
integrating GMSL2 may require additional effort compared to
simpler options such as Ethernet and USB, as GMSL2 requires
additional deserialization hardware.
synchronization capabilities and whether it supports hardware
triggering. For advanced applications, synchronizing the camera
exposure with the timestamp of other modalities is desirable.
In multi-camera setups, particularly for stereo depth estimation,
synchronized exposure and triggering may be required. Lastly,
the availability of software drivers (e.g., in ROS1, ROS2,
availability of in-built white balancing and color correction
features can save signicant development effort.
selected
industry-standard
range. The three selected camera systems are detailed in
Appendices C to E.
NUC-1340PD5
Orin 32GB
Rogue Carrier Board
TierIV C1
Hesai XT32
Raspberry PI
Compute Module 4 32GB
Analog Devices
ADIS16475-2
Robotic System
Alphasense
Core Research
StereoLabs
Livox MID360
UbiSwitch Module
Totalstation
Fig. 12: Communication Overview: The camera sensors are directly connected to the processing compute units to reduce network load.
Camera - Recipe
Interface (GMSL, Ethernet, USB, ...)
Camera Sensor (Size, FPS, Shutter, Dynamic Range, ...)
Lens (Distortion, FoV, ...)
Timing (Synchronization, Triggering, Latency)
Driver Support  Host Device Requirements
2) LiDARs: There exists a variety of LiDAR types, each
offering trade-offs in range, accuracy, intensity sensing, multi-
return features, wavelength, scan pattern, FoV, point density,
laser disparity, beam divergence, noise characteristics, and
cost. Most of these trade-offs are straightforward; however,
we would like to highlight laser disparity as a key factor,
where higher disparity may be desirable for navigation
while low disparity is favorable for localization. Further,
time synchronization is especially crucial for LiDAR. Unlike
LiDARs effectively produce point estimates in time.
For Boxi, we chose spinning LiDARs over solid-state Li-
DARs as the latter typically have a smaller FoV, are less mature,
and are designed mainly for autonomous driving. Boxi primarily
targets legged robots operating at lower velocities but higher ac-
celerations and with omnidirectional movement, therefore bene-
ting from the extended FoV of spinning LiDARs. More details
about the two selected LiDARs are available in Appendix F.
LiDAR - Recipe
Type (Spinning, Solid State...)
Characteristics (FoV, Accuracy, Rate, Distance, Pointss )
Timing (Synchronization, Triggering, Latency)
Driver Support  Host Device Requirements
3) IMUs: IMUs can be classied into different categories:
their pricing and stability. Most IMUs used in mobile robots
are consumer-grade IMUs and are based on MEMS technology.
Key performance metrics include gyroscope and accelerom-
eter bias instability, hysteresis, temperature drift, aging effects,
as well as the random walk characteristics of angular and linear
velocity noise. However, datasheet specications often do not
reect the actual conditions of the target use case. As a result,
other error sources, such as temperature dependence or specic
surement errors in real-world applications. When choosing an
input range, specically the maximum acceleration and angular
for IMU is typically crucial for the motion characteristics of
the target application. Generally, for all IMUs, it is crucial
to ensure that the measurement aggregation conguration and
point-of-percussion  correction is available to the user.
Different IMUs require different communication protocols
(Serial, SPI, I2C, CAN, Ethernet), which, in combination
with time synchronization and setting up on-device ltering
selecting an IMU, we recommend considering software support,
bias instability, aging effects, temperature sensitivity, and the
availability of low-level drivers provided by the manufacturers,
which can signicantly reduce integration efforts. More details
of all selected IMUs are provided in Appendix G.
IMU - Recipe
Time synchronization requirements
Gyroscope and Accelerometer instability
Maximum Acceleration and Angular Velocity Rating
Software Support - Reducing Integration Effort
4) GNSS: Global Navigation Satellite System (GNSS) pro-
vides drift-free position estimation, making it indispensable
for global navigation. Typical position accuracy ranging from
1 m to 10 m. Many receivers support external atmospheric
correction data to compute a Real-Time Kinematic (RTK) so-
approximately 1 cm in the best case. GNSS receivers typically
support single-antenna or dual-antenna setup; the latter not
only yields improved positional accuracy, but also provides
heading estimation. The selection of antennas and receiver
capabilities ultimately dictates which satellite constellations
can be used. Moreover, the considerations of electromagnetic
interference (EMI) turned out to be critical when selecting an
antenna placement, as noted in Section V-D1. The atmospheric
correction data can be received from one or more nearby base
stations via the online GNSS correction providers or a radio
link from a local base station. In environments where real-
time communication to the base station is not possible, Precise
Point Positioning (PPP) solutions can be employed. With PPP,
correction data is received through single-path communication
via satellite over a large geographic area, resulting in an
X in [m]
Y in [m]
X in [m]
Y in [m]
Alphasense
X in [m]
Z in [m]
X in [m]
Z in [m]
Alphasense
Fig. 13: Visualization of LiDAR and Camera FoV: (Far Left) Top-down view of HDR FoV projected on at ground plane. The black outline
represents the rectied FoV; The distorted FoV overlaps while the undistorted one does not overlap. (Left Center) Top-down view of the
CoreResearch camera FoV projected on the ground plane. Overlap in both distorted and undistorted FoV. (Right Center) Side view of FoV
for CoreResearch, HDR, and ZED2i cameras, with ZED2i tilted downward at a 15angle and rotated 180in its image axis for better space
management within Boxi. (Far Right) FoV of Hesai and Livox LiDARs.
accuracy up to 2.5 cm. More details about our GNSS receiver
and postprocessing software are available in Appendix H.
GNSS - Recipe
Dual or Single Receiver (Baseline)
Single or multi-band support
Online Corrections (None, RTK, PPP)
EMI (Antenna Placement, Receiver Placement)
Postprocessing Support (Correction data)
Synchronization against GNSS Time
Connectivity  Driver Support  Host Device Requirements
5) Network Switch:
Reliable communication between
Ethernet-enabled sensors and computers requires a switch or
a managedunmanaged router. Key considerations include the
number of ports, port speeds, form factor, cost, and switching
capacity. One desirable feature is the hardware support for PTP
time synchronization (IEEE 1588v2), as using a non-compliant
switch can increase time synchronization errors. More details
about the selected switch are provided in Appendix I.
Switch - Recipe
Management (active or passive)
Switching Capacity, Speed, Size
PTP-enabled
6) Compute: To establish a versatile research platform
capable of deploying various algorithms, we require both a
CUDA-accelerated NVIDIA GPU and an x86 CPU architecture.
Due to the large amount of raw sensory data, two computers
are required to perform the required raw data processing and
allow for online image compression (see Section V-F). This, in
addition to the demand for accurate time-stamping and other
a NUC, Jetson, and Raspberry Pi (see Figure 12). More details
for each selected module are provided in Appendix J.
Compute - Recipe
Workload Analysis (CPU, GPU)
Reduce Compute Units to Minimum
Hardware Acceleration (NVEC H.265, GPU)
Connectivity (Ethernet, USB-X, SPI, GMSL, PCI-E ..)
Time synchronization capabilities (PPS, PTP)
C. Mechanical
The entire payload has been designed with a strong focus on
positioning accuracy and stiffness while minimizing weight.
1) Component Placement: In general, the component place-
ment requires balancing multiple, often conicting, objectives.
The most important criterion is the safety of each component,
specically the LiDAR sensors and the GNSS antennas, which
require a rollover cage. Apart from thermal, dust-proong,
and water-proong considerations, the FoV for each sensor is
essential for the placement of the exteroceptive components.
During autonomous operation, each sensor should capture
a comprehensive but overlapping view of the environment,
which is also key for calibration and multi-modal algorithms.
For example, sufcient overlap between LiDARs and cameras
allows calibration of the extrinsic parameters using inexpensive
targets rather than requiring a dedicated calibration room. Fig-
ure 13 illustrates the overlapping FoV between all cameras and
LiDARs. When mounted on an ANYmal robot with a standing
height of approximately 0.8 m, the cameras capture the ground
in front of the feet of the robot starting at a distance of 0.75 m.
This setup effectively balances peripheral coverage and ground
visibility for local navigation. The front-facing CoreResearch
stereo pair has a baseline of 11 cm, which results in accurate
depth estimates up to a distance of 20 m and enables a direct
comparison to the ZED2i with rolling shutter.
The Livox is mounted upside down, allowing for accurate es-
timation of the terrain directly around the robot, which is crucial
for 3D near-eld mapping. The Hesai is mounted horizontally
on top of the camera bundle for optimal FoV for localization.
In addition, the primary GNSS antenna is rigidly connected
to the lid through standoffs and a copper-shielded element.
Our empirical ndings showed that the EMI resulting from the
LiDARs interferes with the GNSS signal (see Section V-D3).
Component Placement - Recipe
Interference Analysis
Field of View Analysis
Protection  Ease of Calibration
2) Tolerances and Assembly: To meet the 10 kg weight
requirement while maintaining structural integrity, Boxi (base
and lid) is machined from a single AL-6061 T4 aluminum
mono-block with a minimum wall thickness of 2 mm. This
material offers a favorable strength-to-weight ratio and enables
in-house manufacturing.
All parts follow ISO 2768-1 with a ne tolerance class
(0.05 mm). A full 3D CAD model was created to account for
connector spacing and cabling constraints. Positioning pins are
used at all interfaces, including between the lid and body and
for all sensor mounts, to minimize offsets due to screw backlash
and machining tolerances. A protective cage is mounted on
the lid to house the Livox unit and prism while shielding the
LiDARs from impact.
Tolerances and Assembly - Recipe
Material (Weight, Stiffness, Number of Interfaces)
Thermal behavior of the Material
Manufacturing (Cost, Precision, Quantity)
Positioning Pins, Thread-locking Adhesives, Torque
Consider Assembly (Plugging in Connectors)
3) Thermal Considerations: The primary thermal load
within Boxi originates from the two main compute modules
and the DC-DC converter. To balance cooling performance,
based cooling strategy. All heat-generating components were
strategically placed at the rear, while cameras and LiDAR
sensors were placed at the front of Boxi to limit the adverse
effects of thermal expansion. To improve thermal management,
cooling ns were integrated beneath the two compute modules,
complemented by a single radial fan to actively circulate air.
To verify the efcacy of the cooling system and better un-
derstand the impact of temperature changes on the deformation
of Boxi and, in turn, the effect on extrinsic calibration, we con-
ducted a thermal Finite Element Method (FEM) simulation un-
der maximum thermal load. The maximum heat generation from
all signicant sources adds up to 92 W dissipation. The FEM
simulation results shown in, Figure 14, indicate a low overall de-
formation in the sensor area. Under maximum load, the relative
rotation between the sensors remains within 0.004, which is
less than the machine tolerance and the accuracy of the calibra-
tion. The simulation validates our thermal design and shows that
aluminum AL6061-T4 is sufcient, even though it has double
the thermal expansion coefcient of regular stainless steel .
We also veried the thermal design of Boxi under normal load
(recording, see. Section V-F) with an ambient temperature of
19 C. We monitored the maximum temperature of the housing
using a Fluke Thermal Camera and measured the internal CPU
PJetson40W
Fig. 14: (a) Thermal deformation from heat generated by the compute
modules. The deformation is amplied and visualized for clarity, with
the mounting points assumed to remain at a constant temperature. (b-
d) Thermal imaging of the box during recording.
temperature of the NUC at 55 C and the Jetson at 51 C.
The entire housing remained well below 30 C, while the fan
operated at 30  speed. The CPT7 with 43 C and all cameras
and LiDARs with a temperature below 40 C remained well
within the operation specications.
Thermal Considerations - Recipe
Heat Sources (Compute Units, Sensors)
Ambient Conditions (Temp. Range, Humidity Levels)
Thermal FEM Simulation (Angular and Linear Deformation)
4) Advanced Topics: As Boxi is intended to be mounted on
a legged robotic platform, we also considered topics such as
these topics are provided in Appendix O.
5) Recommendations and Reections: The dust- and water-
proof design proved to be highly effective, particularly with the
ns strategically placed on the exterior. Integrating an off-the-
shelf modular cable pass-through was both simple to integrate
and offered exibility. When designing rollover cages, it is
crucial to always assume the worst-case scenario and identify
potential failure points under various conditions. While the
compact form-factor fan was desirable, it generates substantial
noise at high RPMs.
We initially used the manufacturers camera FoV to design
the housing. However, this specication was applied to rectied
of the image because of the sheye effect. Manufacturing
the base body from a single block of aluminum signicantly
improved stiffness, precision, and thermal properties while sim-
plifying the design. However, this approach may be impractical
and prohibitively expensive for commercial applications.
D. Electronics and Communication
1) Power: Boxi supports a wide input voltage range from
18 V to 60 V for compatibility across platforms. Power is dis-
tributed via a custom PCB integrating two step-down converters:
a 12 V, 300 W (Traco TEQ 300-4812WIR) and a 5 V, 15 W.
To mitigate EMI and ground loop issues, we used ground-
isolated converters, added input lter coils, and implemented
a star ground layout. Distinct connector types were chosen
for different power levels to prevent misconnection and ensure
electrical safety. We recommend analyzing required voltage
levels and power consumption, considering the inrush currents
of all components, and hot-plug battery exchange if desired.
Power - Recipe
Power Analysis (Voltage Levels, Power, Inrush Current)
Distribution (Cabling, Polarized Connectors)
Best practices to prevent EMI
2) Communication and Synchronization: Extending on the
basic knowledge on communication and time synchroniza-
tion provided in Appendix K, components that operate on
independent clocks require active synchronization to mitigate
drift. Typical consumer-grade crystal oscillators drift at a
rate of 50 ppm (parts per million), leading to 29.9 ms drift
per 10 min. Two common synchronization protocols are PTP
(e.g., ptp4l, phc2sys) and NTP (e.g., chrony). PTP is
generally preferred when reliable network connections are
mentioned before, the GNSS module CPT7 is used as a clock
reference and PTP Grandmaster clock. Since the switch in
Boxi does not support hardware PTP synchronization (IEEE
1588v2; see Section V-B5), we conducted a 43 h continuous,
time synchronization drift analysis, showcasing high-quality
time-synchronization over the entire period. More details and
results on this experiment can be found in Appendix L.
a) Synchronization - Camera and LiDAR: The time
synchronization of both LiDARs has been established with the
IEEE 1588v2 (2008) standard PTP protocol, which indicates
PTP clock accuracy 1 s and PTP clock drift 1 s s1.
Both LiDARs provide a per-point timestamp for accurate
integration to downstream tasks. Different from the LiDARs,
cameras require an exposure time of around 10 ms for normal
lighting conditions. Ideally, the timestamp should correspond to
the midpoint of the exposure for each pixel. In the case of our
rolling shutter HDR cameras, one needs to calculate the expo-
sure time for every i-th horizontal line of the image separately:
rst. Similarly to the LiDAR case, compensating for this
different trigger and readout time improves state estimation
and requires precise synchronization . Manufacturer data
is not available for the ZED2i, regarding exposure time or
global shutter compensation.
b) Synchronization - IMUs: All 7 IMUs available within
Boxi are time synchronized to a varying level of precision. The
Raspberry Pi Compute Module 4 handles the time synchro-
nization ADIS and STIM320 using hardware timestamping via
custom-written kernel modules. The other IMUs are synchro-
nized by the manufacturer-provided drivers. More information
about the IMU time synchronization details can be found in
the Appendix M.
c) Synchronization - Verication: When it comes to time
of the claimed accuracy. While tools such as ptp4l provide
the estimated time tracking offset  (see Appendix L), this
is harder to achieve for sensors synchronized with other means.
Since we have IMUs connected to each PC, the time syn-
chronization accuracy can be validated by aligning the angular
velocities of the IMUs over time, similar to the approaches
in [23, 31]. We have developed a standalone, open-source tool
for verifying time synchronization between two IMU time series
measurements at arbitrary rates. We ran the verication tool
across all recorded missions to obtain a condence interval per
mission using the HG4930 as a reference IMU. We applied the
tool to three 30 s snippets per axis for each mission. The results
are shown in Table VI and indicate that the offset between all
IMUs can be estimated up to a standard deviation of around
0.5 ms and is constantly below 4.5 ms, which can be attributed
to the exposure time of the IMU and internal time delays. The
details of the time synchronization validation tool and procedure
are provided in Appendix N. For verication of the camera
AP20-IMU
TABLE VI: Evaluation of the IMU Software Alignment Tool across
dataset. We provide alignment results for each axis, along with the
mean and standard deviation.
time synchronization, there is the option to hardware trigger an
external LED; however, this procedure is time-intensive and out
of the scope of this work . Lastly, all IMUs, and LiDARs
operate in free-running mode. It was sufcient for our use
case to only trigger cameras of the same type simultaneously.
Synchronization - Recipe
Bandwidth and Latency
Precision Time Protocol (PTP) (Preferred)
Network Time Protocol (NTP) (Unreliable Network)
Hardware Timestamping (Linux Kernel Modules)
Verication Tools
3) Electro Magnetic Interface (EMI): EMI mitigation in
Boxi focused on minimizing both emissions and internal in-
terference. Analog signal integrity was prioritized by placing
sensitive components, such as the GNSS receiver, outside the
enclosure and using additional EMI absorbers. Testing showed
that maximizing the distance between the GNSS antennas and
the main housing, avoiding direct contact with the metal struc-
interference. Inside the enclosure, all signal lines use shielded
cables where possible. An exception is the 1 Gbps Ethernet
Two openings in the aluminum housing on the side panels are
tted with 3D-printed plastic parts, enabling the mounting of
Wi-Fi antennas within Boxi. This design represents a trade-
off between compact integration and EMI mitigation. As an
EMI - Recipe
EMI Emission Requirements
Signal Integrity (GNSS Sensitive)
E. Calibration
all sensors, covering the calibration methods, procedures, and
verication techniques. These calibrations comprise the intrinsic
and extrinsic calibration of all cameras with respect to each
other (the bundle of all cameras), the extrinsic calibration of
each LiDAR, jointly, to all CoreResearch cameras, the extrinsic
calibration of each IMU to the front-facing global shutter cam-
bundle. Additionally, the IMU drift parameters are calibrated.
Alphasense
Kalibr (ours)
TotalRecon
Camera Bundle
LiDAR Cal
Fig. 15: An overview of the calibrations done. The arrows indicate
the transforms that are explicitly calibrated.
Given the large number of available sensors  2 LiDARS,
10 cameras, 7 IMUs, and a reector prism  we carefully
designed the calibration method, setup, and procedure.
1) Calibration Chain: Central to the calibration of our sys-
tem are the intrinsic and extrinsic calibrations of the 10 cameras.
Since cameras cover a wide FoV, sufcient overlap is available
to robustly calibrate the extrinsic parameters by detecting a
calibration target placed in various poses. The camera bundle
calibration forms the foundation for calibrating all other sensors
in the subsequent steps of the calibration chain Figure 15.
2) Camera Intrinsic and Extrinsic: For camera intrinsic and
extrinsic calibration, we re-implemented Kalibr [26, 56] to
allow for online calibration and feedback during the recording
session. The calibration can be decomposed into an intrinsic and
extrinsic phase. During the intrinsic phase, the operator moves
an AprilGrid  in front of all cameras until sufcient samples
are collected (see Appendix P1). Then, in the extrinsic phase,
it is required to statically place the target at different locations
around the Boxi or move the payload itself until sufcient
static samples are collected to avoid time synchronization
Fig. 16: LiDAR reconstruction of the calibration target using camera-
detected poses and the optimized extrinsic calibration. The recon-
structed corner locations are sub-millimeter accurate.
errors and motion blur (see Appendix P1). We periodically
estimate the covariance of the intrinsic and extrinsic parameters
in real-time to provide feedback on the sufciency of the
data collected. We used the Kannala-Brandt  equidistant
distortion model for the wide-angle sheye CoreResearch [53,
64] and HDR cameras, and the rad-tan distortion model for
the ZED2i. More details on why we chose the AprilGrid can
be found in Appendix P2. Similarly, details on reprojection
errors across all cameras are provided in Appendix P3
In Table VII, we show the numerical evaluation of the
estimates of the extrinsic and intrinsic parameters, which are
also presented live to the user during the calibration procedure.
The standard deviation of the translation and rotation for
all cameras is excellent. The total intrinsic and extrinsic
calibration procedure takes around 5 min compared to the
default Kalibr workow of multiple hours.
3) Camera to IMU: Before calibrating the camera-IMU ex-
trinsic parameters, we collect static data of the IMUs over 18 h
to model the Allan variances of each of the IMUs using . We
then perform camera-IMU extrinsic calibration using the front-
facing global shutter CoreResearch cameras using Kalibr .
To collect camera data capable of capturing the dynamic motion
required for the camera-IMU calibration accurately, we set the
camera frame rate to 17 Hz, with an exposure time of 1 ms.
With this exposure time, we also employ uniform articial
lighting of the static calibration target to aid in the detection
of the calibration target. More details about the motions and
individual IMUs can be found in the Appendix Q
4) LiDAR to Camera Extrinsic: To efciently perform the
LiDAR to camera extrinsic calibration, we used a variant of the
intensity alignment extrinsic calibration DiffCal . Unlike
plane matching, which requires many samples spanning differ-
ent orientations, this intensity-matching method can recover
the calibration using a single sample. Additionally, intensity
matching allows one to incorporate partial observations of the
calibration target in the LiDAR point cloud, which is typical
for our Hesai LiDAR with a narrow FoV. Allowing for partial
observations enables the collection of calibration data closer
to the cameras, where the detected poses are more accurate.
For this calibration, we used a different calibration target
formed of a checkerboard pattern. Each LiDAR is individually
calibrated against the 5 CoreResearch cameras. Using the
Translation (m)
Rotation ()
Error (px)
CoreResearch-Front-Center
CoreResearch-Front-Right
CoreResearch-Front-Left
CoreResearch-Left
CoreResearch-Right
HDR-Front
HDR-Left
HDR-Right
ZED2i-Left
ZED2i-Right
TABLE VII: Typical results from a full camera-camera intrinsic and extrinsic calibration. The spatial transforms are with respect to
ZED2i camera during calibration, yet the result is precisely consistent with the stereo baseline of the camera.
Fig. 17: Example calibration data samples used for the LiDAR to
camera calibration. Hesai LiDAR points are overlayed on the image,
and colored by each points intensity channel. The calibration target
reects the LiDAR beams with intensity contrast between the black
and white squares seen in the camera image.
calibrated extrinsic parameters between the cameras, the
detected calibration target poses from each camera are fused and
jointly calibrated with respect to each LiDAR, without rening
the intra-camera extrinsic parameters. More details on the cali-
bration target and data collection can be found in Appendix R.
Figure 16 shows a dense, reconstructed point cloud of
the calibration target, colored by the intensity channel. This
reconstruction has the coordinate system of the calibration
target as its origin. From this dense reconstruction, we use a
line-based corner detection algorithm (similar to ) to detect
the locations of the checkerboard corners in the reconstruction.
Figure 17 shows the sub-millimeter consistency between the
detected corners and their true positions on the canonical
calibration target. In Figure 17, we show example overlays of
the LiDAR points on the calibration target, after optimization.
Another example of camera-LiDAR overlay during rapid
motion is provided in Appendix R1.
5) Camera to Prism: Due to the lack of existing methods,
we developed a custom calibration procedure, TotalRecon, to
calibrate the front-facing CoreResearch cameras to the total
station prism. Using TPS measurements and camera detections
of a static calibration target, we solve for a 9-DoF state: the
prism position relative to the camera, and the 6-DoF pose of
the target with respect to the total station. To ensure accurate
target detection, the camera is positioned within 1 m of the
target. The total station tracks the prism while Boxi is held static
at various poses around the target using a tripod. Sufcient
samples are collected to excite all translational and rotational
degrees of freedom. We validate the calibration by checking the
consistency of prism estimates across all cameras, transforming
them to the front-center camera frame using known extrinsics.
The routine yields 3 mm cross-camera consistency and achieves
accuracy below the mechanical mounting tolerance.
Calibration - Recipe
Calibration Requirements and Chains
Available Calibration Methods (CAD, Tools)
Calibration Targets and Procedure Requirements
Calibration Intervals
Change of Calibration (Time, Temperature)
Calibration Verication
F. Software
The hardware and software design of Boxi were co-
developed. To ensure ease of use for researchers, the software
is based on Ubuntu 20.04 LTS with ROS1 Noetic and ROS2
the Raspberry Pi Compute Module 4 to enable IIO support
and accurate kernel timestamping. For all ROS1 components,
we ran a single roscore instance per PC and enabled inter-
PC communication using the fkie multi-master package ,
which utilizes gRPC under the hood.
1) Recording: In line with best practices established in
previous research [28, 53], we only run the required sensor
drivers and prioritize the recording of raw driver data to mini-
mize artifacts, minimize processing time during recording, and
preserve future processing exibility. As an example, the UDP
packets are recorded for Hesai LiDAR instead of point clouds,
and the serialized SVO2 les are recorded for the ZED2i RGB-
D camera to maintain maximum delity. We implemented a dis-
tributed recording system, which stores data on the Jetson, NUC,
and CPT7, minimizing network bandwidth and distributing load
across machines. Specic details about image compression and
recording details can be found in Appendix R1. In summary,
recording for 10 minutes results in 14.4 GB HDR, 5.77 GB
and >100 MB for all IMU and other auxiliary status, diagnostic
During this recording, the Jetson exhibited higher CPU usage
(72.0) compared to the NUC (49.2), while only the Jetson
showed GPU activity (7.0). The NUC utilized the network
at 58.2 Mbps, whereas the Jetson showed no network activity.
at 18.7 Mbps, compared to 13.5 Mbps on the NUC.
Software - Recipe
Co-development of Hardware and Software
Testing of all Software on Prototype (Desk Setup)
Ensuring Driver and Hardware Compatibility
Adaptable and Modular Software Design
Hardware Acceleration (Video Compression, GPU)
Simplicity  Maintainability  Documentation
G. Limitations - Boxi Design
Starting with the mechanical design, Boxi lacks a robust
rollover cage to protect the system under the full load of a
legged robot tipping over. While the lid provides sufcient
stiffness for mounting, it cannot absorb the energy generated
during falls, especially under the robots full weight.
To address this issue, the development of a newly designed
protective rollover cage is essential. The cage should be fully de-
coupled from Boxi and directly mounted onto the robot, offering
enhanced protection and durability. During our thermal simula-
units. However, the heat generated by individual sensors is also
non-neglectable and currently not taken into consideration.
While we can measure highly accurate time synchronization
via PTP offsets, starting the development with more verication
tools in mind would have helped. The developed IMU align-
ment tool conrmed good time synchronization. Further, we
did not implement verication, such as active LED targets, to
validate the manufacturers rolling shutter effect compensation.
The decision to use the Jetson Orin with the compact
ConnectTech carrier board was motivated by its smaller form
factor. However, the absence of real-time interrupt pins resulted
in a Raspberry Pi being required to manage the STIM320 and
ADIS IMUs. This introduced further complexities, including
a custom Raspberry Pi PCB design and PTP synchronization.
In retrospect, avoiding the integration of IMUs such as the
ADIS and STIM320 due to their limited ROS1ROS2 driver
supportwould have signicantly reduced integration efforts
and further improved the maintainability of the software stack.
Comprehensive environmental testing, including shock and
vibration resistance assessments, has yet to be performed.
Although extrinsic and intrinsic calibrations have demonstrated
stability over time, broader environmental validation remains
a critical area for future work.
VI. LESSONS LEARNED AND DISCUSSIONS
1. Hardware and software should be jointly developed:
The interdependence of software and hardware turned out to be
the most difcult part to get right. An exhaustive analysis of
software and hardware requirements and the interdependence
between components is essential.
2. Select components for the application: While technical
specications in datasets are important, having access to good
software support and easy integration can be more important
and justify additional cost, weight, size, or worse specications.
synchronization
targeted
based on the application and the software. State-estimation and
SLAM tasks might require s or ms level time synchronization
between LiDAR and IMUs.
4. Develop software before hardware: We recommend
implementing the software early in the design process to
identify any compatibility issues.
5. Develop verication tools for everything: How can
you verify that the software functions as intended? The
development of tools to ensure time synchronization and data
integration is essential.
6. Perform rapid-prototyping during development: We
recommend building a cheap mock-up setup and 3D-printed
prototypes before moving to advanced CNC milling.
7. Calibration targeted for you application: Dene your
calibration requirements, and which tools and setups are
required to achieve it. An overlapping FoV, use of cheap
desirable for us.
8. Keep it simple: What is better than a ptp-enabled switch?
Using no switch. We recommend using as few compute
implement custom PCBs if they signicantly simplify the
design. The same applies to the software: e.g. integrating
ROS1 and ROS2 inherently increased the complexity.
VII. CONCLUSION
In conclusion, Boxi has proven to be an invaluable tool,
enabling the collection of the GrandTour dataset with high-
quality ground truth reference data. Through the algorithmic
performance analysis (Section IV), we demonstrated that sensor
payload design should consider the target application, software
Out of the selected components, NovAtel SPAN CPT7 and
Leica Geosystems components (custom AP20 and MS60) stood
out as vital components, due to their excellent software support
and performance, despite their cost. The design of the power
distribution board also contributed to a clean assembly and
minimized EMI problems, with the placement of the GNSS
antenna being the only challenge we encountered.
set of sensors, which will include the two LiDARs, three
HDR cameras, and a single updated next-generation NVIDIA
Jetson processor, alongside a PTP-enabled switch for improved
synchronization. These renements will further enhance Boxis
for future research and applications in robotics. Lastly, a
drawback of Boxi is the lack of auxiliary modalities such as
the usefulness and effectiveness of the sensor payload in various
environments.
REFERENCES
Cpt7 product sheet, 2024. URL
DownloadProdukteNovAtelSPANCPT7Datenblatt
DatasheetNovAtelCPT7.pdf. Accessed: 2025-01-21.
Inertial explorer, 2024. URL
WaypointContentInertialExplorerOverviewofIE.
htm. Accessed: 2025-01-21.
The Engineering ToolBox (2005). Metals - temperature
expansion
engineeringtoolbox.comthermal-expansion-metals-d
859.html.
Sameer Agarwal, Keir Mierle, and The Ceres Solver
Team. Ceres Solver, 10 2023. URL
ceres-solverceres-solver.
Michael Bloesch, Michael Burri, Hannes Sommer, Roland
recursive estimation for mobile robots. IEEE Robotics
and Automation Letters, 3(1):573580, 2017.
Leonardo Brizi, Emanuele Giacomini, Luca Di Gi-
rome. arXiv preprint arXiv:2404.11322, 2024.
A. Bry, C. Richter, and N. Roy. Spatial and temporal cali-
bration of multi-sensor systems with application to stereo
vision. In IEEE International Conference on Robotics
and Automation (ICRA), pages 38633870, 2015.
Russell Buchanan. Allan variance ros, November 2021.
Keenan Burnett, David J Yoon, Yuchen Wu, Andrew Z
A multi-season autonomous driving dataset. The Interna-
tional Journal of Robotics Research, 42(1-2):3342, 2023.
Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas
vehicle datasets. The International Journal of Robotics
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh
Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes:
A multimodal dataset for autonomous driving. In Pro-
ceedings of the IEEECVF conference on computer vision
and pattern recognition, pages 1162111631, 2020.
Alexander Carballo, Jacob Lambert, Abraham Monrroy,
David Wong, Patiphon Narksri, Yuki Kitsukawa, Eijiro
multiple 3d lidar dataset. In 2020 IEEE intelligent vehicles
symposium (IV), pages 10941101. IEEE, 2020.
Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang,
Peter Carr, Simon Lucey, Deva Ramanan, et al. Argo-
Proceedings of the IEEECVF conference on computer
vision and pattern recognition, pages 87488757, 2019.
Kenny Chen, Ryan Nemiroff, and Brett T Lopez. Direct
lidar-inertial odometry: Lightweight lio with continuous-
time motion correction.
In 2023 IEEE international
conference on robotics and automation (ICRA), pages
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Uwe Franke, Stefan Roth, and Bernt Schiele.
cityscapes dataset. In CVPR Workshop on the Future of
Datasets in Vision, volume 2, page 1, 2015.
Andrei Cramariuc, Aleksandar Petrov, Rohit Suri, Mayank
camera miscalibration detection. In 2020 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 49975003. IEEE, 2020.
Igor Cvisic, Ivan Markovic, and Ivan Petrovic. Enhanced
calibration of camera setups for high-performance visual
odometry. Robotics Auton. Syst., 155:104189, 2022. URL
Efe Daum, Maxime Vaidis, and Franois Pomerleau.
Benchmarking ground truth trajectories with robotic total
stations. arXiv preprint arXiv:2309.05134, 2023.
Timoth
