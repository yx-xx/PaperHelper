=== PDF文件: RLDG Robotic Generalist Policy Distillation via Reinforcement Learning.pdf ===
=== 时间: 2025-07-22 15:52:18.524578 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Reinforcement Learning
Charles Xu1,
Qiyang Li1
Jianlan Luo1,
Sergey Levine1
1University of California, Berkeley.  Project Leads
Use case 3: RL on Sub-task
Use case 2: Multiple RL Policies
Pretrained OpenVLA
"Insert the USB"
Use case 1: Single RL Policy
Pretrained Octo
"Insert the USB"
Precise and Contact Rich Insertion
Generalization to Unseen Scenarios
Composition for Long Horizon Tasks
Fig. 1: RLDG improves generalist robot policies like OpenVLA and Octo by training specialist RL policies and using them
to generate high-quality fine-tuning datasets. It has the flexibility to distill knowledge from multiple RL policies trained on
individual narrowly scoped tasks into a single generalist. It can also be applied to the most critical sub-task of a long-horizon
manipulation task, improving the success rate at the bottleneck while leveraging human demonstrations on parts of the task
where it suffices. The resulting fine-tuned generalist policies are capable of precise manipulation, generalization to unseen
AbstractRecent advances in robotic foundation models have
enabled the development of generalist policies that can adapt
to diverse tasks. While these models show impressive flexibility,
their performance heavily depends on the quality of their training
data. In this work, we propose Reinforcement Learning Distilled
Generalists (RLDG), a method that leverages reinforcement
learning to generate high-quality training data for fine-tuning
generalist policies. Through extensive real-world experiments
on precise manipulation tasks like connector insertion and
RL-generated data consistently outperform those trained with
human demonstrations, achieving up to 40 higher success rates
while generalizing better to new tasks. We also provide a detailed
analysis that reveals this performance gain stems from both
optimized action distributions and improved state coverage. Our
results suggest that combining task-specific RL with generalist
policy distillation offers a promising approach for developing
more capable and efficient robotic manipulation systems that
maintain the flexibility of foundation models while achieving the
performance of specialized controllers. Videos and code can be
found on our project website
I. INTRODUCTION
Recent advances in robotic foundation models have demon-
strated impressive capabilities in understanding and executing
diverse manipulation skills [7, 4, 3, 35, 16, 2, 38, 19, 5].
By leveraging Internet-scale pretraining and grounding with
robot actions, these models can achieve zero-shot and few-
shot generalization across various domains. Deploying these
models typically requires fine-tuning them with task-specific
data to adapt to the target task or domain. The quality of this
fine-tuning data is therefore critical to the performance of the
resulting policies. While human teleoperation is a common
and accessible source for such data, human demonstrations
often contain inconsistencies in execution quality and style.
These variations make it challenging for foundation models
to learn robust policies, as they must cope with imperfections
and inconsistencies inherent in human demonstrations. This
challenge affects all robotic tasks but becomes particularly
pronounced in scenarios requiring precise control and dex-
and consistency of demonstration data even more crucial for
effective policy learning.
To tackle this challenge, we propose Reinforcement Learn-
ing Distilled Generalist (RLDG), a simple yet effective method
that leverages reinforcement learning to generate high-quality
training data for robotic foundation models. While directly
fine-tuning foundation models with reinforcement learning is
in principle possible, it also presents practical challenges in
terms of optimization stability, computational costs, as well as
issues raised in value function pretraining .
generate high-quality trajectories through reward maximiza-
policies compared to human demonstrations. The approach is
cies using sample-efficient real-world RL frameworks [24, 26]
until convergence, then collect data from these policies to
fine-tune robotic foundation models. This procedure is simple
and flexible, offering several benefits. First, it provides an
automated approach to generate large amounts of high-quality
training data without requiring the effort of human teleop-
training is significantly more cost-effective than collecting hu-
man demonstrations. Second, by combining the optimization
capabilities of RL with the strong generalization of foundation
mance while generalizing to novel scenarios. Finally, RLDG
provides a valuable solution for complex multi-stage tasks by
using RL data to address the bottleneck step that hinders the
performance of the overall task.
Through extensive experiments across multiple manipula-
tion tasks with well-defined reward functions, we demonstrate
that generalist policies like OpenVLA  and Octo
achieve superior performance when finetuned with RL data
compared to human demonstrations, specifically for tasks
where RL can learn effective controllers. For precise manipu-
lation tasks such as tight-fitting connector insertions presented
in Fig. 3, RLDG achieves 30 higher success rates on
average. This performance gap widens further when evalu-
ating generalization: policies trained with RLDG demonstrate
significantly better transfer to novel scenarios, with on average
50 higher success rates. Notably, as we will show in Sec-
tion IV, achieving comparable performance to RLDG would
require 6-10x more human demonstrations. For complex tasks
such as precise insertion, RLDG can achieve perfect success
rates (100), while policies trained on human demonstrations
plateau at 90 even with significantly more data.
Our key contribution is RLDG, a simple yet effective
method that leverages reinforcement learning to generate high-
quality training data for fine-tuning pre-trained robotic foun-
dation models, providing an automated alternative to human
demonstrations. Through extensive experiments on robotic
manipulation tasks, we demonstrate that RLDG achieves 30-
50 higher success rates compared to conventional fine-tuning
with human demonstrations while requiring 6-10x less data.
with human demonstrations in multi-stage tasks, enabling
better overall performance by using RL data for critical phases
while maintaining the benefits of human demonstrations for
other phases.
Our results suggest a promising direction for robotic learn-
high-quality training data for foundation models. This synergy
enables more capable robotic systems that can both execute
skills precisely and generalize effectively to new scenarios
through natural language instructions while reducing reliance
on human demonstration data collection.
II. RELATED WORK
Our work finetunes robotic foundation models with data
generated by reinforcement learning policies, which could be
seen as an instance of policy distillation . By combining
these approaches, we develop a general technique for training
robust robotic policies that leverage both the performance of
RL policies and the flexibility of foundation models. Thus, we
survey related work across these three key areas and examine
their intersections.
Foundation models for robotics. Recent advances in vision-
language foundation models have enabled the development
of generalist robotic policies that can understand and execute
diverse tasks through natural language instructions. Such mod-
els [4, 3, 16, 35, 2, 38, 19, 5, 8] leverage large-scale pretraining
on Internet-scale vision-language data followed by finetun-
ing on robot demonstrations. While these approaches show
impressive generalization capabilities across a wide range of
with precise manipulation tasks that require careful alignment
and contact-rich interactions (see Section IV). This challenge
comes from the limitations in the demonstration-based learn-
ing approach  human demonstrations, while diverse and
contact-rich manipulation tasks. Thus, robotic systems that can
perform these challenging tasks more effectively than humans
are highly desirable. By definition, simply imitating human
behaviors would not achieve this goal. RLDG addresses this
limitation by complementing the semantic understanding of
foundation models with the robust behaviors learned through
reinforcement learning, enabling precise manipulation while
maintaining the flexibility and generalization capabilities of
foundation models.
Reinforcement learning for robotic manipulation. Rein-
forcement learning has been successfully applied to learn
complex robotic manipulation skills in the real world through
direct interaction with the environment [24, 26, 29, 18, 14, 15,
in learning challenging tasks like precision insertion [22, 39,
manipulation . A key advantage of RL is its ability to
discover optimal action distributions through trial-and-error
compared to pure imitation learning [23, 26, 24]. However,
to achieve broader generalization across different tasks and
environment instrumentation such as resetting the environment
to different initial conditions for each trial, while this is in
principle possible, it would quickly become impractical when
the number of tasks and situations grows. RLDG bridges this
gap by combining the strengths of both approaches, using
RL to learn optimal behaviors for specific challenging tasks,
then distilling these precise capabilities into foundation models
while preserving their broad generalization abilities.
Policy distillation and knowledge transfer. The idea of
distilling multiple specialized policies into a single more
general policy has been explored extensively in the RL and
robotics literature , including methods that use RL to
distill into general-purpose neural networks [31, 28], methods
that employ bidirectional constraints between specialists and
generalists [36, 9], and methods that focus on continual
learning [32, 34].
These approaches have demonstrated that careful distillation
can preserve the essential behavioral characteristics of expert
policies while potentially adding beneficial properties like
improved generalization or reduced computational require-
ments. While prior work has explored policy distillation in
various contexts, our work introduces two key innovations:
(1) we show that distilling RL policies into foundation models
that leverage large-scale pre-training yields better performance
than training from human demonstrations while exhibiting
better generalization capabilities than specialized RL policies,
and (2) we demonstrate that for precise manipulation tasks,
using RL-generated data for fine-tuning foundation models
produces superior performance compared to using human
available. Together, these findings establish RLDG as a practi-
cal approach for enhancing foundation models with specialized
RL capabilities while maintaining their broad generalization
abilities.
III. REINFORCEMENT LEARNING DISTILLED GENERALIST
Reinforcement Learning Distilled Generalist (RLDG) is a
simple yet effective method for enhancing generalist policy
performance through the distillation of specialized RL policies.
In RLDG, we train RL policies for individual tasks and then
use these policies to generate training data that can be used
to fine-tune a single generalist robotic manipulation policy,
such as OpenVLA  or Octo . Although specialized RL
policies can achieve high performance on specific tasks, they
often lack zero-shot generalization and robustness to distur-
bances. Conversely, generalist policies excel at generalization
but can struggle to achieve high performance when trained
on human demonstrations, for example due to suboptimal
data or modality mismatches between human demonstrators
and robot policies (e.g., different viewpoints, memory, and
task knowledge). RLDG bridges this gap through knowledge
to finetuning on human demonstrations, while demonstrating
stronger generalization capabilities compared to the original
RL policies. This distillation approach through data generation
with RL is agnostic to both the choice of RL algorithm and
generalist policy architecture, making it flexible to any model
choice. Furthermore, it offers flexibility to train and collect
data with separate RL policies trained on multiple narrowly
scoped tasks (such as one policy for each connector in the
Connector Insertion task). We can also elect to train
RL on the bottleneck segments of a long-horizon task that
require the most precision and benefit the most from RL-
generated data, while leaving the less critical parts for humans
to demonstrate. This simplifies the RL training complexity,
improves data diversity for better generalist performance, and
avoids training RL on unnecessarily long-horizon tasks.
A. Online RL Training
We can formulate each robotic task as a Markov decision
process (MDP), where the state st consists of RGB images and
proprioceptive information, and actions at represent desired
end-effector movements. The policy objective (atst) is to
maximize the expected discounted return:
at(atst)
st1P (st1st,at)
tR(st, at)
where 0 defines the initial robot configurations, P represents
the systems transition dynamics, and R : S  A R is a
reward function encoding the task objectives.
While RLDG is agnostic to the choice of RL algorithm,
we implement RLDG using HIL-SERL  motivated by its
sample efficiency and high performance for learning precise
real-world manipulation skills from pixel input. It incorporates
human interventions with RLPD  to efficiently learn visuo-
motor policies that consistently achieve 100 success rate by
maximizing (1).
B. Experience Collection
After training RL experts for each of the tasks provided to
out the converged policies. Since we transfer knowledge from
RL into the generalist policy only through this data, we have
the flexibility to mix experience from multiple sources. For
tasks that involve separate RL policies per manipulation object
like Connector Insertion, we rolled out each policy and
constructed a balanced fine-tuning dataset consisting of equal
number of episodes per object. In cases where RL is only
trained on a segment of the task like FMB Assembly, we
combine the RL rollouts with human demonstrations for the
remainder of the task.
C. Generalist Policy Finetuning
Robot generalist models are often pre-trained on diverse
large-scale datasets before being fine-tuned to improve perfor-
mance its performance on a particular task while preserving
the generalization capabilities form the diverse pre-training. In
tune these generalist models. Specifically, suppose we have a
pre-trained policy 0, we fine-tune it with task-specific dataset
D(st,at) with the following supervised learning objective:
L()  E(st,at)D[log (atst)].
We evaluate the efficacy of our method by fine-tuning two
pre-trained robot generalist models using different action
parametrization.
OpenVLA.
7B-parameter
language-action model built on Llama 2 . It takes a
Ethernet
3-pin XLR
DisplayPort
Seen Training Connectors
Unseen Test Connectors
C. FMB Insertion
A.  Connector Insertion
D.  FMB Assembly
Seen Training Scenario
Unseen Test Scenario
B. Object Pick and Place
Fig. 3: Illustrations of tasks used to evaluate RLDG. (A) Precise Connector Insertion includes three training objects and
four unseen test objects for evaluating policy generalization. (B) Pick and Place involves an unseen scenario that tests the
policys visual robustness to different backgrounds and objects. (C) FMB Insertion involves inserting a pre-grasped object
in a moving board while (D) FMB Assembly starts with the object on the table and involves an additional grasping phase.
Franka Emika
RealSense D405
Wrist Camera
3Dconnexion
SpaceMouse
Fig. 4: We use a Franka Emika Panda arm with a parallel jaw
gripper teleoperated by a 3Dconnexion SpaceMouse device.
There is a single RealSense D405 camera mounted on the
robots wrist for image observations.
single image as observation input along with a language
instruction. It predicts 7-dimensional actions which are dis-
cretized into 256 bins per dimension autoregressively using the
standard cross-entropy loss. To fine-tune the model on our RL-
generated dataset, we use the public model weights pre-trained
on 970 thousand Open X-Embodiment dataset  and apply
Low Rank Adaptation (LoRA) , a popular parameter-
efficient fine-tuning method.
Octo. Octo is another open-source generalist robotic policy,
designed to adapt to diverse sensory inputs and action spaces
efficiently. Different from OpenVLA, Octo predicts continuous
actions with a diffusion head, which excels at modeling
multimodal distributions, helpful for imitating human demon-
strations . To predict an action, the transformer backbone
takes in the tokenized observation and goal, then outputs a
readout embedding e, which is used to condition the denoising
process trained on the standard DDPM objective . We take
the pre-trained Octo-Base model, remove its secondary image
on our RL-generated dataset.
IV. EXPERIMENT AND RESULTS
Our experiments aim to evaluate RLDG in terms of its
ability to improve over both imitation learning methods for
training generalist policies (in terms of performance), and
its ability to improve over more specialized RL policies
(in terms of generalization). We use a test suite of tasks
that require precise and delicate manipulation, and thus are
particularly challenging for imitation learning methods. We
focus on two main questions: (1) Is the RLDG approach for
training generalists using data from RL more effective than the
conventional approach of training on demonstration data? (2)
Is the generalist policy that results from RLDG training more
effective at generalizing than the RL policies used to generate
the training data?
Fig. 5: Success rate comparison of OpenVLA and Octo policies fine-tuned with RLDG versus conventional methods using
human demonstrations. Both generalists trained with RLDG consistently outperform their counterparts trained with the same
number of successful expert human demonstrations in both training and unseen scenarios.
A. Experimental Setup and Tasks
Our robot setup for all experiments is shown in Fig. 4.
The arm tracks end-effector commands with a 1kHz low-level
impedance controller. Data collection, RL, and Octo policies
command actions at 10Hz, while OpenVLA runs at 4Hz due
to inference speed limitations. The action space for all policies
is a 6-dimensional end-effector delta pose in the wrist frame
and 1 binary gripper action for tasks that involve grasping.
We collect expert demonstrations using a SpaceMouse device
that streams 6D end-effector twists matching the RL and the
generalist policys action spaces. The RL policys observation
space consists of a single 128  128 wrist RGB image along
with end-effector pose, velocity, and wrench measurements.
For the generalist policies, we fine-tune only using the wrist
camera image as input.
We evaluate RLDG on four real-world manipulation tasks
that present distinct challenges. These include high-precision
contact-rich tasks that typically challenge generalist mod-
improve performance, and multi-stage assembly tasks that
leverage RLDGs ability to compose skills. Through these
unseen configurations.
Connector Insertion. This task requires inserting various
electronic connectors into their matching ports, which requires
sub-millimeter precision and dexterity to deal with the intricate
contact dynamics during alignment. We train separate RL
policies and use them to collect data on USB, Ethernet,
and VGA connectors before distilling them into a single gen-
eralist policy. We also use Type-C, HDMI, Display Port,
and 3-pin XLR connectors to evaluate the policys zero-shot
generalization performance.
Pick and Place. We also test our method on a pick-and-place
location and places it in a bowl. To test generalization, we also
evaluate on an unseen scenario by replacing the training object
and background as shown in Fig. 3. With this experiment,
we aim to demonstrate RLDGs effectiveness on tasks that
generalist policies are often used for and benchmarked on.
FMB Insertion. We also use the single object insertion task
of FMB , a common and reproducible benchmark for
comparing robotic manipulation methods. This task involves
inserting a pre-grasped object into a matching opening with
1.5mm tolerance at randomized positions. We utilize this
task primarily for our analysis experiments in Section V.
FMB Single Object Assembly. This task stems from the
FMB Single-Object Multi-Stage Assembly, which adds a
grasping phase on top of the FMB Insertion task above.
We use this multi-stage task to demonstrate RLDGs ability
to enhance overall task performance by distilling RL data
for the precision-critical insertion phase while using human
demonstrations for the grasping and transport phases.
More details on the experiment tasks and training procedure
can be found in the Appendix.
B. RLDG vs. Conventional Fine-tuning
In this section, we seek to answer Question 1 by compar-
ing generalist policies fine-tuned using RLDG and standard
generalist fine-tuning via imitation learning. For each task,
we fine-tune OpenVLA and Octo on RL-generated data as
described in Sec. III, and on expert human demonstrations.
For a fair comparison, we use the same task setup, training
of successful episodes for both methods. The only difference
is the source of the data (RL vs. human). We aim to evaluate
whether RL policies are a better source of training data for
generalist models than conventional human demonstrations in
terms of resultant policy performance.
a) Success rate.: We present the success rate of each
policy and method in Fig. 5. On each task, both OpenVLA and
Octo fine-tuned with RL-generated data consistently achieved
higher success rates than their counterparts trained with human
Fig. 6: Cycle time comparison between policies trained with RL data versus human demonstrations. NA for RL in FMB
Assembly denotes policy not trained on the whole task, while NA for fine-tuned policies denotes no successes recorded.
The RL-trained policies generally achieve faster execution times across tasks, demonstrating the efficiency benefits of using
RL-generated data for policy training.
Fig. 7: Success rate of OpenVLA policies fine-tuned on differ-
ent sizes of RL-generated and human-collected datasets. When
evaluated on seen (VGA) and unseen (Type C) Connector
Insertion tasks, RLDG shows superior sample efficiency,
requiring significantly fewer demonstrations to achieve perfect
success rate in both scenarios while the performance of
conventional method saturates in the unseen case.
On the precise FMB Insertion and Connector Insertion
from higher quality training data, OpenVLA with RLDG saw
33 and 23 higher success rates, respectively, compared
to the baseline. The benefit of RLDG is equally pronounced
for Octo, where it improved the success rate by 10 and
than OpenVLA. We found that RLDG also improved the
success rate for Pick and Place from 1620 to 1920 for
OpenVLA and 120 to 420 for Octo. Furthermore, the training
task performance boost of RLDG also carried over to unseen
evaluation scenarios. OpenVLA with RLDG achieved over 2
times higher success rate than OpenVLA with human data
for unseen Connector Insertion, while applying RLDG
on Octo increased the success rate from 020 to 420 in
the unseen Pick and Place task. When we strategically
combine human demonstrations with RL-generated data on
the FMB Assembly task, the resulting OpenVLA policy also
significantly outperformed the version trained purely on hu-
man demonstrations. It achieved 2020 successes with RL-
generated data compared to 1220 with human demonstrations,
suggesting the flexibility and effectiveness of RLDG for im-
proving the bottleneck of a long-horizon task.
b) Scaling analysis.: To further investigate the effec-
tiveness of RLDG, we conduct a scaling experiment study-
ing the success rate of OpenVLA policies on a seen VGA
connector and an unseen Type-C connector when fine-tuned
on different numbers of RL-generated and human-collected
episodes. We present the results in Fig. 7. For the VGA
rate with just 45 RL episodes, compared to 300 required
from human demonstrations to achieve the same success rate.
on the unseen Type-C connector insertion, while OpenVLA
trained on human demonstrations plateaued at a 90 success
rate even with 900 demonstrations. These results strongly
suggest that fine-tuning generalist policies using RLDG is
more sample-efficient and leads to higher performance than
human demonstrations for both in-distribution and unseen
c) Cycle time.: As shown in Fig. 6, the generalist policies
trained with RL data consistently have faster cycle times
compared to those trained with human demonstrations across
RLDG decreased the cycle time between 0.3 to 2.3 seconds
per task, while Octo saw little improvement on average.
This improvement can be attributed to the inherent speed
optimization in RL training through temporal discounting,
Fig. 8: Action distribution visualization for RL data and human demo
data for the FMB insertion task. We visualize the first two dimensions of
the dataset actions after filtering all the transitions in the dataset where the
end-effector positions are close to the position shown in the image on the left
(xy coordinates are both within 4mm and z coordinate is within 10mm).
The robot arm needs to move in the -x direction and in the -y direction
to reach the insertion point. The first two dimensions of the action space
corresponds to the control of the x and y position of the end-effector position
correspondingly. Human actions are clustered around the center of the action
space whereas the RL actions are more optimized, and mostly found near the
correct corner (bottom-left) of the action space.
which is then distilled into the generalist policy by collecting
trajectories that solve the task faster than the human expert.
solving the tasks than the original RL policy. For OpenVLA,
we primarily attribute this deficiency to the control frequency
gap between the RL policys 10Hz and OpenVLAs 4Hz,
changing the system dynamics and lowering the maximum
velocity of the arm. We believe the speed of OpenVLA can
be significantly improved if inference can be sped up to match
the RL policy frequency. For Octo, the fine-tuned policies were
unable to fit the fine-tuning dataset perfectly, leading to lower
success rate and longer cycle time overall.
C. Generalization of RLDG vs. Original RL Policies
To address Question 2, we compare the generalization
performance of generalists trained using RLDG against that
of the original RL policies used to generate the data. As
shown in Fig. 5, the RL policy success rate quickly degraded
from 2020 for the training scenario to 120 for the unseen
scenario of the Pick and Place task. In contrast, OpenVLA
and Octo with RLDG achieved 1020 and 420 success rates
respectively on the same task. Additionally, the multi-task
capabilities of OpenVLA and Octo allowed fine-tuning on
multiple connector data in the Connector Insertion task,
achieving 7380 and 5080, respectively, when evaluated across
4 unseen connectors, whereas the best of the three RL policies
trained on single connectors recorded only 4980 successes.
Our experimental results on challenging dexterous manip-
ulation tasks demonstrated several key advantages of RLDG.
Generalist robot policies trained on RL-generated data using
RLDG consistently achieve higher performance across all
tasks compared to conventional fine-tuning methods using
human demonstrations. Compared to directly using the RL
policies that generated the data, RLDG also demonstrated
much greater generalization capabilities and robustness to
unseen test scenarios.
Fig. 9: Fine-tuning success rate on the FMB insertion task with different
fine-tuning data sources and varied dataset sizes (from 25 trajectories to
300 trajectories). Human: demo trajectories collected by human teleoperators.
Human  RL actions: the same human demo trajectories but with all the
actions relabeled by a trained RL agent. RL: rollouts collected by the RL
agent. RL data consistently provide better fine-tuning performance than human
data. Human  RL actions closes the gap mostly, suggesting that most of the
benefits of RL data come from it having better action quality.
V. ANALYSIS: WHY IS RL DATA BETTER THAN HUMAN
We have shown that fine-tuning generalist policies with
RL data yields superior performance compared to training on
human data. However, it is unclear where these benefits are
coming from. In this section, we analyze the source of the
benefits in two parts. The first part focuses on studying the
benefits of RL actions and the state distribution in RL data
in isolation. The second part focuses on dissecting the failure
modes of the fine-tuned policies on each individual task.
A. Is RL data better because of better action or state distri-
To answer this question, we use the FMB insertion task and
create a mixed dataset where we take the human data and
relabel the actions using action samples from the RL policy.
Comparing the fine-tuning performance of the mixed dataset
with the purely RL data and the purely human demo data
would allow us to see the benefits of the actions and the state
distribution in isolation. As shown in Figure 9, mixing human
states and RL actions yields a better fine-tuning success rate
than using fully human data (more than 50 improvements
when fine-tuning on 255075 trajectories), while still being
worse than using fully RL data. This suggests that while RL
action and state distribution both contribute to the fine-tuning
performance improvements, action quality is the factor that
contributes to the performance improvement the most. Figure 8
shows a comparison of human and RL actions. We can see that
the RL action distribution assigns more density on the correct
direction (bottom-left) that moves the end-effector towards
the insertion point whereas human action distribution focuses
mostly around the middle with a slight bias towards the correct
direction. This suggests that RL actions are more optimal than
human actions, resulting in the better sample efficiency for
fine-tuning we observe in Figure 9.
B. Qualitative Analysis: Failure Modes
To further understand why RL-generated data leads to better
observed that policies trained with RL-generated data consis-
tently helped overcome alignment issues in precise, contact-
rich tasks and reduced premature gripper closure during grasp-
ing. Videos of each policy on each task can be found on our
project website (
a) Connector and FMB Insertion.: In both tasks, RL
data eliminated a stuck state where the object contacts
the board but fails to align properly. Human demonstration
policies often maintained contact pressure without necessary
exploratory movements. Furthermore, RL data also improved
approach trajectories, preventing early descents that caused
connectors to catch on socket lips.
b) Pick and Place.:
RL data improved grasp relia-
demonstration-trained policies. However, an interesting RL-
specific failure mode was observed: objects were sometimes
dropped too early, bouncing out of the bowl. This likely
resulted from RLs speed optimization, where objects were
released immediately after clearing the bowls edge, but the
distilled policy lacked precise timing.
c) FMB Assembly.: While both OpenVLA policies per-
formed similarly in grasping and transport phases as they are
trained on the same human data, the performance gap emerged
during insertion, with RL data better addressing alignment
issues much like in the insertion tasks. Octos failure was due
to consistent grasping errors where the fingers are in front of
the object, likely due to the lack of good depth perception.
VI. DISCUSSION AND LIMITATIONS
In this work, we presented RLDG, a simple method that
fine-tunes generalist policies on high-quality data generated
by RL policies. We demonstrated that generalist policies
fine-tuned using RLDG consistently outperform those trained
with human expert demonstrations on a suite of real-world
challenging precise manipulation tasks. Our method can be
applied in real-world robotic manipulation tasks that require
a large amount of training data or where policy performance
using human demonstrations saturate. Our work also opens up
avenues for making autonomous improvements of generalist
policies more scalable and tractable. First, our method assumes
access to reward functions for fine-tuning tasks which may
present difficulties when the task rewards are hard to specify.
Possible future directions include autonomously generating
fine-tuning tasks with reward functions (e.g., using pre-trained
VLMs) such that there is no need for manual task specification.
success but the speed in doing so. Such an objective does
not necessarily result in policies that are robust in distillation
errors. For example, on the Pick and Place task, the policy
fine-tuned on RL-generated data always tries to place the
object immediately after it has moved close enough to the
goal location but sometimes drops the object too early (see
Section V-B).
We also note that the human demonstrations in this study
were collected by an author with extensive experience using
the tele-operation interface. We acknowledge that it may
raise concerns about the quality of the demonstrations being
suboptimal and the fairness of the comparison to fine-tuning
on RL data. However, we stress that the demonstrator made
a sincere effort to collect the best possible 
