=== PDF文件: FAST Efficient Action Tokenization for Vision-Language-Action Models.pdf ===
=== 时间: 2025-07-22 09:41:35.385310 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Vision-Language-Action Models
Karl Pertsch,1,2,3, Kyle Stachowicz,2,
Brian Ichter1, Danny Driess1, Suraj Nair1, Quan Vuong1, Oier Mees2, Chelsea Finn1,3, Sergey Levine1,2
1Physical Intelligence, 2UC Berkeley, 3Stanford
AbstractAutoregressive
sequence
Transformer-based
vision-language
can be tremendously effective for capturing complex and
generalizable robotic behaviors. However, such models require
us to choose a tokenization of our continuous action signals,
which determines how the discrete symbols predicted by the
model map to continuous robot actions. We find that current
approaches for robot action tokenization, based on simple
poorly when learning dexterous skills from high-frequency
robot data. To address this challenge, we propose a new
compression-based tokenization scheme for robot actions, based
on the discrete cosine transform. Our tokenization approach,
Frequency-space Action Sequence Tokenization (FAST), enables
us to train autoregressive VLAs for highly dexterous and
high-frequency tasks where standard discretization methods fail
completely. Based on FAST, we release FAST, a universal robot
action tokenizer, trained on 1M real robot action trajectories.
It can be used as a black-box tokenizer for a wide range of
robot action sequences, with diverse action spaces and control
frequencies. Finally, we show that, when combined with the
0 VLA, our method can scale to training on 10k hours of
robot data and match the performance of diffusion VLAs, while
reducing training time by up to 5x.
I. INTRODUCTION
dously effective for capturing complex and generalizable
robotic behaviors both from scratch [8, 72, 54, 6, 20, 65]
and using models pre-trained for next-token prediction on
Internet-scale image-text corpora [10, 40, 66, 7, 68]. How-
continuous action signal, which determines how the discrete
symbols predicted by the model map to continuous robot
actions [67, 34, 42, 12]. It is widely known that a good choice
of tokenization can be critical to the performance of sequence
models [58, 60]. Prior robotic policies of this sort typically
use nave tokenization strategies based on a per-dimension,
per-timestep binning scheme [9, 10, 40]. We find that such
methods perform poorly when learning dexterous skills with
high-frequency control (see Figure 2, right). We observe that
correlations between time steps are a major challenge for
nave tokenization strategies when predicting sequences of
: Core contributors
Correspondence to: researchphysicalintelligence.company
Fig. 1: We propose FAST, a simple yet effective approach
for tokenization of robot action trajectories via time-series
compression. FAST enables training of autoregressive VLAs
that solve complex dexterous manipulation tasks and gener-
alize broadly to new scenes. We use it to train 0-FAST,
a generalist robot policy that matches the performance of
the state-of-the-art 0 diffusion VLA on dexterous and long-
horizon manipulation tasks, while training 5x faster (top).
future actions, i.e., action chunks, as is common for high-
frequency control. Highly correlated action tokens diminish
the effectiveness of the next token prediction objective used
in autoregressive VLAs. Intuitively, in such cases low token
prediction loss can often be achieved with mappings as trivial
as simply copying the most recent action token, leaving models
in poor local optima.
In this work, we propose a new tokenization strategy
from first principles. Our key insight is that robot action
signals need to be compressed before training, to reduce
correlation between consecutive tokens. We take inspiration
from compression-based tokenization strategies, such as the
byte-pair encoding method commonly used by language mod-
els [27, 60]. However, since robotic actions are continuous,
the corresponding compression strategy should be chosen
accordingly. We therefore base our method off of the discrete
cosine transform (DCT) encoding, which is widely used for
compressing continuous signals such as images (e.g., JPEG
compression). We find that the resulting tokenization approach,
Frequency-space Action Sequence Tokenization (FAST), en-
ables us to train autoregressive VLA policies via simple
next token prediction (see Figure 2, left) for highly dexter-
ous and high-frequency tasks where standard discretization
methods fail entirely. Additionally, FAST for the first time
enables efficient VLA training on the recently introduced
DROID dataset , a large-scale multitask in-the-wild
robot manipulation dataset. The resulting policy is the first
language-conditioned generalist manipulation policy that can
be successfully evaluated zero-shot in unseen environments,
simply by prompting it in natural language.
Based on FAST, we develop FAST, a universal robot ac-
tion tokenizer, trained on 1M real robot action trajectories that
cover a large diversity of robot embodiments, action spaces
and control frequencies. We demonstrate that the FAST to-
kenizer effectively tokenizes a wide range of robot action
and is a good off-the-shelf tokenizer for training autoregressive
VLA models. When integrated with the 0 VLA, FAST-based
autoregressive VLAs scale to training on 10k hours of robot
data and achieve performance comparable to diffusion-based
VLAs across a variety of tasks, while reducing training time
by up to 5x (see Figure 1).
II. RELATED WORK
Tokenization for language, text, and audio. Tokenization is
a key component of training pipelines for modern transformer-
based autoregressive sequence models, and the choice of
tokenization approach can have significant impact on model
training and downstream performance . While there are
multiple works exploring the training of tokenization-free
language models [28, 56] that directly operate on bit streams,
most language models today rely on a text tokenization
stage prior to training. A common approach is byte pair
encoding [27, 58], which compresses input text by merging
frequently occurring token sequences into new tokens. For
Vision-Language-Action Model
Vision-Language-Action Model
Vision-Language-Action Model
fold the shirt
fold the shirt
fold the shirt
OpenVLA-style
Data Control Frequency (Hz)
Comparing Action Tokenizers across
Control Frequencies
FAST Action
tokenization
Frequency
Robot Data
Fig. 2: Left: FAST tokenization enables training of autoregres-
sive Transformers for dexterous robot control via simple next
token prediction. Right: FAST outperforms popular binning
tokenization schemes used by e.g. OpenVLA , particularly
for high-frequency robot data, achieving better task completion
progress on real-world evaluations that increases with higher
control frequency.
produced by a pre-trained vision encoder , and full au-
toregressive image input-output can be achieved with a vector-
quantizing autoencoder [22, 62]. Similar approaches can be
extended to the video domain . In audio generation and
speech synthesis, which share the time-series structure of ac-
tion prediction, state-of-the-art models typically encode time-
series audio data using either frequency-domain spectrogram
images  or using learned vector quantizers .
Vision-language-action models. Recently, multiple works
have developed generalist robot policies [9, 54, 6, 10, 20, 40,
datasets [55, 39, 63, 24, 49, 35]. One promising approach for
training generalist policies are vision-language-action models
tune vision-language models, that are pre-trained on internet-
scale image and text data, for robot control. This has multiple
billions of parameters, provides policies with the necessary
expressivity for fitting large robot datasets. Reusing weights
pre-trained on internet-scale datasets also improves the ability
of VLAs to follow diverse language commands and generalize,
e.g., to new objects and scene backgrounds [10, 40, 70, 66, 37].
Most VLA models today are confined to rather simple, low-
frequency control tasks, particularly models that use the most
common autoregressive VLA design [10, 40]. We show that
this is a direct consequence of the action tokenization schemes
employed by these models, which make training on dexterous
tasks challenging. We introduce a new action tokenization
approach that allows us to train the first autoregressive VLAs
on dexterous and high-frequency robot data.
Action representations for VLA training. Prior works have
explored various action parameterizations for training robot
action representations like language sub-tasks [21, 2, 4], or
keypoints [52, 32, 25, 19]. Such approaches can often learn
from few examples or even perform tasks zero-shot without
any robot examples [52, 32, 25], but require hand-designed
low-level controllers for task execution, limiting their gener-
ality. An alternative approach directly trains VLAs to output
low-level robot control commands given image and language
instruction inputs. The most common design directly embeds
actions into discrete tokens, that can be generated with stan-
dard autoregressive sequence models, like any popular vision-
language model. Existing approaches map from continuous
robot actions to discrete action tokens using a simple per-
that this scheme struggles to scale to high-frequency robot
control tasks. We propose a new tokenization scheme for
robot actions, based on time-series compression techniques,
that allows us to train autoregressive VLAs on high-frequency
data. A number of works have also proposed alternatives
to tokenization, for example by using regression heads or
introducing new weights for diffusion decoding [20, 7, 42, 66].
In comparison, our approach does not require modifications of
the underlying pre-trained transformer model, can easily be ap-
plied to any pre-trained autoregressive transformer model, and
achieves competitive performance to state-of-the-art diffusion-
based VLAs  across many tasks, while being significantly
more compute efficient to train.
Another set of related work explores vector-quantized action
representations [42, 3, 51]. Such approaches train a vector-
quantiz
