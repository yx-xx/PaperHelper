=== PDF文件: Demonstrating ViSafe Vision-enabled Safety for High-speed Detect and Avoid.pdf ===
=== 时间: 2025-07-21 15:12:32.492194 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating ViSafe: Vision-enabled Safety for
High-speed Detect and Avoid
theairlab.orgvisafe
Parv Kapoor, Ian Higgins, Nikhil Keetha, Jay Patrikar, Brady Moon, Zelin Ye, Yao He,
Ivan Cisneros, Yaoyu Hu, Changliu Liu, Eunsuk Kang, Sebastian Scherer
Carnegie Mellon University
Equal Contribution
Intruder
Avoidance triggered
Intruder detected
Avoidance triggered
Intruder platforms used in the real-world flight tests
Exploded view of the ViSafe payload
ViSafe payload mounted on the X6 Ownship
We demonstrate ViSafe, a high-speed vision-only airborne collision avoidance system. Top: Rendering of a real-world flight test log where the
ViSafe system detects an incoming intruder with a 144 kmh closure rate and performs an avoidance maneuver to ensure safe separation. The annotations
showcase the detections and multi-camera tracks from the vision-based aircraft detection and tracking system, while the trajectory shows the log of the
performed real-world avoidance maneuver. The numbered annotations showcase the different stages of the flight test, from intruder detection to avoidance
completion. Bottom: ViSafe payload, ownship, and intruder platforms used in the real-world flight tests.
AbstractAssured safe-separation is essential for achieving
seamless high-density operation of airborne vehicles in a shared
airspace. To equip resource-constrained aerial systems with this
safety-critical capability, we present ViSafe, a high-speed vision-
only airborne collision avoidance system. ViSafe offers a full-
stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with
a custom multi-camera hardware prototype designed under
SWaP-C constraints. By leveraging perceptual input-focused
control barrier functions (CBF) to design, encode, and enforce
safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations.
We evaluate ViSafes performance through an extensive test
campaign involving both simulated digital twins and real-world
flight scenarios. By independently varying agent types, closure
weather and lighting), we demonstrate that ViSafe consistently
ensures self-separation across diverse scenarios. In first-of-its-
kind real-world high-speed collision avoidance tests with closure
rates reaching 144 kmh, ViSafe sets a new benchmark for vision-
only autonomous collision avoidance, establishing a new standard
for safety in high-speed aerial navigation.
I. INTRODUCTION
Collision avoidance systems are critical to enabling safe
operations in shared airspace. The integration of Uncrewed
Aerial Systems (UASs) into an already congested National
Airspace System raises pressing concerns about ensuring the
safe separation of airborne vehicles. Existing solutions, such
as Autonomous Collision Avoidance Systems (ACAS)
and Unmanned Traffic Management (UTM)  frameworks,
have demonstrated effectiveness. However, these systems of-
ten depend on multiple active sensor modalitiessuch as
small UASs due to stringent size, weight, power, and cost
(SWaP-C) constraints. Addressing threats posed by coopera-
tive as well as non-cooperative aerial entities like balloons and
rogue drones while respecting SWaP-C resource constraints
remains an ongoing challenge. As a result, current regulations
impose strict line-of-sight requirements on human operators,
significantly restricting the utility and scalability of UASs.
Cameras offer a lightweight, cost-effective alternative for
enabling safety-critical Detect and Avoid (DAA) capabilities in
sUAS. With the growth of data-driven methods, vision-based
object detection offers a promising direction for tracking small
aircraft in images with low signal-to-noise ratios . How-
systems with downstream provably safe collision avoidance
methods remain challenging.
We present ViSafe, a vision-only airborne collision avoid-
ance system to impart see-and-avoid capabilities to sUAS.
ViSafe builds on our prior work AirTrack , which uses
high-resolution detection and tracking networks to detect
aerial objects. We extend AirTrack to the multi-camera setting
by using multi-view fusion to track detected intruder posi-
tions across multiple cameras. Additionally, we formulate the
downstream vision-based collision avoidance problem from a
control theoretic perspective using Control Barrier Functions
(CBFs) , which provide provable guarantees of safety and
are suited for runtime monitoring and response.
CBFs have been successfully applied in various domains, in-
cluding safe navigation [19, 6], robotic manipulation [55, 39],
and industrial automation systems . Their deployment for
safe collision avoidance in airspaces has also been investigated
[16, 38, 42]. While existing work assumes global availability
of information and uses proprioceptive information, ViSafe
removes this assumption in the formulation. We identify key
challenges in deploying CBFs in the wild and provide insights
for deploying these techniques for high-speed collision avoid-
ance. Moreover, we offer an edge compute-focused solution
for real-time deployment on resource-constrained platforms.
designing our formulations, where we leverage the empirical
performance statistics of our vision inference system (Air-
Track ) to account for uncertainty in state estimation. This
is achieved using a multi-view Kalman Filter and the ASTM
F3442F3442M standard (satisfied by AirTrack) to determine
the range profile in which intruder detections are reliable.
Bridging the gap between theoretical formulations and real-
world safe behavior requires extensive testing. To address
operating environment in simulation. Using Nvidia Isaac Sim
, we render realistic camera feeds for use in our visual
detection algorithm, providing large-scale and diverse scenario
benchmarking abilities. These tests were run using the same
hardware as the real-world payload, thereby minimizing our
sim-to-real gap for testing.
test our avoidance algorithms at multiple real-world testing
facilities. We created representative configurations in the field
to analyze our systems collision avoidance capabilities. We
conducted approximately 80 hours of flight testing across
two outdoor testing locations to validate our hypotheses.
We observe similar collision avoidance performance in both
simulation and field testing, which further lends credibility to
our hardware-in-the-loop simulation fidelity.
The main contributions of this work are as follows:
1) Multi-view vision-only aircraft detection  tracking and
CBF-based collision avoidance system that assumes no
global availability of information or communication.
2) Custom-built
hardware
simultaneously
streams multiple camera inputs, provides state estimation,
performs deep learning model edge inference, and com-
putes avoidance maneuvers on board in real time.
3) Digital twin and hardware-in-the-loop simulation to per-
form DAA benchmarking and performance analysis under
different agent types, closure rates, interaction geome-
4) First-of-its-kind real-world flight tests demonstrating that
ViSafe ensures safe aerial separation in encounter scenar-
ios with a closure rate of up to 144 kmh.
II. RELATED WORK
A. Collision Avoidance logics
One of the seminal works in airborne collision avoidance
was the Traffic Collision and Avoidance System (TCAS) ,
which functions on a cooperative surveillance mechanism. The
agents communicate their position and intent using Mode S
developed. There are variants of this algorithm for different
agent types in different airspaces (ACAS Xa, Xu), etc. The
key factor driving the development of ACAS algorithms is
the availability of extended surveillance data using ADS-B,
which enables aircraft to assess potential collision risks and
coordinate maneuvers collaboratively. These logics involve
generating cost tables for agent states and possible actions
through simulation and optimization . These tables are then
used for Resolution Advisories (RAs) during deployment.
sors and information to provide RAs. Deploying these sensors
on small UAS is challenging, and there is a large push toward
sensor miniaturization for this purpose. The investigation of
ACASXu with visual inference information is a relatively
unexplored area . Additionally, most avoidance logics are
designed for large UAS, and hence, resolution advisories tend
to be relatively simple. However, small UAS can be in dense
airspaces with other UAS where a finer discretization of action
space is needed. This can lead to state space explosion, where
cost tables can be prohibitively large.
Our proposed technique works with vision inference data
and does not depend on radar and ADS-B data. Additionally,
our RAs are generated at runtime using efficient mixed integer
linear programming (MILP) solvers, which are suited for more
fine-grained control of UAS. Moreover, our technique can
factor in nominal control inputs, which incorporate the liveness
requirement of reaching a goal. It helps our agent stray not
too far from the originally planned trajectory.
B. Control Barrier Functions for Aerial Collision Avoidance
There has been a recent surge in using CBFs to ensure
the safe separation of UAS. Squires et al. identify key
challenges with designing CBFs for collision avoidance and
propose a construction technique. However, they assume in-
formation availability at all times during an episode and
test their formulations in simulation without sensor noise.
of CBFs in a decentralized setup with message passing to
communicate control outputs. However, they do not consider
non-cooperative setups in which all agents do not provide in-
formation. Our proposed technique assumes a non-cooperative
setup in which no information about other agents is explicitly
communicated to the controlled agent. Moreover, we deal
with information unavailability, where the CBF is only active
when intruder information from our vision-based detection and
tracking module is available.
C. Aircraft Detection and Tracking
tracking systems have employed modular approaches in-
corporating established computer vision techniques such as
frame stabilization, background-foreground separation, and
flow-based object tracking [30, 13, 15, 11, 29, 36]. In these
modular approaches, frame stabilization was achieved using
optical flow or image registration methods [35, 45, 48], while
regression-based motion compensation and morphological op-
erations were utilized to highlight potential intruders .
Background subtraction and traditional machine learning algo-
tors to identify aircraft [46, 13, 44]. Furthermore, temporal
filtering was used to reduce the false positives from these
morphological operations. Lastly, track-before-detect methods
have also been explored, where the tracking is usually man-
aged using methods like Hidden Markov Models, Kalman
our visual detection and tracking system, AirTrack , uses
deep learning-based modules, the overall sequential pipeline
of performing frame alignment, detection, tracking, and false
positive filtering is strongly motivated by the success of past
modular  traditional approaches.
Recent advancements have leveraged deep learning, par-
ticularly convolutional neural networks (CNNs), for object
detection [12, 22, 53, 24]. In particular, for aircraft detection,
the challenge is detecting small objects within high-resolution
tive than traditional anchor-based methods like YOLO and R-
CNN . Given the data-centric nature of these methods,
this has led to a proliferation of aircraft detection datasets
[1, 50, 49, 54, 43]. This insight aligns with findings in related
fields such as face detection and object detection in aerial im-
agery [21, 26, 31]. For vision-based aircraft detection systems
where computational efficiency is crucial, fully convolutional
networks that predict heatmaps are the most effective [24, 25].
for aircraft tracking, using a management system to handle
track initiation and termination and associating new detec-
tions with existing tracks using algorithms like the Hungarian
method [9, 30, 13]. For small objects, where bounding box
IoU is unreliable due to sensitivity to positional deviations,
integrated detection and tracking approaches offer a superior
alternative . Motivated by this trend, our AirTrack
employs an anchor-free detection and tracking system.
III. PROBLEM DEFINITION
We model the problem of maintaining safe separation be-
tween two aerial agents: the ownship agent and the intruder
agent. The ownship refers to the primary controlled agent
and the intruder refers to a potentially unknown agent in the
configuration space which doesnt have any communication
with the ownship agent. Only passive sensing, specifically
keeping the two agents outside each others collision volume
at all times. We model the collision avoidance problem in the
horizontal plane and assume constant velocity vectors for the
intruder agent inspired by . The separation requirement
can be formally defined as:
label {eq1 } forall t in T { operation} bullet d{ownship-intruder} geq d{thresh}
distance
threshold
downshipintruder is the relative horizontal distance between the
agents. For notational simplicity, we will refer to this distance
as d in our formulations. Additionally, Toperation is the time
of operation of the ownship agent, and Eq. (1) formally states
the problem of safe separation for the entire operation time.
As can be inferred from (1), this is equivalent to a forward
invariance property. In order to encode and enforce this
simple unicycle dynamics model for our ownship agent:
{own }}  beg
in { bmatrix}
{own})}  v{own} sin {(chi {own})}  0  0 end {bmatrix}  begin {bmatrix} 0  0  0  0  1  0  0  1 end {bmatrix} u
u T  [ dot {v}{own} dot {chi }{own}]
heading (yaw) angle. The control input u R2 consists of
the rate of change of speed and heading, i.e., vown and own.
mimick real world deployment scenarios.
u geq  u{m in } , u leq u{max}  forall t in T{operation}
Onboard System
Digital Twin
Real World
AirTrack Visual Detector
Motor Control
3D Projection  Kalman Filter
Safety Violation Assessment
Global Path
Follower
Overview of our ViSafe framework for real-world testing and hardware-in-the-loop simulation: Firstly, the onboard sensors or digital twin
simulation stream the multi-cam videos to the AirTrack visual detection module, which detects the intruder across multiple views. Then, these detections,
along with the ownship state information, are sent to the multi-view fusion and coordinate frame conversion module, which then tracks the intruder and sends
the intruder state information along with the ownship state information to the CBF. The CBF uses the nominal global plan and the safety violation assessment
to compute modifications to the nominal control input in case of violation. This safe control output is then sent to the drone autopilot system for execution.
This loop continues to operate in real-time until the flight test is complete.
These control constraints are particularly important for our
use case since, for some aircraft, loiter safety maneuvers are
actually impossible and would lead to an aircraft crash.
{x{ int}}
begi n {bmatri
x} v{int} cos {(chi {int})}  v{int} sin {(chi {int})}  end {bmatrix}
heading (yaw) angle. Overall, we make two assumptions:
earlier. Second, the intruder agent is agnostic to the ownship
agents maneuvers, i.e., there are no interaction effects. This
assumption also implies that the township agent possesses
no collision avoidance algorithm, since that would lead to
interaction effects. These assumptions are made to keep the
problem tractable but remain practical in todays airspace.
IV. ViSafe FRAMEWORK
An overview of our ViSafe framework (Fig. 2) is as follows:
While the visual detection module (Section IV-A) provides an
image-level intruder detection, the intruder state information is
transformed to the North East Down (NED) coordinate system
and broadcasted to the safety controller in its expected input
format (Section IV-B). The safety controller (Section IV-C)
encodes the desired safe separation constraints as well as our
actuation constraints and computes modifications to nominal
control input in case of violation. These constraints are derived
using our defined CBF. This modified control action is then
converted into low-level drone commands and executed.
A. Visual Detection Module
We extend the state-of-the-art aircraft detection  tracking
modified the AirTrack algorithm (originally focused on single-
camera inference) to incorporate multi-camera inputs and
enable multi-camera tracking. Furthermore, we also upgraded
the SWaP-C hardware, implementing efficient multi-camera
image sharing (with zero memory copies) and further opti-
mizing the deep learning model inference to operate at the
desired frequency required to facilitate high-speed collision
avoidance on the edge. The overall design of our Detection
Module comprises Frame Alignment, Detection, Secondary
are two consecutive grayscale image frames, and it outputs
a list of tracked objects with various attributes. The model
is trained on the Amazon Airborne Object Tracking (AOT)
Frame Alignment: The goal of this module is to align the
successive frames in a video so that the ego-camera motion
can be discarded, thereby helping to distinguish foreground
objects from the background. This is achieved by predicting
the optical flow between two successive input image frames
and the confidence of the predicted flow. The input images are
cropped from the center-bottom to cover high-texture details
below the horizon, and a ResNet-34 backbone architecture
with two prediction heads is used for alignment. The prediction
is made at 132 scale of the input, and low confidence offset
predictions are rejected.
detection module and a secondary detection module. The
primary module takes in two full-resolution (1224  1024)
grayscale image frames that have been aligned, while the
secondary module receives a smaller 15th sized crop of the
image around the top k detector outputs (based on confidence)
from the primary module. The network is fully-convolutional
(HR-Net-W32) and it produces five outputs: a center heatmap
indicating the objects center, bounding box size, center offset
from the grid to the objects center, track offset of the objects
center from the prior frame, and object distance in log scale.
Secondary Classifier: A ResNet-18 module is used as a
binary classifier for false-positive rejection. The module takes
cropped regions around the bounding boxes from the detection
module as input and predicts whether the crop is an aircraft
or not. This improves the overall precision.
Image-level Tracking: We use an offset tracking vector-
based approach . The predicted track offset vector from the
detection module is subtracted from the current object center
to find the location in the prior frame. If the location matches
existing tracks within a threshold, the detection is associated
with an image-level track; otherwise, a new track is initialized.
to provide precise image-level aircraft tracks and distance from
the ownship. For ViSafe, we run AirTrack on video streams
from two cameras at 8 Hz on our Nvidia Orin AGX compute
system (described in Section V-E). We encourage the readers
to refer to the AirTrack  paper for further details.
B. Multi View Fusion  Coordinate Frame Conversion
Once AirTrack provides an image-level intruder track (from
any one of the multiple cameras), we unproject the track
location in the image to 3D leveraging the predicted distance
of the aircraft, center of the predicted bounding box and the
calibrated intrinsics of the respective camera. Then, the 3D
position of the image-level track in the camera coordinate
frame is transformed to the local frame of the 3DM-GQ7
GNSINSS module by using the multi-camera and IMU ex-
trinsic calibration. Since, the GNSS module provides us the
heading of the ownship with respect to the North direction,
we can transform the 3D image-level intruder track position
to a North-East-Down (NED) coordinate frame centered about
the GNSS module of the ownship. Now given the 3D position
[xt,yt,zt] of the image-level track, the range d, azimuth  and
elevation  of the intruder in the ownships NED coordinate
frame can be calculated as:
beg in { alig
n e d}  d
r t  {{xt}
{yt}2  {zt}2}   theta  arctan left (frac {yt}{xt}right )   phi  -arcsin left (frac {zt}{d}right ) end {aligned}
We leverage a multi-view fusion module to track and fuse
the intruder positions across multiple cameras. At the start, for
the first image-level intruder track, we initialize two Kalman
Filters to perform tracking and fusion of new observations
for the same intruder. We empirically observe that the range
(d) is noisier than the angles (  ) since the angular
estimates depend solely on the 2D location of the detection
Encounter geometry and information required for vision-enabled
collision avoidance. The velocity of the ownship vown and heading with
respect to North own are obtained from the ownship odometry. The intruders
range d, azimuth , velocity vint, and heading with respect to North int are
obtained using the visual detection module.
while the range is predicted by the monocular deep learning
model (AirTrack). It is possible to use a single Kalman Filter
by specifying independent process noises; however, we find
that the coupling of range measurements degrades the angular
two Kalman Filters, each with a constant velocity motion
model (one for angles and one for range). Through decoupling,
the range filter accounts for high uncertainty in range while
the angle filter ensures smooth tracking. Subsequently, for
each new image-level track across the different cameras, we
use the angular distance to determine if the track associates
with pre-existing 3D intruder tracks. If it associates, we fuse
the new observation with the two pre-existing Kalman Filters.
Kalman Filters. In this way, we can use Kalman Filters (unique
to each intruder) to initialize, associate, and track different
intruders in 3D. Each 3D intruder track is configured to
propagate the intruder position and velocity to the downstream
supervisory safety controller at a frequency of 24 Hz.
As shown in Fig. 3, the supervisory safety controller de-
scribed in the subsequent sub-section expects the multi-view
fusion module to output the range d, the angle of the line
of sight to the NED frame centered about the intruder ,
the velocity of the intruder vint along the North and East
direction and the heading of the intruder with respect to the
North direction int. The tracks from the multi-view fusion
module already provide the range d, and  can be computed as
. Furthermore, using the ownships odometry information
(vown  own) and the track information (range d, range rate
components of vint as follows:
begin { aligned}   { v{i nt }} {No
rth}  { v {own}} { No rth}     dot {d}cos {theta }  ddot {theta }sin {theta }   {v{int}}{East}  {v{own}}{East}  dot {d}sin {theta }  ddot {theta }cos {theta } end {aligned}
can be obtained as:
b e gin {a
l igned}
int}}  arctan left (frac {{v{int}}{East}}{{v{int}}{North}}right ) end {aligned}
level intruder detections to the expected input format of the
supervisory safety controller.
C. Supervisory Safety Controller
The three key requirements for this controller are:
Certified safety: Enforcement of a strict notion of safety
while working with vision inputs
Run time safety: Reactive low latency solution that can
make decisions in real-time
Performant safety: Preserving original mission require-
ments and introducing a minimal modification to the
original control input provided by a nominal controller
Existing techniques, such as reachability analysis and regret
minimization for MPC, often conflict with these three require-
ments. For example, reachability analysis-based techniques are
certified and can generate controllers that preserve mission
outputs. However, they tend to be computationally expensive,
compromising the reactive requirement. Considering these
three key requirements, we used a Control Barrier Function
(CBF) based Quadratic Program (QP) for our supervisory
safety controller. CBFs were introduced in  to formally
certify the safety of robotic control systems. We assume we
are given a nonlinear control affine system defined by
label {eq0} dot {x}  f(x)  g(x) u
where x X , f and g are locally Lipschitz continuous
functions and u R is the set of inputs to the system.
Let us say our safe set is defined by Xs X , then the
goal of safe control is to keep this set forward invariant with
respect to the dynamics Eq. (8). Mathematically, this forward
invariance of the set is described as
for a ll  x(t { 0}
t  in mathcal {X}{s} bullet x(t) in mathcal {X}{s} quad t geq t0
Let C : {x X h(x) 0} be a zero sub-level set of a
function h : X R. Then h is a valid control barrier function
for C given dynamics Eq. (8) if there exists an extended class
Lf h(x)  L  g h(x)u
l eq - alpha (h(x) quad forall x in mathcal {C}
drift f(x) and control g(x) dynamics, respectively. Note this
formulation is slightly different than the one defined in
since we define the safe set to be a zero-level subset instead
of a superset. However, since we change the inequality sign,
we can still ensure forward invariance using this condition.
This design choice is inspired by .
QP-based controller. Our supervisory controller enforces our
safety and actuation constraints. We devise this controller
using our defined control barrier function. First, let our safe set
be defined as C, then a straightforward distance maximization-
based CBF would be:
h( x )  d{ thresh} - d label {eq:nomcbf}
b egin  {
a li g n ed} x  in C : h(x) leq 0 x notin C : h(x) > 0 end {aligned}
dthresh represents the minimum distance threshold that should
be maintained between the two agents. However, since the
relative degree of pure distance maximization-based CBF
with respect to u is 2, there would be singularity with the
CBF proposed in Eq. (11). Inspired by , we propose the
following CBF:
{eq12}  h( x)  c  d{n}{thresh} - d{n} - kdot {d} k
where c > 0,n > 0 and k > 0 are hyperparameters to be tuned.
Our defined CBF takes negative values when the agents are
safely separated and positive values when the requirement is
violated. The CBF condition we enforce for forward invariance
is as follows:
do t  {h}(x) leq - lambda h(x)
We choose the penalty term as h(x) itself as is often done
for CBFs.  is another hyperparameter that controls the rate
of change of the barrier function. Note that the non-linear
constraint h(x) 0 is not necessarily a subset of d dthresh
when d > 0. However, as demonstrated in , the system
remains forward invariant within the set h(x) 0d dthresh
under the application of the control law h(x) h(x). Our
final QP formulation is defined as follows:
n {aligned}
min  {u}
u - u{nom} {2}  textrm {s.t.} quad  dot {h}(x) leq - lambda h(x) u leq u{max}  u geq u{min}  end {aligned} label {eq:qp}
where unom is the control input provided by the nominal
controller and umax and umin encode the actuation constraints.
formulation using post-processed visual detection information
and ownership odometry information (vown  own). First, we
derive the values of h in terms of our control input u using
our encounter geometry as illustrated in Fig. 3.
be g in {a li gned}  dot {h}(x)  nd{n-1}dot {d} -kddot {d}  end {aligned} k
begi n {al i gned}  d ot {d }   v{own}cos {(alpha - chi {own})}  v{int}cos {(alpha - chi {int})}  end {aligned} label {eq:ddot}
(b) Overtake
(a) Head-on
(c) Crossing
Diversity of the airborne collision testing scenarios: (a) The various encounter geometries used for real-world  simulation flight testing. (b)
The diverse weather and lighting conditions that were used to evaluate ViSafes robustness in simulation. In the simulation, based on the chosen encounter
We compute d by taking the projection of vown and vint
along the line of sight (LOS). To compute d, we differentiate
Eq. (15) with our original assumption of constant intruder
velocity vector (i.e. vint  0 and int  0 ) to get:
begin  {ali g ned}   ddot {d }   d ot {v
} {own }cos  {(alpha -  chi  {own})
}  dot {chi }{own}v{own}sin {(alpha - chi {own})}    frac {(v{own}sin {(alpha - chi {own})} v{int}sin {(alpha - chi {int})}){2}}{d} label {eq:dddot} end {aligned}
Now given u
, we can rewrite Eq. (16) as
egin { align
ed}  dd o t {d}
begin  {bma t rix} cos {(al p ha - c
hi {own})}  v{own}sin {(alpha - chi {own})}  end {bmatrix}top u    frac {(v{own}sin {(alpha - chi {own})} v{int}sin {(alpha - chi {int})}){2}}{d} label {eq:dddotrewrite} end {aligned}
As can be observed, the constraints on h are affine in u and
fit the form of a QP, as defined in Eq. (13), which can be
solved to compute the desired safe control usa fe. We use the
following tuned hyperparameters for the final formulation of
We use a simple PD controller as our nominal controller,
where the computed desired safe control usa fe is then con-
verted into low-level drone control actions in the form of
velocity and yaw setpoints, which are executed on our ownship
agent by the flight controller (PX4 in Isaac Sim and Ardupilot
on the real-world drone hardware).
V. ViSafe TESTING
A. Experiment Design
We consider a two-agent interaction scenario. The ViSafe-
enabled ego agent is tested against an airborne intruder in
various collision geometries. We start by identifying three
classes of interaction scenarios for our two-agent setup: Head-
agent and the intruder on a single straight-line converging
path moving towards each other. An Overtake scenario has
the agents on a single straight-line path with a higher-speed
ego agent in the trail of a low-speed intruder. A Lateral
scenario has the ego and intruder on perpendicular converging
paths. Additionally, since our visual detection module can be
sensitive to the intruder being above or below the horizon,
we investigate both possibilities for our agents, leading to 6
total scenarios. Our six configuration setups are described in
Fig. 4. These experiments are performed in both a high-fidelity
digital-twin simulation and real-world settings. Table I shows
the various agents, collision geometries, commanded ground
B. ViSafe Hardware Prototype
ViSafe uses custom hardware as shown in Fig. 1. The
hardware has the following major components:
1) Cameras: 6 x Sony IMX264 cameras providing a total of
220FOV horizontally and 48vertically.
2) Compute: NVIDIA AGX Orin Developer Kit
3) Camera
LI-JXAV-MIPI-
ADPT-6CAM-FP
EXPERIMENTAL CONFIGURATIONS
Scenario ID
Intruder
Collision Geometry
Hor. Closure Rate
Location
Multirotor (10 ms)
Aurelia X6 (10 ms)
20 ms (72 kmhr)
Nardo Airfield
Multirotor (5 ms)
Aurelia X6 (10 ms)
Overtake
5 ms (18 kmhr)
Nardo Airfield
Multirotor (5 ms)
Aurelia X6 (10 ms)
Crossing
11.18 ms (40 kmhr)
Nardo Airfield
VTOL (30 ms)
Aurelia X6 (10 ms)
40 ms (144 kmhr)
Nardo Airfield
Multirotor (10 ms)
Aurelia X6 (10 ms)
20 ms (72 kmhr)
TABLE II
DIGITAL TWIN  HARDWARE-IN-THE-LOOP BENCHMARKING
Separation Minima (m)
Risk Ratio
Number of violations
Scenario
Above Horizon - E1, E2, E3
Below Horizon - E1, E2, E3
TABLE III
REAL WORLD BENCHMARKING
Separation Minima (m)
Risk Ratio
Number of violations
Scenario
Above Horizon - E1, E2, E3
Below Horizon - E1, E2, E3
4) State Estimation: 3DM-GQ7 GNSINSS module with a
dual-GPS antenna setup.
5) ADS-B In: uAvionix PINGMAVLink based ADS-B
receiver (not used)
The payload is securely mounted below the ownship on a
custom mounting plate. The entire payload weighs approx.
1.6 kgs, including the 3D-printed protective casing. The deep-
learning pipeline and the collision avoidance algorithm de-
scribed in Section IV run onboard the payload in real-time.
The payload communicates with the ownship autopilot via
the MavLink protocol to issue navigation commands in the
form of yaw turn rates. We use DroneKit  to command
the ownship autopilot to execute the avoidance maneuvers.
The entire system is agnostic to the ownship platform and is
designed to be mounted easily on most systems.
C. Reaction Time Profile for Successful Detect  Avoid
Based on the empirical performance statistics of our vision
inference system (AirTrack ), we determine the range pro-
file in which the intruder tracks are reliable and the maximum
reaction time available for the ViSafe system. Leveraging the
empirical numbers from the ASTM F3442F3442M standards
(satisfied by AirTrack using a Bell 407 intruder with a frontal
diagonal length of 4.27 m), the minimum number of pixels
at which AirTrack can reliably detect the object with a 95
probability of track comes out to be 14 pixels. Using the ap-
proximate calibrated focal length of our multi-camera payload,
the maximum distance dmax at which the object can be detected
comes out to be 163.2  l, where l is the frontal diagonal
length of the intruder. For our real-world intruder platforms
(shown in Fig. 1), the reliable maximum distance comes out
as dHexarotor
287m and dVTOL
525m. Thus, for real-world
Head-on scenarios, the maximum available reaction time for
the M600 intruder is 14.35s, and for the VTOL intruder, it
is 13.12s. This short maximum reaction time signifies the
need for accurate, fast, and on-edge DAA, which we find is
empirically satisfied by our ViSafe system as shown in both
benchmarking (Table II and III) and profiling (Table IV).
D. Simulated Digital-Twin Experiment Setup
We first perform experiments in a digital twin of the
test environment. The digital twin enables low-cost, low-risk
TABLE IV
VISAFE SYSTEM PROFILING
Component
Performance
AirTrack Detection  Tracking (GPU)
Multi-View Fusion  State Estimation (CPU)
CBF-based Avoidance Control (CPU)
50 Hz (solving a QP)
Mean CPU Utilization
Mean GPU Utilization
Peak Memory Consumption
testing in a more diverse and controllable setting.
In order to test the scenario as realistically as possible, we
create a digital twin of the real-world testing site. We use
the Pegasus Simulator , which is a framework built on
top of NVIDIA Omniverse and IsaacSim . While design-
ing the assets, we use ArcGIS CityEngine , which takes
satellite imagery, terrain maps, and structure information from
OpenStreetMap to build a 3D model of the test site, including
buildings and roads. We also procedurally added trees and a
realistic sky with clouds and weather conditions as depicted
in Figure 4. The experiments were carried out on a desktop
computer with an Nvidia 3080Ti and an Nvidia A6000 running
Ubuntu 20.04.
To simulate real-world hardware performance, we transmit
the rendered images to the onboard processing units for detec-
tion and tracking. This allows us to conduct hardware-in-the-
loop (HITL) experiments within the digital-twin framework.
day and cloud covers, three representative collision configura-
to a total of 60 distinct scenarios. For the above scenarios, we
simulated 200 trajectories per scenario, leading to a total of
12000 trajectories.
E. Real World Experiment Setup
We provide details on the real-world experiments to
test the ViSafe system. Real-world flight tests are con-
ducted at two distinct locations. A dedicated testing fa-
cility at the Nardo Flight Test Field  (Valentin Vas-
silev Memorial Field), USA [FAA Identifier: 77PA],
ondary test-site in Leesburg, VA, USA are utilized for exper-
iments. All tests are performed under the FAA 14 CFR Part
107 guidelines. The Nardo flight test airfield has an unpaved
runway with a total length of 1800ft (550m) and a width
of 100ft (30.5m). The Leesburg field is an open test range.
Table I. The field experiments use three airborne platforms, as
shown in Figure 1. Their details are as follows:
1) Aurelia X6 Standard: A hexa-rotor ownship.
2) Hexarotor: A multi-rotor low-speed intruder.
3) VTOL: A quad-plane vertical takeoff and landing fixed-
wing aircraft that acts as our high-speed intruder.
F. Metrics
We compare our technique with a nominal planner without
an avoidance strategy. By default, the nominal plans violated
Overtake
Scenario Type
HROC (ms) shifted by minimum value
Shifted Horizontal Rate of Closure (HROC) - Above and Below Horizon
Above Horizon (Nominal)
Above Horizon (ViSafe)
Below Horizon (Nominal)
Below Horizon (ViSafe)
Average horizontal rate of closure comparisons across different
encounter geometries in real-world testing: Higher values indicate that
agents are moving apart, showcasing diverging  safe trajectories. Under
different testing scenarios, it can be seen that ViSafe consistently shows a
significant boost in average HROC over the nominal plan.
the safe separation clause, and we include it solely as a refer-
ence point to quantify ViSafes gains via metrics. Our metrics
are inspired by the operational standards proposed for collision
avoidance systems by the Radio Technical Commission for
Aeronautics (RTCA) . These are the key metrics which
1) Probability of Near Mid Air Collision P(NMAC): This
metric measures the probability that two agents come
within a predefined unsafe distance. Lower values signify
better collision avoidance.
2) Separation Minima: This metric captures the minimum
distance between agents during an episode. Higher values
indicate greater safety margins.
3) Horizontal Rate of Closure (HROC): This metric mea-
sures the relative velocity at which two agents approach
each other, with negative values indicating impending
collisions. Higher values reduce collision risks.
4) Risk Ratio: This metric measures the ratio of collisions
compared to a baseline (nominal planner). Lower values
indicates higher safety improvements over the baseline.
G. Results and Discussion
In this section, we highlight the key findings of comparing
ViSafe with a nominal planner without an explicit avoidance
strategy. The results for simulated digital-twin and real-world
experiments are summarized in Tables II and III.
1) ViSafe enables high-speed collision avoidance [H1]:
Our results demonstrate the advantage of ViSafe in reducing
the collision risk over nominal plan. By design, The nominal
planner exhibited high P(NMAC) values across Head-On,
ments. ViSafe drastically reduces the P(NMAC) for the same
configurations. Our method significantly reduces collision risk
and improves safety margins, especially in critical configu-
rations such as head-on and overtaking situations. The total
number of loss of separations is reduced significantly by ViSafe
for all testing configurations in the real world.
No Clouds
Overcast
Dusk Clear
Dusk Overcast
Fog Clear
Fog Overcast
Weather Conditions
HROC (ms) (shifted)
Shifted HROC Values by Weather Conditions - Nominal vs ViSafe
Horizontal rate of closure comparisons across different weather conditions in the digital twin: Higher values indicate that agents are moving
The results in Fig. 5 indicate that ViSafe improves the
HROC values compared to the nominal planner. In the head-
on scenario, HROC improved from -19.25 , text {ms} (nominal) to
-13.83 , text {ms} (ViSafe), demonstrating that our method facilitates
safer approaches by reducing the rate of closure. Similar trends
are observed in lateral and overtaking cases, where ViSafe
maintains a higher and safer HROC, effectively mitigating
dangerous approaches.
2) ViSafe is robust to weather and lightning conditions
[H2]: We also test the robustness of ViSafe to weather and
lighting conditions in simulation. The results are highlighted
in II. Our risk ratios increase due to detection and tracking
failures for some weather conditions. However, our overall
risk ratios with respect to all trajectories are still considerably
lower than the nominal values across all configurations. ViSafe
also achieves higher HROC values than the nominal ones for
all weather conditions, indicating safe  divergent trajectories
as highlighted in Fig. 6.
3) Digital-twin provides a reliable estimate of real-world
performance [H3]: Our results indicate that the performance
metrics observed in the digital-twin environment closely align
with those obtained via real-world testing. As summarized
in Tables II and III, ViSafe demonstrates higher separation
minima and low-risk ratios in all configurations, with slightly
higher risk ratios in simulation compared to real-world results.
We hypothesize that this is due to larger simulated data points
that capture more edge cases. In addition, the lower risk ratios
observed in real-world tests (indicating higher safety perfor-
mance) suggest that the digital twin environment provides a
conservative estimate of ViSafes performance. These findings
underline the importance of the digital twin as a reliable proxy
for predeployment evaluation of safety and finding the lower
bound. By identifying and remedying edge case behavior in
VI. LEARNED CHALLENGES AND LIMITATIONS
A. Learned Challenges
There were multiple challenges observed while trying to
develop and test ViSafe. We list a few of them.
1) Lack of global information availability: CBF guaran-
tees invariance when information is available throughout
the episode. Since we use vision inference to trigger
Theoretical guarantees provided by traditional CBFs can
be compromised without making modifications to the
design of the CBF itself.
2) Actuation constraints and singularities: Actuation con-
straints for the ownship drone proved challenging in the
CBF design process. We had to theoretically guarantee
safety while ensuring we always had a control output that
kept us safe. This was achieved by careful hyperparameter
tuning for a specific ownship system. In the future, a
more principled approach, such as automatic safety index
3) Inaccuracies in vision-based inference: Vision-based
state estimation is not perfect; therefore, false positives
can often throw the safety module off. We had to use
Kalman filtering to ensure minimal false positives were
a part of the CBF to ignore out-of-distribution inputs,
leveraging Robust CBFs .
4) Safety guarantees: We acknowledge that the CBF can
only provide a formal safety guarantee given always
accurate global perception, i.e., no false negatives, which
is not the case in the real world. However, we believe
that ensuring compliance with standards and conducting
operability studies supported by empirical statistical anal-
ysis (similar to our extensive HITL digital twin testing)
can help build valid safety cases for DAA systems.
5) Real world testing constraints: Testing CBFs on actual
drones and collecting data is a time-consuming process,
and only a limited number of experiments can be run. Ad-
before testing, making the process cumbersome. Also,
unexpected disturbances like LOS, high wind speeds, and
data corruption make testing in the real world challeng-
ing. In the future, we plan to use the drone telemetry data
we have collected to test our approach and fine-tune it,
further exploring ways to improve simulation fidelity.
B. Limitations
Across our wide array of simulation and real-world tests,
we find that our current system struggles when the intruder is
below the horizon. As acknowledged in the benchmarking of
our prior work (AirTrack ), the primary reason is that the
vision-based inference system fails to establish a consistent
track when the background changes fast. Furthermore, as
shown in prior work , this performance behavior can be
attributed to biases or shortcuts learned by the model given
the training data distribution. We aim to leverage optical
flow and temporal information in future work to mitigate
this issue. Lastly, for the design of our CBF, we make the
following assumptions: the intruder has a constant velocity
plane parallel to the ground plane, and the intruder doesnt
react to the ownships actions. Although these are used to
make the problem tractable, we aim to investigate interaction-
aware avoidance without explicit communication in future
work using adaptive control barrier functions, improving the
versatility of our ViSafe system.
VII. CONCLUSION
In this work, we present a comprehensive solution for
high-speed vision-only airborne collision avoidance. Our full-
stack solution, ViSafe, deploys perceptual input-focused con-
trol barrier functions (CBF) for safe self-separation in dy-
namic environments. Through simulated hardware-in-the-loop
digital twin testing and real-world flight tests, we exemplify
ViSafes reliability in the face of heterogeneous agents, vary-
ing encounter geometries, and other environmental factors.
ViSafes successful deployment in real-world high-speed col-
lision avoidance tests (at closure rates of up to 144 kmh)
establishes its effectiveness as a cornerstone for autonomous
aerial navigation systems.
ACKNOWLEDGMENTS
Approved for public release; distribution is unlimited. This
research was sponsored by DARPA (W911NF-18-2-0218).
The views, opinions, andor findings expressed are those of
the author(s) and should not be interpreted as representing the
official views or policies of the Department of Defense or the
U.S. Government. The authors thank Rebecca Martin, Helen
their support with the field testing of ViSafe. We also thank
John Keller for helping out with the payload.
REFERENCES
Airborne object tracking dataset.
opendata.awsairborne-object-tracking. 3, 4
drones using the mavlink protocol. URL
comdronekitdronekit-python. 8
Procedural City Generator  3D City Maker  ArcGIS
CityEngine  esri.com.
arcgisproductsarcgis-cityengineoverview.
[Accessed
Isaac Sim  developer.nvidia.com.
nvidia.comisaacsim. [Accessed 11-12-2024]. 9
Nvidia isaac sim, 2022.
comisaac-sim. 2
Ayush Agrawal and Koushil Sreenath. Discrete control
barrier functions for safety-critical control of discrete
systems with application to bipedal robot navigation.
In Robotics: Science and Systems, 2017.
URL https:
api.semanticscholar.orgCorpusID:1780280. 2
Aaron D. Ames, Samuel Coogan, Magnus Egerst-
Tabuada. Control barrier functions: Theory and appli-
cations.
In 2019 18th European Control Conference
MJ Kochenderfer. Optimized airborne collision avoid-
ance in mixed equipage environments. Lincoln Labora-
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos,
and Ben Upcroft. Simple online and realtime tracking. In
2016 IEEE international conference on image processing
(ICIP), pages 34643468. IEEE, 2016. 3
Joseph Breeden and Dimitra Panagou.
Robust control
barrier functions under high relative degree and input
constraints for satellite trajectories, 2021.
URL https:
arxiv.orgabs2107.04094. 10
Ryan Carnie, Rodney Walker, and Peter Corke. Image
processing algorithms for uav sense and avoid.
Proceedings 2006 IEEE International Conference on
Robotics and Automation, 2006. ICRA 2006., pages
Xueyun Chen, Shiming Xiang, Cheng-Lin Liu, and
Chun-Hong Pan.
Aircraft detection by deep convolu-
tional neural networks. IPSJ Transactions on Computer
Vision and Applications, 7:1017, 2014. 3
Debadeepta Dey, Christopher Geyer, Sanjiv Singh, and
Matt Digioia. Passive, long-range detection of aircraft:
towards a field deployable sense and avoid system. In
Field and Service Robotics, pages 113123. Springer,
Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi,
Qingming Huang, and Qi Tian.
triplets for object detection.
In Proceedings of the
IEEECVF international conference on computer vision,
Giancarmine Fasano, Domenico Accardo, Anna Elena
logical filtering and target tracking for vision-based uas
sense and avoid. In 2014 International Conference on
Unmanned Aircraft Systems (ICUAS), pages 430440.
Junjie Fu, Guanghui Wen, and Jinde Cao. High-order
control barrier function based collision avoidance forma-
tion tracking of constrained fixed-wing aircraft. In 2023
5th International Conference on Industrial Artificial In-
telligence (IAI), pages 16, 2023. doi: 10.1109IAI59504.
lad Moghassem Hamidi, and Sebastian Scherer.
aircraft detection and tracking. In 2023 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
Asma Hamissi, Amine Dhraief, and Layth Sliman.
comprehensive survey on conflict detection and resolu-
tion in unmanned aircraft system traffic management.
IEEE Transactions on Intelligent Transportation Systems,
Marvin Harms, Mihir Kulkarni, Nikhil Khedekar, Martin
Neural control barrier
functions for safe navigation, 2024. URL
orgabs2407.19907. 2
Yao He, Ivan Cisneros, Nikhil Keetha, Jay Patrikar, Zelin
Scherer. Foundloc: Vision-based onboard aerial localiza-
tion in the wild. arXiv preprint arXiv:2310.16299, 2023.
Peiyun Hu and Deva Ramanan. Finding tiny faces. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 951959, 2017. 3
Sunyou Hwang, Jaehyun Lee, Heemin Shin, Sungwook
deep convolutional neural network in small unmanned
aircraft systems.
In 2018 AIAA Information Systems-
AIAA Infotech Aerospace, page 2137, 2018. 3
Marcelo Jacinto, Joao Pinto, Jay Patrikar, John Keller,
Rita Cunha, Sebastian Scherer, and Antonio Pascoal.
Pegasus simulator: An isaac sim framework for mul-
tiple aerial vehicles simulation.
In 2024 International
Conference on Unmanned Aircraft Systems (ICUAS),
Jasmin James, Jason J Ford, and Timothy L Molloy.
Learning to detect aircraft for long-range vision-based
sense-and-avoid systems. IEEE Robotics and Automation
Jasmin James, Jason J Ford, and Timothy L Molloy.
Below horizon aircraft detection using deep learning for
vision-based sense and avoid.
In 2019 International
Conference on Unmanned Aircraft Systems (ICUAS),
pages 965970. IEEE, 2019. 3
Nikhil Varma Keetha, Chen Wang, Yuheng Qiu, Kuan
evolving graph embedding for object identification. In
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 84078416, 2022.
JE Kuchar and Ann C Drumm.
The traffic alert and
collision avoidance system. Lincoln laboratory journal,
John Lai, Jason J Ford, Peter OShea, and Rodney
Walker. Hidden markov model filter banks for dim target
detection from image sequences. In 2008 Digital Image
John Lai, Luis Mejias, and Jason J Ford.
Airborne
vision-based collision-detection system. Journal of Field
John Lai, Jason J Ford, Luis Mejias, and Peter OShea.
Characterization of sky-region morphological-temporal
airborne collision detection. Journal of Field Robotics,
Dong Liang, Zongqi Wei, Dong Zhang, Qixiang Geng,
Liyan Zhang, Han Sun, Huiyu Zhou, Mingqiang Wei, and
Pan Gao. Learning calibrated-guidance for object detec-
tion in aerial images. arXiv preprint arXiv:2103.11399,
Changliu Liu and Masayoshi Tomizuka.
Control in a
safe set: Addressing safety in human-robot interactions.
In IEEEACM International Conference on Human-Robot
Guido Manfredi and Yannick Jestin. An introduction to
acas xu and the challenges ahead. In 2016 IEEEAIAA
35th Digital Avionics Systems Conference (DASC), pages
Rebecca Martin, Clement Fung, Nikhil Keetha, Lujo
mation for improving robustness in long range aircraft
detection. In 2024 IEEERSJ International Conference
on Intelligent Robots and Systems (IROS), pages 10431
Jeffrey W McCandless.
Detection of aircraft in video
sequences using a predictive optical flow algorithm.
Optical Engineering, 38(3):523530, 1999. 3
Luis Mejias, Scott McNamara, John Lai, and Jason Ford.
Vision-based detection and tracking of aerial targets for
uav collision avoidance. In 2010 IEEERSJ International
Conference on Intelligent Robots and Systems, pages 87
Timothy L Molloy, Jason J Ford, and Luis Mejias.
Detection of aircraft below the horizon for vision-based
detect and avoid in unmanned aircraft systems. Journal
of Field Robotics, 34(7):13781391, 2017. 3
Tamas G. Molnar, Suresh K. Kannan, James Cunning-
Collision avoidance and geofencing for fixed-
wing aircraft with control barrier functions, 2024. URL
Muhammad Ali Murtaza, Sergio Aguilera, Vahid Azimi,
and Seth Hutchinson. Real-time safety and control of
robotic manipulators with torque saturation in operational
space. In 2021 IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 702708,
Andreas Nussberger, Helmut Grabner, and Luc Van Gool.
Aerial object tracking from an airborne platform. In 2014
international conference on unmanned aircraft systems
(ICUAS), pages 12841293. IEEE, 2014. 3
Michael P. Owen, Sean M. Duffy, and Matthew W. M.
Edwards.
Unmanned aircraft sense and avoid radar:
Surrogate flight testing performance evaluation. In 2014
IEEE Radar Conference, pages 05480551, 2014. doi:
Jay Patrikar, Joao Dantas, Sourish Ghosh, Parv Kapoor,
Ian Higgins, Jasmine J Aloor, Ingrid Navarro, Jimin Sun,
Ben Stoler, Milad Hamidi, et al. Challenges in close-
proximity safe and seamless operation of manned and
unmanned aircraft in shared airspace.
arXiv preprint
Jay Patrikar, Joao Dantas, Brady Moon, Milad Hamidi,
Sourish Ghosh, Nikhil Keetha, Ian Higgins, Atharva
airspace operations. Scientific Data, 12(1):468, 2025. 3
Stavros Petridis, Christopher Geyer, and Sanjiv Singh.
Learning to detect aircraft at low resolutions. In Inter-
national Conference on Computer Vision Systems, pages
474483. Springer, 2008. 3
Vladimir Reilly, Haroon Idrees, and Mubarak Shah.
Detection and tracking of large number of targets in wide
area surveillance. In European conference on computer
Artem Rozantsev, Vincent Lepetit, and Pascal Fua. Fly-
ing objects detection from a single moving camera. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 41284136, 2015. 3
Minimum Operational Performance Standards
for Airborne Collision Avoidance System X (ACAS X)
(ACAS Xa and ACAS Xo), Volume I and Volume II. DO
385A. RTCA, 2023. Approved by the RTCA Program
Management Committee (PMC) as documented in June
Falk Schubert and Krystian Mikolajczyk.
Robust reg-
istration and filtering for moving object detection in
aerial videos.
In 2014 22nd International Conference
on Pattern Recognition, pages 28082813. IEEE, 2014.
Tianjun Shi, Jinnan Gong, Shikai Jiang, Xiyang Zhi,
Guangzhen Bao, Yu Sun, and Wei Zhang. Complex opti-
cal remote-sensing aircraft detection dataset and bench-
IEEE Transactions on Geoscience and Remote
Elysia Smyers, Sydney Katz, Anthony Corso, and
Mykel J Kochenderfer. Avoidds: Aircraft vision-based
intruder detection dataset and simulator.
Advances in
Neural Information Processing Systems, 36:80768091,
Eric Squires, Pietro Pierpaoli, and Magnus Egerstedt.
Constructive barrier certificates with applications to
fixed-wing aircraft collision avoidance.
In 2018 IEEE
Conference on Control Technology and Applications
(CCTA), pages 16561661, 2018. doi: 10.1109CCTA.
Eric Squires, Pietro Pierpaoli, Rohit Konda, Samuel
constraints with applications to decentralized fixed-wing
collision avoidance. CoRR, abs1906.03771, 2019. URL
Vladan Stojnic, Vladimir Risojevic, Mario Mustra, Ve-
dran Jovanovic, Janja Filipi, Nikola Kezic, and Zdenka
Babic. A method for detection of small moving objects
in uav videos. Remote Sensing, 13(4):653, 2021. 3
Bo Yang, Dongjian Tian, Songliang Zhao, Wei Wang,
Jun Luo, Huayan Pu, Mingliang Zhou, and Yangjun
Robust aircraft detection in imbalanced and sim-
ilar classes with a multi-perspectives aircraft dataset.
IEEE Transactions on Intelligent Transportation Systems,
Mingxin Yu, Chenning Yu, M-Mahdi Naddaf-Sh, Devesh
planning for manipulators with control barrier function-
induced neural controller, 2024. URL
Weiye Zhao, Tairan He, Tianhao Wei, Simin Liu, and
Changliu Liu. Safety index synthesis via sum-of-squares
Xingyi Zhou, Vladlen Koltun, and Philipp Krahenbuhl.
Tracking objects as points. In European Conference on
Computer Vision, pages 474490. Springer, 2020. 3, 5
