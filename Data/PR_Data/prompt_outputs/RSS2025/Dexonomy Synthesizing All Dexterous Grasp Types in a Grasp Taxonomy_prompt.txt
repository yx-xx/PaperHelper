=== PDF文件: Dexonomy Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy.pdf ===
=== 时间: 2025-07-22 09:42:51.838123 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Grasp Types in a Grasp Taxonomy
Jiayi Chen1,2, Yubin Ke1,2, Lin Peng2, He Wang1,2,3
1Peking University, 2Galbot, 3Beijing Academy of Artificial Intelligence
Equal contribution, Corresponding author
Fig. 1: For any grasp type in the GRASP taxonomy , any object, and any articulated hand, our pipeline efficiently
synthesizes contact-rich, penetration-free, and physically plausible dexterous grasps, starting from only one human-annotated
grasp template to specify an initial hand pose and contact information per hand and grasp type.
AbstractGeneralizable
dexterous
grasping
suitable
grasp types is a fundamental skill for intelligent robots. Devel-
oping such skills requires a large-scale and high-quality dataset
that covers numerous grasp types (i.e., at least those categorized
by the GRASP taxonomy), but collecting such data is extremely
challenging. Existing automatic grasp synthesis methods are often
limited to specific grasp types or object categories, hindering
scalability. This work proposes an efficient pipeline capable
of synthesizing contact-rich, penetration-free, and physically
plausible grasps for any grasp type, object, and articulated hand.
Starting from a single human-annotated template for each hand
and grasp type, our pipeline tackles the complicated synthesis
problem with two stages: optimize the object to fit the hand
template first, and then locally refine the hand to fit the object
in simulation. To validate the synthesized grasps, we introduce
a contact-aware control strategy that allows the hand to apply
the appropriate force at each contact point to the object. Those
validated grasps can also be used as new grasp templates to
facilitate future synthesis. Experiments show that our method
significantly outperforms previous type-unaware grasp synthesis
baselines in simulation. Using our algorithm, we construct a
dataset containing 10.7k objects and 9.5M grasps, covering
31 grasp types in the GRASP taxonomy. Finally, we train a
type-conditional generative model that successfully performs the
desired grasp type from single-view object point clouds, achieving
an 82.3 success rate in real-world experiments. Project page:
I. INTRODUCTION
Dexterous grasping is a fundamental skill for intelligent
though dexterous grasping has received increasing attention,
most prior work focuses on whether a dexterous hand can
successfully grasp an object, rather than considering how to
grasp it. As a result, the dexterous hand loses its dexterity and
becomes functionally similar to a large parallel gripper. True
dexterous grasping is not merely about grasping with dexter-
ous hands, but about grasping dexterously with appropriate
grasp types based on the task requirement. For example, when
a robot needs to securely grasp an apple or hold a knife to cut,
it should use a power grasp to envelop the object. Conversely,
when grasping a lightweight or flat object from the table, a
precision grasp using the fingertips would be more suitable.
To develop such intelligent skills, there are two key chal-
task and (2) generating high-quality grasps for specified types
and objects. The first challenge is a high-level decision-making
problem and can take advantage of recent advances in large
vision-language models, e.g., GPT-4o , as a temporary
solution. However, the second challenge is less studied and
represents a significant bottleneck, which is the main focus
of this paper. To address the problem of type-aware grasp
synthesis with data-driven methods, the first step is to build a
large-scale grasp dataset that at least includes most of the grasp
types in the GRASP taxonomy . However, collecting and
annotating grasp data, particularly for multi-fingered hands in
contact-rich scenarios, remains a big challenge.
Several approaches have been explored for automatically
synthesizing a large-scale dexterous grasp dataset, but most
of them suffer from various limitations. Analytical grasp
synthesis methods [47, 44, 25, 8, 7] are often applicable
to any object, but most of them are type-unaware and the
synthesized grasps only belong to limited types. This is
because specifying flexible grasp types solely through ana-
lytical metrics is challenging. Moreover, these methods often
produce unnatural hand poses, as they prioritize force closure,
which does not always align with human habits. Another line
of research [55, 48, 52] focuses on transferring functional
dexterous grasps by mapping object contact regions. While
these methods generate more human-like grasps and support
a wider range of grasp types, they are limited to objects
that are geometrically similar or axis-aligned with the initial
In this work, we propose a novel pipeline based on sampling
and optimization to address these challenges. As shown in
Figure 1, our algorithm can efficiently synthesize high-quality
dexterous grasps for any grasp type, object, and articulated
hand and grasp type. Our synthesized grasps achieve rich
hand-object contact (e.g., > 10 hand links within 2 mm
of the object for power grasps), guarantee penetration-free
poses via collision mesh verification, and satisfy force closure
under six-axis testing in MuJoCo   all with shared
hyperparameters across grasp types, objects, and hands.
Our key insight is that grasping can be framed as a geomet-
ric matching problem, where the hand and object should align
through contact points. We begin by introducing a human-
annotated grasp template that specifies the initial hand pose
and contact information (i.e., points and normals). Unlike
previous methods that directly adjust the hand pose to fit
the object, we first use a lightweight stage that samples and
optimizes the object pose to match the hand contacts defined in
the grasp template. This stage supports hundreds of thousands
of initial samples processed in parallel on a single GPU and
leaves only a small number of promising results for the next
stage. The combination of dense sampling and optimization
helps avoid local optima without sacrificing efficiency.
After aligning the object pose, the hand only needs a slight
refinement to get a good grasp. This dual-stage design not
only eases the hand refinement stage, but also ensures that the
final grasp remains similar to the initial grasp template and
thus remains natural. In contrast to most prior work [24, 47,
to refine the hand pose, we propose a novel method based on
the transposed Jacobian control in MuJoCo. This approach is
key to achieving rich contacts while ensuring no penetration,
with minimal coding effort and parameter tuning.
assess their ability to withstand external forces applied to the
object. Unlike previous work [47, 60] that designs heuristics to
squeeze the hand for applying force on the object, which is not
suitable for all grasp types, we design a contact-aware control
strategy that computes the desired forces for each contact point
and controls the hand to apply them approximately, also based
on the transposed Jacobian control. Finally, high-quality grasps
that pass the simulation tests can be used to construct new
grasp templates, reducing the need for human annotations and
broadening the range of objects that can be grasped.
Experiments show that our method greatly outperforms
previous type-unaware grasp synthesis baselines in simulation,
especially under more challenging testing conditions (i.e.,
smaller friction coefficients) and with a wider variety of ob-
jects (i.e., from Objaverse ). Using our proposed pipeline,
we also build a large-scale dataset covering different grasp
types. This dataset further enables training a type-conditional
generative model that generates desired grasp types for novel
objects from single-view point clouds, achieving a success
rate of 82.3 on the Shadow hand in real-world experiments.
annotation UI for collecting semantic grasps on the specified
object regions with only a few mouse clicks.
In summary, our main contributions are:
An efficient pipeline to synthesize high-quality grasps for
any grasp type, object, and articulated hand, starting from
one human-annotated template per hand and grasp type.
A large-scale dataset containing 9.5M grasps and 10.7k
A type-conditional generative model that can use the
specified grasp types to grasp novel objects in the real
An annotation UI for collecting semantic grasps with only
a few mouse clicks.
II. RELATED WORK
A. Analytical Grasp Synthesis
Analytical grasp synthesis aims to find good grasps, typ-
ically measured by wrench-based force closure metrics [21,
40]. These methods generally assume complete knowledge
of the objects geometry for the metric calculation, making
them more suitable for data preparation than for real-world
For parallel grippers and suction cups, where hand-object
contact patterns are simple and the dimension of the hand pose
is low (around 7), many work [32, 33, 19, 4] randomly sample
a large number of grasps and select those with better metrics
as the final results. For dexterous hands, which have more
complex contact patterns and higher pose dimensions (often
exceeding 20), random sampling is insufficient to find good
sampling-based optimization, such as simulated annealing,
since the commonly used metrics are non-differentiable. More
recent approaches [27, 43, 44, 47, 6, 25, 8, 7] introduce
differentiable energies to leverage gradient-based optimization.
requiring many iterations to adjust the hand pose, which can be
poses. Our approach benefits from both random sampling and
gradient-based optimization, greatly reducing the complexity.
B. Grasp Taxonomy
The GRASP taxonomy  categorizes 33 distinct grasp
types based on human daily activities. Many previous analyt-
ical methods [6, 7, 25, 8] can only synthesize limited grasp
47] supports contact beyond the fingertip, they can only syn-
thesize some simple grasp types, e.g. warping fingers around
the object, and typically have high randomness and cannot
specify a desired type. Some work on functional dexterous
grasping [23, 48, 1, 52, 61] t
