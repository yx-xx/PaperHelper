=== PDF文件: DemoGen Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning.pdf ===
=== 时间: 2025-07-22 09:42:17.559614 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：for Data-Efficient Visuomotor Policy Learning
Zhengrong Xue123, Shuying Deng1, Zhenyang Chen2, Yixuan Wang1, Zhecheng Yuan123, Huazhe Xu123
1Tsinghua University, 2Shanghai Qi Zhi Institute, 3Shanghai AI Lab,
Equal contribution
demo-generation.github.io
Disturbance Resistance
Obstacle Avoidance
Dexterous Hands
Deformable
Bimanual
Poor Generalization
Strong Generalization
One Human-Collected Demo
"One-Shot" Imitation
Versatile Skills  Platforms
Extended O.O.D. Capabilities
Fig. 1: DemoGen is a fully synthetic approach for automatic demonstration generation. DemoGen promotes the spatial generalization ability
of visuomotor policies and can facilitate one-shot imitation by adapting one human-collected demonstration into novel object configurations.
DemoGen applies to various manipulation tasks and platforms and can be extended to enable additional out-of-distribution capabilities.
AbstractVisuomotor policies have shown great promise in
robotic manipulation but often require substantial human-
collected data for effective performance. A key factor driving
the high data demands is their limited spatial generalization
ferent object configurations. In this work, we present DemoGen,
a low-cost, fully synthetic approach for automatic demonstration
generation. Using only one human-collected demonstration per
by adapting the demonstrated action trajectory to novel object
configurations. Visual observations are synthesized by leveraging
3D point clouds as the modality and rearranging the subjects
in the scene via 3D editing. Empirically, DemoGen significantly
enhances policy performance across a diverse range of real-world
manipulation tasks, showing its applicability even in challenging
scenarios involving deformable objects, dexterous hand end-
be extended to enable additional out-of-distribution capabilities,
including disturbance resistance and obstacle avoidance.
I. INTRODUCTION
Visuomotor policy learning has demonstrated remarkable
competence for robotic manipulation tasks [8, 65, 17, 63], yet
it typically demands large volumes of human-collected data.
State-of-the-art approaches often require tens to hundreds of
demonstrations to achieve moderate success on complex tasks,
such as spreading sauce on pizza  or making rollups with
a dexterous hand . More intricate, long-horizon tasks may
necessitate thousands of demonstrations .
One key factor contributing to the data-intensive nature of
these methods is their limited spatial generalization [44, 47,
encoders [35, 41, 36, 63], exhibit limited spatial capacity,
typically confined to regions adjacent to the demonstrated ob-
ject configurations. Such limitation necessitates repeated data
collection with repositioned objects until the demonstrated
configurations sufficiently cover the full tabletop workspace.
This creates a paradox: while the critical actions enabling
dexterous manipulation are concentrated in a small subset of
contact-rich segments, a substantial portion of human effort is
spent teaching robots to approach objects in free space.
A potential solution to reduce redundant human effort is
to replace the tedious relocate-and-recollect procedure with
automatic demonstration generation. Recent advances such as
MimicGen  and its subsequent extensions [22, 20, 24]
have proposed to generate demonstrations by segmenting the
demonstrated trajectories based on object interactions. These
object-centric segments are then transformed and interpolated
into execution plans that fit desired spatially augmented object
configurations. The resulting plans are then executed through
open-loop rollouts on the robot, termed on-robot rollouts, to
verify their correctness and simultaneously capture the visual
observations needed for policy training.
Despite their success in simulation, applying MimicGen-
style strategies to real-world environments is hindered by the
high costs of on-robot rollouts, which are nearly as expensive
as collecting raw demonstrations. An alternative is to deploy
via sim-to-real transfer [38, 48, 60], though bridging the sim-
to-real gap remains a significant challenge in robotics.
In this work, we introduce DemoGen, a data generation
system that can be seamlessly plugged into the policy learning
workflow in both simulated and physical worlds. Recognizing
the high cost of on-robot rollouts represents a major barrier
to practical deployment, DemoGen adopts a fully synthetic
pipeline that efficiently concretizes the generated plans into
spatially augmented demonstrations ready for policy training.
For action generation, DemoGen develops the MimicGen
strategy by incorporating techniques from Task and Motion
Planning (TAMP) [11, 6, 33], similar to the practice in the
recently released SkillMimicGen . Specifically, we decom-
pose the source trajectory into motion segments moving in
free space and skill segments involving on-object manipulation
through contact. During generation, the skill segments will be
transformed as a whole according to the augmented object
motion planning to connect the neighboring skill segments
after transformation.
With the processed actions in hand, a core challenge is
obtaining spatially augmented visual observations without
relying on costly on-robot rollouts. While some recent work
leverages vision foundation models to manipulate the appear-
ance of subjects and backgrounds in robotic tasks [59, 4, 2],
these techniques are not directly applicable to modifying the
spatial locations of objects in an image, as 2D generative
models generally lack awareness of 3D spatial relationships,
such as perspective changes .
DemoGen employs a more straightforward strategy: it se-
lects point clouds as the observation modality and synthesizes
the augmented visual observations through 3D editing. The
key insight is that point clouds, which inherently live in the 3D
augmentations. Generating augmented point cloud observa-
tions is reduced to identifying clusters of points corresponding
to the objects or robot end-effectors and then applying the
same spatial transformations used in the generated action
plans. Notably, this strategy also applies to contact-rich skill
ters that undergo uniform transformations. Furthermore, the
artificially applied transformations on point clouds accurately
reflect the underlying physical processes, thereby minimizing
the visual gap between real and synthetic observations.
evaluating the performance of visuomotor policies trained on
DemoGen-generated datasets from only one human collected
demonstration per task. To assess the impact of DemoGen
on spatial generalization, we adhere to a rigorous evaluation
protocol in which the objects are placed across the entire
tabletop workspace within the end-effectors reach.
We conduct extensive real-world experiments, showing that
DemoGen can be successfully deployed on both single-arm
and bi-manual platforms, using parallel-gripper and dexterous-
hand end-effectors, from both third-person and egocentric
observation viewpoints, and with a range of rigid-body and
deformablefluid objects. Meanwhile, the cost of generating
one demonstration trajectory with DemoGen is merely 0.01
seconds of computation. With such minimal cost, DemoGen
significantly enhances policy performance, generalizing to
un-demonstrated configurations and achieving an average of
74.6 across 8 real-world tasks. Additionally, we demon-
strate that simple extensions under the DemoGen framework
can further equip imitation learning with acquired out-of-
distribution generalization capabilities such as disturbance
resistance and obstacle avoidance. The code and datasets will
be open-sourced to facilitate reproducibility of our results.
Please refer to the project website for robot videos.
II. RELATED WORKS
A. Visuomotor Policy Learning
Represented by Diffusion Policy  and its extensions [63,
tion learning methods that learn to predict actions directly from
visual observations in an end-to-end fashion . The end-to-
end learning objective is a two-edged sword. Its flexibility
enables visuomotor policies to learn dexterous skills from
human demonstrations, extending beyond rigid-body pick-and-
place. However, the absence of structured skill primitives
makes such policies intrinsically data-intensive.
The conflicts between the huge data demands and the great
expense of robotic data collection have driven the growing
attention to data-centric research. Such efforts include more
efficient data collection systems [10, 7, 30], collaborative
gathering of large-scale datasets [37, 27], and empirical studies
on data scaling [66, 31]. Instead of scaling up via pure human
can help save much of the human effort.
B. TAMP-Based Imitation Learning
Attempting to develop manipulation policies from only a
handful of demonstrations, data-efficient imitation learning
methods often build on the principles of Task and Motion
Planning (TAMP) [11, 6, 33], while incorporating imitation
learning to replace some components in the TAMP pipeline. A
common approach is to learn the end-effector poses for picking
and placing [64, 45, 55, 57, 18, 46]. The whole trajectories
are generated using motion planning toolkits  and then
executed in an open-loop manner. Some methods extend this
idea to more complex scenarios by learning to estimate the
states of manipulated objects in the environment and replaying
demonstrated trajectory segments centered around the target
Button-Large
Single Demo
Sparse Demos
Dense Demos
Full Demos
Button-Small
Fig. 2: Qualitative visualization of the spatial effective range. The grid maps display discretized tabletop workspaces from a birds-eye
view under different demonstration configurations. Dark green spots mark the locations where buttons are placed during the demonstrations.
Each grid cell corresponds to a policy rollout with the button placed at that location. Blue, yellow, green, and gray grids denote successful
executions for the Button-Large, Button-Small, both tasks, and no tasks, respectively.
objects [25, 49, 12, 13].
