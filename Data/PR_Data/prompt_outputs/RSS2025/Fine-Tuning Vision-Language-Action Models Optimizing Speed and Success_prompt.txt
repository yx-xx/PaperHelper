=== PDF文件: Fine-Tuning Vision-Language-Action Models Optimizing Speed and Success.pdf ===
=== 时间: 2025-07-22 09:42:56.579194 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Fine-Tuning Vision-Language-Action Models:
Optimizing Speed and Success
Moo Jin Kim1
Chelsea Finn1
Percy Liang1
AbstractRecent vision-language-action models (VLAs) build
upon pretrained vision-language models and leverage diverse
robot datasets to demonstrate strong task execution, language
following ability, and semantic generalization. Despite these
tuning to achieve good performance, yet how to most effectively
fine-tune them is unclear given many possible strategies. In
this work, we study key VLA adaptation design choices such
as different action decoding schemes, action representations,
and learning objectives for fine-tuning, using OpenVLA as our
representative base model. Our empirical analysis informs an
Optimized Fine-Tuning (OFT) recipe that integrates parallel
and a simple L1 regression-based learning objective to altogether
improve inference efficiency, policy performance, and flexibility
in the models input-output specifications. We propose OpenVLA-
art on the LIBERO simulation benchmark, significantly boosting
OpenVLAs average success rate across four task suites from
76.5 to 97.1 while increasing action generation throughput
by 26. In real-world evaluations, our fine-tuning recipe enables
OpenVLA to successfully execute dexterous, high-frequency con-
trol tasks on a bimanual ALOHA robot and outperform other
VLAs (0 and RDT-1B) fine-tuned using their default recipes,
as well as strong imitation learning policies trained from scratch
(Diffusion Policy and ACT) by up to 15 (absolute) in average
success rate. We release code for OFT and pretrained model
checkpoints at
I. INTRODUCTION
Recent vision-language-action models (VLAs)robot poli-
cies built by fine-tuning pretrained vision-language models on
large-scale robot datasets for low-level robotic controlhave
demonstrated strong task performance, semantic generaliza-
tasks [4, 34, 24, 23, 7, 15, 8, 53, 58, 3]. Despite their strengths,
fine-tuning is crucial for satisfactory deployment of VLAs on
novel robots and tasks, yet it is unclear what the most effective
approach for adaptation is given the large design space. A
robotics practitioner who wishes to fine-tune a VLA to a new
robot setup and task may default to using the same training
recipe used for pretraining (or a parameter-efficient variant),
but it is not obvious whether this would yield the best policy,
and there is limited empirical analysis of alternative fine-tuning
approaches in the literature.
1Stanford University. Correspondence to: Moo Jin Kim <moojinkcs.
stanford.edu>, Chelsea Finn <cbfinncs.stanford.edu>, Percy
Liang <pliangcs.stanford.edu>.
Prior work has begun exploring VLA adaptation strategies,
with Kim et al.  proposing parameter-efficient fine-tuning
via LoRA. However, their autoregressive action generation
remains too slow (3-5 Hz) for high-frequency control (25-
50 Hz), and both LoRA and full fine-tuning of autoregressive
VLAs often yield unsatisfactory performance in bimanual ma-
nipulation tasks [54, 27, 3]. While recent approaches improve
efficiency through better action tokenization schemes [2, 38],
achieving 2 to 13 speedups, significant latency between
action chunks (e.g., 750 ms for the recent FAST approach )
still limits real-time deployment on high-frequency bimanual
robots. Exploring alternative VLA adaptation approaches that
achieve both satisfactory speed and quality remains an under-
explored area of research.
In this work, we study key design decisions for adapt-
ing VLAs to novel robots and tasks using OpenVLA, a
representative autoregressive VLA, as our base model. We
examine three key design choices: action decoding scheme
(autoregressive vs. parallel generation), action representation
(discrete vs. continuous), and learning objective (next-token
prediction vs. L1 regression vs. diffusion). Our study reveals
several key insights that build on each other: (1) parallel
decoding with action chunking not only boosts inference
efficiency but also improves success rates on downstream tasks
while enabling greater flexibility in the models input-output
specifications; (2) continuous action representations further
improve model quality compared to discrete representations;
and (3) fine-tuning the VLA with an L1 regression objective
yields comparable performance to diffusion-based fine-tuning
while offering faster training convergence and inference speed.
Building on these insights, we introduce OpenVLA-OFT:
an instantiation of an Optimized Fine-Tuning (OFT) recipe that
integrates parallel decoding and action chunking, continuous
action representations, and an L1 regression objective to en-
hance inference efficiency, task performance, and model input-
output flexibility while maintaining algorithmic simplicity.
We conduct experiments on both the standardized LIBERO
simulation benchmark and dexterous tasks on a real bimanual
ALOHA robot. In LIBERO, OpenVLA-OFT establishes a new
state of the art by achieving 97.1 average success rate across
four task suites, outperforming both fine-tuned OpenVLA
policies  (76.5) and 0 policies  (94.2) while
achieving a 26 speedup in action generation with 8-step
action chunks. For real-world ALOHA tasks , we augment
our recipe with FiLM  for enhanced language grounding,
Fig. 1: OpenVLA-OFT on the bimanual ALOHA robot. Our Optimized Fine-Tuning (OFT) recipe enhances fine-tuned OpenVLA policies through
improved inference efficiency, model quality, and input-output flexibility. The resulting OpenVLA-OFT policies execute diverse dexterous manipulation tasks
on a real-world bimanual robot at high control frequencies (25 Hz). The  suffix indicates the integration of feature-wise linear modulation (FiLM) ,
which strengthens language grounding in tasks where accurate language understanding is critical for success.
denoting the augmented recipe as OFT. OpenVLA-OFT
successfully executes dexterous bimanual tasks like folding
clothes and manipulating target food items based on the users
prompt (see Figure 1), outperforming both fine-tuned VLAs
(0 and RDT-1B) and prominent imitation learning policies
trained from scratch (Diffusion Policy and ACT) by up to 15
(absolute) in average success rate. With 25-timestep action
base OpenVLA, demonstrating that our new fine-tuning recipe
enables real-time robot control with strong task performance
and language following ability.
II. RELATED WORK
Prior works have leveraged language and vision founda-
tion models to enhance robotic capabilities, using them as
pretrained visual representations that accelerate robotic policy
learning [30, 33, 31, 20, 32], for object localization in robotics
tasks [9, 47], and for high-level planning and reasoning [1, 17,
fine-tuning vision-language models (VLMs) to directly predict
low-level robotic control actions, producing vision-language-
and generalization to out-of-distribution test conditions
and unseen semantic concepts. These works focus primarily on
model development, while we focus on developing a recipe for
fine-tuning such models, justifying individual design decisions
with insights that we gain from our empirical analysis.
Despite the importance of fine-tuning for real-world VLA
remains limited. While Kim et al.  study various parameter
update strategies and from their findings show that LoRA fine-
tuning enables effective adaptation to single-arm robots oper-
ating at low control frequencies (< 10 Hz), their analysis does
not extend to bimanual robots with high control frequencies
(25-50 Hz), a more complex control scenario. We address
this gap by exploring VLA adaptation design decisions for fast
inference and reliable task execution on a real-world bimanual
manipulator with a 25 Hz controller.
Recent works by Belkhale and Sadigh  and Pertsch
et al.  improve VLA efficiency through new action to-
kenization schemes, using vector quantization or discrete co-
sine transform-based compression to represent action chunks
(sequences of actions) with fewer tokens than simple per-
dimension binning (as used in RT-2  and OpenVLA ).
While these approaches achieve 2 to 13 speedups for
autoregressive VLAs, we explore design decisions beyond
autoregressive modeling, which remains inherently limited by
iterative generation. Our parallel decoding approach, when
paired with action chunking, achieves significantly greater
(0.07 ms for single-arm tasks with one input image and 0.321
ms for bimanual tasks with three input images).
Another line of research [54, 27, 3] demonstrates effective
VLA fine-tuning for high-frequency, bimanual manipulation
using generative approaches like diffusion or flow match-
Fig. 2: Key design decisions for VLA fine-tuning. Left: Comparison between autoregressive decoding, which generates actions sequentially, and parallel
next-token prediction and continuous action values with L1 regression or diffusion modeling objectives. The original OpenVLA training scheme includes
autoregressive decoding, discrete actions, and next-token prediction.
ing. While these diffusion-based VLAs achieve higher action
throughput than autoregressive VLAs by generating multi-
timestep action chunks simultaneously, they introduce compu-
tational trade-offs through slower training and multiple denois-
ing or integration steps at inference time. Furthermore, these
diffusion VLAs vary considerably in architecture, learning
specificationsand which design elements most significantly
impact performance remains unclear. Through controlled ex-
regression objective can match more complex approaches in
task performance while achieving significantly greater infer-
ence efficiency.
low-level control policy in addition to a slower VLA ,
in this work we fine-tune the base VLA end-to-end with
techniques that enable high efficiency without needing a
separate controller. Further, unlike other works that collect
online interaction data to continually update specialist policies
via reinforcement learning , here we focus on the sim-
pler imitation learning paradigm in which we develop high-
perf
