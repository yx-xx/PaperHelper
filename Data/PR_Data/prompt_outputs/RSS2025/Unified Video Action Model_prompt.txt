=== PDF文件: Unified Video Action Model.pdf ===
=== 时间: 2025-07-21 14:24:37.236184 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Unied Video Action Model
Shuang Li
Yihuai Gao
Dorsa Sadigh
Shuran Song
Stanford University
Observation
Forward Dynamics Inverse Dynamics
Video Model
Observation
Joint Latent
Action Diffusion
Video Diffusion
Joint Sequence Model
PolicyPlanner
(b) Random masked training supports exible Input-Output conguration
Predicted
(a) UVA Overview
Unied  Video Action Model
Fig. 1: Unied Video Action Model. (a) UVA features a joint video-action latent representation and decoupled video-action decoding. The
joint latent representation effectively models the underlying relationships between video and action sequences, while the decoupled diffusion
enables high-speed action inference by bypassing video generation. (b) By leveraging masked training, UVA supports exible input-output
combinations for actions and videos. This versatility allows the model to function as a robot policy, video model, forward or inverse dynamics
AbstractA unied video and action model holds signicant
promise for robotics, where videos provide rich scene infor-
mation for action prediction, and actions provide dynamics
information for video prediction. However, effectively combining
video generation and action prediction remains challenging, and
current video generation-based methods struggle to match the
performance of direct policy learning in action accuracy and
inference speed. To bridge this gap, we introduce the Unied
Video Action model (UVA), which jointly optimizes video and
action predictions to achieve both high accuracy and efcient
action inference. The key lies in learning a joint video-action
latent representation and decoupling video-action decoding. The
joint latent representation bridges the visual and action domains,
effectively modeling the relationship between video and action
sequences. Meanwhile, the decoupled decoding, powered by two
lightweight diffusion heads, enables high-speed action inference
by bypassing video generation during inference. Such a uni-
ed framework further enables versatile functionality through
masked input training. By selectively masking actions or videos, a
single model can tackle diverse functions beyond policy learning,
such as forward and inverse dynamics modeling and video
generation. Via an extensive set of experiments, we demonstrate
that UVA can serve as a general-purpose solution for a wide range
of robotics tasks without compromising performance compared
to methods tailored for specic applications. Results are best
viewed on our website.
I. INTRODUCTION
A unied video and action model that jointly learns an
agents actions and their effects on visual observations holds
great promise for robotics  videos provide rich environmental
context for predicting actions, while actions reveal how inter-
actions drive visual changes, enabling more accurate modeling
of real-world dynamics. However, despite its promise, previous
approaches have often failed to fully realize this potential.
A key challenge lies in the inherent mismatch between the
requirements of action and video generation. Action modeling
demands high temporal speed to capture dense, ne-grained
tion to produce high-delity visual outputs, which often results
in slower processing speeds.
Previous policy learning approaches have struggled to bal-
ance these conicting requirements, often focusing on one
aspect at the expense of the other. For instance, action only
methods like [10, 25, 51] bypass video generation entirely.
While such approaches reduce computational complexity, they
overlook the benets of video generation  adding observation
supervision helps the model learn scene dynamics, which
reduces overtting to action history and enhances robustness
to visual disturbances. On the other hand, video generation
methods such as [14, 28] often rst generate high-resolution
videos and then predict actions based on the generated videos.
While this hierarchical approach can utilize existing video
slower processing speeds and the propagation of errors from
the generated video into action prediction.
To address these limitations, we propose UVA, a Unied
Video and Action Model designed to simultaneously model
videos and actions  capturing the underlying interactions be-
tween visuals and actions to enhance task understanding, while
maintaining high-speed action prediction during inference. We
propose the following three design choices to achieve this:
1) Unied Latent Video-Action Representation: UVA
introduces a unied latent representation that integrates both
visual and action data. Unlike traditional video generation
based policy methods which rely on a hierarchical video
and action generation, UVA is trained simultaneously with
supervision from both video and action data. This enables
the model to capture the intricate dynamics shared between
the visual and action domains with reduced computational
overhead. Utilizing the rich scene information encoded in the
latent representation unlocks UVAs superior performance in
understanding complex environments and delivering precise
action predictions.
2) Decoupled Video-Action Diffusion for Fast Inference:
To further enhance efciency and achieve inference speed
comparable to action-only methods, UVA decouples video
generation from action prediction. During training, the model
employs two lightweight diffusion heads to decode video
observations and actions from the unied latent space. At
generation entirely, directly utilizing the latent representation
for fast action prediction. This design enables real-time policy
deployment without sacricing performance, as it still retains
the rich representations learned during training from both
visual motions and robot action trajectories.
3) Mask Training for Flexibility: The ability to predict
both videos and actions through unied representations further
unlocks the potential to perform a diverse set of functions
using masked training. UVA can handle versatile functions that
go beyond traditional policy learning by masking inputs and
outputs as needed, as illustrated in Figure 1. This versatility en-
ables the model to tackle complex scenarios, such as operating
as a forward or inverse dynamics model, learning effectively
from video-only datasets where action labels are unavailable,
or simultaneously performing both low-level control and high-
level planning.
We evaluate UVA on seven publicly available benchmarks
to assess its diverse capabilities. UVA outperforms or matches
state-of-the-art approaches, demonstrating particularly strong
performance in multi-task settings. For instance, UVA outper-
forms the best baseline by 20 in success rate on PushT Mul-
titask [10, 16] and by 5 on Libero10 . The experiments
show that UVA can serve as a general-purpose framework
for different robotics tasks without compromising performance
compared to methods tailored for specic applications. In sum,
the unied video action model is:
that are tailored for robot policy learning [10, 25] or
inates the need for video generation during policy in-
duces the costs of the denoising process. As a result,
UVA achieves a similar speed as Diffusion Policy ,
making it practical for robot applications.
as a forward dynamics model for planning, as an inverse
dynamics model to generate actions, a video generation
II. RELATED WORK
There is a rich literature on video generation where the dom-
inant approach includes diffusion-based methods [3, 4, 18, 21
of related work involves the use of masked training in robot
learning. In this section, we will review both video generation
and masked training approaches used in robotics.
Video Generation for Policy Learning: Video models aid
policy learning by simulating task dynamics and predicting
future states. Models like [14, 28] leverage video generation
techniques to produce high-quality videos, which are then used
for action prediction. PAD  jointly trains video generation
and action prediction; however, it cannot predict future ac-
tions independently of future image generation, resulting in
slower inference. The work by  leverages video models to
generate object ow as an intermediate representation, which
captures physical interactions and is used to predict actions
for skill transfer across different robotic embodiments and
environments. In , a video diffusion model is ne-tuned
on robotics tasks, with the latent representations from the pre-
dicted videos serving as inputs to a policy network for action
prediction. Existing approaches that utilize video generation
for policy learning often suffer from slow inference speeds,
making them impractical for real-world applications. Addition-
as low-level policies  or image-tracking techniques ,
to extract actions from the generated videos. As a result, the
nal action accuracy suffers from compounded errors in video
generation and action prediction.
Video Generation as Dynamics Models: Video models can
serve as dynamics models by predicting future states condi-
tioned on current observations and actions, enabling robots to
simulate and plan tasks. GameGen-X  introduces a diffu-
sion transformer model for generating and controlling open-
world game videos, enabling interactive simulations. Genie
utilizes a foundation world model to transform static images
into interactive 3D environments, providing rich simulations
for embodied agents. Additionally,  demonstrates how
diffusion models can act as real-time game engines, gener-
ating dynamic and interactive scenarios to facilitate decision
making. These advancements highlight the versatility of video
generation models in robotic applications. In this work, we
propose a unied video and action model, showcasing its
ability to address both policy learning and dynamics modeling
within a single framework.
Masked Training: Recent works in robotics have explored
masked training techniques [31, 34, 48]. For example, Liu
et al.  and Wu et al.  randomly mask observations
and actions and reconstruct the missing portions. Their results
show that masked training improves generalization to down-
stream tasks and enables the model to be used for various
applications. However, these methods primarily rely on low-
dimensional state observations rather than videos, which are
Transformer
Observation
Repeat to
get N tokens
Action Chunk
Channel-wise
Flatten as N tokens
Future Observation Ot1
Actions At
Noisy O!"
Diffusion
Noisy At
Masked Future
Observation
Temporal-wise Concatenation
Diffusion
Flatten as N tokens
Fig. 2: Network Architecture. Given historical observations {Oth1, . . . , Ot} and corresponding action chunks {Ath, . . . , At1}, the
model predicts future observations {Ot1, . . . , Oth} and actions {At, . . . , Ath1}. Each image observation is represented as a sequence of
N tokens, and each image corresponds to an action chunk with L actions sampled at a higher frequency. During training, future observations
are used with randomly masked tokens, while at inference time, the model starts from an empty image to predict the complete image. History
using diffusion heads.
more natural but harder to predict. Radosavovic et al.
rst pretrain a model to predict actions or observations using
masked training and then netune the model or use a linear
probe for downstream tasks. Their work focuses solely on
action prediction results. In contrast, our method does not
require netuning for downstream tasks and can be directly
applied to various functions beyond policy learning, such as
video generation, forward dynamics, and inverse dynamics.
III. UNIFIED VIDEO ACTION MODEL
In robotics, we are interested in learning generalizable
policies that map observations to actions. However, this ob-
jective often tends to overt the training data, thereby limiting
the ability of learned policies to adapt to new scenarios.
In contrast, video generation [4, 36] demonstrates strong
generalization to novel scenes and supports training on datasets
without actions. However, effectively leveraging video data for
policy learning presents challenges such as the ability to match
the high temporal speed required for outputting dense, ne-
grained motions. In this section, we discuss our approach to
leveraging video-generation methods for robotics tasks.
Problem Statement: Given a sequence of image observations
{Oth1, . . . , Ot} and action chunks {Ath, . . . , At1},
dict the future actions {At, . . . , Ath01} and observations
{Ot1, . . . , Oth0}, where h0 is the future horizon. Each
action chunk, e.g., At 2 RLm consists of L actions, and each
action has m dimensions. We set h  h0 in the experiments.
For simplicity, we refer to both as h in the following sections.
We rst introduce the model with complete video and
action inputs and outputs (III-A-III-C). We then discuss
how masked training can exibly learn from any combination
of video and action data (III-D), enabling UVA to perform
various functions, including policy learning, video generation,
forward and inverse dynamics, and integrated policy and
planning.
As shown in Figure 2, our method encodes the history of
observations and actions (III-A), along with masked future
observations (III-B), and passes them to the Transformer
. For the masked observations, we randomly mask tokens
within the future observation frames during training and train
the model to reconstruct them. During inference, the model
generates the full set of tokens, starting from an empty
sequence. In III-C, we then discuss the choice of decoupling
video-action diffusion for fast inference addressing the high
temporal speed demand of robot policies.
A. Encode History
We rst process the historical image observations through
a pre-trained VAE encoder (kl-f16)  to obtain their latent
representations. Each image is encoded into a latent map of
dimensions Rwhc, where w and h represent the width and
and processed by a fully-connected (FC) layer, projecting each
element into a d-dimensional latent vector. Thus, each image
is represented as a sequence of N visual tokens, each with
d-dimensional features.
For history actions, we use a higher sampling frequency
compared to observations, as observations typically exhibit
redundancy and minimal changes over short time intervals.
Each image observation (e.g., Oth1) corresponds to L
actions within an action chunk (e.g., Ath). We repeat the
action chunk M times to match the number of visual tokens
as shown in Figure 2. The repeated sequence is then passed
through an FC layer, and converted into a sequence of N
action tokens, each with a d-dimensional latent representation.
These history visual and action tokens serve as conditions for
predicting future observations and actions.
B. Masked Autoencoder for Observation Prediction
Our work is closely related to [8, 27]. Their method focuses
on image generation conditioned on class labels. It begins by
generating a subset of visual tokens for the image and then
sequentially predicts additional tokens based on the previously
generated ones, following an autoregressive process to com-
plete the image. This step-by-step autoregressive approach has
been shown to outperform the single-step generation of all
visual tokens simultaneously. To facilitate step-by-step pre-
During training, some visual tokens are randomly masked, and
the model is trained to reconstruct these masked tokens.
We follow this setting for video prediction. Future ob-
servation frames {Ot1, . . . , Oth} are processed similarly
to historical observations: they are passed through a VAE
encoder to extract latent representations, followed by an FC
with a d-dimensional latent vector. Some tokens are randomly
masked out during training. These visual tokens are concate-
nated channel-wise with historical visual tokens and action
latent features. Latents from h different time steps are then
temporally concatenated with latent representations from other
time steps to produce a N h latent sequence. The resulting
sequence is passed through a Transformer to fuse the video
and action information, resulting in a set of joint video-
action latent representations, {Zt1, . . . , Zth}, where each
latent (e.g., Zt1) contain N latent tokens. These joint video-
action latent tokens are then used to reconstruct the future
observations and corresponding action chunks.
For tasks that involve language instructions, such as
cross-attention in the Transformer. The language input is
encoded into a d-dimensional token using the CLIP  text
encoder. To emphasize the language, this token is repeated M
times and appended to the N h video-action tokens, resulting
in a total of N h  M tokens. These tokens are then passed
through the Transformer to fuse the multimodal information.
We take the rst N h outputs of the Transformer as the joint
video-action latent representations, {Zt1, . . . , Zth}.
To minimize information leakage across different frames,
we consistently mask the same positions across all video
frames. At inference time, the model generates complete
videos by predicting all tokens starting from an empty se-
quence. At each autoregressive generation step, visual tokens
at the same position across all video frames are generated
simultaneously as shown in Supplementary X-A. Unlike
image generation conditioned on class labels or text, historical
observations provide rich contextual information about the en-
vironment. We found that a single-step generation is sufcient
to generate high-quality observations, while using additional
steps can further enhance the quality.
C. Decoupled Video and Action Diffusions
Previous video generation-based policy learning methods
rely on hierarchically generating videos rst and then predict-
ing actions, leading to slow speed and accumulated errors.
To address this, we propose decoupling video and action
prediction while training them jointly. During training, video
generation helps the latent representations Z capture more
detailed scene information, which benets action prediction.
During policy inference, where speed is crucial, the decoupled
design allows us to skip video generation and decode only
the actions. Similarly, for video generation, where quality is
the priority, we can perform multi-step autoregressive video
generation while bypassing action decoding.
We introduce two lightweight diffusion decoders for action
and video prediction (see Figure 2). Instead of performing the
denoising over the entire model , our approach restricts the
denoising process to the lightweight decoders, delivering more
efcient performance. This design preserves the generative
strengths of diffusion models while signicantly reducing
inference time.
The joint latent Z serves as the conditioning input for the
diffusion decoders. The video diffusion decoder processes each
latent token zi 2 Zt1  {z1, . . . , zN} to predict individual
patches in the video frame, which are then reshaped and
sent to the VAE decoder to reconstruct the full frame Ot1.
For the action diffusion decoder, all latent tokens in Zt1
are aggregated using a convolutional layer, followed by an
MLP layer, to produce an action latent. This latent encodes
both visual and action-related information for the current step
and serves as the condition for the action diffusion model to
generate the action chunk At. We use the diffusion head (base
size) from  for both action and video prediction.
During training, the decoders learn to predict the noise
added to noisy action chunks or video patches. The action
diffusion loss [21, 37, 39] is dened as:
Laction(Z, A)  E,k
k(A(k)k, Z)k2i
where A(k) represents the noisy actions, is the added noise,
k is the diffusion timestep, Z is the joint video-action latent
Lvideo(Z, O)  E,k
ki (Oi,(k)k, zi)k2
where Oi,(k) represents the i-th noisy visual token in the video
frame O(k) at diffusion timestep k, N is the total number of
visual tokens in a video frame, i is the added noise to the i-th
visual token, zi is the latent token in Z, and (Oi,(k)k, zi)
is the predicted noise for the i-th token.
The total loss at each time step is the combination of the
action and video diffusion losses: L  Laction  Lvideo. The
overall loss is calculated as the sum of these losses over the
time horizon h. During policy inference or video generation,
the decoders iteratively rene pure noise into actions or videos
using the learned denoising process.
D. Masked Training with Flexible Objectives
Instead of training the model solely on the task of predicting
future observations and actions based on historical data, we
propose a masked training approach with multiple training ob-
jectives using a unied framework. As illustrated in Figure 1,
the model is trained on ve distinct tasks by varying input
and output combinations. Unused components are masked and
replaced with a learned mask token. The action loss and video
loss are selectively applied to supervise the model depending
on the specic task.
This training approach enables us to fully utilize the data in
various combinations and supports the use of incomplete data,
such as video data without corresponding actions. This masked
training strategy enables the model to perform a diverse
range of functions, including acting as a robot policy, video
policy and planner. For instance, when given only image
model to generate action labels from videos. Additionally, this
strategy helps prevent overtting to specic tasks, enhancing
the models overall versatility and robustness.
IV. EVALUATION
In the following sections, we evaluate UVAs capacities as
policy V, a video generator VI, a forward dynamics model
tailored for the corresponding application.
V. UVA AS POLICY
We rst investigate the effectiveness of UVA on policy
learning. As noted by Kim et al. , different policy designs
excel in different settings. Their experiments show that while
Diffusion Policy  performs better in single-task setups,
it falls behind OpenVLA in multi-task scenarios. To com-
prehensively evaluate the performance of UVA as a policy,
we conduct extensive evaluations in single-task and multi-task
(Figure 4). For simulation tasks, we used the same random
seeds for different methods for a fair comparison. For real-
world tasks, to minimize evaluation bias, all evaluations use
public benchmarks with released datasetsno additional
training data were collected.
A. Simulation Benchmarks
Single-Task Evaluation: We rst evaluate single-task scenar-
compare UVA with the baselines on the PushT [10, 16] and
Toolhang  tasks. We report the success rates of the best-
performing checkpoint, averaging across 50 rollouts for PushT
and Toolhang, respectively.
Multi-Task Evaluation: We train one policy for multiple
task goals dened by image or text. We introduce a new
varying target T positions. We evaluate the best-performing
checkpoint over 50 rollouts and report its average reward.
Libero10  has 10 tasks. We evaluate each task in 50
different environments with varying random seeds and report
the average rewards across all 10 tasks. See Supplementary
X-B for details.
Single Task
ToolHang
Multi-Goal Push-T
Multi-Task Evaluation
Multi-Goal Libero10
Fig. 3: Simulation Environments. We evaluate UVA and baselines
in both single-task and multi-task settings. In the multi-task scenario,
the goal can be dened through the image observations (PushT-M)
or language descriptions (Libero10).
Single-Task "
Multi-Task "
Libero10
UVA-action
TABLE I: Policy Learning Results in Simulation. UVA has
higher success rate than the baselines in most settings, with a strong
performance in multi-task scenarios. Speed is measured by a single
action trajectory inference. All methods, except OpenVLA, infer 16
action steps per trajectory with 8 executed steps. OpenVLA infers
one action at a time, so it is run 8 times to match the inference time
for 8 executed actions.
B. Real-world Benchmarks
Training Data: We use two publicly available datasets in-
troduced by  and  without collecting any additional
training data. Both benchmarks collect data using the handheld
UMI  device. We used three tasks, including Cup Arrange-
and tested them on the ARX X5 arm.
Single-Task Evaluation: We train a single-task policy on the
Cup task and directly compare it with the Diffusion Policy
model provided by the author , both trained on the same
data. We evaluate each method over 20 rollouts with varying
initial congurations and report the average success rate.
Multi-Task Evaluation: We train one model with all three
tasks and then evaluate their performance on each task in-
dependently. We randomly selected 500 episodes from each
dataset and combined them into a dataset to train both our
model and Diffusion Policy . Since the training data were
collected independently in prior works, all evaluation cases are
Out-of-Distribution (OOD), involving unseen environments,
Cup Arrangement
Towel Folding
Mouse Arrangement
Training Data
Out-of-Distribution Multi-Task Testing Scenarios
Towel Folding (33.3)
Mouse Arrangement (33.3)
Fig. 4: Real-World Out-of-Distribution Evaluation. We use the training data provided by prior works [11, 46]. The test scenario is
signicantly out-of-distribution with unseen environments, objects, robots, and even gripper colors. The numbers in the parentheses show
the percentage of such category of test cases in evaluation. Please refer to our website for all evaluation rollouts.
Single-Task "
OOD Multi-Task "
TABLE II: Success Rate on Real-World UMI  tasks. We
compare UVA with DP-UMI which is designed for UMI tasks. UVA
performs worse than DP-UMI in the single-task setting but achieves
better performance in the multi-task setting. Both UVA and DP-UMI
use 16 denoising steps. Speed is measured by inferring a single action
trajectory consisting of 16 actions.
we include cases with varying initial congurations, object
(green), as shown in Figure 4. Each policy is evaluated over 60
for details.
C. Baselines
We compared with the following alternative methods, all
methods are trained or ne-tuned on the same data as our
model and tested using the same random seed and initial states.
Diffusion Policy  is a state-of-the-art visuomotor
policy model. We use both CNN-based network [DP-C]
and Transformer-based design [DP-T] from their original
implementation for all simulation tasks. For real-world
is optimized for UMI data. It leverages a CLIP-pretrained
ViT-B16  vision encoder, signicantly improving
visual understanding. We refer to it as [DP-UMI].
OpenVLA  is a state-of-the art vision-language-
action (VLA) built on 7B Llama 2  for multi-task
setting. It is trained on a diverse dataset encompassing a
wide range of robots, tasks, and environments. We ne-
tune OpenVLA on each task to optimize its performance.
0  is an open-source VLA model designed for
general-purpose robot control. It employs a ow match-
ing based architecture to generate continuous action se-
quences. Trained on a diverse dataset spanning multiple
robot embodiments and tasks, 0 demonstrates strong
zero-shot and ne-tuned performance. We use the of-
cially released checkpoints: 0 and 0-FAST, both ne-
tuned on Libero tasks.
UniPi  is a video-based policy model that generates
videos rst and then predicts actions based on the gener-
ated videos. Since the ofcial implementation is not avail-
relies on pixel-wise video generation, which results in
slower video generation speed. For action inference, we
train a model that processes two consecutive generated
video frames using a pretrained ResNet-50 for the PushT
and PushT-M tasks, and a ResNet-152 for the Tool Hang,
UVA-action is an ablation of UVA, where the video
generation part is excluded, and the model is trained
solely as a policy model. This baseline aims to evaluate
the effectiveness of joint video and action training.
D. Policy Learning Results
We evaluate policy learning results with UVA compared
to the baseline methods on a few different axes: 1) action
prediction accuracy, 2) inference speed, 3) robustness to visual
of joint video-action modeling.
Action Prediction Accuracy (Simulation Tasks): In Table I,
we compare UVA with baseline methods in both single-task
and multi-task settings. We use the same random seed for our
method and baselines to ensure a fair comparison.
Simulation Single-Task: Our method is able to match the
performance of the state-of-the-art model DP-C and signi-
cantly outperform other video-based methods such as UniPi
and vision-language-action model OpenVLA.
Simulation Multi-Task: Our method is particularly strong
in the multi-task setting. Specically, UVA surpasses the
best baseline by 20 on the PushT-M task and by 5
on the Libero10 benchmark. This result demonstrates that
UVA model is able to better learn and leverage the general
dynamics that are shared across tasks and, therefore, improve
overall performance in the multi-task setting.
ne-tuned from pretrained checkpoints using the entire Libero
Libero10. Moreover, 0 and 0-FAST use third-person and
wrist-view images plus robot proprioception, while UVA relies
only on third-person images. Despite its smaller size, limited
Action Prediction Accuracy (Real-World Tasks): Table II
shows the results of real-world tasks. We ensure a fair compar-
ison by keeping the initial placement of objects and grippers
identical across different methods for each test rollout.
Real-World Single-Task: First, we evaluate the policy perfor-
mance in a single-task setting. This evaluation aims to compare
our method with a strong baseline in prior works by replicating
a similar evaluation setup. Overall, UVA performs comparable
with DP-UMI, which is optimized with this particular training
dataset. We noticed that the dataset contains extensive recov-
ery data from the moments of failure to correct the policy.
This data is particularly useful for models without history
observations included in the recovery data. In contrast, our
model uses a longer history, which is advantageous for tasks
requiring longer memory. However, in this case, the collected
failure recovery data is less impactful for our model, as its
longer memory window prioritizes learning from extended
temporal patterns. While we could shorten our models history
than tailoring it to this specic task and training data.
Real-World Multi-Task: We train a single model using our
method and evaluate it on three tasks individually. DP-UMI is
trained and tested in the same manner for a fair comparison.
For each task, we test the methods on 20 different cases,
as shown in Figure 4. Our approach demonstrates superior
performance in the multi-task setting, achieving a 15 higher
success rate on the Cup task and a 40 higher success rate
on the Mouse task compared to DP-UMI.
In general, our method can successfully complete the task
with distractor objects or changing backgrounds but fails when
the background color is the same as the objects. Its visual
understanding could be enhanced by training on additional
video data without action labels. DP-UMI performs well on
GoalColor
BgObject
Fig. 5: Visual Disturbances on PushT. Tasks are performed under
altered visual conditions, including changes in background color,
distracting background objects, and goal color.
Normal " BgColor " BgObject " GoalColor "
TABLE III: Visual Generalization Results on PushT with Vi-
sual Disturbances. Our method and UniPi, both video generation
approaches.
the towel task but shows less stability when handling pick-
and-place cups and grabbing the mouse. However, it performs
well across different backgrounds due to its use of a pre-
trained vision encoder. When the gripper color is changed to
an unseen green, the performance of both methods slightly
decreases compared to the orange gripper used for training.
to the unseen gripper. Please refer to our website for details.
Inference Speed: Speed is evaluated based on a single action
trajectory inference. All methods, except OpenVLA, infer 16
action steps per trajectory with 8 executed steps. OpenVLA
infers one action at a time, requiring 8 runs to match the
inference time for 8 executed actions. UniPi generates raw
pixel videos, resulting in signicantly slower inference.
Our method achieves faster inference speeds by performing
diffusion iterations only on the lightweight action head, rather
than the entire network as DP-C and DP-T. Thanks to the
decoupled design, video generation can be skipped during
policy inference, further improving efciency. For simulation
C and DP-T, we follow their original implementations and also
perform denoising over 100 steps. With the same number of
diffusion steps (Table I), our method achieves faster inference
compared to DP-C and DP-T. Both 0 and 0-FAST achieve
faster inference speed than other methods.
For real-world tasks (Table II), both UVA and DP-UMI use
16 denoising steps for real-time manipulation. We found that
the UVA Attention module in the Transformer accounts for
half of the inference time, making UVA slightly slower than
DP-UMI. With future improvements, such as replacing the
Attention with Flash Attention, our model could achieve faster
speeds. We note that, although DP-UMI uses a pretrained
Fig. 6: Robustness to History Length on PushT-M. Typical policy
learning frameworks such as DP-C  often experience performance
drops with increased history length due to overtting, while our
model maintains robust performance.
ViT encoder, its model size (171.27M parameters) is smaller
than DP-C (262.69M parameters). This explains why DP-C is
slower than UVA in Table I and DP-UMI is faster in Table II.
performance across diverse settings.
Robustness to Visual Disturbances: We have shown that
UVA is robust to visual disturbances in the real-world multi-
task setting in Figure 4. All tests are unseen during training,
and even with more challenging distractor objects and back-
To more rigorously evaluate this visual generalization ca-
procedurally altering visual conditions in the PushT environ-
ment. The modications include changes to the background
on scenarios outside the training distribution, as the model
was trained only in the standard environment in Figure 3. The
results show that video generation methods, such as UniPi
and UVA, exhibit superior performance in handling visual
disturbances. For example, with changes in goal color, UniPi
achieves a success rate of 40, UVA achieves 64, while
OpenVLA only reaches 32.
Robustness to History Length: Prior policy learning meth-
as the history length increases as shown in Figure 6 evaluated
on the PushT-M task. In contrast, UVA can effectively adapt to
longer history inputs. By jointly predicting video and action,
our model sustains robust performance even as the history
length grows. This highlights the better potential of UVA for
tasks that require reasoning over extended temporal contexts.
Effect of Joint Video-Action Modeling: We evaluate this by
comparing UVA with a baseline (UVA-action) that removes
the video generation part. As shown in Table I, this modi-
cation led to reduced performance compared to the complete
in improving policy learning.
VI. UVA AS A VIDEO GENERATOR
UVA can function as a video generation model by bypassing
the action diffusion head during inference. We compare UVA
Libero10  CupArrange
UVA(1 step)
UVA(8 steps)
TABLE IV: Video Generation Results. Our method outperforms
UniPi in both simulated (Libero10) and real-world (Cup Arrange-
ment) environments. The masked autoencoder training enables au-
toregressive video generation, with 8 steps performing better than a
single step. FVD  results are reported.
with UniPi on video generation results across two datasets:
Libero10 and Cup Arrangement, in Table IV. Performance is
evaluated by computing the Frechet Video Distance (FVD)
for 500 videos generated by each method. FVD is a
metric for assessing video quality by evaluating visual delity
and temporal coherence. It compares statistical properties of
feature representations from real and generated videos, using
a pre-trained Inated 3D ConvNet . Lower FVD scores
indicate greater similarity.
The masked autoencoder training in our method (III-B)
facilitates video generation in an autoregressive manner, where
visual tokens are generated across all video frames in parallel
during the rst stage. In the subsequent stage, the next set of
tokens is predicted sequentially, conditioned on the previously
generated tokens. This iterative token prediction process con-
tinues until the entire video is generated. See Supplementary
X-A for details. In Table IV, we show that even with a
1-step process, our method outperforms UniPi on the more
challenging Cup Arrangement task, while using 8 steps further
improves performance.
Figure 7 shows the generated videos. Conditioned on the
history observations, UVA can predict future observations that
closely match the ground truth. Even with a single autore-
gressive step, it can generate realistic video frames, and with
additional steps, the details become more rened. In contrast,
UniPi occasionally produces blurry images, such as the Cup
Folding task and the Libero10 task (left). Additionally, UniPi
may fail to generate some objects entirely, as demonstrated in
the Libero10 task (the second moka pot in the right gure).
We believe that with more computational resources and larger
video generation models, both UVA and UniPi could achieve
further improvements. However, given similar computational
VII. UVA AS A FORWARD DYNAMICS MODEL
Our model can perform forward dynamics predictions
Ot1  fforward(Ot, At). To evaluate its effectiveness, we use
it to guide the behavior of a pretrained policy model, such
as the DP-C. We evaluate this approach in a block-pushing
to a specied square and another block to a second square,
each randomly assigned. During testing, we aim to control
the policy to complete specic tasks, such as pushing the red
block to the red square (R-R) or the red block to the green
Future Observation Prediction
Condition
Future Observation Prediction
Condition
arrangement
arrangement
Towel fold
Towel fold
Put both
moka pots on
the stove
Put both the
cream cheese
box and the
butter in the
Libero 10
Libero 10
Fig. 7: Video Generation Results on Validation Set. UVA generates high-quality videos that closely match the ground truth, with 8
autoregressive steps further enhancing detail compared to a single autoregressive step. In contrast, UniPi occasionally produces blurry (Cup
arrangement) or mismatched images (Towel fold and Libero10 left) and may fail to generate some objects (Libero10 right, the second moka
pot). With the similar computational resources, UVA consistently outperforms UniPi.
square (R-G). The evaluation considers four distinct settings,
as illustrated in Figure 8. Each setting is tested 10 times with
varying initial positions of the objects and the robot. Notably,
a perfect policy model in this setup would achieve a maximum
average success rate of 50, since training only requires one
block to be pushed to any square, while testing species exact
target assignments for each block.
At each step, we sample 100 trajectories of 16 future
actions using DP-C. The sampled actions, along with historical
observations by functioning as a forward dynamics model. For
each trajectory, we calculate a reward based on the predicted
The reward is computed as the distance between the blocks and
their target squares, which are identied from the predicted
Initial Observation
Red-Green
Green-Red
Green-Green
Fig. 8: Forward Dynamics Model on Block Pushing Task. During
proper action that moves a specic object to a specic target.
R-R " R-G " G-R " G-G " Avg. "
GT-Dynamics
TABLE V: Success Rate on Block Pushing. Our model functions as
a forward dynamics model to guide the behavior of pretrained policy
for trajectory selection increases the success rate to 60. Using a
ground-truth simulator provides an upper bound success rate of 75.
frames. We then execute the rst 6 steps of the selected
trajectory and resample new trajectories until the task is
completed or the episode ends. DP-C can complete the tasks
with a success rate of 38 in Table V. By leveraging our
model to guide the policy, the success rate increases to 60.
For comparison, we evaluate the performance of using a
ground-truth simulator to render the sampled trajectories and
select the best ones. Even with the ground-truth simulator,
the success rate is limited to 75 due to suboptimal sampled
trajectories and errors in object detection. While our model
performs worse than the simulator, it still signicantly en-
hances performance and guides the pretrained policy models
to complete required tasks.
VIII. UVA AS A INVERSE DYNAMIC MODEL
In this section, we evaluate the effectiveness of the proposed
method in an inverse dynamics setting, where actions are
inferred as At  finverse(Ot, Ot1) using UMI data. For the
UMI Cup Arrangement data, the robots actions are naturally
aligned with camera movements, enabling straightforward
evaluation of action prediction accuracy using the ground truth
camera poses obtained from motion capture (Mocap). Notably,
our model had never encountered this test data during training.
camera pose) generated by the inverse dynamics model used in
UniPi  and a well-engineered SLAM system used in UMI
, where the SLAM system requires an additional mapping.
The UMI actions in the training data are derived from SLAM.
pute the L2 distance to the ground truth actions from Mocap,
as shown in Table VI. The UniPi inverse dynamics model pre-
dicts actions from two consecutive images, which may lead to
discontinuities in the predicted actions over time. In contrast,
our method predicts 16 actions simultaneously, resulting in
Position
Rotation
UniPi Inverse Dynamics
Visual Inertial SLAM
TABLE VI: Inverse Dynamics. L2 distances between predicted
actions and ground truth actions from Mocap. The table compares
positions and rotation results of UVA, the UniPi inverse dynamics
UniPi inverse dynamics model, while SLAM achieves the best
accuracy but is more complex to implement.
more consistent and temporally coherent predictions. The ac-
tion errors produced by SLAM were 0.41 cm for position and
0.30 degrees for rotation. While UVA exhibited slightly higher
errors than SLAM, it still demonstrated strong performance
with position errors under 1 cm and rotation errors around 1
degree. These results highlight the generalization capability of
UVA for action prediction, even on unseen data, and suggest
that it could serve as a viable alternative to SLAM, which is
difcult to calibrate and suffers from a high failure rate.
IX. DISCUSSION
jointly models and separately decodes video and actions. This
design enables us to fully leverage video data as additional
prediction by skipping video decoding during inference. The
framework inherently supports masking training, allowing it
to fulll various robotics functions, including acting as a
and a combined policy and planner. By fully utilizing video
and action data in diverse congurations, our model reduces
overtting to specic tasks, outperforms previous methods,
and demonstrates versatility for multi-purpose applications.
Limitation and Future Work: One limitation of our frame-
work is that it does not currently leverage large amounts of
actionless video data, which could provide valuable additional
supervision. As a result, our method occasionally achieves
only comparable performance to the DP-UMI on real-world
tasks. We believe that pretraining the model on web-scale
video datasets could signicantly enhance its generalization
dict modalities beyond video and action, such as sound and
more comprehensive and versatile framework. This remains a
promising direction for future research.
ACKNOWLEDGMENT
We would like to thank Huy Ha for his valuable advice on
video recording and website design. We also thank Amber Xie,
Austin Patel, Jennifer Grannen, Vincent de Bakker, John So,
Max Du, and Vidhi Jain for their important feedbacks on the
paper draft. We are grateful to Mengda Xu, Suneel Belkhale,
Xiaomeng Xu, Fanqi Lin, Lirui Wang, and Tianhong Li for
helpful discussions. We would like to express our gratitude to
Chi Cheng, Zhenjia Xu, Chuer Pan, Zeyi Liu, Huy Ha, Fanqi
to the shared UMI dataset. Finally, we want to thank everyone
who contributed their computing resources to help us train the
This work was supported in part by the Toyota Re-
search Institute, NSF Award 1941722, 2143601, 2037101,
TIMAT project. The views and conclusions contained herein
are those of the authors and should not be interpreted as
necessarily representing the ofcial policies, either expressed
or implied, of the sponsors.
REFERENCES
Dosovitskiy Alexey. An Image Is Worth 16x16 Words:
Transformers for Image Recognition at Scale.
preprint arXiv:2010.11929, 2020.
Kevin Black, Noah Brown, Danny Driess, Adnan Es-
language-action ow model for general robot control,
2024. URL  orgabs2410.24164.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel
Stable Video Diffusion: Scaling Latent Video Dif-
fusion Models to Large Datasets.
arXiv preprint
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy
as World Simulators, 2024.
Jake Bruce, Michael D Dennis, Ashley Edwards, Jack
Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al.
International Conference on Machine Learning, 2024.
Joao Carreira and Andrew Zisserman. Quo Vadis, Action
Recognition? A New Model and the Kinetics Dataset. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 62996308, 2017.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and
William T Freeman.
Image Transformer.
In Proceedings of the IEEECVF
Conference on Computer Vision and Pattern Recognition,
Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and
Hao Chen.
video generation.
arXiv preprint arXiv:2411.00769,
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchel, Russ Tedrake, and Shuran
Song. Diffusion policy: Visuomotor policy learning via
action diffusion. The International Journal of Robotics
Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau,
Benjamin Burchel, Siyuan Feng, Russ Tedrake, and
Shuran Song. Universal Manipulation Interface: In-the-
Wild Robot Teaching Without In-the-Wild Robots. arXiv
preprint arXiv:2402.10329, 2024.
Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, and
Suneel Belkhale. Action-free reasoning for policy gen-
eralization. arXiv preprint arXiv:2502.03729, 2025.
Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo,
Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang
Autoregressive Video Gen-
eration Without Vector Quantization.
arXiv preprint
Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Or
Abbeel. Learning universal policies via text-guided video
generation. Advances in Neural Information Processing
Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam-
ing Transformers for High-Resolution Image Synthe-
In Proceedings of the IEEECVF Conference on
Computer Vision and Pattern Recognition, pages 12873
Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez,
Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny
Implicit
Behavioral Cloning.
Conference on Robot Learning
(CoRL), November 2021.
Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping
Autoregressive Generation in Video Diffusion Models.
arXiv preprint arXiv:2406.10981, 2024.
Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin
Factorizing Text-to-Video Generation by Explicit Image
Conditioning. arXiv preprint arXiv:2311.10709, 2023.
Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen
Prediction with action: Visual policy learning via joint
denoising process. In The Thirty-eighth Annual Confer-
ence on Neural Information Processing Systems, 2024.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,
Piotr Dollar, and Ross Girshick. Masked Autoencoders
Are Scalable Vision Learners.
In Proceedings of the
IEEECVF Conference on Computer Vision and Pattern
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
Diffusion Probabilistic Models.
Advances in Neural
Information Processing Systems, 33:68406851, 2020.
Jonathan Ho, William Chan, Chitwan Saharia, Jay
Imagen Video: High Denition Video
Generation with Diffusion Models.
arXiv preprint
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Diffusion Models.
Advances in Neural Information
Processing Systems, 35:86338646, 2022.
Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu
Chaochao Lu, and Jianyu Chen. Video Prediction Policy:
A Generalist Robot Policy with Predictive Visual Repre-
sentations. arXiv preprint arXiv:2412.14803, 2024.
Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted
Ethan Foster, Grace Lam, Pannag Sanketi, et al. Open-
arXiv preprint arXiv:2406.09246, 2024.
Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and
Joshua B Tenenbaum. Learning to act from actionless
videos through dense correspondences.
arXiv preprint
Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and
Kaiming He. Autoregressive Image Generation Without
Vector Quantization. arXiv preprint arXiv:2406.11838,
Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sud-
Carl Vondrick. Dreamitate: Real-World Visuomotor Pol-
icy Learning via Video Generation. CoRL, 2024.
Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen,
Jiacheng You, and Yang Gao. Data scaling laws in im-
itation learning for robotic manipulation. arXiv preprint
Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang
Knowledge Transfer for Lifelong Robot Learning. Ad-
vances in Neural Information Processing Systems, 36,
Fangchen Liu, Hao Liu, Aditya Grover, and Pieter
Abbeel. Masked Autoencoding for Scalable and General-
izable Decision Making. Advances in Neural Information
Processing Systems, 35:1260812618, 2022.
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn.
What Matters in Learning from Ofine Human Demon-
strations for Robot Manipulation.
In arXiv preprint
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Amanda Askell, Pamela Mishkin, Jack Clark, et al.
Learning Transferable Visual Models from Natural Lan-
guage Supervision.
In International Conference on
Machine Learning, pages 87488763, 2021.
Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg,
Trevor Darrell, and Jitendra Malik.
Robot Learning
with Sensorimotor Pre-Training. In Conference on Robot
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bjorn Ommer. High-Resolution Image
Synthesis with Latent Diffusion Models, 2021.
Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor,
Andrew Pierson, Ankush Gupta, Austin Waters, Aaron
van den Oord, Daniel Tanis, Dumitru Erhan, Eric Lau,
Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang,
Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina
Mikoaj Binkowski, Mohammad Babaeizadeh, Moham-
mad Taghi Saffar, Nando de Freitas, Nick Pezzotti,
Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung,
Robert Riachi, Ruben Villegas, Rui Qian, Sander Diele-
Tom Hume, Vikas Verma, Weizhe Hua, William Zhu,
Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du, and
Yutian Chen. Veo. 2024.
Ganguli.
Unsupervised
Learning
Nonequilibrium
Thermodynamics.
In International Conference on
Machine Learning, pages 22562265, 2015.
Yang Song and Prafulla Dhariwal.
Improved Tech-
niques for Training Consistency Models. arXiv preprint
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
Based Generative Modeling Through Stochastic Differ-
ential Equations. arXiv preprint arXiv:2011.13456, 2020.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever.
Consistency Models.
arXiv preprint
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and ne-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.
Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Ku-
vain Gelly.
Towards Accurate Generative Models of
arXiv preprint
Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi
Fruchter. Diffusion models are real-time game engines.
arXiv preprint arXiv:2408.14837, 2024.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
and Illia Polosukhin. Attention Is All You Need. Ad-
vances in Neural Information Processing Systems, 2017.
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan
mad Taghi Saffar, Santiago Castro, Julius Kunze, and
Dumitru Erhan. Phenaki: Variable Length Video Gen-
eration from Open Domain Textual Descriptions.
International Conference on Learning Representations,
Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkor-
eit. Scaling Autoregressive Video Models. arXiv preprint
Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai,
Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu,
Jianmin Bao, Yuhui Yuan, et al. ART-V: Auto-Regressive
Text-to-Video Generation with Diffusion Models.
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 73957405, 2024.
Philipp Wu, Arjun Majumdar, Kevin Stone, Yixin Lin,
Igor Mordatch, Pieter Abbeel, and Aravind Rajeswaran.
Masked Trajectory Models for Prediction, Representa-
chine Learning, pages 3760737623, 2023.
Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gor-
don Wetzstein, Manuela Veloso, and Shuran Song. Flow
as the cross-domain manipulation interface. CoRL, 2024.
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
Srinivas. VideoGPT: Video Generation Using VQ-VAE
and Transformers.
arXiv preprint arXiv:2104.10157,
Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea
Finn. Learning ne-grained bimanual manipulation with
low-cost hardware.
arXiv preprint arXiv:2304.13705,
