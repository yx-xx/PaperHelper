=== PDF文件: Safe Beyond the Horizon Efficient Sampling-based MPC with Neural Control Barrier Functions.pdf ===
=== 时间: 2025-07-21 13:46:21.461107 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Safe Beyond the Horizon: Efficient Sampling-based
MPC with Neural Control Barrier Functions
Ji Yin1, Oswin So2, Eric Yang Yu2, Chuchu Fan2, and Panagiotis Tsiotras1
AbstractA common problem when using model predictive
control (MPC) in practice is the satisfaction of safety specifications
beyond the prediction horizon. While theoretical works have shown
that safety can be guaranteed by enforcing a suitable terminal
set constraint or a sufficiently long prediction horizon, these
techniques are difficult to apply and thus are rarely used by prac-
solve this problem, we impose a tradeoff between exact recursive
box dynamics by learning an approximate discrete-time control
barrier function and incorporating it into a variational inference
MPC (VIMPC), a sampling-based MPC paradigm. To handle the
resulting state constraints, we further propose a new sampling
strategy that greatly reduces the variance of the estimated optimal
time planning on a CPU. The resulting Neural Shield-VIMPC
(NS-VIMPC) controller yields substantial safety improvements
compared to existing sampling-based MPC controllers, even under
badly designed cost functions. We validate our approach in both
simulation and real-world hardware experiments. Project website:
I. INTRODUCTION
Model Predictive Control (MPC) is a versatile control ap-
proach widely used in robotics applications such as autonomous
of deformable objects , to name just a few. These methods
address safety by incorporating state and control constraints
into the finite-horizon optimization problem, ensuring that the
system remains safe over the prediction horizon [12, 49, 59, 74].
is often overlooked by practitioners, potentially leading to the
violations of safety constraints at future timesteps.
This is a well-known problem in the field of MPC. The
question of whether a sequence of safe control actions can
always be found under an MPC controller has been studied
extensively in the literature under the name of recursive
feasibility [14, 32, 42, 50, 52]. A simple method of achieving
recursive feasibility is by enforcing a control-invariant terminal
set constraint at the end of the prediction horizon [14, 42].
for general nonlinear systems. Methods such as bounding the
system dynamics [13, 14, 26] often result in over-conservative
1Ji Yin and Panagiotis Tsiotras are with D. Guggenheim School of Aerospace
tsiotras}gatech.edu
2Oswin So, Eric Yang Yu and Chuchu Fan are with the Department
of Aeronautics and Astronautics, Massachusetts Institute of Technology,
Equal contribution
Fig. 1: Standard sampling vs. Resampling-Based Rollout
(RBR). In this AutoRally example, the blue dot samples future
trajectories and computes an optimal control that avoids the
black obstacles. RBR rewires all sampled trajectories to be safe,
resulting in a more accurate sampling distribution. In contrast,
the standard approach samples from a Gaussian distribution
and wastes computation on unsafe trajectories.
(HJ) PDE  scale exponentially with the number of state
for systems with more than, say, 5 state variables .
Even without considering recursive feasibility, nonlinear con-
strained optimization is a challenging problem. Traditionally,
constraints are handled using techniques such as interior-point,
sequential quadratic programming and augmented Lagrangian
type methods , which mostly rely on accurate linear or
quadratic approximations of the cost and constraints. However,
these gradient-based methods can get stuck in local minima
or fail to converge when the problem is highly nonlinear.
needed to solve these optimization problems, which is often
not available for black-box systems. Consequently, MPC
controllers that rely on these underlying nonlinear constrained
optimizers [30, 38, 56] inherit the same limitations.
One way to address these previous limitations is to use
sampling-based optimization methods, which have become
popular within both the robotics and model-based reinforcement
learning communities. In particular, variational inference MPC
(VIMPC) [8, 45, 57, 60, 61, 74] has emerged as a popular
family of methods that pose the control problem as an inference
problem instead of an optimization problem through the control
as inference framework [6, 65, 72], and perform variational
inference to approximate the resulting intractable optimal
control distribution. Popular MPC controllers within this family
include Model Predictive Path Integral (MPPI)  and the
Cross-Entropy Method (CEM) [66, 74], which have been
applied to robotics tasks such as autonomous driving ,
bipedal locomotion , manipulation of deformable objects
DPNCBF Training
Estimate Optimal Control
Resampling-based Rollout
Resample
Weighted
Propagate
Fig. 2: Overview of Neural Shield VIMPC (NS-VIMPC). Offline, using DPNCBFs, we collect a dataset and train a NN
approximation B of a DCBF. B is used to impose the DCBF descent condition (16) as a state constraint. Online, we modify
the VIMPC architecture to use resampling-based rollouts to improve the sampling distribution of the Monte Carlo estimator in
the presence of the DCBF state constraints. In addition, we integrate the DCBF safety condition (14b) into the optimization
objective function as in (18).
, and in-hand manipulation , among many others.
Using this framework, constraints can be easily handled by
appropriately manipulating the posterior control distribution
without facing the same optimization challenges as traditional
gradient-based methods. However, the problem of safety beyond
the prediction horizon still remains a challenge and has not
been addressed by existing works that employ VIMPC.
In this work, we present a novel sampling-based MPC
approach that provides safety beyond the prediction horizon by
using control barrier functions to enforce the control-invariant
set constraint for VIMPC controllers. To tackle the challenge
of finding control-invariant sets for general nonlinear systems,
we extend our previous work on learning neural network
approximations of control barrier functions using policy neural
control barrier functions (PNCBF)  to the discrete-time
case. Inspired by particle filtering and sequential Monte Carlo
handling state constraints in VIMPC that significantly improves
the sampling efficiency and enables real-time planning on a
CPU. Results from both simulation and hardware experiments
suggest that the resulting Neural Shield-VIMPC (NS-VIMPC)
controller outperforms existing MPC baselines in terms of
Contributions. We summarize our contributions below.
We extend policy neural CBF (PNCBF) to the discrete-time
case and propose a novel approach to train a discrete-time
PNCBF (DPNCBF) using policy evaluation.
We propose Resampling-Based Rollout (RBR), a novel
sampling strategy for handling state constraints in VIMPC
inspired by particle filtering, which significantly improves
the sampling efficiency by lowering the variance of the
estimated optimal control.
Simulation results on two benchmark tasks show the efficacy
of NS-VIMPC compared to existing sampling-based MPC
controllers in terms of safety and sample efficiency.
Hardware experiments on AutoRally , a 15 scale
autonomous driving platform, demonstrate the robustness
of NS-VIMPC to unmodeled dynamical disturbances under
adversarially tuned cost functions.
II. RELATED WORK
Sampling-based MPC. Sampling-based MPC has become
a popular alternative to traditional MPC methods that use
gradient-based solvers, in part due to the advent of parallel
computing and the recent advances in GPU hardware. As a
gradient-free method, sampling-based MPC can be applied
to any problem without requiring specific problem structures.
MPPI  is a popular sampling-based MPC approach that
formulates a variational inference problem, then solves it in the
case of the Gaussian distribution, having strong connections to
stochastic optimal control  and maximum entropy control
. Separately, the Cross-Entropy Method (CEM)  has
become popular in the reinforcement learning community ,
in part due to its simplicity. Both approaches were recently
shown to be part of the VIMPC family of MPC algorithms .
VIMPC family to include different choices of divergences
and sampling distributions beyond Gaussians [45, 57].
Safety in Sampling-based MPC. Recent research efforts
assess the risk associated with uncertain areas in the state
space during the exploration phase, and enhance MPPIs
safety by incorporating a risk penalty into its cost function
[5, 85]. While these methods empirically enhance safety, they
lack formal assurances. Another class of MPPI alternatives
leverage an auxiliary tracking controller to follow the MPPI
output trajectories [62, 77], improving robustness against
unforeseeable disruptions, but these improvements are limited
when the simulation-to-reality gap is significant. More recently,
Control Barrier Functions (CBF) [3, 4, 82] have been used
to provide formal safety guarantees for MPPI controllers
[71, 84, 86]. However, these methods use distance functions as
Our proposed Neural Shield MPPI (NS-MPPI) controller,
as a specific form of the proposed NS-VIMPC framework,
effectively resolves this critical issue of safety under input
constraints identified above.
Different Proposal Distributions for VIMPC. Many works
investigate changing the sampling distribution of VIMPC to
improve the performance of the sampling-based controller.
The MPPI variant in  uses covariance steering to assign
a terminal covariance to the sampling distribution, but this
relies on expert knowledge on how the covariance should be
designed.
Normalizing flows are used in  to approximate the opti-
mal sampling distribution, but this does not address the problem
of recursive feasibility on its own. Another direction looks
at changing the effective sampling distribution by modifying
the underlying dynamical system to be more amenable to
calculations. The works [62, 77] leverage an auxiliary tracking
output of the stable tracking controller. However, this requires
the construction of such an auxiliary tracking controller, which
can be difficult to perform for arbitrary nonlinear discrete-time
systems. In contrast, our proposed resampling-based rollouts
(RBR) can be viewed as a way of easily improving the proposal
distribution without the need for any specialized problem
structure.
Duality between Control and Inference. The proposed
resampling strategy in our work is similar in spirit to [63, 80]
in that factor graphs, a method used originally for estimation,
is adapted for control purposes. In  a linear optimal
control was used to improve the performance of particle filters.
first to adopt the resampling mechanism from particle filters to
improve the performance of sampling-based MPC controllers.
Control Barrier Functions. Designing CBFs with control-
invariant safe sets is not a trivial task. As a result, previous
works only seek saturating CBFs that do not consider input
constraints [48, 78, 81]. Some works use hand-tuned CBFs
to prevent saturation [17, 75], which can lead to overly
conservative safe sets. Other works develop CBFs with input
constraints for specific types of systems [18, 19]. The emerging
neural CBFs [64, 88, 90, 91] allow for more general dynamics
utilizing machine learning techniques; however, they can still
saturate the control limits.
Reachability. Recent works in the safety community have
realized that the value function of the reachability problem is a
valid CBF that takes input constraints into account [16, 36, 69].
In continuous-time, the reachability problem can be formulated
as a Hamilton-Jacobi PDE  and is conventionally solved
using grid-based numerical PDE solvers . However, grid-
based methods suffer from the curse of dimensionality and
cannot be practically applied to systems with a state space
larger than 5 dimensions . To resolve this problem, recent
works have looked at using learning-based methods at solving
reachability problems in both continuous-time  and discrete-
time [27, 37]. In particular, the value functions of both the
avoid  and reach-avoid problems  are CBFs. Though
early works focused on the use of optimal value functions ,
recent works any policy is a CBF, which is closely related to
the ideas of backup-CBFs . While this idea has been used
for the avoid problem continuous-time settings  and for the
reach-avoid problem in discrete-time settings , it has not
so far been used for the avoid problem in discrete-time, which
TABLE I: Relationship between DPNCBF and other reachabil-
ity methods.
Discrete
(Arbitrary) Policy-Conditioned
Bansal and Tomlin
Fisac et al.
Hsu et al.
So et al.
He et al.
DPNCBF (ours)
is the approach we take in this work with Discrete-time Policy
Neural Control Barrier Function (DPNCBF). We summarize the
relationship of this work with existing deep reachability-based
methods in Table I.
III. PROBLEM FORMULATION
We consider the discrete-time, nonlinear dynamics
xk1  f(xk, uk),
with state x X Rnx and control u U Rnu. Let UK
denote the set of control trajectories of length K. Following
the MPC setup, we assume that a cost function J : UK R
encoding the desired behavior of the system is given. Moreover,
we consider state constraints defined by an avoid set A X
described as the superlevel set of some specification function
The goal is then to find a sequence of controls u
{u0, u1, . . . , uK1} UK that minimizes the cost function J
while satisfying the system dynamics (1) and safety constraints
xk A for all k 0.
A. Variational Inference MPC For Sampling-based Optimiza-
To solve the above optimization problem, we deviate
from traditional MPC solvers and use a variational inference
MPC formulation to solve the problem via sampling-based
optimization. To this end, we make use of the control-as-
inference framework  to model the problem. Specifically,
let o be a binary variable that indicates optimality such that,
for all controls u UK,
p(o  1  u) exp(J(u)),
We assume a prior p0(u) on the control trajectory. The optimal
distribution can then be obtained via the posterior distribution
Z1 exp(J(u))p0(u),
where Z :
exp(J(u))p0(u) du denotes the unknown
normalization constant. Since sampling from p(u  o  1) is
posterior p(u  o  1) with a tractable distribution qv(u)
parametrized by some vector v by minimizing the forward KL
p(u  o  1) qv(u)
In the special case of qv being a Gaussian distribution with
mean v and a fixed control input covariance , intrinsic to the
robotic system of interest , we can solve (5) in closed-form
to obtain the optimal vas (see Section B1 for details)
v Ep(uo1)[u].
While this expectation cannot be readily computed because
p(u  o  1) is intractable to sample from, we can use
importance sampling to change the sampling distribution to
some other distribution r(u) that is easier to sample from,
leading to
p(u  o  1)
Z1 exp(J(u))p0(u)
Using samples u1, . . . , uN drawn from r, we compute a Monte
Carlo estimate v of the optimal control sequence v(see
Section B2 for details) as follows,
Note that the Z in (u) is canceled out in the computation of
i in (10) and hence can be ignored.
Remark 1 (Self-normalized importance sampling). Note
that the weights i in (9) are not from the regular importance
sampling estimates in (8) due to the normalization by
the sum of the weights in (10). Instead, (9) is a self-
normalized importance sampling estimator (SNIS), which
uses an estimate of the weights  but results in a biased,
though asymptotically unbiased, estimator. This fact is not
present in many existing works on both VIMPC (e.g.,
[57, 74]) and MPPI (e.g., ). See Section B2 for more
details.
Remark 2 (Connections to MPPI and CEM). In the case
where we choose p0(u)  q0(u) and r(u)  qv(u) for
some previous estimate of the optimal control sequence v,
the above variational inference MPC framework reduces to
MPPI (see Section B3 for details). Moreover, Cross-Entropy
Method (CEM) also falls in the VIMPC framework [57, 74].
B. Constraint Handling In Variational Inference MPC
One advantage of sampling-based MPC is that it is simple
to incorporate hard constraints. One can include an indicator
function in the cost function that heavily penalizes constraint
violations (e.g., see [8, 10, 76, 77]). Specifically, for some
large constant C > 0, we can modify the cost function as
Jnew(u)  J(u)  C
From the inference perspective (3), we can interpret Jnew (11)
as saying that o  ocost oconstraint, where,
p(ocost  1  u) exp(J(u)),
p(oconstraint  1  u) exp
only within the prediction horizon and does not consider the
probability of staying safe beyond the prediction horizon. To
tackle this problem, we will use a discrete-time control barrier
function (DCBF), which we introduce in the next section, to
enforce that the states remain within a control-invariant set.
C. Discrete-time Control Barrier Functions (DCBF)
A discrete-time control barrier function (DCBF)  asso-
ciated to the avoid set A is a function B : X R such that1
B(x) 0 inf
uU B(f(x, u)) B(x) (B(x)), (14b)
where  is an extended class- function . As in , we
restrict our attention to the class of linear extended class-
(B(x))  a  B(x),
The following theorem from  proves that a controller
satisfying the condition (14b) renders the sublevel set S
{x  B(x) 0} forward-invariant.
Theorem 1 ([84, Property 3.1]). Any control policy  : X U
satisfying the condition
B(f(x, (x))) B(x) (B(x)),
renders the sublevel set S  {x  B(x) 0} forward-
invariant.
safety beyond the prediction horizon, is to enforce the condition
(16) at every time step of the optimization problem. Another
point to note is that S is a control-invariant set, and thus the
constraint B(xK) 0 can be imposed as a terminal state
constraint to guarantee recursive feasibility for MPC as is
done classically [14, 42]. Thus, one can try to enforce these
constraints by incorporating them into the cost function as
in (11) , i.e., modify the cost according to one of the
following options,
Jnew(u)  J(u)  C
1B(f(x,u))B(x)>(B(x)),
Jnew(u)  J(u)  C
B(f(x, u)) B(x)  (B(x))
1Note that we use the opposite sign convention as compared to .
large C, samples that violate the DCBF constraint will have
a normalized weight of near zero, rendering these samples
useless. Conservative DCBFs may cause the majority of the
samples to violate the constraint (16) and hence have zero
wasted computation. Second, while constructing a function B
that satisfies (14a) is relatively simple, it is much harder to
construct a function B that also satisfies (14b), contrary to the
case with (continuous-time) control barrier functions [69, 88]
(see also Remark 3 below). Consequently, many works that
integrate control barrier functions into MPC often only propose
functions for which (14a) holds and not (14b) , rendering
the safety guarantees of Theorem 1 invalid.
In the next section, we address these two problems via a
novel resampling method that reuses computations from zero-
weight samples and learns a DCBF that tries to respect both
(14a) and (14b) using policy value functions.
Remark 3 (Differences between discrete-time and continu-
ous-time control barrier functions under unbounded controls).
Note that in the continuous-time case where having an
unbounded control space U, control-affine dynamics, and
a non-zero B
u are sufficient to guarantee that B is a valid
control barrier function. This is because (14b) generally
holds for a function B that satisfies (14a) in the continuous-
time case under these assumptions. However, the same does
not apply to discrete-time under general nonlinear dynamics
f since (16) is nonlinear in u, let alone the fact that robotic
systems in real life are unable to exert infinite forces and
hence generally do not have unbounded controls.
Remark 4 (Constraint satisfaction in the variational infer-
ence framework). One potential issue with incorporating
DCBF constraints into the cost function J is that, while
p(u  o  1) may have zero density on the set of controls that
violate the DCBF constraint, this does not necessarily hold
for the approximating distribution q since we are minimizing
the forward KL divergence . In particular, the mean v
of qv, which is the control to be used, may not satisfy
the DCBF constraint. One way to guarantee that v does
satisfy the DCBF constraint is to assume that the set of
controls that satisfy the DCBF constraint is itself convex
(see Appendix F). However, this is an unrealistic assumption
that is often violated by state constraints such as obstacle
avoidance. Despite these shortcomings, we observed in our
experimental results that this method of enforcing DCBF
constraints on p(u  o  1) indeed drastically improved
safety. Alternatively, this problem can be solved by checking
for constraint satisfaction of the mean v, and if not satisfied,
replacing it with any of the rollouts that do satisfy the
exploration of this issue as future work.
IV. NEURAL SHIELD VIMPC
In this section, we propose Neural Shield VIMPC (NS-
VIMPC), a sampling-based MPC paradigm that efficiently
samples trajectories using a DCBF modeled using a neural
network. We illustrate the proposed NS-VIMPC algorithm in
A. Approximating DCBF Using Neural Policy Value Functions
k denote the state at time k following the control policy
: X U. Define the policy value function V h, as,
V h,(x0) : max
We then have the following theorem.
Theorem 2. V h, satisfies (14a) and (14b) and is a DCBF.
V h,(xk) h(xk),
V h,(xk) V h,(f(xk, (xk))).
Using the definition of the avoid set A (2) and (20), it
follows that V h,(x) > 0 for all x A, satisfying the first
condition (14a) of a DCBF. When V h,(xk) 0, we have
(V h,(xk)) 0, and (21) implies that,
V h,(f(xk, (xk)))V h,(xk) 0 (V h,(xk)). (22)
Since (xk) U, this implies the second condition (14b).
Although we have constructed a DCBF from (19), the
challenge is that the policy value function V h, cannot be
easily evaluated at arbitrary states since the maximization in
(19) is taken over an infinite horizon. To fix this, we train a
neural network approximation V h,
of V h,, extending the
approach of  to the discrete-time case. To begin, we first
rewrite (19) in a dynamic programming form,
V h,(x0)  max
k), V h,(x
We can then train a neural network V h,
to approximate the
value function V h, by minimizing the loss
(x0) max
k), V h,
over all states x0. One problem, however, is that the minimizer
of (24) is not unique. For example, if h(x) h for all x,
then V h,
h is a minimizer of (24) but does not necessarily
satisfy (19). To fix this, we follow the approach of [27, 67] and,
inspired by reinforcement learning , introduce a discount
factor  (0, 1) to define the discounted value function
V h,,(xk)  max
k), (1)h(xk)V h,,(x
and the corresponding loss L,
(xk) V h,,
(xk)  max
k)  V h,,
directly in (16),
we first take the maximum with h and use V h,,
defined as
(x) : max{h(x), V h,,
This guarantees that V h,,
(x) h(x) and hence the zero
sublevel set of V h,, will be a subset of h. Hence, imposing
the state constraint V h,,(x) 0 will, at the very least,
prevent violations of the original state constraints during the
prediction horizon, and potentially also induce a state constraint
that is closer to the true control-invariant set than the original
sublevel set of h.
Remark 5 (Neural Network Verification of DCBFs). We
emphasize that our goal here is to obtain a good approxi-
mation of a DCBF B using a neural policy value function
, and not necessarily to obtain a true DCBF. Verifying
whether the learned V h,,
is a true DCBF requires neural
network verification which can be intractable or inconclusive
(see Appendix 1 in  on the NP-completeness of the NN-
verification problem). This is especially true in the discrete-
time case where the condition (16) may not be affine in the
control u.
using an approximation of a DCBF is sufficient for enabling
the use of much shorter prediction horizons without sacrificing
B. Efficient Sampling Using Resampling-based Rollouts
We tackle the problem of wasted samples with zero weights
by drawing inspiration from the sequential Monte Carlo
and particle filter  literature, and by performing a
per-timestep resampling during the rollout, which we call
Resampling-Based Rollouts (RBR). Specifically, the control-
as-inference problem formulation (3) gives us a temporal
decomposition of p(oconstraint  1  u) (13) in the case of
state constraints when C , as follows
p(oconstraint  1  u)
p(u  o  1) p(ocost  1  u)p(u)
By treating the term on the right as a measurement model,
particle filtering  can be used to solve the control-as-
inference problem. The update of the particle filter weights wi
at time step k for particle i is written as
Fig. 3: Resampling-based Rollouts (RBR). Inspired by particle
resample any samples that violate the state constraints among
the set of safe samples. In this example, the two particles
at states xb
1 satisfy the constraints and have weights
1  1. The particle at state xa
1 violates constraints
and thus has a weight of wa
1  0. Consequently, both xa
the control prefix ua
0 are resampled away and replaced with
equal probability by either xb
1 and their control prefixes
respectively (in this example, xc
1 was chosen). Note that ua
can be left untouched, though is now applied from xc
1 instead
of the unsafe state xa
to their weights wi
k to obtain a new set of particles ui. Due to
the indicator function in the weight update (31), the weights are
either 0 or 1. Assuming there exists a particle that satisfies the
state constraint, applying systematic resampling  results in
rewiring particles that violate the constraint to particles that
still maintain safety, reusing the computation from zero-weight
samples (see Fig. 3). If all particles violate the constraint,
we do not resample. In this case, since we want to minimize
constraint violations, we still use the cost term (18) such that
trajectories with higher constraint violations have higher costs.
satisfaction for each particle. This gives a total of N(K 1)
queries per rollout.
While we can prove that this resampling is unbiased from the
particle filter perspective, we also provide a more direct proof
of this fact without the analogy to particle filters.
Theorem 3. For a probability density function f and set S,
let a be sampled from the conditional density f(x  x S),
and let b be sampled from the unconditional density f, such
that a and b are independent. Define the rewired random
variable b to be equal to b if b S and a otherwise, i.e.,
b  1bSb  1bSa
x S), such that b and a have the same distribution, b
The proof is given in Section D2. Consequently, we can
use b in a Monte Carlo estimator and still obtain unbiased
in Section D3).
Corollary 1. For any function w, the Monte Carlo estimate
of E[w(x)] under the conditional density f(x  x S) using
random variables a and b is unbiased, i.e.,
2w(b)]  E[w(x)].
Carlo estimator, since most of the samples do not violate the
state constraints and hence contribute to the weighted sum
with non-zero weight. We can also theoretically prove that
resampling improves the variance of the resulting Monte Carlo
estimator. In the following theorem, we show how resampling
reduces the exponential growth of the variance on the prediction
horizon to a constant factor in the limit as the number of
samples N goes to infinity.
Theorem 4. Let the horizon K > 0, consider U
[1, 1], and define the avoid set and the dynamics such
that u [0, 1]K is safe, and unsafe otherwise. Let the
prior distribution p(u) be uniform on [1, 1]K and let
p(o  1  u) be the indicator function for u [0, 1] such
that the posterior distribution p(u  o  1) is uniform
on [0, 1]K, and the proposal distribution r(u)  12K
is uniform on U. Then, the variance of the Monte Carlo
estimator of the optimal control law
p(ui  o  1)
grows exponentially in K, i.e.,
Var[vk]  1
Using resampling, the variance is upper-bounded by,
Var[vk,resample]  O
The proof of Theorem 4 is given in Section D4. As shown
in Theorem 4, although the variance is still exponential in
K using resampling, the base of the exponential decreases
to 1 exponentially in the number of samples N. In other
using the proposed RBR is bounded by a constant factor.
This novel approach reduces the variance of the Monte Carlo
estimator of the optimal control law in a way that mirrors
the relationship between Sequential Importance Sampling
(i.e., particle filters without resampling), where the variance
increases exponentially with the horizon length, and Sequential
Monte Carlo (i.e., particle filters with resampling), where the
(asymptotic) variance only increases linearly .
Another method to theoretically quantify the improvement
in the variance of the estimator is via the effective sample
size (ESS) [24, 25]. ESS is defined as the ratio between the
variance of the estimator with N samples from the target and
the variance of the SNIS estimator . It can be interpreted as
the number of samples simulated from the target pdf that would
provide an estimator with variance equal to the performance
of the N-sample SNIS estimator. However, since the ESS is
computationally intractable, the approximation (made formal
is more often used in practice. The following theorem shows
that performing RBR results in either the same or higher
tion D5).
Theorem 5. Let w  [w1, . . . , wm, 0, . . . , 0] denote the un-
normalized weight vector without resampling, where the last
N m entries are zero due to violating the safety constraints.
Let w  w  c  [w1, . . . , wm, cm1, . . . , cN] denote the
unnormalized weight vector resulting from safe resampling,
where c  [0, . . . , 0, cm1, . . . , cN]. Let w  w w1
w w1 denote the normalized weights.
Suppose that the weights of the resampled trajectories c are
not drastically larger than the weights of the original
trajectories w, i.e.,
ESS using RBR is no smaller than the [
without resampling, and is strictly greater if the inequality
in (38) is strict. In other words,
C. Summary of NS-VIMPC
We now summarize the NS-VIMPC algorithm, shown in
algorithm to learn the policy value function for a user-specified
policy (Section IV-A). In our experiments, we chose this to
be Shield MPPI (S-MPPI) . Online, we use the learned
DPNCBF to enforce the DPCBF constraint (16) to try to
enforce safety beyond the prediction horizon. During sampling,
we use RBR to efficiently sample control sequences {ui} from
the raw samples {ui} to satisfy the DPNCBF constraint, thus
improving the sample efficiency of the Monte Carlo estimate
of the optimal control. The sampled control sequences {ui}
are then used to compute the estimate of the optimal control v
using (9). As in MPC fashion, we only execute the first control
distribution qv for the next iteration.
V. SIMULATIONS
We first performed simulation experiments to better under-
stand the performance of the proposed Neural Shield VIMPC
(NS-VIMPC) controller. Although many sampling-based MPC
controllers fall under the VIMPC family with different choices
Fig. 4: AutoRally Trajectories. We visualize the trajectories
of the three MPPI baselines under a challenging target velocity
of 15 ms1. Both MPPI and S-MPPI veer off course and crash
while NS-MPPI stays within the track even under Gaussian
disturbances.
Crash Rate
Deterministic
Stochastic
Col. Rate
Velocity
Target Velocity (ms)
Fig. 5: Varying target velocities on AutoRally. Our NS-MPPI
achieves the lowest crash and collision (Col.) rates under both
deterministic and stochastic dynamics. While the collision rate
is close to 1 for every method in the stochastic environment,
NS-MPPI achieves a crash rate of near 0.
of the prior p0 and r, we choose to instantiate the MPPI
algorithm (see Section B1 for details), and call the resulting
controller Neural Shield-MPPI (NS-MPPI).
Baseline methods. We compared NS-MPPI against the follow-
ing sampling-based MPC methods.
Baseline MPPI (MPPI) , which forward simulates a set
of randomly sampled trajectories for optimal control.
Shield MPPI (S-MPPI) , which extends MPPI by taking
h in (2) to be a DCBF and by adding the DCBF constraint
violation into the cost as in (18).2
MPPI but with the weight   1 for only the k-lowest cost
trajectories and 0 otherwise. This corresponds to an average
of the k-lowest cost trajectories.
In all simulations, we use S-MPPI as the control policy
to learn the DPNCBF. We provide further details on the
simulation experiments in Appendix A.
2We remove the local repair step from S-MPPI as this requires known
gradients from the dynamics and would make the method gradient-based
Control Horizon
Success Rate
Control Horizon
Crash Rate
Fig. 6: Varying control horizon on Drone. (a) Only NS-
MPPI has a crash rate of zero with a control horizon of 10,
demonstrating the benefit of enforcing the DCBF constraint
for maintaining safety beyond the prediction horizon.
A. Simulations on AutoRally
We first compare all methods on the AutoRally  testbed,
a 15 scale autonomous racing car. The goal for this task
is to track a given fixed velocity without exiting the track.
The vehicle collides when it contacts the track boundary and
crashes when it fully exceeds the track boundary.
We tested our algorithm under both deterministic dynamics
and stochastic dynamics with a Gaussian state disturbance
added at each timestep. We visualize the resulting trajectories
in Fig. 4 with a target velocity of 15 ms1. This is a very high
target velocity, as previous works considered at most velocities
of 8 ms1  or 9 ms1 . Under this challenging speed,
both MPPI and S-MPPI frequently veer off course. In contrast,
the proposed NS-MPPI successfully retains the vehicle within
the confines of the track, thereby ensuring safety.
crash rate, collision rate, and velocity in Fig. 5 over 20
trials. With higher velocities, the vehicle has less time to turn,
increasing the likelihood of leaving the track and colliding or
crashing. We see that NS-MPPI consistently outperforms all
other methods in both settings without having a significantly
lower average velocity.
B. Simulations on Drone
We next tested our algorithm on Drone, a simulated planar
quadrotor that incorporates ground effects arising from the
intricate interaction between the blade airflow and the ground
surface. The goal is for the drone to navigate as fast as possible
through a narrow corridor close to the ground. We vary the
control horizon for 500 sampled trajectories and plot the results
in Fig. 6. Only NS-MPPI has a crash rate of zero over all
control horizons, while all other controllers have high crash
rate in this challenging task.
To understand why this is the case, we visualize the
trajectories for NS-MPPI and S-MPPI with a control horizon
of 10 in Fig. 7. Due to the DCBF constraint, NS-MPPI starts
descending to avoid the obstacles even before any of the original
collision constraints are violated. On the other hand, S-MPPI
Fig. 7: Differences between NS-MPPI and S-MPPI. We
compare the sampling trajectory ui (green) and estimated
optimal trajectory v (orange) under a control horizon of 10.
NS-MPPI descends early enough to avoid collisions due to
the DCBF constraints despite none of the sampled trajectories
violating any constraints due to the short control horizon.
Fig. 8: RBR needs 5X less samples. On AutoRally, applying
RBR to S-MPPI significantly reduces the number of sampled
trajectories needed to achieve the same level of safety (190 to
the collision rate by 74 at 50 sampled trajectories.
is unaware of the obstacles beyond the prediction horizon
and keeps accelerating rightward until collision. This suggests
that enforcing the DCBF constraint even with an approximate
is beneficial for safety with short control horizons.
C. Deeper Investigation Into RBR
We next perform various case studies to better understand
the sample efficiency benefits of the proposed RBR method.
RBR improves sample efficiency by 5X. To isolate the effects
of RBR without the other improvements, we considered a new
method that extends S-MPPI with RBR (S-MPPI w RBR),
and compared it against the original S-MPPI across varying
numbers of sampled trajectories on AutoRally over 100 trials
in Fig. 8. S-MPPI with RBR achieves the same collision rates
as S-MPPI without RBR while using 5 times fewer trajectories.
This matches our expectations, both from the intuition that RBR
results in a more closely aligned proposal distribution (e.g.,
see Fig. 1), and from the theoretical results in Section IV-B
that show that RBR improves the quality of the estimator.
RBR achieves larger [
ESS. RBR theoretically improves
claim empirically on AutoRally and plot the results in Fig. 9.
While MPPI and S-MPPI exhibit values of [
ESS concentrated
Log(ESS)
Frequency
Effective Sample Size (ESS) Frequencies
Fig. 9: NS-MPPI has larger [
ESS. The proposed NS-MPPI
achieves larger effective sample sizes, verifying that RBR
enables more efficient use of samples and computations.
S-MPPI w RBR
Crash Rate
S-MPPI w RBR
Collision Rate
Fig. 10: AutoRally with obstacles. (a) While NS-MPPI
consistently clears the entire track, S-MPPI often crashes into
obstacles and walls. (b) NS-MPPI has 85 fewer crashes
compared to S-MPPI. Adding RBR alone to S-MPPI fixes
62 of the crashes made by S-MPPI.
near 1, the [
ESS values for NS-MPPI are more uniformly
sampled trajectories and thus a better use of the available
computational resources.
RBR is especially beneficial in harder environments. To
further stress-test RBR, we ran 50 trials with 100 sampled
trajectories on a harder variant of AutoRally with obstacles
of various sizes. Note that the presence of obstacles close to
the center of the track violates the assumption in Remark 4
regarding the safety of the mean v. Comparing the trajectories
from S-MPPI and NS-MPPI in Fig. 10a, S-MPPI is unable
to avoid crashing into walls and obstacles since it relies on a
heuristic DCBF. On the other hand, NS-MPPI avoids crashes
in almost all runs. We also compare against S-MPPI w RBR
and plot the crash and collision rates in Fig. 10b, where we
see that RBR constitutes a large fraction of the performance
reduction between S-MPPI and NS-MPPI. Nevertheless, RBR
alone is not sufficient, and the use of an approximate DPNCBF
is still required to bring the crash rate to near 0.
To demonstrate the enhancement in sampling efficiency
achieved by the proposed sampling method, we provide visual
representations of trajectory sampling distributions obtained
from simulations in environments with obstacles in Fig. 1. As
demonstrated by Fig. 1 (b), the RBR concentrates trajectory
TABLE II: Performance and Timing Comparison
Controller
Crash Rate
Collision Rate
Control Rate (Hz)
S-MPPI (Valid DCBF)
NS-MPPI (Valid DCBF)
Crash Rate
Control Horizon K
S-MPPI (Valid DCBF)
NS-MPPI (Valid DCBF)
Obstacle
Rollouts
Trajectory
Fig. 11: RBR improves performance even with valid DCBF.
(a) As the estimator variance grows exponentially with K, S-
MPPI crashes at larger K despite using a valid DCBF. Using
RBR (NS-MPPI) mitigates this, maintaining safety across all
horizons. (b) We visualize this exponential growth in variance
by comparing the rollouts of the two methods at K  10.
S-MPPI is unable to sample a control sequence that turns left
for all K  10 steps with only N  50 samples.
samples within a feasible narrow passage that satisfies the
DCBF safety criteria. Conversely, the standard sampling ap-
misallocating samples to unsafe zones and thereby failing to
sufficiently explore safe areas. This inadequacy leads to a
collision with an obstacle.
Enhanced sampling efficiency enables safe real-time plan-
ning on a CPU. We compare the computation times of different
VI-MPC controllers on AutoRally with a 12 m s1 target speed,
horizon of K  15 and small sample size of N  30
trajectories per optimization iteration (Table II). NS-MPPI
achieves the lowest crash and collision rates, albeit with a
slightly lower control rate of > 70 Hz that suffices for most
robotic tasks. S-MPPI suffers from over 11 times larger crash
and collision rates, while both MPPI and CEM controllers
exhibit crash and collision rates close to 1.
RBR improves performance even with a valid DCBF. We
further investigate whether RBR is beneficial when we have a
valid DCBF by performing experiments on Dubins, a Dubins
car that travels with fixed velocity, using only N  50 samples
(Fig. 11). Here, NS-MPPI (Valid DCBF) uses RBR while
S-MPPI (Valid DCBF) does not. Both methods use a valid
DCBF. At K  1, RBR has no effect since no resampling
estimator variance grows exponentially in the horizon length
K (Section IV-B), S-MPPI crashes at K 7, where N  50
samples is not enough to sample a trajectory that satisfies the
DCBF constraints at all timesteps. Using RBR mitigates this
Narrow corridor unsafe
from state discretization
Grid HJ Value Function
Control Horizon
Success Rate
Control Horizon
Crash Rate
NS-MPPI (Grid HJ)
Fig. 12: Value functions from grid-based solvers suffer in
multi-fidelity problems. (a) Using a value function from a grid-
based HJ reachability solver results in much lower success rates
at longer control horizons compared to the learned DPNCBF.
(b) Due to state discretization, the narrow corridor is marked
cross the corridor to remain safe.
exponential growth, and NS-MPPI remains safe across all time
horizons. See Appendix G for more details and additional plots.
D. Comparing DPNCBF against grid-based reachability.
DPNCBF. In particular, we question whether the value function
obtained via continuous-time techniques can be used in place
of the proposed DPNCBF.
Grid-based reachability suffers with multi-fidelity problems.
We compare against a grid-based reachability method solved
using methods from continuous-time HJ reachability
(denoted NS-MPPI (Grid)) on Drone in Fig. 12. The height
of the narrow corridor is much smaller than the length of the
overall space. Consequently, given the 6-dimensional state-
space of Drone, the grid size used by the grid-based solver is
too coarse to capture the safe region in the narrow corridor
accurately and marks the narrow corridor as unsafe, blocking
traversal. This causes NS-MPPI (Grid HJ) to have a much
lower success rate than NS-MPPI at longer control horizons.
Optimal safe controls from continuous-time reachability
can cause crashes in discrete-time. Finally, we compare
against a concurrent work DualGuard-MPPI  that uses the
value function solved using continuous-time HJ reachability as
a safety filter during the rollout process of MPPI on Dubins
(Fig. 13), where we add an additional box-constraint  2
on the heading. For this experiment, to rule out crashed caused
by errors in the learned DPNCBF, we wrote a discrete-time
grid-based solver and used the corresponding value function
as the DCBF. Different from our method, DualGuard uses the
value function as a safety filter during the rollout, replacing the
original control with the continuous-time optimal safe control
when the current value function is unsafe V (x) > , with a
DualGuard (1)
DualGuard (2)
Timestep
Dist to obstacle
Timestep
Unfiltered
Filtered
Constraint Region
Fig. 13: Optimal safe controls from continuous-time reachability can cause crashes in discrete-time. The optimal safe
control from continuous-time is different from the optimal control in discrete-time. Consequently, using the value function from
continuous-time HJ reachability as a safety filter during the rollout process (i.e., concurrent work DualGuard-MPPI ) leads
to crashes, even if a margin  is added to try to account for this gap by being more conservative.
Fig. 14: AutoRally Track Setup. The site is equipped with
a spacious carport, a 3-meters-tall observation tower, and a
storage shed . Each turn of the track is numbered as Ti,
with T1 is the first turn and T9 is the last.
static margin  added to try to account for the gap between
continuous-time and discrete-time. However, Fig. 13 shows
that neither small nor large value of  can bridge this gap,
and DualGuard-MPPI violates safety constraints in both cases.
When the margin is small (  1), the safety filter acts too
late to prevent collision with the obstacle. When the margin
is larger (  2), the safety filter acts early and avoids the
due to the zero-order hold used in discrete-time.
VI. HARDWARE EXPERIMENTS
MPC baselines on hardware using the AutoRally experimental
platform . Testing was conducted on a dirt track outlined by
drainage pipes, as shown in Fig. 14. All computations use the
onboard Intel i7-6700 CPU and Nvidia GTX-750ti GPU .
See Appendix E for more details on the hardware experiments.
A. Robustness Against Adversarial Costs
Robots depend on sensors such as cameras to gather
information about their environment, and use the collected data
to construct suitable cost functions. Deep Neural Networks
(DNNs) are frequently utilized for object detection, classifi-
Fig. 15: Adversarial cost. With a cost function that rewards
pletes 10 laps collision-free.
susceptible to adversarial attacks , which can mislead the
algorithms into generating incorrect outputs. These attacks often
involve altering the appearance of the object of interest directly
, eventually resulting in erroneous cost functions. In this
hardware experiment, we tested the robustness of the proposed
NS-MPPI to these erroneous or misspecified cost functions
by using an adversarial cost function on the actual AutoRally
hardware platform. Namely, instead of penalizing infeasible
system states, we reward unsafe states by assigning negative
the S-MPPI and MPPI baselines.
The results are depicted in Fig. 15. As MPPI only uses cost
the closest boundary, resulting in a crash. S-MPPI crashes
the vehicle at the first turn (T1) as well, but it covers a
greater distance than MPPI. Only NS-MPPI achieves a collision-
free completion of 10 laps. Nonetheless, there are noticeable
oscillatory movements of the car, indicating susceptibility to
attractive costs driving it toward boundary collisions.
Fig. 16: Safety under unsafe user inputs. Using an erro-
neously large velocity target, both MPPI and S-MPPI crash.
The visualization on the bottom row visualizes simulation
trajectories that lead to crashes in the same turn.
B. Safety Under Unsafe User Input
Exceeding speed limits is a significant contributor to road
accidents  due to noncompliance by many drivers. The pro-
posed Shield VIMPC control framework can smartly manage
speeds supervised by the NCBF to achieve safety, even given
unsafe user inputs. To this end, we tested MPPI, S-MPPI, and
the proposed NS-MPPI on the AutoRally hardware using large,
unsafe target velocities to examine their safety performance.
The results are shown in Fig. 16. Designating high target
velocities encourages maximizing speed, thus disrupting the
equilibrium in the controllers cost structure by diminishing
the emphasis on cost penalties associated with obstacles. As
a result, MPPI causes the vehicle to deviate from the path,
resulting in a crash at the second turn (T2), while S-MPPI
impacts at the seventh turn (T7). In contrast, NS-MPPI ensures
VII. LIMITATIONS
One limitation of our approach is that the DPNCBF is only
an approximation of a DCBF. As such, the typical safety
guarantees of DCBFs may not hold if the function is not
a true DCBF. Moreover, as noted in Remark 4, the use of
variational inference means that vmay not satisfy the state
constraints despite the optimal distribution p(u  o  1) having
zero density at states that violate the state constraints. While
theoretically we were only able to show that vstays safe
under the assumption that UK
safe is convex, empirical results in
Fig. 10 show that NS-MPPI can perform well despite the fact
safe is typically not convex.
VIII. CONCLUSION
In this study, we have adapted the policy neural control
barrier function (PNCBF) to a discrete-time setting and
utilized the resulting discrete-time policy neural control barrier
function (DPNCBF) to supervise the proposed resampling-
based rollout (RBR) method. This novel method enhances
sampling efficiency and safety for all variational inference
model predictive controllers (VIMPCs). Leveraging RBR, we
developed the neural shield VIMPC (NS-VIMPC) control
framework and demonstrated its benefits for safe planning
through the novel neural shield model predictive path integral
(NS-MPPI) controller. We conducted tests of the NS-MPPI,
benchmarking its performance against state-of-the-art sampling-
based MPC controllers using both an autonomous vehicle and
a drone. Our simulations and experimental data indicate that
the NS-MPPI outperforms existing VIMPC methods in terms
of safety and sampling efficiency. The improved sampling
efficiency allows NS-MPPI to reach similar performance levels
as other VIMPC methods, while using significantly fewer
sampled trajectories, facilitating real-time control on a CPU
rather than necessitating costly GPU resources.
We have used the novel RBR method to concentrate
sampled trajectories on safe regions. While this approach
can significantly improve sampling efficiency in terms of
further enhance performance, we may, for instance, integrate a
Control Lyapunov Function  or a Control Lyapunov Barrier
Function  with the novel RBR approach to sample safe
and higher-performance trajectories.
ACKNOWLEDGMENTS
This work was funded by the National Science Foundation
(NSF) under CAREER award CCF-2238030 and award CNS-
REFERENCES
Manuel Acosta and Stratis Kanarachos. Tire lateral force
estimation and grip potential identification using neural
squares. Neural Computing and Applications, 30(11):
Mohamadreza Ahmadi, Andrew Singletary, Joel W. Bur-
agent POMDPs via discrete-time barrier functions. In
IEEE 58th Conference on Decision and Control (CDC),
pages 47974803, Nice, France, Dec. 11  13, 2019.
Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and
Paulo Tabuada. Control barrier function based quadratic
programs for safety critical systems. IEEE Transactions
on Automatic Control, 62(8):38613876, 2016.
Aaron D. Ames, Samuel Coogan, Magnus Egerstedt,
Gennaro Notomista, Koushil Sreenath, and Paulo Tabuada.
Control barrier functions: Theory and applications. In
18th European Control Conference (ECC), pages 3420
Ermano Arruda, Michael J. Mathew, Marek Kopicki,
Michael Mistry, Morteza Azad, and Jeremy L. Wyatt.
Uncertainty averse pushing with model predictive path
integral control. In IEEE-RAS 17th International Confer-
ence on Humanoid Robotics (Humanoids), pages 497502,
Hagai Attias. Planning by probabilistic inference. In
International Workshop on Artificial Intelligence and
Somil Bansal and Claire J Tomlin. Deepreach: A deep
learning approach to high-dimensional reachability. In
2021 IEEE International Conference on Robotics and
Automation (ICRA), pages 18171824. IEEE, 2021.
Lucas Barcelos, Alexander Lambert, Rafael Oliveira,
Paulo Borges, Byron Boots, and Fabio Ramos. Dual
Online Stein Variational Inference for Control and Dy-
namics. In Proceedings of Robotics: Science and Systems,
Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti.
Model-predictive control via cross-entropy and gradient-
based optimization. In Proceedings of the 2nd Conference
on Learning for Dynamics and Control, volume 120 of
Proceedings of Machine Learning Research, pages 277
Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan
and Byron Boots. STORM: An integrated framework
for fast joint-space model-predictive control for reactive
manipulation. In Conference on Robot Learning, pages
Javier Borquez, Luke Raus, Yusuf Umut Ciftci, and Somil
Bansal. Dualguard mppi: Safe and performant optimal
control by combining sampling-based mpc and hamilton-
jacobi reachability. arXiv:2502.01924, 2025.
Camille Brasseur, Alexander Sherikov, Cyrille Collette,
Dimitar Dimitrov, and Pierre-Brice Wieber. A robust
linear MPC approach to online generation of 3d biped
walking motion. In IEEE-RAS 15th International Confer-
ence on Humanoid Robots (Humanoids), pages 595601,
Jos Manuel Bravo, Daniel Limn, Teodoro Alamo, and
Eduardo F. Camacho. On the computation of invariant sets
for constrained nonlinear systems: An interval arithmetic
approach. Automatica, 41(9):15831589, 2005.
Wen-Hua Chen, John OReilly, and Donald J. Ballance.
On the terminal region of model predictive control for non-
linear systems with inputstate constraints. International
Journal of Adaptive Control and Signal Processing, 17
Yuxiao Chen, Mrdjan Jankovic, Mario Santillo, and
Aaron D Ames.
Backup control barrier functions:
Formulation and comparative study. In 2021 60th IEEE
Conference on Decision and Control (CDC), pages 6835
Jason J Choi, Donggun Lee, Koushil Sreenath, Claire J
value functions for safety-critical control. In 2021 60th
IEEE Conference on Decision and Control (CDC), pages
Andrew Clark. Verification and synthesis of control barrier
functions. In IEEE Conference on Decision and Control
(CDC), pages 61056112, Austin, TX, Dec. 13  17,
Wenceslao Shaw Cortez and Dimos V. Dimarogonas. Safe-
by-design control for EulerLagrange systems. Automat-
Wenceslao Shaw Cortez, Xiao Tan, and Dimos V. Di-
marogonas. A robust, multiple control barrier function
framework for input constrained systems. IEEE Control
Systems Letters, 6:17421747, 2021.
Li Danjun, Zhou Yan, Shi Zongying, and Lu Geng.
Autonomous landing of quadrotor based on ground effect
modelling. In Chinese Control Conference (CCC), pages
Charles Dawson, Sicun Gao, and Chuchu Fan.
control with learned certificates: A survey of neural
and control. IEEE Transactions on Robotics, 39(3):1749
Christopher De Sa, Megan Leszczynski, Jian Zhang,
Alana Marzoev, Christopher R Aberger, Kunle Olukotun,
and Christopher R. High-accuracy low-precision training.
arXiv preprint arXiv:1803.03383, 2018.
Arnaud Doucet, Nando De Freitas, and Neil Gordon.
An introduction to sequential Monte Carlo methods.
Sequential Monte Carlo Methods in Practice, pages 314,
Arnaud Doucet, Adam M. Johansen, et al. A tutorial
on particle filtering and smoothing: Fifteen years later.
Handbook of Nonlinear Filtering, 12(656-704):3, 2009.
Vctor Elvira, Luca Martino, and Christian P. Robert.
Rethinking the effective sample size.
International
Statistical Review, 90(3):525550, 2022.
Mirko Fiacchini, Teodoro Alamo, and Eduardo F. Ca-
macho. On the computation of local invariant sets for
nonlinear systems. In IEEE Conference on Decision and
Jaime F. Fisac, Neil F. Lugovoy, Vicen Rubies-Royo,
Shromona Ghosh, and Claire J. Tomlin.
Bridging
Hamilton-Jacobi safety analysis and reinforcement learn-
In International Conference on Robotics and
Automation (ICRA), pages 85508556, Montreal, Canada,
Manan S. Gandhi, Bogdan Vlahov, Jason Gibson, Grady
predictive path integral control: Analysis and performance
guarantees. IEEE Robotics and Automation Letters, 6(2):
GeorgiaTech.
AutoRally.
Online. Available: https:
autorally.github.io, 2022. Accessed: Nov. 9, 2024.
Philip E Gill, Walter Murray, and Michael A. Saunders.
optimization. SIAM Review, 47(1):99131, 2005.
Brian Goldfain, Paul Drews, Changxi You, Matthew
autonomous driving. IEEE Control Systems Magazine,
Ravi Gondhalekar, Jun-ichi Imura, and Kenji Kashima.
Controlled invariant feasibilityA general approach to
enforcing strong feasibility in MPC applied to move-
blocking. Automatica, 45(12):28692875, 2009.
Neil J. Gordon, David J. Salmond, and Adrian F.M. Smith.
Novel approach to nonlinearnon-Gaussian Bayesian state
estimation. In IEE Proceedings F (Radar and Signal
Processing), volume 140, pages 107113, 1993.
Amira Guesmi, Muhammad Abdullah Hanif, Bassem
attacks for camera-based smart systems: Current trends,
future outlook. IEEE Access, 11:109617109668, 2023.
Emily Hannigan, Bing Song, Gagan Khandate, Maximil-
ian Haas-Heger, Ji Yin, and Matei Ciocarlie. Automatic
snake gait generation using model predictive control. In
IEEE International Conference on Robotics and Automa-
tion (ICRA), pages 51015107, Paris, France, May 31
Tairan He, Chong Zhang, Wenli Xiao, Guanqi He,
Changliu Liu, and Guanya Shi. Agile But Safe: Learning
Collision-Free High-Speed Legged Locomotion.
Proceedings of Robotics: Science and Systems, Delft,
Kai-Chieh Hsu, Vicen Rubies-Royo, Claire Tomlin, and
Jaime F Fisac. Safety and Liveness Guarantees through
Reach-Avoid Reinforcement Learning. In Proceedings of
Wilson Jallet, Antoine Bambade, Nicolas Mansard, and
Justin Carpentier.
Constrained differential dynamic
approach.
In IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 13371
Ghassen Jerfel, Serena Wang, Clara Wong-Fannjiang,
Katherine A. Heller, Yian Ma, and Michael I. Jordan.
Variational refinement for importance sampling using the
forward kullback-leibler divergence. In Uncertainty in
Artificial Intelligence, pages 18191829, 2021.
Zichao Jiang, Junyang Jiang, Qinghe Yao, and Gengchao
Yang. A neural network-based pde solving algorithm with
high precision. Scientific Reports, 13(1):4479, 2023.
Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and
Mykel J. Kochenderfer. Reluplex: An efficient SMT solver
for verifying deep neural networks. In Computer Aided
Eric C. Kerrigan and Jan M. Maciejowski.
Invariant
sets for constrained nonlinear discrete-time systems with
application to feasibility in model predictive control. In
Proceedings of the 39th IEEE conference on decision and
Genshiro Kitagawa. Monte Carlo filter and smoother for
non-Gaussian nonlinear state space models. Journal of
Computational and Graphical Statistics, 5(1):125, 1996.
Jacob Knaup, Kazuhide Okamoto, and Panagiotis Tsiotras.
Safe high-performance autonomous off-road driving using
covariance steering stochastic model predictive control.
IEEE Transactions on Control Systems Technology, 31
Alexander Lambert, Fabio Ramos, Byron Boots, Dieter
Stein variational model
predictive control. In Proceedings of the Conference on
Robot Learning, volume 155 of Proceedings of Machine
Learning Research, pages 12781297, 1618 Nov, 2021.
Sergey Levine. Reinforcement learning and control as
probabilistic inference: Tutorial and review. arXiv preprint
Juncheng Li, Frank Schmidt, and Zico Kolter. Adversarial
camera stickers: A physical camera-based attack on deep
learning systems. In International Conference on Machine
Lars Lindemann and Dimos V. Dimarogonas. Control
barrier functions for signal temporal logic tasks. IEEE
Control Systems Letters, 3(1):96101, 2018.
Bjrn Lindqvist, Sina Sharif Mansouri, Ali-akbar Agha-
Nonlinear
MPC for collision avoidance and control of UAVs with
dynamic obstacles.
IEEE Robotics and Automation
Johan Lfberg. Oops! I cannot do it again: Testing for
recursive feasibility in MPC. Automatica, 48(3):550555,
Kostas Margellos and John Lygeros. HamiltonJacobi
formulation for reachavoid differential games. IEEE
Transactions on Automatic Control, 56(8):18491861,
David Q. Mayne, James B. Rawlings, Christopher V. Rao,
and Pierre O.M. Scokaert. Constrained model predictive
Ian M. Mitchell. The flexible, extensible and efficient
toolbox of level set methods.
Journal of Scientific
Anusha Nagabandi, Kurt Konolige, Sergey Levine, and
Vikash Kumar.
Deep dynamics models for learning
dexterous manipulation. In Conference on Robot Learning,
pages 11011112, Virtual, Nov. 16  18, 2020.
National
Administration
(NHTSA). Speeding.
speeding. Accessed: Jan. 27, 2025.
Jorge Nocedal and Stephen J. Wright.
Numerical
Optimization. Springer, 1999.
Masashi Okada and Tadahiro Taniguchi.
Variational
inference MPC for Bayesian model-based reinforcement
learning. In Conference on robot learning, pages 258272,
Art B. Owen. Monte Carlo theory, methods and examples,
Thomas Power and Dmitry Berenson. Keep it simple:
Data-efficient learning for controlling complex systems
with simple models.
IEEE Robotics and Automation
Thomas Power and Dmitry Berenson.
Variational In-
ference MPC using Normalizing Flows and Out-of-
Distribution Projection.
In Proceedings of Robotics:
Science and Systems, New York City, NY, June 27
July 1, 2022.
Thomas Power and Dmitry Berenson.
Learning a
generalizable trajectory sampling distribution for model
predictive control. IEEE Transactions on Robotics, 40:
Jintasit Pravitra, Kasey A. Ackerman, Chengyu Cao,
Naira Hovakimyan, and Evangelos A. Theodorou. L1-
adaptive MPPI architecture for robust and agile control
of multirotors. In IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 76617666,
Las Vegas, NV, Oct. 25  29, 2020.
Mohamad Qadri, Paloma Sodhi, Joshua G. Mangelson,
Frank Dellaert, and Michael Kaess. InCOpt: Incremental
constrained optimization using the Bayes tree.
IEEERSJ International Conference on Intelligent Robots
and Systems (IROS), pages 63816388, Kyoto, Japan, Oct.
Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen,
and Chuchu Fan. Learning safe multi-agent control with
decentralized neural barrier certificates. arXiv preprint
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar.
On stochastic optimal control and reinforcement learning
by approximate inference. In International Joint Confer-
ence on Artificial Intelligence, Beijing, China, Aug. 3
Reuven Rubinstein.
The cross-entropy method for
combinatorial and continuous optimization. Methodology
and Computing in Applied Probability, 1:127190, 1999.
Oswin So and Chuchu Fan.
Solving stabilize-avoid
optimal control via epigraph form and deep reinforcement
learning. In Robotics: Science and Systems, Daegu, South
Oswin So, Ziyi Wang, and Evangelos A. Theodorou.
Maximum entropy differential dynamic programming. In
International Conference on Robotics and Automation
(ICRA), pages 34223428, Philadelphia, PA, May 23
Oswin So, Zachary Serlin, Makai Mann, Jake Gonzales,
Kwesi Rutledge, Nicholas Roy, and Chuchu Fan. How to
train your neural control barrier function: Learning safety
filters for complex input-constrained systems. In IEEE
International Conference on Robotics and Automation
(ICRA), pages 1153211539, Yokohama, Japan, May 13
Richard S. Sutton and Andrew G. Barto. Reinforcement
Chuyuan Tao, Hunmin Kim, Hyungjin Yoon, Naira Hov-
augmentation in sampling-based control algorithm for
sample efficiency. In American Control Conference (ACC),
pages 34883493, Atlanta, GA, June 8  10, 2022.
Marc Toussaint and Amos Storkey. Probabilistic inference
for solving discrete and continuous state Markov decision
processes. In Proceedings of the 23rd International Con-
ference on Machine Learning, pages 945952, Pittsburgh,
Efstathios Velenis, Emilio Frazzoli, and Panagiotis Tsio-
tras. Steady-state cornering equilibria and stabilisation
for a vehicle during extreme operating conditions. Inter-
national Journal of Vehicle Autonomous Systems, 8(2-4):
Ziyi Wang, Oswin So, Jason Gibson, Bogdan Vla-
Theodorou.
Variational Inference MPC using Tsallis
Divergence. In Proceedings of Robotics: Science and
Tianhao Wei and Changliu Liu. Safe control with neural
network dynamic models. In Learning for Dynamics and
Control Conference, pages 739750, Palo Alto, CA, June
Grady Williams, Paul Drews, Brian Goldfain, James M.
model predictive control: Theory and applications to
autonomous driving.
IEEE Transactions on Robotics,
Grady Williams, Brian Goldfain, Paul Drews, Kamil
Robust sampling based model predictive control with
sparse objective information. In Robotics: Science and
Sean Wilson, Paul Glotfelter, Li Wang, Siddharth Mayya,
Gennaro Notomista, Mark Mote, and Magnus Egerstedt.
The Robotarium: Globally impactful opportunities, chal-
control of multirobot systems. IEEE Control Systems
Zhe Wu, Fahad Albalawi, Zhihao Zhang, Junfeng Zhang,
Helen Durand, and Panagiotis D. Christofides. Control
Lyapunov-barrier function-based model predictive control
of nonlinear systems. In American Control Conference
(ACC), pages 59205926, Milwaukee, WI, June 27  29,
Mandy Xie, Alejandro Escontrela, and Frank Dellaert.
A factor-graph approach for optimization problems with
dynamics constraints. arXiv preprint arXiv:2011.06194,
Bin Xu and Koushil Sreenath.
Safe teleoperation of
dynamic UAVs through control barrier functions. In IEEE
International Conference on Robotics and Automation
(ICRA), pages 78487855, Brisbane, Australia, May 21
Xiangru Xu, Paulo Tabuada, Jessy W. Grizzle, and
Aaron D. Ames. Robustness of control barrier functions
for safety critical control. IFAC-PapersOnLine, 48(27):
Ji Yin, Zhiyuan Zhang, Evangelos Theodorou, and Pana-
giotis Tsiotras. Trajectory distribution control for model
predictive path integral control using covariance steering.
In International Conference on Robotics and Automation
(ICRA), pages 14781484, Philadelphia, PA, May 23
Ji Yin, Charles Dawson, Chuchu Fan, and Panagiotis
Tsiotras. Shield model predictive path integral: A com-
putationally efficient robust MPC method using control
barrier functions. IEEE Robotics and Automation Letters,
Ji Yin, Zhiyuan Zhang, and Panagiotis Tsiotras. Risk-
aware model predictive path integral control using con-
ditional value-at-risk. In IEEE International Conference
on Robotics and Automation (ICRA), pages 79377943,
Ji Yin, Panagiotis Tsiotras, and Karl Berntorp. Chance-
constrained information-theoretic stochastic model predic-
tive control with safety shielding. 63rd IEEE Conference
on Decision and Control, pages 653658, Dec. 1619,
Changxi You and Panagiotis Tsiotras. Vehicle modeling
and parameter estimation using adaptive limited memory
joint-state UKF. In American Control Conference, pages
Hongzhan Yu, Chiaki Hirayama, Chenning Yu, Sylvia
scalable dynamic obstacle avoidance. In IEEERSJ Inter-
national Conference on Intelligent Robots and Systems
(IROS), pages 1124111248, Detroit, MI, Oct. 1  5,
Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin
Chen. An optimal control approach to particle filtering.
Songyuan Zhang, Kunal Garg, and Chuchu Fan. Neural
graph control barrier functions guided distributed collision-
avoidance multi-agent control. In Conference on Robot
Songyuan Zhang, Oswin So, Kunal Garg, and Chuchu
Fan. GCBF: A neural graph control barrier function
framework for distributed safe multi-agent control. IEEE
Transactions on Robotics, 2025.
Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan.
A survey on deep learning-based fine-grained object
classification and semantic segmentation. International
Journal of Automation and Computing, 14(2):119135,
APPENDIX A
DETAILS ON THE SIMULATION EXPERIMENTS
A1 AutoRally Environment
The numerical simulations use the AutoRally platform, a 15-scale autonomous vehicle system capable of executing high-speed,
limit-handling maneuvers in complicated environments . Figure17 shows the AutoRally hardware. In this study, we define a
crash as an event when the vehicle makes contact with the track boundary, constructed from soft drainage pipes, resulting
in the inability to continue driving. We define a collision as an event when the vehicle contacts the track boundary but can
continue driving.
Fig. 17: AutoRally chassis and compute box .
A2 AutoRally Dynamics Modelling
We model the AutoRally using a rear-wheel drive, single-track bicycle model . The vehicle system is represented in
curvilinear coordinates using the centerline of the track as the reference curve. Using a curvilinear coordinate system provides a
more intuitive interpretation of the vehicles position and heading relative to the track, as compared to Cartesian coordinates.
To this end, we consider a nonlinear, continuous-time system,
x  F(x, u),
where the system state of the AutoRally is given by,
and where vx is the longitudinal velocity, vy is the lateral velocity, and  is the yaw rate. The front and rear wheel angular
velocities are denoted by F and R. The yaw error and the lateral distance error from the centerline of the track are denoted
by e and ey, respectively (see Fig. 18). The variable s is the curvilinear position along the track centerline. The control input
to the system (A.1) is,
where  is the steering angle and T denotes the values for throttle input (if positive) or braking input (if negative). The dynamics
of a single-track dynamic bicycle model  used to model (A.1) are given by,
vx  fF x cos  fF y sin   fRx
vy  fF x sin   fF y cos   fRy
(fF y cos   fF x sin ) F fRyR
R  (R, T),
e   vx cos e vy sin e
ey  vx sin e  vy cos e,
s  vx cos e vy sin e
where m and Iz are the vehicles mass and moment of inertia about the vertical axis, respectively. The radius of the front
wheel is rF , and the moment of inertia of the front wheel about the front axle is IF . The curvature of the track centerline
Fig. 18: Schematic of the dynamic bicycle model.
at position s is (s). The front and rear tire frictional forces are denoted by fF x, fF y, fRx, fRy, where the subscripts F, R
indicate front and rear tires and x, y indicate longitudinal and lateral directions. These frictional forces are computed using the
ellipse model in ,
fix  fizix,
fiy  fiziy  i(, i),
where i  F, R indicates front or rear wheels, fiz is the normal force acting on the tire. The tire friction coefficients ix, iy
are dependent on wheel slip angles and the intrinsic tire parameters.
Inspired by , we use a neural network to model the residual error i(, i), where  is steering angle, i  arctan(viyvix)
is wheel slip angle. i is trained using experimental data collected from the AutoRally. The rear wheel angular acceleration R
is given by (A.4e). Modelling R is challenging, because it is determined by a variety of mechanical and physical conditions,
including nonlinear tire behavior, dynamic load transfer, road surface irregularities and the complex interaction between these
components. To this end, we use a data-driven approach and utilize a neural network (, ) trained on experimental data to
model the rear wheel angular acceleration R.
The nonlinear, continuous-time system (A.1) is discretized and converted to a discrete-time system using Euler integration,
xk1  f(xk, uk)  xk  F(xk, uk)t,
where t  tk1 tk is the discretization time interval. We employ an Unscented Kalman Filter (UKF) on the discrete-time
system (A.6) to identify the vehicle and tire parameters using the approach in . The resulting parameters of the vehicle model
include the vehicle mass m  22 kg, moment of inertia Iz  1.1 kg  m2, lF  0.34 m, lR  0.23 m, IF  0.10 kg  m2
and front wheel radius rF  0.095 m. The parameters in Pacejkas Magic Formula used by the tire friction elipse model
are computed as tireB  4.1, tireC  0.95, tireD  1.1.
A3 Drone Dynamics Modelling
We model the two-dimensional drone dynamics using the following equations,
m cos  g,
where the total thrust F and the torque  are given by,
F  F1  F2,
where is the length from the center of the drone to the center of each rotor, and F1, F2 are thrust forces generated by the left
and right rotors, respectively. The thrust forces are computed considering the ground effect , given by,
F1  Fin1
F2  Fin2
where Fin1, Fin2 are the input thrust commands for the left and right rotors, and  is an intrinsic property of the drone. The
radius of the rotors is r and the heights from the rotors to the ground are zr1 and zr2, which are given by,
zr1  z sin ,
zr2  z  sin ,
where  is the pitch angle.
APPENDIX B
PROOFS FOR VARIATIONAL INFERENCE MPC
B1 Derivations of the Variational Inference Updates
Let qv be the density function of a Gaussian distribution with mean v and covariance , i.e.,
qv(u)  Z1
2(u v)1(u v)
where the normalization constant Zq is independent of the mean v. We wish to solve the variational inference problem
p(u  o  1) qv(u)
Expanding and ignoring terms unrelated to v, we get
p(u  o  1) qv(u)
p(u  o  1) log
p(u  o  1)
p(u  o  1) log (p(u  o  1)) du
p(u  o  1) log(qv(u)) du
p(u  o  1) log(qv(u)) du  O(1)
p(u  o  1)
2(u v)T1(u v)  log Zq
du  O(1)
2(u v)T1(u v)  log Zq
2(u v)T1(u v)
Taking the derivative with respect to v and setting to zero, we get
v Ep(uo1) [u] .
B2 Derivations of the Self Normalized Importance Sampling Estimator
We next derive the self-normalized importance sampling (SNIS) estimator v in (9) of vin (8). Note that, since Z
exp(J(u))p0(u) du is unknown, we cannot compute the weights (u) in (8) directly. Hence, the regular importance
sampled Monte Carlo estimator
is not computable. Instead, we can use the SNIS estimator , which can be derived as follows. First, note that Z can be
written as the expectation of exp(J(u)), i.e.,
Z  Ep0(u) [exp(J(u))] .
exp(J(ui))p0(ui)r(ui),
where Z is a normal importance sampled Monte Carlo estimator of Z. If we use the estimate Z to compute the weights
(u) in the normal importance sampled Monte Carlo estimator v (B.11), we obtain the SNIS estimator as in (9) and (8)
exp(J(ui))p0(ui)
j1 exp(J(uj))p0(uj)r(uj)
exp(J(ui))p0(ui)
j1 Z1 exp(J(uj))p0(uj)r(uj)
Z1 exp(J(ui))p0(ui)
j1 Z1 exp(J(uj))p0(uj)r(uj)
B3 MPPI as a Special Case of the Variational Inference Update
Theorem 6. When p0(u)  q0(u) : qv0(u) and r(u)  qv(u) for some previous estimate of the optimal control sequence
qv(u)  Z1
2(u v)1(u v)
By choosing p0(u)  q0(u) and r(u)  qv(u), we have that
2(u v)1(u v)
2(u v)1(u v))
2u1u u1v  1
exp(u1v  1
(u)  Z1 exp(J(u))p0(u)
Z1 exp([J(u)  u1v]) exp(1
exp([J(u)  u1v]),
where the last line follows since terms not dependent on u can be canceled out between the numerator and denominator in
APPENDIX C
NCBF TRAINING DETAILS
The definition of the avoid set is given by (2),
where the avoidance heuristic h(x) is used to define the avoid set. Furthermore, the definition of the policy value function
V h,(x0) given by (19), and the ensuing training losses (24), (26) are all dependent on h(x). A suitable selection of the
avoidance heuristic h(x) must ensure that its corresponding avoidance set A encompasses all system states that overlap with
any obstacles. Additionally, the heuristic should improve the quality of information captured by the loss function (26), thereby
making it more effective for neural network training. To this end, a reasonable choice of the avoidance heuristic utilizes
coordinates in (A.2),
h0(x)  w2
where wI  1.5 m is half of the track width, and ey is the lateral deviation of the vehicle CoM to the track centerline. The
visualization of h0(x) is demonstrated by the orange curve in Fig. 19.
Fig. 19: Avoidance heuristic visualization. The orange curve shows the original avoidance heuristic h0, while the blue curve
demonstrates the modified avoidance heuristic h used for training the NCBF.
(x) of the policy value function V h0, given by (19) may not accurately
distinguish the unsafe states in the avoid set A0  {xh0(x) > 0} from the rest of the state space, due to the fact that trained
neural networks can suffer from insufficient accuracy and precision [22, 40]. This is demonstrated by the orange line and its
error margins in Fig.20. To alleviate this problem, we introduce discontinuity to h0(x) for enhanced tolerance of policy value
function modelling errors while maintaining the same avoid set (2), such that,
where our choice of the avoidance heuristic is given by,
if ey < wI,
if wI ey wO,
if wO ey,
where wO is thecrash width. If wO < ey, the vehicle crashes and needs to be reset. If wI ey wO, the AutoRally collides
with the soft drainage pipes but can still continue driving. As shown by the blue curve in Fig. 19, h(x) is designed to reduce
the error of the avoid set of the resulting NCBF by introducing a discontinuity around zero. This is further demonstrated in Fig.
Fig. 20: Improving accuracy and precision of NCBF modeled avoid set boundary by using the modified heuristic h(x). When
using the original avoidance heuristic h0(x) to supervise the NCBF training process, the resulting modeled avoid set boundary
can be anywhere within ey (a, b) due to model errors. Instead, using h(x) to supervise the training process results in a
modeled avoid set with an accurate boundary shown by the dashed blue line, despite model errors.
Fig. 21: NCBF Training. The left figure shows collected system states (yellow dots) in AutoRally simulations. The right figure
visualizes the resulting Neural CBF. The NCBF B(x) takes the 8-dimensional state x as input. The red color indicates an
unsafe region where B(x) > 0, while the blue color indicates a safe region where B(x) 0.
The orange dots in the left plot in Fig. 21 represent the vehicle states collected using the training data. The dots outside of
the track show that the autonomous car went off the track at several turns. These data are used to train a neural DCBF, leading
to a visualization as demonstrated by the plot on the right in Fig. 21,in which the NCBF B(x) is mapped onto the 2D Cartesian
plane by utilizing mean values from the training dataset for the remaining state dimensions. The red color shows the area where
B(x) > 0 , and as a result, the neural CBF slows down the Autorally when it approaches sharp turns to ensure safety.
From Fig. 22, we see that the learned NCBF B(x) has a smaller safe set (negative level set) than the avoid set heuristic
h(x), as it turns positive earlier than h(x) when the vehicle approaches the track boundary. When used as a safety filter, B(x)
can slow down the autonomous vehicle much earlier, making its negative level set control-invariant.
APPENDIX D
PROOFS FOR RESAMPLING-BASED ROLLOUTS (RBR) SUPERVISED BY A CBF
D1 Proof of Theorem 1
the identity function . Since B(x1) (Id ) B(x0), it follows that,
B(xk) (Id )k B(x0).
Since (Id ) is a class- function for a (0, 1), it follows from B(x0) 0 that B(xk) 0. Hence, the set S is forward
invariant for system (1).
Fig. 22: A simulation example of the value change of the learned Neural CBF B(x) compared to the avoid set heuristic h(x).
The orange trajectory produced by the standard MPPI results in a collision with the track boundary. The square purple dot
shows the point where B(x) shifts from negative to positive values, corresponding to the purple vertical line in the curve
plot above, which shows the value change of the heuristics B(x), h(x) along the MPPI trajectory. The dots along the track
centerline are observations collected starting at the square red dot, used to augment the training data.
D2 Proof of Theorem 3
f(b  a, b)  1bS(b b)  1bS(b a).
Let f denote the marginal density of b. Then, using the independence of a and b,
f(b  a, b)f(a  a S) da
Simplifying the inner integral first using properties of the Dirac delta function gives
f(b  a, b)f(a  a S) da
1bS(b b)  1bS
(b a)f(a  a S) da
1bS(b b)  1bSf(b  b S).
1bS(b b)  1bSf(b  b S)
f(b)1bS  f(b  b S)
1bSf(b) db
f(b  b S)P(b S)  f(b  b S)P(b S)
f(b  b S).
D3 Proof of Corollary 1
2E[w(b)],
2E[w(x)],
E[w(x)].
D4 Proof of Theorem 4
We prove the two claims separately.
Lemma 7. The variance of the Monte Carlo estimator of the optimal control law is
Var[vk]  1
[1,1]K p(u  o  1)uk du
[1,1]K 1u[0,1]uk du
[0,1]K uk du
Using the formula for computing the variance of an importance sampled Monte Carlo estimator , we then have that
Var[vk]  1
(ukp(u  o  1) v
(ukp(u  o  1) 1
[0,1]K(uk)2 du 2K
[0,1]K uk du  1
Proof of second claim. We now prove the second claim. When performing resampling-based rollouts, only if ui
k lies in [1, 0]
for all k  0, . . . , K 2 does resampling not occur, and the output ui
k [1, 0] for k  0, . . . , K 2. Otherwise, we have
k [0, 1] for k  0, . . . , K 2. The last control ui
K1 is never resampled. Hence, the probability that all N
samples lie in [1, 0]K1  [1, 1] is equal to 2N(K1).
For all timesteps except the last, we write the joint probability density function (over all N samples) as
For convenience, let t : 2N such that P(u1:N
[0, 1])  1 t for k < K 1, and
2(1 t)K1 ui
We will compute the expectation of vk using the law of total expectation. To this end, we have that
2(1 t)K1 ui
2(1 t)K1 ui
2(1 t)K1 ui
(1 t)K1Eu1:N
2(1 t)K1 ui
We now split into two cases. When k  K 1,
K1[0,1] ui
K1[0,1]ui
K1[0,1]1ui
Let c : Eu1:N
2(1t)K1 for convenience. Computing the conditional variances, for u1:N
have that
For u1:N
2(1 t)K1 ui
We now split into two cases.
Case 1: k  K 1. Looking at the first term of in (D.46),
K1)2  u1:N
K1[0,1](ui
K1[0,1](ui
For the second term in (D.46), we have
K1  u1:N
K1[0,1]1uj
K1[0,1]ui
K1[0,1]ui
Substituting the two terms (D.49) and (D.54) into (D.45), yields
Case 2: k < K 1. Looking at the first term of in (D.46),
k)2  u1:N
k)2  u1:N
k)2  u1:N
From Theorem 3, the marginal of any resampled controls will all be uniform over [0, 1]. Hence,
k)2  u1:N
For the second term in (D.46),
K1[0,1]1uj
To make this computation easier, let ij  1 denote the event that ui
k and uj
k for i  j were resampled from the same control,
i.e., ui
k and uj
k are the same random variable, and therefore it follows that
k  ij  1  u1:N
k and uj
k are independent. Hence,
k  ij  1  u1:N
Combining the two terms (D.61) and (D.69), we thus have that
Var[vk]  Esign of u1:N
Varsign of u1:N
The first term gives
1  P(u1:N
The second term gives
2  P(u1:N
Var[vk] 3
D5 Proof of Theorem 5
To begin, we expand w to obtain
Simplifying the first term, yields,
Substituting (D.91) back into the parenthesis (D.89) gives us that
Simplifying the parenthesis in (D.94), we obtain
Using our assumption that c is not drastically larger than w in (38) then gives us the desired result.
APPENDIX E
DETAILS ON VIMPC COST DESIGNS
In both simulations and experiments, MPPI variants using DCBF to ensure safety utilized a state-dependent cost function:
k )  (xm
k xg)Q(xm
whereas the standard MPPI employed the cost,
k )  (xm
k xg)Q(xm
k xg)  1(xm
where Q  diag(qvx, qvy, q , qF , qR, qe, qey, qs) represents the cost weights, and xg  diag(vg, 0, . . . , 0) specifies the target
velocity. The collision cost function is defined as:
k is within the track,
otherwise.
Unlike standard MPPI, Shield-MPPI and NS-MPPI incorporate a DCBF constraint violation penalty but do not include an
explicit collision cost.
APPENDIX F
CONSTRAINT SATISFACTION ON THE VARIATIONAL DISTRIBUTION
One way of guaranteeing that the variational distribution q(u) satisfies the constraints xk
strong (but unrealistic) assumption that the set of controls that satisfy the constraints is a convex set. Specifically, for generic
state constraints x
safe UK as
safe(x0)
u UK  xk
We then have the following lemma.
Lemma 8. Assume UK
safe(x0) is convex for all states x0
safe(x0). Then, the mean v(8) of the variational distribution qv (and the state trajectory resulting from following v) will
also satisfy the constraints, i.e.,
safe(x0).
outside UK
safe(x0), its support is contained within UK
safe(x0). Since UK
safe(x0) is convex, the integral of u over p(u  o  1) will
also be contained within UK
safe(x0). Hence, vUK
safe(x0).
APPENDIX G
RBR PERFORMANCE USING A VALID DCBF
Valid DCBF on DubinsCar. We evaluate a valid DCBF on DubinsCar with x  [px, py, ], u  [ ] (Fig. 23). We also add a
high-cost region and a constraint that  2.
At K  1, RBR has no effect (no resampling). With U  R, a valid DCBF allows safe sampling with just N  50, and both
S-MPPI and NS-MPPI avoid crashes.
As estimator variance grows exponentially with K, S-MPPI crashes at larger K. RBR (NS-MPPI) mitigates this, maintaining
safety across all horizons.
Higher K is needed to avoid the high cost region. The cost slowly increases with K 10 from estimator variance.
Control Horizon K
Cum. Cost of Safe Traj
Crash Rate
Control Horizon K
S-MPPI (Valid DCBF)
NS-MPPI (Valid DCBF)
Obstacle
High cost
Sampled Rollouts
Trajectory
Fig. 23: (a) Cumulative cost of safe trajectories using valid DCBF on DubinsCar with N  50. MPPI is unsafe for K < 15
and omitted. (b) Higher K is needed to avoid the high-cost region.
