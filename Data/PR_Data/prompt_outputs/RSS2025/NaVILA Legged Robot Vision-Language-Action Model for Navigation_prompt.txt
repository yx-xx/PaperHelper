=== PDF文件: NaVILA Legged Robot Vision-Language-Action Model for Navigation.pdf ===
=== 时间: 2025-07-22 09:59:51.954275 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Model for Navigation
An-Chieh Cheng1,
Yandong Ji1,
Zhaojing Yang2,
Zaitian Gongye1
Xueyan Zou1
Jan Kautz3
Erdem Byk2
Hongxu Yin3,
Sifei Liu3,
Xiaolong Wang1,3,
1UC San Diego
Move forward out of the room. Turn right at the end. Proceed to the grass and stop in front of the soccers.
Walk forward along the way. Turn a little left and keep going straight. Stop in front of the red door.
Turn left immediately. Go straight forward. Step on the mat. Keep going forward.
Stop when you are very close to the trash can.
Move forward along the way. Turn left at the yellow fire hydrant.
Go forward along the slope and stop in front of the door.
Walk forward, when seeing the stair bars, turn right and walk around the stairs until reaching the hallway.
Turn right and walk along the hallway, stop in front of a bathroom.
Workspace
Move forward and exit the office area. And then turn right and stop near the blue trash can.
Fig. 1: Real-world demonstration of NaVILA: Upon receiving human instructions, NaVILA uses a vision-language model to process RGB
video frames and employs locomotion skills to execute the task on a robot. The robot successfully handles long-horizon navigation tasks
and operates safely in challenging environments.
AbstractThis paper proposes to solve the problem of Vision-
and-Language Navigation with legged robots, which not only
provides a flexible way for humans to command but also allows
the robot to navigate through more challenging and cluttered
scenes. However, it is non-trivial to translate human language
Equal contribution, ordered alphabetically.  Equal advising.
instructions all the way to low-level leg joint actions. We propose
Action model (VLA) with locomotion skills. Instead of directly
predicting low-level actions from VLA, NaVILA first generates
mid-level actions with spatial information in the form of language,
(e.g., moving forward 75cm), which serves as an input for a
visual locomotion RL policy for execution. NaVILA substantially
improves previous approaches on existing benchmarks. The
same advantages are demonstrated in our newly developed
benchmarks with IsaacLab, featuring more realistic scenes, low-
level controls, and real-world robot experiments.
I. INTRODUCTION
The ability to perform Vision-and-Language Navigation
(VLN) has become a foundational component in modern
robotics systems. With VLN, a robot is expected to navigate
around unseen environments without a provided map following
a language instruction [16]. This not only offers a better inter-
face for humans, but also strengthen cross-scene generalization
through languages. In this paper, we further extend the study
of VLN with legged robots (e.g., quadruped or humanoid).
Using legs instead of wheels allows robots to navigate in more
challenging and cluttered scenarios. As the examples shown
in Fig. 1, our robot can navigate through a messy laboratory
space with narrow walkways, transition from room to room in
a house, as well as tackle outdoor challenging environments
such as uneven terrains with small rocks, holes, and troughs.
To translate language to action, the robot needs to reason
about the input language, and perform closed-loop planning
as well as low-level control. With the recent advancement in
Large Language Models (LLMs) and Vision-Language Models
(VLMs), several end-to-end Vision-Language-Action (VLA)
systems have been developed [79]. These systems fine-tune
a general-propose VLM with large-scale robot manipulation
demonstrations to produce low-level actions. While unifying
reasoning and execution in a single model is fascinating and
shows encouraging results, it is worth diving deeper into the
the quantized low-level commands? After all, LLMs and
VLMs were primarily trained with natural language. Unifying
reasoning and execution becomes challenging when we need
to convert that reasoning into precise, non-verbal actions.
Inspired by the recent progress on VLM [10, 11] for spatial
location and distance reasoning, we propose NaVILA, a two-
level framework for legged robot VLN: A VLM is fine-
tuned to output a mid-level action (VLA) in the form of
language such as turn right 30 degrees, and a low-level
visual locomotion policy is trained to follow this instruction
for execution. The mid-level action output of the VLA conveys
the location and direction information without the low-level
commands. The advantages of this framework are three-fold:
(i) By decoupling low-level execution from VLAs, the same
VLA can be applied across different robots by swapping
the low-level policy; (ii) Representing actions as mid-level
language instructions enables VLA training with diverse data
This enhances reasoning capabilities without overfitting out-
puts to specific low-level commands and can leverage real-
world data for generalization; (iii) NaVILA operates on two
distinct timescales: the VLA, typically a large and computa-
tionally intensive model, runs at a lower frequency, providing
high-level navigation commands; while the locomotion policy
operates in real-time. This dual-frequency approach allows the
locomotion policy to handle sophisticated obstacle avoidance
and increases overall robustness.
To train the VLA, we demonstrate how to (i) integrate
historical context and current observations in VLN within
existing VLM frameworks, (ii) create a specialized navigation
prompt tailored for VLN tasks, (iii) utilize real-world data
from YouTube human touring videos to improve navigation
in continuous environments, and (iv) introduce a carefully cu-
rated dataset blend designed to enhance VLN generalizability.
These strategies allow us to fine-tune a general-purpose image-
based VLM into a navigation-focused agent while simultane-
ously training it on general vision-language datasets, thereby
maintaining its broad generalization capabilities. Moreover,
this is the first work to show that direct training on human
videos improves navigation in continuous environments.
To train robust locomotion skills, we employ a single-
stage approach to learn vision-based locomotion policy. We
construct a height map from raw LiDAR point clouds and
introduce randomization to bridge the sim-to-real gap. This
controller takes the output from our VLA model, converts
it into command velocities, and tracks these velocities by
controlling the positions of the joints. This end-to-end ap-
proach enables the training of visual locomotion skills that
are both robust and safe, facilitating deployment in real-world,
challenging environments (e.g., strong sunlight or near certain
transparent surfaces).
In our experiments, we show that our VLA significantly
outperforms the state-of-the-arts on classic VLN benchmarks,
with over 17 improvement in success rate. Additionally, our
single-stage locomotion policy outperforms previous policy
distillation-based methods by a substantial margin. To better
simulate the challenges of locomotion navigation in VLN, we
introduce a new benchmark, VLN-CE-Isaac, using Isaac Sim.
This benchmark considers detailed robotic joint movements
and interactions with environments, which prior VLN works
have not explored. In our VLN-CE-Isaac experiments, our
vision-based policy outperforms the blind policy by a signif-
icant margin, showing a 14 improvement in success rate.
We also demonstrate that our VLA can be deployed across
different robots (Unitree Go2, Unitree H1, Booster T1), each
using distinct locomotion skills. Finally, we deploy NaVILA in
the real world, exhibiting impressive robustness and achieving
an 88 success rate on 25 instructions, including a 75
success rate on complex instructions across diverse scenes.
II. METHOD
NaVILA integrates high-level visual language understand-
ing with low-level locomotion control (Fig.2). It employs a
VLM to process single-view images and generate waypoint
instructions in natural language, which a locomotion policy
translates into precise joint movements for real-time robot
control. The synergy between the VLMs reasoning and the
locomotion policys execution enables NaVILA to generalize
across diverse environments. We first describe how we tame
VLMs for high-level VLN in Sec.II-A, then outline our robot
configuration and locomotion policy in Sec. II-B.
Move forward 75 cm.
Walk forward and turn right.
Pass the rug and stop by the desk.
Instruction
Positions
History Views
Velocity
Commands
Proprioception
Prior Actions
Joint Pos.  Vel.
Orientation
Angular Velocity
Height Map
Current View
Fig. 2: NaVILA is a two-level framework combining high-level visual language understanding with low-level locomotion control. Our VLA
model processes single-view images to produce mid-level actions in natural language, which are then converted into precise joint movements
by an advanced low-level locomotion policy. This integration allows for strong generalization and adaptability across different real-world
A. Taming VLMs for Vision Language Navigation
VLN requires processing video inputs as observations. A
common approach to handling video inputs in VLMs is
through video encoders . However, recent progress in
VLMs has largely been driven by the availability of image-
text data. While there have been efforts to extend this success
to video encoders, the lack of large, high-quality video-
text datasets has limited their pre-training. To address this
our approach. These models exhibit stronger generalization
abilities and possess broader knowledge, making them more
suitable for tackling the generalization challenges in VLN.
ily of efficient VLMs for both understanding and generation.
VILAs pre-training has proven particularly effective for multi-
image reasoning, making it especially suitable for VLN tasks
where understanding sequential image relationships is critical.
VILA Preliminary. VILA consists of three main compo-
sion encoder processes the input images, converting them
into a sequence of visual tokens. These tokens are then
downsampled and mapped into the language domain via
an MLP projector. Afterward, the projected tokens, along
with text tokens, are sent to the LLM for auto-regressive
generation. When handling videos, VILA uniformly sampled
frames at regular intervals. It puts all the frame information
before any text. A typical prompt for describing a video
might look like frame3frame6frame9...Tell
me about this video. Notably, with sequence par-
allel training , VILA can include frames up to 1024.
VILA undergoes a 3-stage training process: first, it pre-trains
a connector between the frozen LLM and vision backbones
using alignment data ; then it pre-trains both the connector
and the LLM using text-image interleaved corpus [21, 22]; and
LLM) with instruction tuning data [20, 23].
Navigation Prompts. In vision-language navigation tasks,
images from different time steps serve two distinct purposes.
The image at time step t represents the current observation,
which is crucial for a VLN agent to make immediate decisions
(e.g., turning right at an intersection or stopping when the goal
is reached). On the other hand, frames before time step t are
historical frames that function as a memory bank, helping the
agent track overall progress (e.g., remembering the starting
the next step). Uniformly sampling frames at regular intervals,
as done in VILA, is not ideal because it doesnt differentiate
between these two types of representations. Therefore, we first
extract the most recent frame t as the current observation and
then uniformly sample frames from the preceding t1 frames,
ensuring the first frame is always included. Additionally, since
current and historical observations serve different roles, we
distinguish them in our task prompt using textual cues like a
video of historical observations: for memory
frames and current observation: for the latest frame.
that could complicate the LLMs learning process. Instead, we
adhere to our design principle of keeping both the input and
output of LLM in the language domain to fully leverage the
reasoning capabilities of the pre-trained LLM. By integrating
these tokens for historical and current observations with the
navigation instruction, we construct a navigation task prompt,
as shown in Fig. 2.
Learning from Human Videos. Recent studies [2426] have
shown that collecting trajectory-instruction pairs from human
videos can enhance navigation capabilities. However, prior
work has been limited to discrete navigation settings and has
mainly used real videos for pre-training to reduce domain
gaps or improve landmark understanding, rather than for
directly training navigation models. Extending this approach
to continuous settings presents a significant challenge due to
the difficulty of obtaining continuous action labels. Recent
advances in metric-pose estimation in the wild have now made
this feasible, enabling us to extract spatial understanding from
human videos and train navigation models directly.
Our data pipeline, shown in Fig. 4, starts with 2K egocentric
touring videos from YouTube, which provide a rich source of
real-world data to learn robot navigation from human behavior.
We process these videos into 20K diverse and representative
History Views
Current View
Vision Encoder
Down Sample  Projector
The next action is
Walk forward and turn right.
Pass the rug and stop by the desk.
Imagine you are a robot programmed for navigation tasks.
You have been given a video of historical observations:
and current observation:
Your assigned task is:
Analyze this series of images to decide your next move,
which could involve turning left or right by a specific degree,
moving forward a certain distance, or stop if the task is completed.
Large Language Model
Navigation Prompt
196 tokens
196  t tokens
Fig. 3: Overview of our VLA framework. We denote the purple blocks ( ) as memory tokens sampled from historical frames, and the red
blocks ( ) as the current observation tokens. denotes trainable parameters. In our experiments, we tested configurations with 8 to 64
frames for t.
trajectories using entropy-based sampling . Next, we es-
timate camera poses using MASt3R  to extract step-by-
step actions, and we generate natural language instructions for
each trajectory using VLM-based  captioning followed by
LLM  rephrasing. This approach allows us to leverage
human demonstrations for continuous navigation, a capability
that was previously non-trivial to achieve.
Supervised Fine-tuning Data Blend. Effective Supervised
Fine-tuning (SFT) data is crucial for developing a robust
vision-language action model. The model should specialize
in embodied tasks while avoiding overfitting to specific ac-
tions. It should also generalize well to real-world scenarios
while retaining broad-world knowledge. Thanks to NaVILAs
modular framework, which offers exceptional scalability and
is straightforward. This flexibility allows us to enhance gener-
alizability for navigation. Our SFT data blend is designed from
four perspectives: (1) Navigational data from real videos, (2)
Navigational data from simulations, (3) Auxiliary navigational
For simulated navigational data, the available VLN datasets
in continuous environments are limited, with only R2R-
CE  and RxR-CE  providing sparse path points con-
verted from discrete VLN versions. We leverage both datasets
within the Habitat simulator, using a shortest path follower
to generate action sequences along the geodesic shortest path.
This results in step-wise navigation videos, where each sample
comprises a (t  1)-frame video and the corresponding oracle
action at time step t. To encourage the LLM to generate
continuous value labels for distances and angles, we merge
consecutive actions (e.g., combining two forward 25 cm steps
into a single forward 50 cm step), with a maximum of three
consecutive actions. This merging process not only reduces
dataset size for more efficient processing but also introduces
greater diversity in actions, mitigating overfitting. Addition-
sentation of the stop actionwe apply a rebalancing technique
for a more even distribution. All navigation-specific data
undergo the previously described frame extraction strategy and
are paired with navigation task prompts.
To further improve scene understanding and address the
limited instructions in R2R-CE and RxR-CE, we incorpo-
rate auxiliary navigational datasets. Following , we use
augmented instructions from EnvDrop  and introduce an
auxiliary task of navigation trajectory summarization. Given
a trajectory video, we sample frames by retaining the first
frame and uniformly selecting historical frames, using the
annotated instructions as labels. The LLM is then tasked
with describing the robots trajectory based on these frames.
To further enhance spatial scene understanding, we integrate
the ScanQA  dataset, which features real-world 3D scan
QA pairs with human-edited questions and free-form answers
grounded in 3D objects. For training, we use multi-view RGB
images from the raw scans to support this task.
incorporate general VQA datasets from [23, 33, 34]. This com-
prehensive dataset design ensures that NaVILA can generalize
effectively to novel scenes and real-world environments.
Training and Inference Paradigm. Our training process
begins with the stage two model of VILA, which has al-
ready undergone visual language corpus pre-training. We then
apply our SFT data blend to train the entire VLM for one
three componentsvision encoder, connector, and LLMare
unfrozen. For the inference phase, we implement a regular
expression parser , to extract action types (such as forward
or turn left) and their corresponding arguments (like specific
distance or angles) from the LLM output. This method has
demonstrated effectiveness in both simulated environments
and real-world experiments, where we empirically found
that all actions throughout all experiments are successfully
matched and mapped.
VLM  LLM
Youtube Videos
Entropy-based Trajectory Sampling
Instructions
Step-wise Actions
Metric Pose Est.
(MASt3R)
Turn right 30 degrees.
Walk down the hallway and
enter the dining room.
Fig. 4: Data pipeline for transforming human touring videos in the
wild into pairwise navigation data within a continuous environment.
We begin by processing the videos into meaningful trajectories
through entropy-based sampling . Then we extract step-wise
actions through metric camera pose estimation , and utilize
VLM  and LLM  to generate instructions.
B. Visual Locomotion Policy
In this section, we begin with a brief overview of the Go2
robot dog, the experimental platform used in this work. Next,
we describe the development of the end-to-end vision-based
control policy, which interprets high-level language navigation
commands from the VLM and converts them into precise joint
movements. This control policy is trained in the Isaac Sim
simulator using Isaac Lab  and then directly deployed to
the real-world robot.
Go2 Robot. As shown in Fig. 5, the robot is equipped with a
LiDAR sensor mounted at the base of its head, broadcasting
point clouds at a frequency of 15Hz. The robot features 18
degrees of freedom (DoFs), comprising 6 DoFs for its base
and 3 DoFs for each of its four legs. In the policy training
the policy only controls the 12 joint motors on the legs.
Interpreting High-level Commands. As in our formulation,
VLM outputs a fixed set of actionable words, such as {move
tions to fixed command velocities {0.5 m s1,
6 rad s1,
6 rad s1, 0} and execute with corresponding time durations
to align with the specific VLM value.
Low-level Action and Observation Space. The action space
a of the control policy is defined as the desired joint position
qd R12, which is converted into torque input for the
simulator using the stiffness and dampness. We adopt the PPO
algorithm  to train the policy. During training, the critic
observes the privileged environment and generates a value
function to update the actor, while the actor only receives
sensor data available in the real world. The observation space
of the critic oc contains the proprioception and velocity com-
mand at the current time step t and a privileged terrain height
scan around the robot. The proprioceptive data includes robot
linear and angular velocity, orientation, joint positions, joint
the real world, and instead, a history of proprioceptive data is
used to infer this information implicitly. The robot perceives
height (m)
(a) Simulation
(b) Real
Height Map
Fig. 5: Height map reconstruction from point cloud. (a) Go2 robot
follows velocity commands while avoiding obstacles in simulation.
Red dots show LiDAR points raycasting from the sensor center to the
terrain mesh. The right image shows a preprocessed height map with
values clipped to sensor constraints; darker colors indicate higher
heights. (b) Safe locomotion near glass. The top-down height map
detects glass surfaces where depth and RGB images fail.
the surrounding terrain using a height map from the LiDAR
Incorporating Height Map from LiDAR Point Cloud.
Given LiDARs superior ability to detect transparent objects
and robust performance under strong sunlight, we chose the
manufacturer-provided LiDAR as the primary sensor for per-
ceiving the robots surroundings and ensuring safe navigation.
The Unitree L1 generates point clouds with a wide field of
view of 360 90, from which we create a 2.5D height map
based on the parameters listed in the Supplementary. For each
voxel grid, the lowest value within the range is selected, and
a maximum filter is then applied over the last 5 lidar point
clouds to smooth the resulting height map.
Training. Different from most existing works [3841] that
utilize the two-stage teacher-student training paradigm, we
adopt a single-stage manner to train the locomotion policy.
Compared to two-stage training, single-stage RL is more time-
efficient as it eliminates the need for policy distillation. Ad-
allowing it to explore and potentially discover novel strategies.
With the support of ray-casting in Isaac Lab, our vision-based
RL policy training achieves a high throughput over 60K FPS
on an RTX 4090 GPU.
III. EXPERIMENTS
We conduct experiments to answer the following questions:
(1) How does our VLAs performance compare to state-of-the-
art methods in VLN-CE benchmarks and general spatial scene
understanding tasks? (Sec. III-A) (2) How does the perfor-
mance of our single-stage visual locomotion policy compare
to policy distillation-based approaches? (Sec. III-B) (3) How to
evaluate locomotion navigation in simulators, and how effec-
tive and flexible is NaVILA in these scenarios? (Sec. III-C) (4)
Can NaVILA pipeline be successfully deployed in real robot
VLN experiments? (Sec. III-D)
A. High-level VLA Performance
VLN-CE Benchmarks. We evaluate our VLA on the VLN-
CE benchmarks, which provide continuous environments for
executing navigational actions in reconstructed photorealistic
indoor scenes. We focus on the val-unseen split in both
R2R (Room-to-Room) and RxR (Room-across-Room) datasets
TABLE I: Comparison with state-of-the-art methods on the Val-Unseen split of R2R-CE  and RxR-CE . indicates methods using
the waypoint predictor from Hong et al. . NaVILA outperforms all methods that do not rely on simulator pre-trained waypoint predictors,
even when those methods leverage additional inputs such as depth, panoramic views, and odometry.
Observation
R2R Val-Unseen
RxR Val-Unseen
Ego2-Map
DreamWalker
HAMTScaleVLN
R2R-CMTP
WS-MGMap
AO-Planner
RGB-Seq2Seq
TABLE II: Cross-dataset performance on the RxR-CE  Val-
Unseen split. All results are obtained without training on the RxR-
CE training set. NaVILA significantly outperforms NaVid , the
current single-view state-of-the-art.
Observation
RxR Val-Unseen
S.RGB Depth Odo.
NE OS SR SPL
WS-MGMap
RGB-Seq2Seq
within VLN-CE, as these are the two most recognized bench-
marks in VLN. We employ the following widely used evalua-
tion metrics for VLN tasks: Navigation Error (NE), Oracle
Success Rate (OS), Success Rate (SR), Success-weighted
Path Length (SPL), and normalize dynamic time wrapping
(nDTW). We show results in Table I, where NaVILA sig-
nificantly surpasses all baselines that do not rely on simulator
pre-trained waypoint predictors in both benchmarks using a
single model. Notably, this also marks the first time a VLN
comparable or superior results to models that use panoramic
This suggests that NaVILAs strong generalization capabilities
can effectively compensate for the limited observations in
RGB views or sensors.
To evaluate the cross-dataset performance, we follow
by training NaVILA exclusively on R2R samples, while
leaving out the RxR training set. We then evaluate its zero-
shot performance on the RxR Val-Unseen split. As shown
in Table II, our method significantly outperforms NaVid,
the current state-of-the-art model, with a substantial 10
improvement in SR.
Spatial Scene Understanding Benchmarks. As a general
navigation agent, robust spatial scene understanding (e.g.,
object localization, referring, and spatial reasoning) is crucial.
To evaluate NaVILAs capabilities in scene understanding, we
conduct evaluations on the ScanQA Validation benchmark, a
widely used dataset for 3D Question Answering. ScanQA is
based on real-world scans, and we use multi-view images from
these scans as input to query NaVILA for answers. As shown
in Table III, NaVILA significantly outperforms the previous
state-of-the-art model, NaviLLM , by a substantial margin
(20 points higher on the CIDEr score). Moreover, when using
64 frames, NaVILAs performance demonstrates superior per-
formance compared to state-of-the-art 3D-based large multi-
modal models [61, 62]. This is particularly noteworthy as
these other models require either 3D scans or RGBD data
with camera poses as inputs, while our method achieves better
results with less observation.
B. Low-level RL Policy Performance
To highlight the advantages of our RL policy over policy
distillation-based approaches, we compared it to Regularized
Online Adaptation (ROA) . In ROA training, the model
first learns a privileged encoder that processes height scan
points and other privileged observations. This privileged en-
coder then supervises an adaptation encoder, which takes the
same 2.5D heightmap as our low-level policy as input. We
evaluated both approaches using three metrics: linear velocity
TABLE III: Evaluation of spatial scene understanding performance
on the ScanQA dataset  Validation split. NaVILA outperforms
current state-of-the-art VLA models and demonstrates superior per-
formance to other 3D LMMs that require additional input, such as
depth or camera pose. Note that indicates 3D LMMs that require
task-specific fine-tuning on the ScanQA dataset.
ScanQA Validation
Bleu-4 Rouge Cider Meteor EM
Task-specific Specialist
VoteNetMCAN
ScanReferMCAN
3D-VisTA
3D Large Multi-modal Models
3D-LLM(flamingo)
3D-LLM(BLIP 2flant5)
Chat-3Dv2
Scene-LLM
2D Vision-Langauge-Action Models
NaVILA (8 frames)
NaVILA (64 frames)
metrics assess how accurately the policy follows velocity
avoidance capabilities. As shown in Table V, our low-level
policy outperforms ROA in all three metrics, particularly
achieving a significantly lower collision rate, demonstrating
the effectiveness of our training approach.
C. Legged Robot Navigation Performance in Simulation
High-fidelity VLN-CE-Isaac Benchmark. Currently, there
are no VLN-CE benchmarks tailored specifically for legged
robots. Existing benchmarks [29, 30] for vision-language nav-
igation are based on the Habitat  simulator, which focuses
on high-level planning without addressing precise low-level
robotic control. For instance, agents in Habitat can navigate
through narrow gaps, such as a 10 cm space between two
or humanoids. To overcome this limitation, we introduce
a new benchmark VLN-CE-Isaac built on Isaac Sim. Isaac
Sims high-fidelity simulation captures detailed robotic joint
movements and interactions with the environment, enabling
comprehensive evaluations of the entire navigation pipeline,
from high-level planning to precise robotic execution. We
incorporate the same scenes from R2R, with robots deployed
in the environment, as shown in Fig. 6. From the 1,839
trajectories in the R2R Val-Unseen split, we select 1,077
traversable trajectories with high-quality meshes to ensure
realistic navigation scenarios. For consistency, we evaluate
performance using the same metrics as prior work.
robotic platforms. To demonstrate this flexibility, we test
our NaVILA model on a Unitree Go2 robot and also a
Unitree H1 robot within the benchmark. To highlight the
effectiveness of the vision-based policy, we compare it against
a proprioception-only (blind) policy. As shown in Table IV,
Fig. 6: VLN-CE-Isaac Benchmark visualization.
TABLE IV: VLN-CE-Isaac evaluation results.
Low-level Observation
VLN-CE-Isaac
Proprio. LiDAR Height Scan
NE OS SR SPL
Unitree Go2
NaVILA-Blind
NaVILA-Vision
Unitree H1
NaVILA-Blind
NaVILA-Vision
TABLE V: Low level policy performance.
Linear Vel.
Angular Vel.
Collision
ROA(wBCLoss)
the vision-based policy outperforms the blind policy by 14
in Success Rate in Go2 settings and 21 in H1 settings,
owing to its superior obstacle avoidance capability. We also
compare NaVILAs with a baseline using Oracles low-level
policy (assuming perfect command execution without realistic
physics). Results show a 15 lower success rate on the
Go2 setup and a 27 lower success rate on the H1 setup
when Oracle policy is not presented. These performance gaps
highlight the increased challenges and realism introduced by
our benchmark. Additionally, we also observe that the success
rate of NaVILA on the H1 robot is significantly lower than
on the Go2, which is expected due to the larger size of the
humanoid robot.
D. Real World Evaluation
We then conduct experiments in the real world, using 25
ple and complex tasks across three types of environments:
ple instructions consist of one or two navigation commands,
where the robot does not need to navigate between rooms (e.g.,
Go to the chair and stop). In contrast, complex instructions
involve three or more commands, requiring the robot to
traverse multiple rooms or landmarks (e.g., Walk out of the
table). We use standard metrics (SR and NE) and compare
NaVILA against GPT-4o, a state-of-the-art VLM known for
its strong generalizability. As shown in Table VI, NaVILA
significantly outperforms GPT-4o in both SR and NE. We
also ablate the effectiveness of adding human videos, and the
TABLE VI: Real-world experiments on quadruped (Unitree Go2)
and humanoid (Booster T1) conducted in different environments
(Workspace, Home, and Outdoor). Simple and Complex refer
to simple and complex instruction-following tasks, respectively. Note
that  indicates models trained without human touring videos.
Workspace
NESRNESRNESRNESRNESRNESR
Unitree Go2
Booster T1
results show that with the help of human videos, the model can
generalize better to outdoor scenes and achieve higher success
rates across all environments. To demonstrate the flexibility
of our two-level approach, we also evaluated it on a Booster
Dynamics T1 humanoid robot, using the same VLA model
without any retraining. Despite variations such as changes in
camera height and camera view angle, NaVILA consistently
outperformes the baselines, highlighting the strong gener-
alization capabilities of our model. Our qualitative results
are presented in Fig.1 and Fig.7. In Fig. 7, we demonstrate
integration with speech recognition, enabling voice-controlled
navigation through our framework. These results highlight the
effectiveness of NaVILA in bridging the gap between vision-
language understanding and real-world navigation tasks.
IV. RELATED WORK
Visual Navigation. Visual navigation has been a long-standing
research topic in robotics [7174]. Classical methods rely
on pre-computed  or geometric maps built with depth
sensors  or monocular cameras while localizing the robot
(SLAM) [77, 78]. More recently, learning-based approaches
using Imitation Learning [79, 80] and Reinforcement Learn-
ing [81, 82] have demonstrated strong performance and ex-
panded applications to Vision-Language Navigation.
Vision-Language Navigation. Vision-Language Navigation
(VLN) is a fundamental challenge in embodied AI, where
agents navigate complex environments using visual cues and
natural language instructions. The field has evolved signifi-
cantly over time. Early research [1, 30, 83] focused on discrete
navigation in simulated environments like MP3D , where
agents teleport between predefined nodes on a navigation
graph [31, 8591]. As foundation models advanced, many
VLN systems improved dramatically by leveraging large-scale
pre-trained models [25, 92] an
