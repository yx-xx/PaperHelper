=== PDF文件: GauSS-MI Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction.pdf ===
=== 时间: 2025-07-22 16:05:37.283277 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Information for Active 3D Reconstruction
Yuhan Xie, Yixi Cai, Yinqiang Zhang, Lei Yang, and Jia Pan
School of Computing and Data Science, The University of Hong Kong, Hong Kong SAR, China
Division of Robotics, Perception, and Learning, KTH Royal Institute of Technology, Stockholm, Sweden
Faculty of Engineering, The University of Hong Kong, Hong Kong SAR, China
Centre for Transformative Garment Production, Hong Kong SAR, China
AbstractThis research tackles the challenge of real-time
active view selection and uncertainty quantification on visual
quality for active 3D reconstruction. Visual quality is a critical
aspect of 3D reconstruction. Recent advancements such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have
notably enhanced the image rendering quality of reconstruction
models. Nonetheless, the efficient and effective acquisition of
input images for reconstructionspecifically, the selection of the
most informative viewpointremains an open challenge, which is
crucial for active reconstruction. Existing studies have primarily
focused on evaluating geometric completeness and exploring
unobserved or unknown regions, without direct evaluation of
the visual uncertainty within the reconstruction model. To
address this gap, this paper introduces a probabilistic model
that quantifies visual uncertainty for each Gaussian. Leveraging
Shannon Mutual Information, we formulate a criterion, Gaussian
Splatting Shannon Mutual Information (GauSS-MI), for real-
time assessment of visual mutual information from novel view-
implemented within an active reconstruction system integrated
with a view and motion planner. Extensive experiments across
various simulated and real-world scenes showcase the superior
visual quality and reconstruction efficiency performance of the
proposed system.
I. INTRODUCTION
reconstruction
attracting
increasing
interest
across various fields, including computer vision[26, 19],
robotics[23,
Radiance
(NeRF) and 3D Gaussian Splatting (3DGS), have
notably enhanced the visual quality of 3D reconstruction
models. However, these techniques necessitate the prior
acquisition of a significant number of images, which can
be laborious, and the extensive sampling of viewpoints may
result in redundancy. Consequently, a challenging issue arises
in effectively and efficiently selecting the viewpoints for
image capture, which is also a critical problem for active 3D
reconstruction.
To enhance the autonomy of robots and enable them to
perform 3D reconstruction tasks in complex environments,
there has been a growing focus on active 3D reconstruction
in recent years [43, 17, 34]. In the active 3D reconstruction
of past observations to actively determine the next viewpoint
for capturing new observation, thus gradually accomplishing
the reconstruction task. The efficient selection of viewpoints
is particularly crucial in this process due to limited onboard
resources such as battery power, memory, and computation
capability. Previous studies on active 3D reconstruction have
primarily relied on evaluating volumetric completeness to
explore all unknown voxels in the environment [14, 43, 34]
or assessing surface coverage completeness [1, 8]. These
approaches overlook the visual quality. By utilizing these
indirect metrics, the resulting visual fidelity of the recon-
struction model cannot be guaranteed. Advanced by radiance
field rendering methods[19, 26], recent works have attempted
to quantify visual uncertainty to directly evaluate the visual
quality of reconstruction models [31, 11, 15].
Despite these efforts, effectively and efficiently assessing
and optimizing visual quality in active 3D reconstruction
remains challenging. To address this, three core issues must
be resolved. Firstly, a robust mathematical model is necessary
to quantify the information obtained from each measurement,
specifically the observed image. This model can serve as a
reconstruction completeness metric for visual fidelity. Sec-
from novel viewpoints without a prior, which can facilitate the
selection of the next viewpoint in the active reconstruction pro-
cess. Lastly, a comprehensive active reconstruction system is
required to autonomously identify a reasonable next viewpoint
with the highest expected information. The system should then
enable the agent to navigate to the selected viewpoint, capture
new data, and iteratively advance the reconstruction process.
To overcome the aforementioned challenges, this paper
proposes a novel view selection metric based on a visual
uncertainty quantification method, from which we develop
a novel active 3D reconstruction system. We first introduce
a probabilistic model that integrates the measurement model
with image loss to quantify the observed information for
each spherical Gaussian in 3D Gaussian Splatting. Based on
Shannon Mutual Information theory, we leverage the prob-
abilistic model to establish the mutual information between
the reconstruction model and observation viewpoint, which
Refinement
Better Synthesis
Uncertainty Reduce
Viewpoint Selection
Online Reconstruction
Sample Next
Viewpoints
Viewpoint
Observed Viewpoint
Image Loss
Probability
GauSS-MI
Illustration of the proposed Gaussian Splatting Shannon Mutual Information (GauSS-MI) method. Upper part: At each active reconstruction step,
once a new observation is obtained, the 3D Gaussian Splatting (3DGS) map is updated and optimized by minimizing the image loss between observed images
and the map. To quantify visual uncertainty, we construct a probabilistic model for each 3D Gaussian ellipsoid by mapping residual image loss onto the
3DGS map. Using this model, we define GauSS-MI, a metric that estimates mutual information between the reconstruction model and a viewpoint. GauSS-MI
enables real-time visual quality assessment from novel viewpoints without a prior, facilitating the selection of the next-best-view to effectively reduce map
uncertainty. Lower part: The active reconstruction process iterates and decreases visual uncertainty, resulting in a high visual fidelity 3D reconstruction result.
measures the expected information gained from an arbitrary
viewpoint for the current reconstruction model. This mutual
information function is termed Gaussian Splatting Shannon
Mutual Information (GauSS-MI), enabling real-time visual
quality assessment from novel viewpoints without a prior.
The GauSS-MI is implemented and integrated into a novel
active 3D Gaussian splatting reconstruction system featuring
a view and motion planner that determines the next best view
and optimal motion primitive. Extensive experiments, includ-
ing benchmark comparisons against state-of-the-art methods,
validate the superior performance of the proposed system
in terms of visual fidelity and reconstruction efficiency. The
implementation of the proposed system is open-sourced on
Github1 to support and advance future research within the
community.
The main contributions of our work are summarized below:
A probabilistic model for the 3D Gaussian Splatting map
to quantify the image rendering uncertainty.
A novel Gaussian Splatting Shannon Mutual Information
(GauSS-MI) metric for real-time assessment of visual
mutual information from novel viewpoints.
An active 3D Gaussian splatting reconstruction system
implementation based on GauSS-MI.
Extensive benchmark experiments against state-of-the-art
methods demonstrate the superior performance of the pro-
posed system in terms of visual fidelity and reconstruction
efficiency.
II. RELATED WORK
The evolution of mapping representations in 3D recon-
struction has driven significant advancements in active re-
construction methodologies. A key distinction between active
and passive reconstruction lies in the process of active view
selection. In this section, we first review active view selection
strategies across various mapping representations. We then
present a detailed review of uncertainty quantification tech-
niques employed in information-based approaches.
A. Active View Selection for 3D reconstruction
The first branch of research in active 3D reconstruction fo-
cuses on geometric reconstruction, utilizing occupancy-based
representations . In this domain, a commonly employed
strategy for determining the next best viewpoint involves con-
structing and evaluating frontiers, which indicate the bound-
ary between mapped and unmapped areas[36, 5, 43]. Addi-
[14, 22] and mutual information [42, 18, 34], to maximize the
information observed at subsequent viewpoints. Recent studies
have also explored surface coverage in active reconstruction,
with or without prior knowledge of the environment[7, 8, 40],
to enhance reconstruction efficiency. However, methods within
this branch mainly rely on occupancy-based map information,
which does not inherently ensure high visual quality in the
resulting reconstructions.
Recent advancements in radiance field representations, such
as Neural Radiance Fields (NeRF)  and 3D Gaussian
Splatting (3DGS) , have significantly enhanced visual
quality in 3D reconstruction, sparking interest in their appli-
cation to active reconstruction for even higher visual fidelity.
tion derived from occupancy maps for viewpoint selection,
rather than directly leveraging the rich visual information
inherent in radiance field maps. For instance, Yan et al.
explored geometric completeness in NeRF by evaluating infor-
mation gain based on volumetric data. Li et al.  introduced
to achieve high visual quality using 3DGS. In NARUTO
, Feng et al. considered implicit uncertainty in geometric
information for active reconstruction with 3DGS.
Considering the implicit model in NeRF, researchers have
explored neural network-based approaches for evaluating im-
plicit uncertainty in visual quality[20, 29, 30, 31]. However,
while these methods successfully evaluate uncertainty in NeRF
for next viewpoint selection, their effectiveness depends heav-
ily on the availability of high-quality datasets for training the
evaluation neural networks.
In contrast to NeRFs implicit modeling approach, 3DGS
provides an explicit representation of the environment using
a collection of spatially distributed spherical Gaussians. This
explicit representation facilitates the direct evaluation of uncer-
tainty in visual quality. In GS-Planner , image loss is di-
rectly incorporated into the occupancy map, enabling the eval-
uation of both geometric and photometric uncertainty. Jiang
et al.  introduced FisherRF, which leverages the Fisher
information matrix to quantify the parameter uncertainty in
radiance maps. Building on this, Xu et al.  extended the
GS-Planner framework by integrating FisherRF and geometric
completeness for more comprehensive uncertainty evaluation.
ActiveGS by Jin et al.  proposed a model for evaluating the
confidence of each Gaussian, which is subsequently used for
viewpoint selection in active reconstruction. In a recent study,
Chen et al.  proposed ActiveGAMER with a silhouette-
based information gain to enhance both geometric and pho-
tometric reconstruction accuracy. Our method also employs
3DGS as the scene representation for active reconstruction.
To enhance the evaluation of observed information in the
Gaussian map, we propose a more comprehensive probabilistic
model that accounts for both reconstruction loss and sensor
measurement noise.
B. Information-theoretic View Uncertainty Quantification
Information theory offers a robust mathematical framework
for active 3D reconstruction, enabling the selection of view-
points that maximize expected information and consequently
reduce the map entropy. Occupancy grid maps, typically
represented as probability models, inherently capture ob-
served information and associated uncertainties. Consequently,
information-theoretic approaches can be directly integrated
into occupancy-based active reconstruction. One common
method constructs information gain from each observation
by occupancy probabilities [14, 22]. Alternatively, mutual
information has been extensively studied for its ability to
quantify the mutual information between a map and obser-
vations. Shannon Mutual Information (SMI) has been proven
to provide guarantees for comprehensive exploration of the
and completeness in active reconstruction tasks. However, its
practical application is hindered by substantial computational
overhead when applied to occupancy mapping, as the compu-
tational complexity scales quadratically with the spatial resolu-
tion of the map and linearly with the numerical integration of
range measurements. To overcome these limitations, Charrow
et al.  introduced the Cauchy-Schwarz Quadratic Mutual
Information (CSQMI) metric, which enables analytical compu-
tation of measurement integration and reduces computational
complexity to linear scaling with the maps spatial resolu-
tion. Subsequent studies have demonstrated the efficiency of
CSQMI in real-time robotic systems for active reconstruction
tasks [2, 28, 33]. To preserve the theoretical guarantee of
Information (FSMI) algorithm, which significantly enhances
the computational efficiency of mutual information evaluation
compared to the original SMI algorithm  by analytically
evaluating integrals.
Information-theoretic uncertainty quantification for radi-
ance field-based approaches can be broadly divided into two
categories. The first category learns an implicit probability
model to estimate information gain for novel viewpoints .
The second employs the Fisher information matrix, derived
from the rendering loss, to quantify information gain[11, 15].
Among these, FisherRF, a recent method based on the Fisher
information matrix, extends its applicability to 3D Gaussian
Splatting (3DGS) . However, FisherRF focuses primarily
on next-best-view (NBV) selection, neglecting the real-time
demands of active reconstruction. To overcome these limita-
the rendering quality. Utilizing the computationally efficient
SMI method, which jointly accounts for uncertainties in the
reconstructed map and measurements, we introduce Gaussian
Splatting Shannon Mutual Information (GauSS-MI), a novel
method for quantifying visual uncertainties. Additionally, we
develop an active reconstruction system based on GauSS-MI,
which achieves high visual fidelity with real-time performance
requirements.
III. OVERVIEW
This paper introduces Gaussian Splatting Shannon Mutual
Information (GauSS-MI) as a metric for efficient next best
view selection in high-visual fidelity active reconstruction. The
proposed method is illustrated in Figure 1. At each active
reconstruction step, we assume that we have a set of previous
observations and a set of next viewpoint candidates. Our goal
is to devise an effective metric for the next best view selection
leveraging the available observed information.
During each active reconstruction step, a new observation
is obtained, and the 3D Gaussian Splatting (3DGS) map is
updated by extending and initializing new Gaussians based
on this new observation. Subsequently, the overall 3DGS map
undergoes iterative optimization based on the loss between
the observed images and the map. To quantify the visual
current observation and the optimized map. By mapping the
image loss onto the 3DGS map, we construct a probabilistic
model for each 3D Gaussian. Subsequently, based on Shan-
non Mutual Information theory, we leverage the probabilistic
MAIN NOTATIONS FOR GAUSS-MI
Notations
Explanation
3D Gaussian splatting map.
The world frame.
A series of ordered Gaussians along a camera ray.
Position of a Gaussian.
Color of a Gaussian.
Opacity of a Gaussian.
Camera pose, or a viewpoint.
Cumulative transmittance of a Gaussian.
Rendered color and observed color.
Rendered depth and observed depth.
Random variable and realization of an observation.
Random variable and realization of luminance for a pixel.
Real probability of a Gaussian.
Odds ratio and log odds of a Gaussian.
Inverse sensor model.
Loss image between the map and the observation.
Hyperparameters.
Entropy of the map.
Mutual Information between the map and the observation.
Iteration of Gaussians.
Iteration of the measurement beams or pixels.
Property based on observation at time k.
Property based on the observations from start to time k.
model to establish the mutual information between the re-
construction model and a viewpoint. This mutual information
function is referred to as Gaussian Splatting Shannon Mutual
Information (GauSS-MI), enabling real-time visual quality as-
sessment from novel viewpoints without a prior. The next best
view is then selected using GauSS-MI, to effectively reduce
the uncertainty of current map. The iterative process leads to a
decrease in visual uncertainty within the reconstruction model,
yielding a high visual fidelity 3D reconstruction result.
The derivation of GauSS-MI is elaborated in Section IV, and
the system implementation details are presented in Section V.
IV. METHODOLOGY
This section presents the probabilistic model for 3D Gaus-
sian Splatting (3DGS) in visual uncertainty quantification,
followed by the formulation of Gaussian Splatting Shannon
Mutual Information (GauSS-MI) for view selection. The main
notations of this section are listed in Table I.
A. 3D Gaussian Splatting Mapping
The proposed system reconstructs the scene by 3DGS,
utilizing a collection of anisotropic 3D Gaussians, represented
by G. Each 3D Gaussian i contains the properties of mean
W and covariance [i]
and ellipsoidal shape in the world frame W, and also optical
properties including color c[i] and opacity [i]. By splatting
and blending a series of ordered Gaussians N, the color C[j]
and depth D[j] for each pixel are synthesized as
where d[i] represents the distance from camera pose  to the
position [i]
W of Gaussian i along the camera ray. We denote
T [i]  [i]
as the cumulative transmittance of Gaussian i along the ray.
At each reconstruction step, the 3D Gaussians are extended
and initialized using the collected RGB-D image and estimated
camera pose . Then the Gaussians iteratively optimize
both their geometric and optical parameters to represent the
captured scene with high visual fidelity.
B. 3D Gaussian probability
To model the information obtained from the 3DGS map
G by a random observation z, we first construct a random
variable r for each Gaussian. As we are going to optimize the
rendering result, we define the probability of a 3D Gaussian
i is reliable for rendering as P(r[i]) (0, 1). Then, the
probability of the 3D Gaussian i is unreliable for rendering is
P(r[i])  1 P(r[i]). Additionally, we denote the odds ratio
o[i] (0, ) and log odds l[i] (, ) of a Gaussian
l[i] : log(o[i]) : log(P(r[i])
P(r[i]))  log(
1 P(r[i]))
We assume each probability of the 3D Gaussian is indepen-
dent. At the initial stage of the mapping, we assume that the
agent has no prior information on the environment, i.e.,
Once a new observation Zk is obtained at time k, the standard
binary Bayesian filter can be used to update the probability
o[i](Z1:k) :  P(r[i]Z1:k)
P(r[i]Z1:k)
P(r[i]Zk)
P(r[i]Zk)
P(r[i]Z1:k1)
P(r[i]Z1:k1)
[i](Zk)o[i](Z1:k1)
where P(r[i]Zk) is the reliable probability of Gaussian i
under the observation Zk. We refer to P(r[i]Zk) as the
inverse sensor model, thereby [i](Zk) is the odds ratio of
the inverse sensor model, which will be constructed and used
for updating the reliable probability P(r[i]Z1:k). We further
use o[i]
on the observations from start to time k.
Given the observation Zk, we construct the P(r[i]Zk) as
P(r[i]Zk)
(LLk)T T [i]  1
Inverse sensor model visualization. The hyperparameters  are
omitted in the figure for simplicity.
be derived as
P(r[i]Zk)
1 P(r[i]Zk)  (LLk)T T [i]
where L, T > 0 are hyperparameters. Lk denotes the loss
between the observation Zk and the map, i.e., a loss image
between the observed groundtruth image and the rendered
image. We compute the loss image by
Lk  cC Ck (1 c)D Dk
where C, D denote the rendered color and depth images from
the reconstructed 3DGS map, Ck, Dk are the groundtruth color
and depth images from observation Zk.
As the 3DGS map optimizes the Gaussians by minimizing
the image loss, we use this loss to construct the inverse sensor
rate. We further visualize the inverse sensor model (7)(8) in
Figure 2 to illustrate the probability update. The Gaussians
associated with observation Zk have T [i] > 0, resulting in the
inverse sensor model P(r[i]Zk) and [i](Zk) being monoton-
ically decreasing with loss Lk. This suggests that lower loss
Lk corresponds to a higher reliable probability of Gaussian
P(r[i]Zk). Additionally, a lower cumulative transmittance T [i]
implies less impact of Gaussian i on observation Zk. Conse-
within the inverse sensor model, e.g., when T [i]  0, we have
P(r[i]Zk)  0.5 and [i](Zk)  1.
To accelerate computation, in implementation, we update
the probability P(r[i]Z1:k) by computing log odds l[i]
Algorithm 1 Probability Update
Loss image
Per pixel
for i 1 to N [j] do
Per gaussian
T [i]  [i]T [j]
Gaussians transmittance
k  T T [i] log LL[j]
Log odds of (7)
T [j]  (1 [i])T [j] Update pixel transmittance
Update probability
log of (6) and substitute (8),
computed by rasterizing the mapping loss Lk as (1). The
probability update algorithm is summarized in Algorithm 1.
C. Gaussian Splatting Shannon Mutual Information
Based on the proposed probability model and Shannon
Mutual Information theory, we then construct the Gaussian
Splatting Shannon mutual information (GauSS-MI) for visual
quality assessment of novel viewpoints.
Given the previous observations Z1:k1, we are interested in
minimizing the expected uncertainty, i.e., conditional entropy,
of the map after receiving the agents next observation zk. In
information theory, the conditional entropy relates to Mutual
Information (MI) by
H(rzk, Z1:k1)  H(rZ1:k1) I(r; zkZ1:k1)
To minimize the conditional entropy H(rzk, Z1:k1) is to
maximize the MI I(r; zkZ1:k1). Note here that we use zk
and Zk to distinguish random variable and realized variable
for the observation at time k.
As we assume that the previous observations Z1:k1 are
given and try to compute the MI for the new observation zk,
in the subsequent of this subsection, we omit the probability
condition Z1:k1 and simplify zk into z. Therefore, the (11)
can be simplified as
H(rz)  H(r) I(r; z)
As z is a random variable with independence among el-
of I(r; z[j]) between z[j] over all measurement beams j
I(r; z[j])
I(r[i]; z[j])T [i]
picture pixel.
From information theory [6, 18], the mutual information
between two random variables is defined and can be organized
I(r[i]; z[j])
:P(r[i], z[j]  Z) log( P(r[i], z[j]  Z)
P(r[i])P(z[j]  Z))
P(z[j]  Z)P(r[i]z[j]  Z) log(P(r[i]z[j]  Z)
P(z[j]  Z)f([i](Z), o[i]
where P(z[j]  Z) is only related to the observation, which
is referred to as the measurement prior. f([i](Z), o[i]
be derived and written in shorthand as
o  1 log( o  1
The detailed derivation is presented in the Appendix A. The
function f(, o) can be interpreted as an information gain
function.
We define the mutual information (14) between the 3DGS
map and the observation as Gaussian Splatting Shannon Mu-
tual Information, GauSS-MI.
D. Computation of Expected GauSS-MI
We further derive the computation of the expected mutual
information (14) for random viewpoints.
1) Measurement prior: We refer to the noise model of RGB
camera in , in which the expectation of the measurement
noise is related to luminance. Thus, we construct the measure-
ment prior P(z) for each pixel j as
P(z[j]m[j])P(m[j])
where P(z[j]m[j]) is the prior probability distribution of the
sensor with respect to luminance m {0,    , 255}. To
compute the expected measurement prior, we define P(m[j])
for m[j]  M [j]
otherwise
where M [j] is the pixels expected luminance, which can be
computed from the expected RGB color (R, G, B) by the
luminance formula M  0.299R  0.587G  0.114B. Thus
the measurement prior (16) can be simplified as
P(z[j])  P(z[j]M [j])
2) Information gain function: As there are no observations
from random viewpoints, computing the loss image Lk for
(Zk) is infeasible; thus, an expectation of Lk is required. We
expect that the rendering result after reconstruction is reliable,
i.e., there is no loss between groundtruth and the 3DGS map
Algorithm 2 GauSS-MI
Per pixel
Rasterize color and f (19) on each pixel
for i 1 to N [j] do
Per gaussian
T [i]  [i]T [j]
Gaussians transmittance
C[j]  c[i]T [i]
Rasterize color
f [j]  log(P [i])T [i]
Rasterize (19)
T [j]  (1 [i])T [j] Update pixel transmittance
M [j] Color2Luminance(C[j])
P(z[j]M [j]) SensorModel(M [j])
I  P(z[j]M [j])f [j]
Update MI
G. Thus, we assume Lk  0 so that 1  0 in f(, o). Then
the information gain function (15) can be derived as,
f [i]  log(o[i]  1
)  log(P(r[i]))
The equation shows that when the reliable probability P(r[i])
is low, the information gain function f will be high, consistent
with the intuition of information gain.
MI can be computed as
I(r[i]; z[j])T [i]
P(z[j]M [j])
f [i]T [i]
P(z[j]M [j])
T [i] log(P(r[i]))
The computation procedure of GauSS-MI is summarized in
Algorithm 2.
V. SYSTEM IMPLEMENTATION
This section details the system implementation of GauSS-
A. System Overview
The proposed active reconstruction system comprises a
reconstruction module and a planning module, as illustrated in
Figure 3. In this work, a mobile robot is equipped with sensors
that can capture color images and depth images and estimate
its pose. Given these messages, the reconstruction module
constructs and updates a 3D Gaussian splatting (3DGS) model
in real-time, while simultaneously generating the 3D Gaussian
probability map. Meanwhile, the planning module creates
a library of candidate viewpoints along with the primitive
Best Primitive
GauSS-MI
Actuator
Mobile Robot
Reconstruction
3D Gaussian
Probability
Reconstruction
Viewpoint Primitive
Next Best View
Selection
View Planner
RGB-D images
Overview of proposed active 3D reconstruction system.
trajectories. The optimal viewpoint and primitive trajectory are
subsequently determined by evaluating both the viewpoints
GauSS-MI and the trajectorys motion energy cost. The robot
then follows the selected primitive trajectory and captures
images from the next-best viewpoint. Given the new obser-
process iterates and results in a high-quality 3D reconstruction
with detailed visual representation.
We then present the view planner for viewpoint sampling
and selection and discuss the autonomous termination condi-
tion design for the proposed system.
B. View Planning
1) Viewpoint Primitive Library: To determine the next best
candidate viewpoints, and choose the next best view within the
candidates. Inspired by the action generation method proposed
[vxy, vz, z]
where vxy and vz represent the body frame linear velocity in
xB yB plane and zB direction, and z is the body frame
angular velocity around the zB axis. We simplify the action of
2-dimensional horizontal movement into 1 dimension, which
can be compensated through the z rotation. The action space
is given by sampling each velocity that,
A  {vxy Vxy, vz Vz, z z}
In this paper, we assume that the sensor, normally with a
limited field of view, is equipped forward, i.e., facing the xB
axis. Thus, in the further forward propagation derivation, we
design the horizontal movement action vxy works on the yB
Given the action   [vxy, vz, z] A , the next viewpoint
is designed by forward propagation with duration time T,
vxyT sin (0  zT)
vxyT cos (0  zT)
Algorithm 3 View and Motion Planner
(t  T) ForwardPropagate((t),)
T MotionPrimitive(X(t),(t  T))
if SafetyCheck(T ) then
I GauSS MI((t  T))
Algorithm2
J MotionCost(T )
if R < wII wJJ then
R  wII wJJ
where ()(n) denotes the n-th derivatives, which constraints
the final state to ensure a stable picture taking on the next
viewpoint. A motion primitive trajectory T from current state
(t)  0 to the next viewpoint (tT)  f can be derived
in closed-form  (detailed in the Appendix B).
state 0, a library of candidate viewpoints f  {f} along
with the primitive trajectories T  {T } can be formed as
a viewpoint primitive library.
2) Next Best View Selection: The total reward for the next
best view evaluation includes the mutual information I (20)
and the motion cost J as
R  wII wJJ
where wI, wJ > 0 are constant reward weights to balance
the range of two components. The motion cost J can be
calculated based on the trajectory T with respect to a specific
mobile robot. The next best view with primitive
T is selected
by optimizing R over all feasible primitives, which is then
assigned to the controller for tracking. The complete procedure
for view and motion planning is summarized in Algorithm 3.
C. Termination Condition
A spherical Gaussian may be deemed rendering reliable
from one perspective, but this reliability may not hold from
another perspective. Specifically, the reliable probability of a
Gaussian P(r[i]) should exhibit anisotropic behavior. In the
four orthogonal horizontal perspectives. The visual reconstruc-
tion completeness of a Gaussian is quantified based on the
average reliable probability denoted as P .
At the beginning of the mapping process, we assume no
prior information about the environment. Consequently, we
initialize the probabilities of all 3D Gaussians as (5). As
detailed in Section IV-B, the probability P(r) for a spherical
Gaussian decreases if it renders out a relatively large image
loss. As the reconstruction process progresses, we expect a de-
crease in the render-ground truth loss and an increase in P(r).
TABLE II
PARAMETERS OF THE PROPOSED SYSTEM
Parameter
hyperparameter on loss L
hyperparameter on cumulative transmittance L
Primitive duration time T
reward weight on information wI
reward weight on motion cost wJ
probability threshold
reconstruction terminate threshold
A Gaussian is considered completely reconstructed when its
average probability exceeds a specified threshold, P > .
The active reconstruction process is regarded complete and
actively terminated once the proportion of fully reconstructed
Gaussians reaches a predefined percentage threshold,
where Ndone represents the number of completely recon-
structed Gaussians, and NGS denotes the total number of
Gaussians in the map.
VI. SIMULATION EXPERIMENTS
In this section, we present a series of simulation experi-
ments designed to validate the proposed method. We begin
by detailing the experimental setup and evaluation metrics.
Based on this, we initially validate the proposed system (Sec-
tion VI-A). Subsequently, we conduct experiments to evaluate
the proposed GauSS-MI metric from multiple perspectives:
the efficiency of next-best-view selection (Section VI-B), real-
time computational performance (Section VI-C), and the effec-
tiveness of uncertainty quantification (Section VI-D). Finally,
we compare the complete system against baseline methods
in Section VI-E and study the termination condition for the
system in Section VI-F.
A. System Validation
1) Simulation Setup: The simulation environment is created
using Flightmare, featuring a configurable rendering en-
gine within Unity2 and a versatile drone dynamics simulation.
A quadrotor is employed as the agent for active reconstruction,
equipped with an image sensor providing RGB-D images at a
resolution of 640480 and a 90 deg Field of View (FOV). The
online 3D Gaussian splatting reconstruction is developed based
on MonoGS, which incorporates depth measurements to
enhance the online reconstruction model. Both the proposed
active reconstruction system and the simulator operate on a
desktop with a 32-core i9-14900K CPU and an RTX4090
GPU. The parameters of the proposed system are summarized
in Table II.
2) Metrics: The evaluation focuses on assessing the visual
quality of the reconstruction results and the efficiency of
the active reconstruction process. Visual quality is evaluated
using Peak Signal-to-Noise Ratio (PSNR), Structural Sim-
ilarity Index (SSIM), and Learned Perceptual Image Patch
Similarity (LPIPS) to quantitatively compare rendered images
from the 3DGS model with a testing dataset of ground-truth
images. Efficiency is measured by calculating the total length
of the reconstruction path P and the number of frames Nf.
To provide a quantitative assessment of the efficiency of
the reconstruction process, we introduce an efficiency metric
that combines visual quality and motion effort, defined as
E  PSNR log Nf. The logarithmic transformation of the
denominator is applied to align with the PSNR calculation.
3) Simulation Result: We actively reconstruct three scenes,
the Oil Drum, the Drilling Machine, and the Potted Plant, to
validate the proposed system. The offline refinement results,
including image rendering and depth rendering, are presented
in Figure 4. The evaluations of visual quality and efficiency are
calculated and summarized in Table III. The Oil Drum is char-
acterized by a relatively simple geometry but detailed texture.
The Drilling Machine exhibits fine geometric structures, while
the Potted Plant features a highly cluttered geometric structure.
The rendering results in Figure 4 demonstrate a detailed visual
fidelity with precise geometric structures, highlighting the
systems capability to capture intricate textures and structures.
B. Comparison Study of Active View Selection
To evaluate the efficiency of the proposed GauSS-MI metric
in selecting the next-best-view for high visual quality recon-
selection using a fixed number of frames.
1) Baselines:
benchmark
selection
approach
quantifies
expected
information gain by constructing the Fisher information
matrix. To ensure a fair comparison, FisherRF is integrated
into our system by substituting the GauSS-MI evaluation I in
(23) with its FisherRF metric. Additionally, a random view
selection policy is implemented as a baseline to highlight the
benefits of using view selection strategies.
2) Results:
The comparative experiment is performed
across three scenes, with the number of frames limited and
gradually increased for each method. We compute the PSNR
values for each test and visualize the results by plots in
Figure 5. The results show that both GauSS-MI and FisherRF
significantly outperform the random policy, demonstrating the
methods effectiveness in next-best-view selection for enhanc-
ing visual quality. While the performance of GauSS-MI and
FisherRF is comparable, GauSS-MI achieves higher PSNR
values in most tests, validating its superior efficiency in active
view selection. The novel view synthesis results for GauSS-
frames Nf  200, are presented alongside the ground truth
High-resolution novel view synthesis of the reconstruction result by the proposed system: color rendering against depth rendering.
TABLE III
EVALUATION RESULTS AND COMPARISON OF SIMULATION EXPERIMENTS
Oil Drum 2
Drilling Machine 2
Potted Plant 2
Visual Quality
Efficiency
Visual Quality
Efficiency
Visual Quality
Efficiency
PSNRSSIMLPIPSNf P(m) E PSNRSSIMLPIPSNf P(m) E PSNRSSIMLPIPSNf P(m) E
1 Simulation scenes are built by Flightmare .
2 Oil drum scene size: 5m  4m  3m. Drilling Machine scene size: 4m  4m  3m. Potted Plant scene size: 5m  5m  5m.
PSNR (dB)
GauSS-MI
FisherRF
PSNR (dB)
Number of Frames  Nf
PSNR (dB)
PSNR results for active view selection with a limited number
of frames. The maximum PSNR value for each test is annotated. The
abbreviations G and F denote GauSS-MI and FisherRF, respectively.
on the left-hand side of Figure 6. These visualizations further
showcase the enhanced visual fidelity reconstruction result of
or textural details. The efficiency on active view selection is es-
pecially advantageous for onboard active reconstruction, where
constrained computational and battery resources necessitate
minimizing the number of frames and reconstruction time.
C. Computational Efficiency
We analyze the computational complexity of the proposed
GauSS-MI method, measure its average runtime, and compare
it with FisherRF , validating the real-time performance of
our metric.
1) Computational Complexity: The computation of GauSS-
in that Eq. (20) projects the information gain function (19)
onto an image. The algorithm is implemented in parallel using
CUDA. Assuming the current 3DGS map with Ng Gaussians,
the image with Np pixels, and Nc candidate viewpoints to
be evaluated, the computational complexity of GauSS-MI is
O(NpNgNc). In contrast, FisherRFs complexity depends on
both candidate and observed views. With No observed views,
FisherRF requires a complexity of O(NpNg(No  Nc)) to
evaluate all candidates, as it has to compute the information
from both observed and candidate views at each decision step.
progresses. GauSS-MI, however, maintains consistent compu-
from our probabilistic model, which quantifies the information
from prior observations with a low computational overhead of
O(2NpNg) during the map update process (Algorithm 1). As a
only candidate views, achieving low and stable computational
2) Runtime: We conducted a complete active reconstruc-
tion experiment to measure the runtime of each method at
each planning timestep, as shown in Figure 7. GauSS-MI
achieves an average runtime of 5.55 ms (182.2 fps), while
FisherRF averages 11.66 ms (85.8 fps). These results corrob-
Oil Drum
Drilling Machine
Potted Plant
Ground Truth
Random (Nf  200)
Ours (Nf  200)
FisherRF (Nf  200)
Active View Selection
Active Reconstruction
Novel view synthesis results compared to ground truth. Left part: Results of active view selection with a fixed number of frames Nf  200.
Right part: Results of active reconstruction, with number of frames Nf specified in Table III.
Number of Frames
Comp. Time (s)
GauSS-MI
FisherRF
Comparison of computation time. Statistics in a complete active
reconstruction process.
orate the computational complexity analysis that GauSS-MI
maintains consistent runtime throughout the reconstruction
dependence on the growing number of observed views.
D. Uncertainty Quantification
To evaluate the uncertainty quantification capability of the
proposed method, we employ sparsification plots and the Area
Under Sparsification Error (AUSE) metrics [13, 11] to evaluate
and compare our method with the state-of-the-art 3DGS-based
uncertainty quantification method, FisherRF .
1) Sparsification Plots: Sparsification plots provide a mea-
surement of the correlation between the estimated uncertainty
and the true errors . If the estimated uncertainty accurately
reflects model uncertainty, progressively removing pixels with
the highest uncertainty should lead to a monotonic decrease
in the mean absolute error (MAE) of the true error image.
The plot of the MAE against the fraction of removed pixels
is called Sparsification Plots, as shown in Figure 8. The
ideal uncertainty ranking can be obtained by ordering pixels
according to their true error relative to the ground truth,
Fraction of Removed Pixels
Average MAE
GauSS-MI
FisherRF
Fig. 8. Sparsification plots. The plot shows the mean absolute error (MAE) of
the true error image against the fraction of pixels with the highest uncertainties
removed. The oracle sparsification represents the lower bound, derived by
removing pixels ranked by ground-truth error. The sparsification plots reveal
the correlation between the estimated uncertainty and the true errors.
yielding the Oracle Sparsification curve. We evaluate the
uncertainty estimates for all images in the test dataset across
three scenes and compute the average sparsification plot. We
compare the result with FisherRF in Figure 8. The plot reveals
that our uncertainty estimate is closer to this oracle, indicating
a stronger correlation between our predicted uncertainties and
actual errors.
2) Area Under Sparsification Error (AUSE): To quantita-
tively assess the divergence between the sparsification plot and
the oracle, we calculate the Area Under Sparsification Error
(AUSE) , which measures the area between the two curves.
The AUSE values for each scene are reported in Table IV. Our
method consistently achieves lower AUSE scores compared to
performance.
TABLE IV
AREA UNDER SPARSIFICATION ERROR (AUSE) RESULTS
Oil Drum
Drilling Machine
Potted Plant
FisherRF
E. Comparison Study of Active Reconstruction
This section evaluates and compares the complete system,
including the proposed view planning and active termination
condition. We select the state-of-the-art baselines employing
different map representations and uncertainty quantification
techniques to validate our systems efficiency on visual quality.
1) Baselines: To evaluate the efficacy of our proposed
active reconstruction system and existing systems, FUEL
and NARUTO . FUEL is a volumetric-based active recon-
struction system with no consideration of visual quality, while
NARUTO is a NeRF-based framework that addresses radiance
field uncertainty with a focus on geometry. For our study,
we implemented the comparison using the open-source codes
for FUEL4 and NARUTO5, employing their default parameter
settings. Each system, including the next best view selection
and path planning algorithm, captured color images in the
three simulation scenes, which are subsequently employed to
3D Gaussian Splatting  for offline model reconstruction.
Evaluation of reconstruction quality and efficiency was con-
ducted using the metrics outlined in Section VI-A2.
2) Results: The quantitative results are presented in Ta-
ble III, while the qualitative visual comparisons are shown in
the right part of Figure 6. Our system demonstrates superior
efficiency across all scenes and attains the highest visual
quality in the Oil Drum and Drilling Machine. In the Potted
Plant scene, NARUTO slightly outperforms our system by
a small margin. However, it is worth noting that NARUTO
completed its reconstruction process after capturing thousands
of images, which contributed to its commendable recon-
struction performance. The extensive 
