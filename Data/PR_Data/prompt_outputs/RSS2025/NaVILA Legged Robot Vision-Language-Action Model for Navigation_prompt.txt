=== PDF文件: NaVILA Legged Robot Vision-Language-Action Model for Navigation.pdf ===
=== 时间: 2025-07-22 09:42:27.378533 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Model for Navigation
An-Chieh Cheng1,
Yandong Ji1,
Zhaojing Yang2,
Zaitian Gongye1
Xueyan Zou1
Jan Kautz3
Erdem Byk2
Hongxu Yin3,
Sifei Liu3,
Xiaolong Wang1,3,
1UC San Diego
Move forward out of the room. Turn right at the end. Proceed to the grass and stop in front of the soccers.
Walk forward along the way. Turn a little left and keep going straight. Stop in front of the red door.
Turn left immediately. Go straight forward. Step on the mat. Keep going forward.
Stop when you are very close to the trash can.
Move forward along the way. Turn left at the yellow fire hydrant.
Go forward along the slope and stop in front of the door.
Walk forward, when seeing the stair bars, turn right and walk around the stairs until reaching the hallway.
Turn right and walk along the hallway, stop in front of a bathroom.
Workspace
Move forward and exit the office area. And then turn right and stop near the blue trash can.
Fig. 1: Real-world demonstration of NaVILA: Upon receiving human instructions, NaVILA uses a vision-language model to process RGB
video frames and employs locomotion skills to execute the task on a robot. The robot successfully handles long-horizon navigation tasks
and operates safely in challenging environments.
AbstractThis paper proposes to solve the problem of Vision-
and-Language Navigation with legged robots, which not only
provides a flexible way for humans to command but also allows
the robot to navigate through more challenging and cluttered
scenes. However, it is non-trivial to translate human language
Equal contribution, ordered alphabetically.  Equal advising.
instructions all the way to low-level leg joint actions. We propose
Action model (VLA) with locomotion skills. Instead of directly
predicting low-level actions from VLA, NaVILA first generates
mid-level actions with spatial information in the form of language,
(e.g., moving forward 75cm), which serves as an input for a
visual locomotion RL policy for execution. NaVILA substantially
improves previous approaches on existing benchmarks. The
same advantages are demonstrated in our newly developed
benchmarks with IsaacLab, featuring more realistic scenes, low-
level controls, and real-world robot experiments.
I. INTRODUCTION
The ability to perform Vision-and-Language Navigation
(VLN) has become a foundational component in modern
robotics systems. With VLN, a robot is expected to navigate
around unseen environments without a provided map following
a language instruction [16]. This not only offers a better inter-
face for humans, but also strengthen cross-scene generalization
through languages. In this paper, we further extend the study
of VLN with legged robots (e.g., quadruped or humanoid).
Using legs instead of wheels allows robots to navigate in more
challenging and cluttered scenarios. As the examples shown
in Fig. 1, our robot can navigate through a messy laboratory
space with narrow walkways, transition from room to room in
a house, as well as tackle outdoor challenging environments
such as uneven terrains with small rocks, holes, and troughs.
To translate language to action, the robot needs to reason
about the input language, and perform closed-loop planning
as well as low-level control. With the recent advancement in
Large Language Models (LLMs) and Vision-Language Models
(VLMs), several end-to-end Vision-Language-Action (VLA)
systems have been developed [79]. These systems fine-tune
a general-propose VLM with large-scale robot manipulation
demonstrations to produce low-level actions. While unifying
reasoning and execution in a single model is fascinating and
shows encouraging results, it is worth diving deeper into the
the quantized low-level commands? After all, LLMs and
VLMs were primarily trained with natural language. Unifying
reasoning and execution becomes challenging when we need
to convert that reasoning into precise, non-verbal actions.
Inspired by the recent progress on VLM [10, 11] for spatial
location and distance reasoning, we propose NaVILA, a two-
level framework for legged robot VLN: A VLM is fine-
tuned to output a mid-level action (VLA) in the form of
language such as turn right 30 degrees, and a low-level
visual locomotion policy is trained to follow this instruction
for execution. The mid-level action output of the VLA conveys
the location and direction information without the low-level
commands. The advantages of this framework are three-fold:
(i) By decoupling low-level execution from VLAs, the same
VLA can be applied across different robots by swapping
the low-level policy; (ii) Representing actions as mid-level
language instructions enables VLA training with diverse data
This enhances reasoning capabilities without overfitting out-
puts to specific low-level commands and can leverage real-
world data for generalization; (iii) NaVILA operates on two
distinct timescales: the VLA, typically a large and computa-
tionally intensive model, runs at a lower frequency, providing
high-level navigation commands; while the locomotion policy
operates in real-time. This dual-frequency approach allows the
locomotion policy to handle sophisticated obstacle avoidance
and increases overall robustness.
To train the VLA, we demonstrate how to (i) integrate
historical context and current observations in VLN within
existing VLM frameworks, (ii) create a specialized navigation
prompt tailored for VLN tasks, (iii) utilize real-world data
from YouTube human touring videos to improve navigation
in continuous environments, and (iv) introduce a carefully cu-
rated dataset blend designed to enhance VLN generalizability.
These strategies allow us to fine-tune a general-purpose image-
based VLM into a navigation-focused agent while simultane-
ously training it on general vision-language datasets, thereby
maintaining its broad generalization capabilities. Moreover,
this is the first work to show that direct training on human
videos improves navigation in continuous environments.
To train robust locomotion skills, we employ a single-
stage approach to learn vision-based locomotion policy. We
construct a height map from raw LiDAR point clouds and
introduce randomization to bridge the sim-to-real gap. This
controller takes the output from our VLA model, converts
it into command velocities, and tracks these velocities by
controlling the positions of the joints. This end-to-end ap-
proach enables the training of visual locomotion skills that
are both robust and safe, facilitating deployment in real-world,
challenging environments (e.g., strong sunlight or near certain
transparent surfaces).
In our experiments, we show that our VLA significantly
outperforms the state-of-the-arts on classic VLN benchmarks,
with over 17 improvement in success rate. Additionally, our
single-stage locomotion policy outperforms previous policy
distillation-based methods by a substantial margin. To better
simulate the challenges of locomotion navigation in VLN, we
introduce a new benchmark, VLN-CE-Isaac, using Isaac Sim.
This benchmark considers detailed robotic joint movements
and interactions with environments, which prior VLN works
have not explored. In our VLN-CE-Isaac experiments, our
vision-based policy outperforms the blind policy by a signif-
icant margin, showing a 14 improvement in success rate.
We also demonstrate that our VLA can be deployed across
different robots (Unitree Go2, Unitree H1, Booster T1), each
using distinct locomotion skills. Finally, we deploy NaVILA in
the real world, exhibiting impressive robustness and achieving
an 88 success rate on 25 instructions, including a 75
success rate on complex instructions across diverse scenes.
II. METHOD
NaVILA integrates high-level visual language understand-
ing with low-level locomotion control (Fig.2). It employs a
VLM to process single-view images and generate waypoint
instructions in natural language, which a locomotion policy
translates into precise joint movements for real-time robot
control. The synergy between the VLMs reasoning and the
locomotion policys execution enables NaVILA to generalize
across diverse environments. We first describe how we tame
VLMs for high-level VLN in Sec.II-A, then outline our robot
configuration and locomotion policy in Sec. II-B.
Move forward 75 cm.
Walk forward and turn right.
Pass the rug and stop by the desk.
Instruction
Positions
History Views
Velocity
Commands
Proprioception
Prior Actions
Joint Pos.  Vel.
Orientation
Angular Velocity
Height Map
Current View
Fig. 2: NaVILA is a two-level framework combining high-level visual language understanding with low-level locomotion control. Our VLA
model processes single-view images to produce mid-level actions in natural language, which are then converted into precise joint movements
by an advanced low-level locomotion policy. This integration allows for strong generalization and adaptability across different real-world
A. Taming VLMs for Vision Language Navigation
VLN requires processing video inputs as observations. A
common approach to handling video inputs in VLMs is
through video encoders . However, recent progress in
VLMs has largely been driven by the availability of image-
text data. While there have been efforts to extend this success
to video encoders, the lack of large, high-quality video-
text datasets has limited their pre-training. To address this
our approach. These models exhibit stronger generalization
abilities and possess broader knowledge, making them more
suitable for tackling the generalization challenges in VLN.
ily of efficient VLMs for both understanding and generation.
VILAs pre-training has proven particularly effective for multi-
image reasoning, making it especially suitable for VLN tasks
where understanding sequential image relationships is critical.
VILA Preliminary. VILA consists of three main compo-
sion encoder processes the input images, converting them
into a sequence of visual tokens. These tokens are then
downsampled and mapped into the language domain via
an MLP projector. Afterward, the pro
