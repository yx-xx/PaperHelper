=== PDF文件: Map Space Belief Prediction for Manipulation-Enhanced Mapping.pdf ===
=== 时间: 2025-07-22 16:13:06.920577 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Map Space Belief Prediction for
Manipulation-Enhanced Mapping
Joao Marcos Correia Marques1
Nils Dengler2,3,4
Tobias Zaenker2,4
Jesper Mucke2
Shenlong Wang1
Maren Bennewitz2,3,4
Kris Hauser1
These authors contributed equally to this work
1. University of Illinois at Urbana-Champaign, IL, USA
2. Humanoid Robots Lab, University of Bonn, Germany
3. The Lamarr Institute, Bonn, Germany
4. The Center for Robotics, University of Bonn, Germany
Robots Belief
Robots Belief
Robots Belief
Scene after manipulation
New instances discovered
uncertainty reduced
New instance uncovered
uncertainty reduced
Time Step
Fig. 1: Example scenario with occlusions in a confined shelf environment. Given a current partial map of the environment (belief t), our planner decides
whether gathering another observation or manipulating the scene would be best to reduce map uncertainty. In this example, first a observation action would
increase environmental knowledge, followed by a push to unveil the hidden can behind the two boxes at time t  2. The predicted belief map is visualized
as a top-down projection of the shelf, ignoring the occluding top shelf board.
AbstractSearching for objects in cluttered environments
requires selecting efficient viewpoints and manipulation actions
to remove occlusions and reduce uncertainty in object locations,
of manipulation-enhanced semantic mapping, where a robot has
to efficiently identify all objects in a cluttered shelf. Although
Partially Observable Markov Decision Processes (POMDPs) are
standard for decision-making under uncertainty, representing
unstructured interactive worlds remains challenging in this
formalism. To tackle this, we define a POMDP whose belief is
summarized by a metric-semantic grid map and propose a novel
framework that uses neural networks to perform map-space be-
lief updates to reason efficiently and simultaneously about object
physics. Further, to enable accurate information gain analysis, the
learned belief updates should maintain calibrated estimates of un-
certainty. Therefore, we propose Calibrated Neural-Accelerated
Belief Updates (CNABUs) to learn a belief propagation model
that generalizes to novel scenarios and provides confidence-
calibrated predictions for unknown areas. Our experiments show
that our novel POMDP planner improves map completeness and
accuracy over existing methods in challenging simulations and
successfully transfers to real-world cluttered shelves in zero-shot
fashion.
I. INTRODUCTION
Active sensing has long been studied in robotics for tasks
such as exploring an unknown environment , complete 3D
object model acquisition , and searching for an unobserved
target object [26, 12, 47]. To build complete maps as efficiently
as possible, Next Best View (NBV) planning  is often
employed to reduce the uncertainty about the map as quickly
as possible. Although NBV planning offers an approach for
static scenes in which the robot simply moves the camera
passively through free space, there are many applications, such
as household and warehouse robotics, in which robots may
need to manipulate the environment in order to gain better
viewpoints [11, 33]. We refer to this problem as Manipulation-
Enhanced Mapping (MEM). MEM offers two significant new
challenges beyond standard NBV problems. First, in order to
decide when and where to manipulate objects, the robot should
reason about how object movement may affect previously
occluded regions. Second, it must anticipate the impact of
manipulations on observed objects and possibly partially-
observed or unobserved objects. For example, pushing boxes
in a grocery shelf backward will move all objects simultane-
ously until the furthest, occluded one, hits a wall.
MEM is related to the mechanical search problem
in which the robot manipulates clutter to reveal a target
object. Prior approaches in mechanical search tend to rely
on restrictive assumptions, such as a static viewpoint, which
ignores a robots ability to look around obstacles [18, 39].
Other studies assume full observability of object dynamics
and poses  or are limited to a fixed set of predefined
objects . These assumptions are too limiting for complex
cluttered scenes like shelves. The most closely-related work to
ours is Dengler et al. , who address these limitations by
training a reinforcement learning policy for viewpoint planning
and learn a push scoring network from human annotations to
derive manipulation actions, switching between manipulation
and manipulation when the information gain from obtaining
novel images of the environment saturates. However, their
approach to switching between action modes is inefficient,
since waiting for information gain saturation to perform a
push results in the agent sampling the environment several
times to reveal details that could have been revealed easily
through manipulation. Furthermore, the proposed method does
not update its environmental map after a push, leaving the
viewpoint planner, conditioned solely on this outdated map,
less capable of exploiting the newly revealed space.
This paper formulates the MEM problem as a Partially
Observable Markov Decision Process (POMDP) in the belief
space of semantic maps. By maintaining map-space beliefs,
our approach is applicable to unstructured cluttered environ-
ments with an arbitrary numbers of objects. The POMDP
computes the next best viewpoint or manipulation action that
maximizes the agents expected information gain over a short
horizon (Fig. 1) in a receding-horizon fashion. Our approach
leverages neural network methods for map-space belief prop-
literature to drastically improve map completion rates and offer
better guidance for object search [12, 47]. The key challenge in
belief propagation with manipulation actions is that they often
reduce certainty when the objects dynamics are unknown
or the robot interacts with unobserved objects. To address
this challenge, we introduce the Calibrated Neural-Accelerated
Belief Update (CNABU) technique to learn belief propagation
models for both observation (obtaining new images from the
environment) and manipulation actions. Confidence calibration
is especially important for belief propagation because over-
confidence in either object dynamics or map prediction can
result in ineffective exploration andor early termination. We
employ evidential deep learning to obtain better off-the-shelf
model calibration .
Our experiments in simulation environments demonstrate
that our proposed MEM planner outperforms prior work
and CNABU-enhanced baselines in terms of metric-semantic
accuracy. Furthermore, we perform hardware experiments with
a UR5 robot equipped with a gripper and an in-hand camera,
demonstrating zero-shot transfer of the learned models, and
showing the efficacy of our method in mapping of cluttered
shelves. An implementation of our method can be found on
Github1.
II. RELATED WORK
A. Next Best Viewpoint Planning
NBV planning is a well-researched approach in the area of
active vision that has been applied to both object reconstruc-
tion and large-scale scene mapping. Generally, NBV consists
of two steps: First sampling view candidates, then evaluating
which candidate is the best. For object reconstruction tasks
1 enhanced map prediction
the object. For large-scale scenes, sampling is more challeng-
ing. Monica and Aleotti  sample at the contour of the
explored scene. Other approaches sample at either predefined
or dynamically detected regions of interest. For the evaluation,
most approaches compute an estimated information gain to
determine the utility of a view. The information gain is often
based on the expected entropy reduction, e.g. by counting
unknown voxels in the field of view. Other approaches like
Hepp et al.  rely on a learned utility to predict the
best view. In this work, we build upon existing concepts of
NBV planing, but enhance them by incorporating manipulation
actions to interactively shape and explore the environment,
allowing the robot to gather richer information and adapt its
strategy based on both observation and interaction.
B. Mechanical Search in Shelves and Piles
Mechanical search algorithms [9, 18, 39] locate and extract
one or multiple target objects from a given scene, while
dealing with confined spaces, occlusion and object occurence
correlations. The task consists of multiple steps, i.e., visual
execution. For visual reasoning, current research demonstrates
that the scene can be effectively explored by interacting with
objects [3, 11, 26, 33] to actively reduce or overcome occlu-
Kim et al.  propose a method for locating and retrieving
a target object using both pushing and pick-and-place actions.
long-term map, and rebuilds environmental knowledge from
scratch with each observation. Therefore, the approach can
lead to unnecessary manipulation actions, as the target may
already be visible from other viewpoints. In the context of
planning for Manipulation Among Movable Objects (MAMO)
[36, 40], Saxena and Likhachev  introduced a method for
object retrieval in cluttered, confined spaces. Despite achieving
strong retrieval performance, their approach depends entirely
on prior knowledge of object shapes and dynamics. Pajarinen
et al.  propose a POMDP formulation for manipulating
objects under uncertain segmentation using a particle belief
prehensile manipulation to enable efficient belief propagation.
Muguira-Iturralde et al.  propose a planner based on two-
level hierarchical search to enable visibility-aware navigation
with movable objects. Their algorithm, Look and Manipulate
Backchaining (LAMB), however, relies on the assumptions
of deterministic action outcomes and on extremely simplified
environment dynamics (grasps always succeed, pushed objects
always slide along axis without rotation), having limited
applicability in the real world.
In this work, we do not focus on retrieving individual
environment. With our long-term occupancy and semantic map
on perfect model knowledge or single-shot scene understand-
complex manipulation behaviors.
C. Learned World Dynamics Models
Many model-based reinforcement learning algorithms learn
environment models from episodic environmental interaction,
often in latent spaces for improved evaluation speed .
These models, however, do not result in a human-interpretable
representation in contrast to our learned map-space dynamics.
Another recent trend leverages the popularity of conditional
video diffusion models to develop interactive pixel-space
simulators [45, 43]. These models focus on short-term visual
fidelity. We posit that explicit 3D maps provide long-term
temporal consistency and calibrated uncertainty measures that
are needed for robotic tasks that involve information gathering.
III. PROBLEM DEFINITION
We address the MEM problem as follows: Consider an
environment with a set of movable objects of varying sizes
and orientations, where some objects may be occluded and
not directly observable from any viewpoint. The arrange-
ment of these objects, along with the fixed support geometry
(e.g., a shelf or table), constitute the workspace configuration
space Cw . The environments initial configuration is un-
observable and denoted c0 Cw.
The robots objective is to create an accurate representation
of the workspace configuration ct after the execution of a
sequence  of actions [a0, . . . , an]. These actions include two
to a specific viewpoint vt V to capture an RGB-D image,
and manipulation actions, where the robot interacts with the
scene (e.g., via pushing or grasping). We address the eye-
in-hand RGB-D camera setting in which the robot does not
receive informative observations during manipulation and must
instead move to a retracted viewpoint to receive valid depth
data due to minimum depth restrictions. Moreover, we do
not integrate views during movement between locations, since
such images are subject to motion blur.
To formalize the problem, let the robots internal represen-
tation of the environment, a belief over metric-semantic maps
explaining object classes over the workspace, be denoted as
t. We assume a closed set of Nclasses semantic classes. Let
the most-likely map according to a belief t be denoted t.
When the robot executes a manipulation action at, it causes
a transition ct 7ct1 Cw according to the environments
dynamics ct1  Dyn(ct, at). Additionally, whenever the
robot takes any action at and gets an observation ot, drawn
according to the observation function Z(otat, ct1), its inter-
nal representation is updated through its belief update function
t1  BelUpdate(t, at, ot).
an optimal budgeted mapping problem. The robot is given
a maximum action budget T, and an initial environment
configuration c0, which is a priori unknown. The task is to
output the most informative sequence of actions  such that
the robots predicted map T , at the last step of the budget,
maximizes its mean Intersection over Union (mIoU) to the
ground truth map GT
which represents cT . So we have:
ToMap(cT )
ct1  Dyn(ct, at)t
t1  BelUpdate(tat, ot), ot Z(otat, ct1), t
where BelUpdate() represents the robots belief update, Z()
is the observation function, and ToMap() yields the metric-
semantic map that corresponds to a known configuration of
the environment. In deployment, the robot cannot accurately
predict GT
ration nor the dynamics of the environment. It may not even
know the number of objects, their shapes, or semantic labels.
IV. METHOD
A. Overview
We model the MEM problem as a Partially Observable
Markov Decision Process (POMDP) in metric-semantic map-
space . To solve this POMDP, the agent should perform a
belief update about the state of the map after both manipu-
lation and observation actions. For manipulation actions, the
belief update propagates through a map transition function
T(t1t, at) that is a priori unknown. For observation
while reasoning over hidden object shapes and arrangements
to reduce uncertainty.
sible maps makes traditional belief updates computationally
infeasible . The approximation that map cells are inde-
leads to compact belief representations but reduces precision
in the belief update. Our CNABU method captures prior
knowledge about the world, such as object contiguity and usual
object sizes and arrangements, in its update. We also leverage
uncertainty-aware deep learning models to predict factorized
belief updates that are better aligned with plausible map con-
figurations and yield actionable quantification of uncertainty.
These models are trained using simulated ground truth to ap-
proximate occlusion reasoning and interaction dynamics, i.e.,
Dyn. Object sizes, classes, occlusion levels, and manipulation
effects should be roughly representative of real scenes, but
our method is tolerant to differences in configuration, number,
class distribution, and moderate shape changes.
tive with the Volumetric Information Gain (VIG) metric ,
since directly modeling the mIoU metric is not feasible, as it
depends on the unobservable workspace configuration. It uses
a 2-step greedy approach that obtains good performance by
exploiting near-submodularity of the VIG function. Moreover,
the 2-step approach obviates the need to sample from the
observation distribution Z.
B. Neural Map Belief Dynamics
Following grid mapping literature , we represent a belief
t over the semantic-metric map using a Bernoulli distribution
Manipulation
Occlusion Aware
Information Gain
Observation
Partial observation
Predicted push dynamics
Executed
manipulation
Belief at time t
Push beliefs
Selection
If observation action selected
Best manipulation action
If manipulation action selected
Executed
observation
Manipulation actions
View actions
Belief at time t1
after manipulation
Belief at time t1
after observation
Best observation
Fig. 2: From a prior map belief, our pipeline predicts a map belief resulting from a set of candidate pushes. It then weighs the information gain from taking
two consecutive independent views given the current belief (orange arrows) or taking a single observation given any of the predicted beliefs after pushing
(blue arrows), selecting the path of highest cumulative information gain and taking its respective first action  either taking the next best view or executing
the best push. IGVt represents the best information gain obtainable from taking two distinct observation actions, whereas IGMt is the best information gain
obtainable through a manipulation action followed by an observation action.
for a cells occupancy and a categorical distribution for a
cells semantic class. Each cell is assumed independent. We
represent the occupancy map as a 3D voxel belief O
RHW D and the semantic map as a 2D birds-eye belief
t RHW Nclasses. The semantic map is 2D for simplicity
because objects are roughly prismatic and stacking is not
allowed in our problem, while the occupancy is 3D because
object heights affect visibility determination. We consider
observations ot O consisting of an RGB-D image with
added semantic labels.
efficient
introduce
Calibrated-Neural Accelerated Belief Update (CNABU) tech-
nique that uses separate neural networks to represent the
posteriors of the viewpoint and manipulation actions in the
factored representation. The first, called observation CNABU,
computes a map belief update after a observation action
o(t, ot, at). The second, called manipulation
action t1 m(t, at), where we drop the observation
because no new observation is generated.
Let t()  P(t) denote the probability density of a
map  under belief t. For any action at and observation ot,
the standard POMDP belief update equation  gives the
posterior belief as:
Z(otat, )
T(, at)t(),
where  ranges over all possible maps, with  being a
normalizing constant. If we wished to project the belief state
into a marginalized occupancy probability of a cell, we would
need to compute:
t1[i, j, k]  Et1[O[i, j, k]  1],
where E is the expected value. A similar equation would hold
for semantic updates. Regardless, the space of possible maps is
far too large  and there is no current method for assessing
map densities t() that accurately accounts for inter-cell
correlations (e.g., object shapes, arrangements, and visibility).
any scene that could produce a belief t via its observations is
inherently a sample of the distribution induced by this belief,
the training process leverages a neural networks averaging
tendency to create an implicit Monte Carlo estimate of Eq. 3.
We postpone the network and training details to Section. IV-G
and proceed to describe their use in MEM.
C. Solving the POMDP
We propose to solve the map-space POMDP by using a
k-step receding horizon greedy planner, as shown in Fig. 2,
which uses Volumetric Information Gain (VIG)  as an
approximation of the true reward. VIG correlates to infor-
mation gain and is a submodular optimization objective in
static scenes , having been used to efficiently solve NBV
planning and sensor placement problems. Due to VIGs sub-
bounded suboptimality, justifying the greedy receding-horizon
strategy. While VIGs submodularity does not hold in general
for dynamic scenes, we assume that manipulation actions do
not increase entropy at a rate that justifies the significant
expense of long lookaheads. However, because a manipula-
tion action does not produce any observation (and hence no
immediate information gain) at least one further observation
must be considered in scoring. Hence, the minimum viable
search horizon at k  2, which yields a balanced tradeoff
between computational efficiency and action quality.
To perform an observation action, the robot chooses from
vi V possible views in a fixed array of camera positions
V to which the robot can move. Furthermore, let t t
be a sampled manipulation action from a set of feasible
manipulation actions. In our two-step greedy search, we only
Semantic
Occupancy
Semantic
2D Occ. Beta t1
Semantic Map t
Changes Beta t1
Occ. UNet Inputs
Occ. UNet Inputs
Manipulation CNABU ()
Observation CNABU ()
Semantic Dirichlet t1
3D Occ. Beta t1
Semantic Map t
Semantic Obs.
Occ. Cell Obs.
Free Cell Obs.
3D Occ. Beta t
Occupancy
Projection
Semantic UNet Inputs
Semantic Map t
Semantic Obs.
2D Occ. Beta t1
Semantic Map t
End Point Map
Start Point Map
Swept Volume
3D Occ. Beta t
3D Occ. Beta t1
Changes Beta t1
Projection
Semantic Dirichlet t1
Learned Modules
Intermediate Results
Static Processes
Semantic UNet Inputs
Fig. 3: Architecture overview of our observation and manipulation CNABU networks. The observation prediction network uses the occupancy posterior beta
and semantic posterior Dirichlet for loss computation, while the manipulation prediction network additionally takes the 2D map of occupancy changes after
the push for loss calculations.
consider two possible kinds of action sequences: taking two
observation actions (vt, vt1) or performing a manipulation
action followed by an observation (t, vt1). This is because
(t, t1) would result in no observation and therefore no
information gain and the information gain of (vt, t1) is
strictly smaller than the VIG of any (vt, vt1), vt1  vt.
Letting IG(vi,    , vin1O) denote the Volumetric
Occlusion-aware Information Gain  for voxels intersected
by the rays from n views (vi    vin1) given belief O ,
the two most informative consecutive views (v
t1), are:
t1)  arg max
IG(vt, vt1O
For manipulation, t
t1  m(t, t) denotes the predicted
belief from the manipulation CNABU when given action
t t as input. We can define the most informative 1-step
t and its associated most informative view v
represent
semantic
H(t, t1)  H(t) H(t1).
IGVt  IG(v
information
gain obtained from two viewpoints, also called ViewPoint
Planning (VPP), while IGMt  IG(v
t1) represents
the best information gain from a push followed by a
selection.
Regt  H(t,
captures
difference
between the current semantic map and the map after the best
Our policy decides the action at to take according to:
if IGVt > IGMt  Regt
otherwise
Where  is a discount factor to account for different
magnitudes between camera array VIGs and whole-map se-
mantic entropy values. Regt serves as a regularization on
the aggressiveness of the selected pushes, as more radical
environment manipulations could potentially reveal more of
the environment, but would cause unnecessary disturbances to
the scene and introduce a lot more uncertainty to the post-
push representations. If at
get the observation ot and use o to obtain the new belief
t1  o(t, at, ot). This search is then repeated in a loop
until the maximum number of actions has been performed, or
a threshold for full map completion has been reached, which
we set to 95 of the semantic voxels with greater than 85
certainty in the belief . When this threshold is reached, the
planner no longer pushes, but still collects novel views.
D. Push Sampling
We consider pushing as our manipulation action. To com-
pute valid push candidates using O
high-confidence frontier points from the shelf entrance and
sample k of them uniformly at random as start points for
the pushes. We test the start points of the k sampled pushes
for collisions against other high confidence voxels in O
After randomly sampling these k unique frontier points, we
determine for each point whether this starting position of a
push will lead to a feasible and valid motion plan. For each
valid start point, we sample a likely occupied point in O
near it to obtain the push direction and sample a push distance
uniformly at random between 50 and 150 mm. We then obtain
a valid motion plan using a sampling based motion planner
[13, 15] and parametrize this plan with .
E. Evidential Posterior Learning
Although we could learn to predict beliefs as functions
evidential posterior networks , as evidential learning is
known improve the calibration of uncertain predictions. An
evidential map belief  consists of belief prior parameters
for each cell of the map belief . Specifically, we store
O RHW D2, a 3D grid of Beta distribution parameters
for each voxel in the map, and S RHW Nclasses, a
grid of Dirichlet distribution parameters for each cell in the
2D map. Let Beta() and Dir() denote the Dirichlet and
Beta distributions, respectively. Therefore, the occupancy and
semantic map beliefs are related to evidential parameters via
O[]  E[Beta(O[])] and S[]  E[Dir(S[])] . We
assume that the initial states O
0 are uninformed and
set to 1, a unit tensor with appropriate dimensions. Note: evi-
dential parameters are maintained for each map belief  in our
algorithm and CNABUs operate by propagating the evidential
parameters (t1 o(t, ot, at) and t1 m(t, at))
followed by an update to the standard belief parameters.
F. Dataset Generation
To train the CNABU models o and m, we collect datasets
on maps, viewpoints, and sampled pushes in simulation.
A total of 14 different object categories from the YCB
dataset  are used and sampled in a shelf board of size
(0.8  0.4  0.4)m. We sample object configurations on
the shelf following a stochastic method that considers class
dependencies and efficient free space coverage for placing
objects. This method allows for the sampling of varied object
detail in Appendix A.
To train the viewpoint belief prediction model o, we
assemble a dataset D  {d1, d2, . . .} where each scene di has
the form (GT, o1,    , on). Here, GT represents the ground
truth 3D metric-semantic voxel map of a shelf environment
with randomly placed objects, and o1,    , on are the depth
and semantically segmented images captured from V, the set
of discrete viewpoints in the environment. The ground truth
semantic labels are used in the rendered images.
To train the manipulation belief prediction model m, the
simulated robot executes a randomly sampled action in synthe-
sized scenes. This produces a dataset Da  {da
each sequence da
post are the ground truth maps before and after manipu-
observations from V as before, taken before the manipulation
is executed.
G. Training CNABU Networks
We now outline the procedure for training the two CNABUs,
whose network architectures are shown in Fig. 3, with further
details given in Appendix B.
We train o from the dataset D by sampling, without
l1), from every scene di at every epoch. This
sampling diversifies the beliefs encountered during training by
varying the emulated observation sequences for each scene.
The loss over the sequence d is computed as follows. We
recursively predict the evidential beliefs t  o(t1, o
up to time l. Also, let yi {0, 1}dim indicate a one-hot
encoded tensor of the ground truth value for a given voxel
i according to GT, where dim is either Nclasses for S
t . For each voxel i in t, define, for its predicted
distribution parameter i
t  yi  (1 yi) i
where denotes element-wise multiplication. We employ the
evidential uncertainty-aware cross-entropy from Sensoy et al.
as the loss, which, for each time step, is given by:
t)Dir(1)
Calibration
L515 Camera
Fig. 4: Real-world environment showing a shelf scenario. The UR5 is
equipped with an Robotiq parallel-jaw gripper and a Realsense L515 RGB-D
camera to create a calibrated representation of the scene.
, where L(i
t) log(ij
tween distributions a and b, 1 is a vector of all ones, type is
o if it is an occupancy loss (dim  2) and s if it is a semantic
loss (dim  Nclasses), and  is an annealing parameter, set
according to Sensoy et al. . The total loss for the sample
d is the sum of the semantic and occupancy losses Lo
over the l observations.
The manipulation CNABU is defined similarly to the obser-
vation CNABU, except that it has an auxiliary output, which
predicts a Beta distribution over a voxel grid modeling the
probability of a given voxel being changed in GT after the
manipulation is executed, which we call diff. Therefore, we
t1  m(t, at)
Training epochs iterate over sequences in Da. For each
without replacement as above. We then recursively obtain
the beliefs from the observation CNABU, t1  o(t, o
for t  0, . . . , l 1. Next, we predict the post-manipulation
belief S
t1  m(l, a) and use GT
post as the
training target. The random sampling of a different number
of observations prior to pushing ensures the CNABU sees
different belief stages during training. We derive the ground
truth for diff, GT
consistency loss, Lcon which is the Mean Squared Error
between l1 and l. This loss serves as a regularization
to encourage the alpha parameters of the distributions in the
unchanged areas of the map to have a similar magnitude.
As before, the network heads are trained using the loss in
Eq. 7 . The total loss for the manipulation sequence is
given by L  LO
l1  Ldiff
l1  Lcon.
V. EXPERIMENTS
We perform four core experiments to evaluate our approach.
provements in map completeness and accuracy compared to
state-of-the-art . Next, we present a series of ablations
of our method and evaluate several interactive baselines. We
Ours wo Pushing
Random  Observation CNABU
Dengler et al.
Action Taken
Occupancy mIoU
Low Occlusion - Occupancy
Action Taken
High Occlusion - Occupancy
Action Taken
Semantic mIoU
Low Occlusion - Semantics
Action Taken
High Occlusion - Semantics
Fig. 5: Simulation results for Manipulation Enhanced Mapping against SOTA and non-pushing baselinesshowing both occupancy and semantic IoUs over
time for each method. Our method outperforms all baselines in highly occluded scenes, while not having degraded performance in low occlusion scenes.
Standard deviation of performance of random baselines over random seeds is represented as shading around each plot.
then present some experiments on the generalization of the
trained CNABUs to within-class shape variations. Finally, we
study the robustness of our system in terms of its zero-shot
transferability to a physical setup. Further evaluations, which
validate the individual CNABUs performance and the use of
VIG as a reward proxy, are provided in Appendices C and D.
A. Experimental Setup
Our task setup consists of a shelf scene with a UR5 arm for
observation and action execution (Fig. 4). The robot is fixed to
a table facing an occluded shelf and equipped with a Robotiq
parallel-jaw gripper for manipulation and an RGB-D camera
for observations. In simulation, the ground truth observations
and segmentations are provided by rendering.
The real-world setup is similar, but with a few notable
differences. For action execution, ROS and MoveIt  are
used. The depth image is obtained from an intel Realsense
L515 camera and the semantic segmentation in the real world
is performed using segment anything 2 (SAM2)  and a
strategy similar to LSeg [25, 19]. We take detected masks
from SAM2 and crop the original image around them. Next,
we extract their CLIP  embeddings and compute their
cosine similarity to the language embeddings of our target
we normalize the similarity scores to classify each mask.
B. Simulation Experiments
These experiments consider both low and high occlusion
scenarios for manipulation-enhanced mapping. We generate
100 low occlusion scenarios via rejection sampling, using our
sampling method described in Appendix A, but keeping only
scenarios for which at least one object cannot be seen from
any of the 300 viewpoints. We then crafted 25 high occlusion
scenarios by hand to be challengingly crowded and with many
objects occluded. We provide examples of each category in
Figs. 10 and 11 in the Appendix. The robot begins with a
naive uniform map prior.
We compare our work (Ours) with the following baselines.
and fine-tuned the network weights provided by the authors
for only 5,000 action steps. Although their experimental setup
closely aligns with ours, our tests introduce a lower upper
shelf board height and more densely sampled object con-
figurations. Second, the Random baseline randomly samples
a set of unique views [vr
n] V and uses standard
metric-semantic occupancy mapping . We also combine
random view selection with our observation belief predictor
ablation of our pipeline that does not use manipulation, Ours
wo pushing.
Metric and semantic mIoU compared to the ground truth
map at time t are plotted in Fig. 5. We observe that the
previous S.O.T.A. for unstructured MEM, Dengler et al. ,
explores efficiently at early stages. However, with more of the
scene uncovered and new areas being harder to observe, its
performance degrades to comparable or worse than random,
particularly in terms of semantic mIoU. We attribute this
performance degradation to two key challenges: the use of
heuristic-based action switching and the lack of map updates
after manipulation. The heuristic switching mechanism relies
on hand-crafted rules to alternate between observation and
manipulation actions, and may not always select the most
informative action, occasionally choosing to push too early,
too often, or to continue observing when manipulation would
be more beneficial. Our POMDP-based action selection over-
comes this problem and does not select manipulation actions
when they are not beneficial for higher information gain. Fur-
update its belief after a push, it requires multiple subsequent
observations to reconcile inconsistencies between the actual
scene and the previously assumed map representation. This
delay in belief correction leads to inefficient re-exploration
and degraded semantic accuracy, as the agent lacks a reliable
signal for where to focus its attention.
scenes even without pushing. In highly occluded scenarios,
pushing is required to make progress after the visible surfaces
Random Push Every 5th
Saturation Pushing
Random Push Every 5th, No Manip. CNABU
Random View, Random Push Every 5th
Action Taken
Occupancy mIoU
Low Occlusion - Occupancy
Action Taken
Occupancy mIoU
High Occlusion - Occupancy
Action Taken
Semantic mIoU
Low Occlusion - Semantics
Action Taken
Semantic mIoU
High Occlusion - Semantics
Fig. 6: Simulation tests of push selection alternatives. Note how our method not only achieves better mIoUs than any of the other methods, it does so
consistently across all the steps, avoiding uninformative or overly aggressive manipulation actions. Also, next-best-view with the observation belief prediction
but without push belief prediction (purple) leads to degraded performance. Standard deviation of performance of random baselines is represented as shading
around each plot.
are observed. Our method uses pushing to achieve significantly
higher mIoUs. Note that its IoU growth is slower early
viewpoint step is taken in the following action.
C. Push Selection Alternatives
Unless otherwise noted, the same belief prediction networks
are provided to each method. Three of the strategies push
regularly at a five-step interval, which is a typical rate at
which observation actions provide diminishing returns. The
its belief using m. The second, Random Push Every 5th, No
Manip. CNABU is the same, but does not use m to update its
belief after pushing. The third, Random View Random Push
Every 5th, chooses random views and randomly pushes every
five steps without using m. Finally, we consider a heuristic
that performs a random push when VIG seems to saturate,
Saturation Pushing. The saturation threshold is when two
consecutive estimates of VIG during NBV differ by less than
2. Saturation pushing uses m after each push to update its
belief. A comparison of selection strategies is given in Sec. F
in the Appendix.
Results are shown in Fig. 6. Each baseline that has a
random component is run three times with different random
seeds. We observe that push belief prediction is beneficial to
manipulation-enhanced mapping, even when random pushes
are being executed. The blue and purple curves show methods
that are not informed by m. Further, we see that the satura-
tion pushing (green) does not observe post-push performance
pushes and our method. Overall, our method still achieves the
best performance, most strikingly in highly occluded scenes.
D. Out of Distribution Shape Experiments
In order to evaluate the robustness of the proposed pipeline
to shape variations within a given class, we modify the
simulation setup. For each individual instance of an object
in a given scene, we randomly and independently rescale
the objects mesh on each of its principal axis by a factor
chosen uniformly at random within [0.8, 1.2], and re-run
our simulation experiments in the high-occlusion scenes with
these out-of-distribution shapes for the strongest baselines in
simulation. As seen in Fig. 8, we observe a decrease in the
agents performance when out of distribution, but note that
in the long run the performance remains consistent with our
ID experiments. This is notable since we do not perform
any sort of shape augmentation during the training of the
observed maps helps them overcome some level of distribution
shift. Naturally, we presume that adding shape augmentation
and more diverse object geometries of the same class during
training should further help reduce this OOD performance gap.
E. Hardware Experiments
We also performed 10 real-world experimental runs on
a UR5 as described in Sec. V-A. All results are collected
in a zero-shot fashion, i.e., no fine-tuning on real data was
performed. We set the budget to 20 steps and sampled a fixed
set of 75 reachable camera poses in front of the shelf for V.
We handcrafted 10 challenging scenes, each with an average
of 18 objects from the YCB dataset , where pushing is
required to reveal other objects. We collect the ground truths
by removing the top of the shelf at the end of each episode
to manually score the final maps. We score each scenario
according to the status of all of the objects present in the
map. Each object in the map is classified in four categories:
1) Correctly Found if the majority of the object is correctly
represented in the map with the right class; 2) Misclassified
But Found if the majority of the object is present in the
occupancy map but is mislabeled; 3) Not Found - if the
majority of the object is absent from the occupancy map and
4) Hallucinated if an object that is not present in the scene is
present in the map. Due to the complexity of precisely resetting
a scene multiple times for different baselines and the fact that
cornflakes
ohio cookies
chewing gum
pasta box
pringles can
baking powder
tomato can
gelatin box
cheez-it box
glass cleaner
coffee can
milk carton
mustard bottle
background
Fig. 7: Qualitative real-world experiment results. Note how VPP and Random baselines are unable to fully explore the environment due to its occlusions,
while our method is able to better explore it via reasonable manipulations. In Step 16, Ours, we highlight in yellow one of the scene objects revealed by
manipulation. We highlight in red a persistent hallucination across all 3 methods, likely due to unreliable semantic segmentation and significant camera noise
at that corner of the shelf .
Ours OoD
Ours wo Pushing OoD
Saturation Pushing OoD
Action Taken
Occupancy mIoU
(a) Occupancy Results (IoU)
Action Taken
Semantic mIoU
(b) Semantic Results (mIoU)
Fig. 8: OoD Results in High Occlusion scenes.
both random  observation CNABU and our approach without
pushing are about as strong as other pushing baselines (see
Figs. 5 and 6), we only compare against them in the physical
experiments. For each model, we report the total quantity of
each detection at time step 20 summed over all 10 trials.
Results in Tab. I show that with zero-shot transfer from sim-
over the compared baselines. Note that all methods compared
use calibrated belief prediction. However, little difference is
seen between viewpoint planning (Ours wo Pushing) and
random viewpoint choices. We expect that this is due to a
domain gap caused by camera noise from the realsense L515
leading to some strong artifacting in the depth images and the
inaccuracies of the open-set sema
