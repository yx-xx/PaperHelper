=== PDF文件: Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation.pdf ===
=== 时间: 2025-07-22 09:42:45.366561 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Bridging the Sim-to-Real Gap for Athletic
Loco-Manipulation
Nolan Fey, Gabriel B. Margolis, Martin Peticco, and Pulkit Agrawal
Improbable AI Lab
Massachusetts Institute of Technology, Cambridge, MA 02139
AbstractAchieving athletic loco-manipulation on robots re-
quires moving beyond traditional tracking rewardswhich sim-
ply guide the robot along a reference trajectoryto task rewards
that drive truly dynamic, goal-oriented behaviors. Commands
such as throw the ball as far as you can or lift the weight as
quickly as possible compel the robot to exhibit the agility and
power inherent in athletic performance. However, training solely
with task rewards introduces two major challenges: these rewards
are prone to exploitation (reward hacking), and the exploration
process can lack sufficient direction. To address these issues, we
propose a two-stage training pipeline. First, we introduce the Un-
supervised Actuator Net (UAN), which leverages real-world data
to bridge the sim-to-real gap for complex actuation mechanisms
without requiring access to torque sensing. UAN mitigates reward
hacking by ensuring that the learned behaviors remain robust
and transferable. Second, we use a pre-training and fine-tuning
strategy that leverages reference trajectories as initial hints to
guide exploration. With these innovations, our robot athlete
learns to lift, throw, and drag with remarkable fidelity from
simulation to reality.
I. INTRODUCTION
General whole-body control comes naturally to animals
after years of evolution, yet it remains a long-standing chal-
lenge in robotics. Fluid whole-body motion requires balancing
multiple competing tasks and constraints that depend on both
the robots morphology and its environment . Recent
work [7, 34] demonstrates that sim-to-real reinforcement learn-
ing (RL), using methods such as Proximal Policy Optimization
(PPO) , is a promising paradigm for learning these behav-
iors by leveraging parallel simulations .
For dynamic, goal-oriented loco-manipulation, it is natural
to train robots with task rewardscommands like throw the
ball as far as possible or lift the weight as quickly as
possible that drive athletic behaviors. However, these task
rewards pose two major challenges: (i) they are prone to
reward hacking, where the policy exploits imperfections in the
guidance. To circumvent these issues, many works on sim-
to-real transfer instead train whole-body controllers (WBCs)
to track dense reference motions [4, 7, 13, 23, 34]. Dense
tracking objectives provide strong regularization by constrain-
ing the policy to adhere to a reference trajectorythereby
reducing reward hackingand they offer a structured path for
1Authors are also affiliated with Computer Science and Artificial Lab-
oratory (CSAIL), the Laboratory for Information and Decision Systems
(LIDS), and the MIT-IBM Watson AI Lab at MIT. Correspondence to
nolanfeymit.edu
Ball Throw
Dumbbell Snatch
Sled Pull
Fig. 1: Sim-to-real transfer of athletic loco-manipulation.
We reduce the sim-to-real gap for a quadruped manipulator
by learning a corrective model for the simulated actua-
tor dynamics based on real-world data, formulated as an
unsupervised actuator net (UAN). Policies trained with the
corrected simulator exhibit improved sim-to-real transfer and
push the limits of the robots physical capabilities in athletic
tasks involving whole-body coordination. Videos of the robots
behaviors are available at
exploration. However, this strategy relies on defining high-
quality reference commands a priori, which in turn demands
access to high-quality reference data. For robots with non-
human morphologies like legged manipulators, obtaining such
data is particularly challenging, and the resulting reference
commands may not capture the optimal, athletic strategies that
a policy might otherwise discover.
To fully harness the benefits of task rewards, it is crucial
to ensure that the simulation faithfully replicates real-world
dynamics. Inaccurate simulation models allow policies to
exploit imperfections, leading to reward hacking, particularly
so when the reward is underspecified. Although techniques
like domain randomization [50, 51, 53] and online system
identification [15, 19, 24, 29, 31, 35] address this by sampling
over parameter distributions, they rely on a priori assumptions
that may not fully capture the complex dynamics of real hard-
ware. For instance, harmonic drive actuators exhibit non-linear
proxies like motor current unreliable for torque estimation.
A promising alternative is to enhance the simulations
physics model directly with real-world data, focusing on
accurately modeling the actuator dynamics. With this moti-
framework for learning corrective actuator models without the
need for torque sensors. UAN is trained using reinforcement
learning to predict corrective torques,   UAN(e), by
minimizing discrepancies between simulated and real-world
joint encoder measurements. In doing so, UAN effectively
bridges the sim-to-real gap even for robots with complex
transmission mechanisms and noisy or unavailable torque
measurements. We evidence the effectiveness of UAN through
sim-to-real demonstrations incorporating the Unitree Z1 Pro
Building on this enhanced simulation environment, we
address the challenge of guided exploration for athletic be-
haviors. Rather than enforcing strict adherence to a reference
In our approach, a WBC is first pre-trained on random base
velocities and end-effector pose commands to establish a
strong motion prior. Then, to learn a new athletic behavior, we
initialize the controller with a reference trajectory and fine-
tune it using a task-specific rewardallowing the policy to
depart from the reference when beneficial.
In summary, our paper presents an easy-to-use training
pipeline for whole-body athletic behaviors that reliably transfer
to reality. First, we employ the Unsupervised Actuator Net
(UAN) to calibrate actuator dynamics and mitigate reward
physics. With this improved simulation environment, we then
pre-train a whole-body controller (WBC) to establish fun-
damental motion skills and fine-tune it with task-specific
rewardsusing a reference trajectory merely as a hint to guide
exploration. This integrated approach enables our robot to
perform dynamic tasks such as throwing, lifting, and dragging
with remarkable fidelity.
II. METHOD
Our training pipeline (see Figure 2) is separated into two
training (Sections II-B and II-C). The real-to-sim calibration
phase involves collecting data on the real robot and training
a UAN to close the sim-to-real gap for non-ideal actuation
mechanisms. Similar to past work [8, 23, 33, 44], our WBC
training is split into two distinct sub-phases: pre-training (Sec-
tion II-B) and fine-tuning (Section II-C). After pre-training, the
policy can track reference trajectories if provided as a sequence
of base velocity and end effector pose commands. During
the fine-tuning phase, the policy observes a reference task
trajectory. This helps warm start exploration when learning a
new task because the policy can simply track these commands
to achieve reasonable task performance. Through training with
the task reward itself rather than a tracking reward, the policy
learns how to depart from the reference trajectory to achieve
higher task performance. Our simulation environments for the
pre-training and fine-tuning phases rely on the same strate-
gies for sim-to-real transfer, including domain randomization
(Section II-B) and a UAN (Section II-A).
Our experiments consider a Unitree B2 quadruped with
a modified Unitree Z1 Pro arm mounted on its back. The
quadruped is 65 cm tall when standing and weighs 60 kg, while
the arm is 74 cm fully extended and weighs 6.8 kg. The system
has 19 actuated joints: 3 for each leg, 6 for the arm, and 1 for
the gripper.
A. Unsupervised Actuator Net
Some actuators are challenging to model in simulation, es-
pecially when they have complex transmission mechanisms. In
such cases, standard domain randomization and online system
identification techniques may be insufficient, and instead, it
is preferable to learn to model the actuator directly from
hardware data. Previous approaches rely on output torque
to learn how to predict the motors torque. Alternatively, we
propose a method for matching the transition dynamics of the
actuator such that
fsim freal(s, ) fsim(s, ).
To influence the simulator dynamics, fsim, we learn a residual
velocity errors, e, and outputs a corrective torque, , for the
simulator such that
UAN freal
fsim (s,   UAN (e)) .
The corrective torques needed to minimize the transition error
are unlabeled, so we parameterize UAN as a neural network
and train it with RL.
1) Architecture and observation space: The network is
designed as a 2-layer MLP with layer sizes [128, 128] and
ELU activations. It is executed at every simulation time step
(5 ms). Assuming each arm joint is identical, a single UAN
is shared across all of the arms actuators, with each actuator
being processed independently by the shared network .
We constrain the observation space to include a history of the
past 20 (equivalent to 100 ms) position and velocity errors
for each relevant actuator. These design choices help prevent
overfitting to other aspects of the training data, such as inertial
coupling. Also, sharing the data across actuators improves
data efficiency. While our training data from individual joints
covers 75 of the operating region of the actuators torque
and velocity, combining data from all arm actuatorseach
Simulator
Reinforcement Learning
AB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXJHoMevGYgHlAsoTZSW8yZnZ2mZkVQsgXePGgiFcyZt4yTZgyYWNBRV3XR3BYng2rjut5Nb
W9Y3MpvF3Z29YPiodHTR2nimGDxSJW7YBqFxiw3AjsJ0opFEgsBWM7mZ6wmV5rF8MOMEYgOJA85o8ZK9YteseSW3TnIKvEyUoIMtV7
xq9uPWRqhNExQrTuemxhQpXhTOC0E01JpSN6A7lkoaofYn80On5MwqfRLGypY0ZK7npjQSOtxFNjOiJqhXvZm4n9eJzXhjThMkNS
rZYFKaCmJjMviZ9rpAZMbaEMsXtrYQNqaLM2GwKNgRveV0rwse5VypX5Vqt5mceThBE7hHDy4hircQw0awADhGV7hzXl0Xpx352PRmnOy
mWP4AfzB3UXj
