=== PDF文件: You Only Teach Once Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations.pdf ===
=== 时间: 2025-07-21 13:44:58.208339 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，要中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：You Only Teach Once: Learn One-Shot Bimanual
Robotic Manipulation from Video Demonstrations
Huayi Zhou, Ruixiang Wang, Yunxin Tai, Yueci Deng, Guiliang Liuand Kui Jia
The Chinese University of Hong Kong, Shenzhen.
Harbin Institute of Technology, Weihai.
The Corresponding Author.
pullpush drawer  pick-place
pour water into the mug cup
pick up  unscrew the bottle
uncover the box with lid
open the delivery box
contact-rich long-horizon complex bimanual tasks (asynchronous)
non-prehensile bimanual tasks (synchronous)
Right Hand Moiton Trajectory
Left Hand Moiton Trajectory
Left Cam
Observation
Right Cam
Observation
Binocular
Left Arm
Right Arm
Parallel
Grippers
Fig. 1: Our proposed YOTO (You Only Teach Once) facilitates various complex long-horizon bimanual tasks. It needs only
a one-shot observation of a single third-person binocular camera to extract the fine-grained motion trajectory of human hands,
which can then be utilized for the dual-arm coordinated action injection and rapid proliferation of training demonstrations.
AbstractBimanual robotic manipulation is a long-standing
challenge of embodied intelligence due to its characteristics of
dual-arm spatial-temporal coordination and high-dimensional
action spaces. Previous studies rely on pre-defined action tax-
onomies or direct teleoperation to alleviate or circumvent these is-
teaching bimanual manipulation is learning from human demon-
strated videos, where rich features such as spatial-temporal
transitions are available almost for free. In this work, we propose
the YOTO (You Only Teach Once), which can extract and then
inject patterns of bimanual actions from as few as a single
binocular observation of hand movements, and teach dual robot
arms various complex tasks. Furthermore, based on keyframes-
based motion trajectories, we devise a subtle solution for rapidly
generating training demonstrations with diverse variations of
manipulated objects and their locations. These data can then
be used to learn a customized bimanual diffusion policy (BiDP)
across diverse scenes. In experiments, YOTO achieves impressive
performance in mimicking 5 intricate long-horizon bimanual
spatial conditions, and outperforms existing visuomotor imitation
learning methods in accuracy and efficiency. Our project link is
I. INTRODUCTION
Bimanual manipulation is an enduring topic in the robotics
community [5, 68, 82, 35, 91, 16, 21]. It has been widely
involved in many other fields such as bionics, high-end
computer vision. Despite this, achieving efficient, precise and
robust manipulation of dual-arm robots to accomplish various
daily tasks remains a difficult research area. Generally, there
are two main challenges: coordination and state complexity
[32, 51]. On the one hand, the two arms working together
need to move alternately or simultaneously in a coordinated,
non-procrastinated manner and avoid collisions with the scene
or each other. This places stringent demands on the control
and scheduling scheme. On the other hand, the total degrees
of freedom of two arms and their respective end effectors
are distributed in a higher-dimensional space than a single
arm. This makes the design of motion planning and action
prediction more challenging. Given these difficulties, it is no
small feat to drive two robot arms to perform tasks that human
toddlers can do with ease, such as uncovering lids, assembling
blocks and lifting large-size objects, let alone mastering many
more complex long-horizon skills.
The mainstream bimanual manipulation research includes
two major branches: explicitly classifying tasks based on pre-
defined taxonomy [82, 45, 32, 51] and implicitly learning from
demonstrations collected by teleoperation [104, 65, 62, 53].
The former often fails to uniformly cover arbitrary tasks
and also limits the flexibility of the robot arm. While the
latter requires substantial training data which is inconvenient
to scale up. And collected demonstrations are intrinsically
non-stationary and despatialized, which is not conducive to
training robust and generalizable action policies. In addition
to taxonomy and teleoperation, an indirect but more plausible
and interpretable route is to learn from human action videos
[4, 107, 25, 13, 47, 69, 42]. This route is based on relatively
mature vision techniques to process human demonstrations and
extract high-level features for generating robot manipulation-
relevant elements. In this paper, we also follow this promising
path. Our dual-arm workbench, hardware settings, and selected
bimanual tasks are shown in Fig. 1. And the overall framework
is shown in Fig. 2.
cluding their location, left-rightness, 3D shape, joints, pose,
using hand-related vision methods [71, 78, 67]. After extract-
ing hand motion trajectories, we do not simply inject step-
wise actions into robots. Because visual perception results
are inevitably erroneous, and real hand motions are jittery
and discontinuous. We thus simplify the consecutive trajectory
into discrete keyframes [38, 80], and assign the corresponding
keyposes to two arms to execute by applying inverse kine-
matics interpolation. Besides, we also record and replay the
order of dual-hand movements (termed as motion mask), which
can help to address the dual-arm coordination issue in long-
horizon bimanual tasks. Now, we successfully obtain a stable
and refined manipulation motion exemplar.
More than that, thanks to the editability of obtained single
demonstrations. First, we change the 6-DoF pose of task-
related objects and adjust corresponding keyposes to let real
robots replay similar actions. Objects can also be replaced with
other ones of analogous shape and size. This auto-rollout oper-
ation is stable and much faster than teleoperation [104, 86]. For
based on a well-taught task. On the other hand, after knowing
the reachable area of manipulators, we can perform geometric
transformation on segmented object point clouds, which can
be extracted by using open vocabulary segmentation [90, 73]
and binocular stereo matching [92, 93]. Such augmentation is
more reliable and efficient than rollout. Therefore, mixing the
above two data expansion schemes, we call it proliferation,
just like the generation of cells.
With sufficient training data, we follow diffusion-based
visuomotor imitation methods [15, 98, 95] and propose a
specialized bimanual diffusion policy (BiDP), which is cus-
tomized for learning long-horizon dual-arm tasks. It has three
major improvements. First, we reduce observations (e.g., 3D
point clouds) from the entire scene to manipulated objects to
accelerate training convergence and eliminate irrelevant terms
[26, 51]. Then, instead of modeling continuous actions, we
choose to predict essential keyposes [55, 89, 41, 99], which
can greatly decrease the diffusion space dimensionality. Third,
we utilize the motion mask to determine the alternating or
synchronous dual-arm moving, and reorganize the bimanual
action space to train a unified action policy. In experiments,
we have verified the high efficiency and effectiveness of BiDP
on challenging bimanual tasks.
We present a paradigm for extracting and injecting dual-
arm movements from a one-shot observation of human
hands demonstration, which supports the fast transfer of
bimanual manipulation skills to two robotic arms.
We develop a solution for rapidly proliferating training
demonstrations based on one-shot teaching, which is
more convenient and reliable than teleoperation.
We propose a dedicated bimanual diffusion policy (BiDP)
algorithm that can efficiently and effectively assist dual-
arm manipulators in imitating complex skills.
Our framework YOTO is compatible with most bimanual
tasks. We verified its effectiveness and superiority on
5 complex long-horizon manipulation tasks (including
synchronous and asynchronous).
II. RELATED WORKS
A. Bimanual Robotic Manipulation
Many bimanual manipulation methods focus on specialized
tasks or primitive skills, such as cloth-folding [56, 6, 19, 88,
[49, 4], throwingcatching [37, 94, 48], scooping , carrying
and dressing . For general bimanual manipulation,
typical research [82, 35, 61, 45, 33, 106, 97] tends to explicitly
classify them into uncoordinated and coordinated, or symmet-
rical and asymmetrical according to task characteristics. Some
homologous approaches assume that two arms form a leader-
follower [52, 32] or stabilizer-actor [29, 51] pair. Most re-
bimanual manipulation by dexterous teleoperating and upgrad-
ing low-cost hardwares of real-world robotics. These similar
works [104, 84, 43, 62, 53, 7] implicitly train an end-to-end
imitation network using massive and diverse teleoperated data,
expecting to get generalized large robotic models. To further
improve dual-arm reachability and dexterity, some studies have
equipped multi-finger hands [50, 86, 79, 20, 14, 23], mobile
footplates [96, 95, 102, 24], tactile feedbacks [50, 20, 12] or
active cameras [17, 14]. In contrast to them, our manipulators
are two fixed-base robot arms with parallel-jaw grippers. We
propose an universal framework that learns bimanual policies
with considering the dual-arm coordination. And the training
data is not collected via teleoperation but proliferated from a
single-shot demonstration.
B. Learn from Human Hand Videos
Human hand videos are valuable resources for learning
complex manipulation behaviors [30, 22, 100, 54, 31]. Ex-
tensive research has leveraged human demonstrations to learn
robot manipulation by extracting rich non-privileged features,
such as keypoints [66, 25, 87], affordances [46, 101, 63], 3D
hand poses [47, 42, 4], motion trajectories [47, 42, 13, 101]
and invariant correspondences [69, 44, 103]. These features
can be tailored to robot-specific variables to alleviate mor-
phology gaps, such as manipulation plans, retargeted motions
Demonstration
(Binocular
Stereo Cam)
segmented
manipulated
Extracted
High-Level
Features
(on Left Cam)
3d hand meshes
hand moiton trajectories
Injected
Keyframes-
based Robot
Components
Refined    Post-Processing
Extraction
Injection
Vision    Techniques
timeline
move move hold move
hold move
hold move hold move move move move move
Auto-Rollout
Verification in
Real-World
Geometric
Transformation
of Point Clouds
Rotation
Translation
Augmentation
Variations
in Location
Category
One-shot
Teaching
Objects Point
Robot State
Bimanual
Diffusion
Predicted
Training
Deployment
Binocular
Vision       Techniques
Objects Point
Predicted
Robot State
Timestep
Fig. 2: The overview of our proposed YOTO. It is a general framework consists of three main modules: (a) the human hand
motion extraction and injection, (b) the training demonstration proliferation from one-shot teaching, and (c) the training and
deployment of a customized bimanual diffusion policy (BiDP). It is best to zoom in to view the details.
and precise actions. Two contemporary works [42, 47] also
propose to use a single human demonstration to learn bimanual
manipulation similar to us. RSRD  roughly recovers 3D
part motion of articulated objects from a monocular RGB
capture arbitrary object in 3D space. OKAMI  applies
the object-aware motion retargeting which is noisy and non-
scheme which is more robust and versatile.
C. Visuomotor Imitation Learning
Visuomotor imitation learning aims to train action predic-
tion policies based on visual observations by exploiting labeled
demonstrations [57, 40, 58, 39, 85, 38, 80]. These learned
policies can drive robots to complete various manipulation
with just dozens of demonstrations, covering long-horizon
[57, 85], dexterous [98, 86] and bimanual [104, 79] tasks.
transformers (ACT) to learn high-frequency controls with
closed-loop feedback in an end-to-end manner. Chi et al.
adopted conditional denoising diffusion models [36, 83, 64]
to represent visuomotor policies in robotics, exhibiting im-
pressive training stability in modeling high-dimensional action
distributions. Ze et al.  incorporated 3D conditioning into
the original diffusion policy , rather than focusing on RGB
images and states as conditions. Yang et al.  combined
SIM(3)-equivariance [96, 76, 8] with diffusion policy, ac-
quiring a more generalizable and sample-efficient visuomotor
policy than [15, 98]. Inspired by them, we propose a bimanual
diffusion policy (BiDP), which adds motion mask as a new
diffusion condition and simplifies visual observations to task-
related object point clouds, making it suitable for learning
bimanual manipulation tasks.
III. HARDWARE SYSTEM
Dual-Arm Placement: Most human video-inspired biman-
ual manipulation works apply humanoid robots [4, 25, 47, 69,
42] or two ipsilateral arms [107, 25] to build workstations.
Some bimanual teleoperations also tend to be anthropopathic
[50, 79, 14, 23] or ipsilateral [86, 12, 20]. Despite the
similarity to human morphology, they are not necessarily op-
timal. Comparatively, it is possible to place two manipulators
opposite each other, as in ALOHA series [104, 24, 1, 105] and
its followers [17, 53, 7]. This heterolateral setup minimizes
the overlap of accessible space and is thus compatible with a
wider range of bimanual tasks. We also adopt the contralateral
placement as shown in the left of Fig. 1, where each arm (Aubo
i51) has a span of approximately 880 mm.
End Effector Selection: Although some methods utilize
multi-fingered dexterous hands as end effectors [86, 79, 14, 23]
and even add tactile sensors [50, 20, 12] to the hands, we still
use two parallel-jaw grippers (with max opening distance 80
mm of each DH-Robotics2), which are easier to control and
interpret. We will show that it is sufficient to complete complex
tasks that are non-prehensile or synchronous.
Camera Observation: Many previous methods adopt the
multi-view RGB observations [104, 53, 7], mainly including
the global third-person camera and the local eye-in-hand
camera. Other works have shown that a single third-person
RGB-D camera [86, 4, 25, 47] is also acceptable. We use a
binocular stereo camera (the DexSense 3D industrial camera3),
similar to commercial RGB-D cameras, but providing raw left
and right images to enable flexible post-processing.
IV. METHOD
In this part, we introduce in detail the proposed framework
in Fig. 2. We firstly give a basic definition of the problem
in Sec. IV-A. Then, a detailed explanation of the three core
modules is presented, which includes the standardized hand
motion extraction and injection process in Sec. IV-B, the
demonstration proliferation solution from one teaching in
Sec. IV-C and the proposed visuomotor bimanual diffusion
policy (BiDP) method in Sec. IV-D.
A. Problem Formulation
In this paper, we mainly consider bimanual robot manipula-
tion tasks, where the agent (e.g., dual manipulators equipped
with parallel-jaw grippers) does not have access to the ground-
truth state of the environment, but visual observations O from
a binocular camera and robots proprioception states S. As for
the action space A  {ap R3, ar SO(3), ag {0, 1}},
it includes the target 6-DoF pose of each robot arm and
the binary openclosed state of the gripper. Note, we focus
on bimanual tasks sharing the same observations O. For the
difference between left and right hands below.
For imitation learning, the agent mimics manipulation plans
from labeled demonstrations D  {(O, A)i}N
is the number of trajectories, O  {Ot, SL
observations of all T steps, and A{AL
t1 are actions
to complete the task. The learning objective can be simply
concluded as a maximum likelihood observation-conditioned
imitation objective to learn the policy :
E(O,A)iD
t0 log (A
strations proliferated from only a single-shot human teaching
and how to improve existing diffusion-based imitation policies
for addressing the bimanual manipulation problem.
B. Hand Motion Extraction and Injection
This part corresponds to the module in Fig. 2 (a).We first
manually demonstrate a long-horizon bimanual task using two
hands on the dual-arm accessible operating table. Then, we
leverage favourable vision techniques to extract rich manip-
ulation features from recorded videos by a single binocular
camera. Extracted features will be post-processed to obtain
keyframes-based motion variables (such as 6-DoF poses and
gripper states) that can drive dual arms.
1) Human Demonstration Capturing: By default, we cap-
ture dual-stream synchronized RGB videos with slight nec-
essary visual difference between left and right cameras to
estimate disparity and depth map. We mainly observe the left
RGB view to extract a series of hand-related features, and
thus always keep both hands visible to the left camera. The
right view is only awakened when accurate 3D information is
needed in a particular frame. This reduces the computational
burden of stereo matching  by at least half.
Algorithm 1 3D Hand Pose Calculation.
hand joints Ihand, index numbers of wrist joint iwri  index-fingertip
iind  ring-fingertip iring, the given chirality  L or  R.
either L or R
Initialize P
j MANO(H
3D hand joints indexing
j[iwri], pind P
j[iind], pring P
j[iring];
liw (pind pwri), lrw (pring pwri);
two 3D lines
vz CROSS PRODUCT(liw, lrw);
Z-axis direction
vz vz(NORMALIZE(vz)  1e-8);
vector normalization
vy  lmid (liw  lrw)2.0;
middle line (Y-axis direction)
vy vy(NORMALIZE(vy)  1e-8);
vector normalization
vx CROSS PRODUCT(vy, vz);
X-axis direction
vrot CONCATENATE([vx, vy, vz]);  final 33 rotation matrix
return vrot;
2) High-Level Features Extraction: Given a video demon-
stration (the left stream) of one specified bimanual task, we
run our vision perception pipeline to obtain the 3D point
trajectories and status of two hands.
3D point trajectories. We first use WiLoR  to detect
bounding boxes of left and right hands in each frame and
then estimate their 3D shapes HL and HR represented by
MANO . Then, we simply track the center point hp,
j ) of each hand and obtain the 3D hands sequence
index among all J frames. The hp,
can be calculated by
averaging several selected points (e.g., five finger tips) from
21 pre-defined joints of the 3D hand model H
As of here, many similar works [42, 47, 14, 23] choose to
retarget the produced continuous trajectories {hp,
j1 to their
end effectors through estimated 3D geometric transformations.
algorithms in left-right classification and 3D shape regression,
we cannot fully trust trajectories directly derived from them. In
achieve continuous and consistent prediction in a given camera
space. This is also pointed out and verified by DexCap .
More examples can be found in Fig. 5. As an alternative, we
propose to project all 3D points {hp,
j1 onto the 2D image,
and then lift these points to 3D by applying the stereo matching
algorithm . The final back-projected 3D point trajectories
are {bhp,
given camera space.
States of two hands. In order to fully map hand movements
to two-fingered grippers, we also need to determine the 3D ori-
entations hr,
and openclosed states hg,
by further observing
3D hands H
j. Here, we can estimate the openclosed state by
detecting if the hand is in contact with an object . If there
is contact, the hand is considered closed (hg,
0), otherwise
open (hg,
1). This is more trustworthy than relying solely
on hands to estimate status. For calculating 3D hand poses
We summarize this process in Alg. 1. To this point, we have
Fig. 3: A detailed example of extracted motion trajectories
with corresponding keyframes of both left hand and right hand.
It is best to zoom in to view the details.
obtained the rough motion trajectories purely based on human
hand videos {(bhp,
cluding the vision-language model Florence-2  and SAM2
) to extract segmented manipulated objects from the left
initial image as our disturbance-free visual observations bO,
which will be further lifted to 3D point clouds eO by applying
stereo matching approaches [92, 93].
3) Robot Actions Injection: Although we have obtained
robot-oriented motion trajectories, their validity and usability
are still concerns. For example, some target poses may be
unreachable for the failed inverse kinematics. Due to agnostic
approach is to replay and verify the rationality of each action
step by step directly on real robots, but this choice is unsafe
and inefficient, considering that the total number of frames J
is usually about 100 to 200.
Keyframes-based motion actions. To this end, we turn to a
more reasonable and safer post-processing, namely keyframes-
based motion simplification and injection. Specifically, we
inherit the abstraction of a consequent demonstration into
discrete keyframes (a.k.a. keyposes) as in C2FARM
and PerAct . Keyframes are important intermediate end-
effector poses that summarize a demonstration and can be
auto-extracted using simple heuristics, such as a change in
the openclose end-effector state or local extrema of veloc-
ityacceleration. This concept is widely used in long-horizon
manipulation studies [55, 89, 41, 99]. Accordingly, we can
just learn to predict the next best keyframe, and use a
sampling-based motion planner to reach it during inference.
We thus simplify trajectories {(bhp,
j1 into a set
of keyframes {(ehp,
K keyframes. K is around 10 in our tasks (K J), which
makes it much more easier to quickly verify and correct errors.
To inject these keyposes into the dual-arm robot, we need
to transform them from the camera coordinate to the robot
coordinate using the pre-measured hand-eye calibration trans-
formation matrix. Usually, a real-robot verification takes about
three minutes. We finally update the verified trajectories into
eA{(eap,
injected K robot actions. An elaborate example of extracted
keyframes is shown in Fig. 3.
Derivation of motion mask. Additionally, we should al-
ways care about the dual-arm spatial-temporal coordination,
which is one of the core issues of bimanual manipulation.
have a time record in every frame, which represents the refined
keyframes-based set eA naturally contains detailed timestamps.
Based on it, we can thus derive the corresponding coordination
strategy C  {(CL
the motion state of a robot arm at the k-th keyframe. The
binary value 0 means holding on, 1 means moving on. Given
this particularity, we name it motion mask to schedule robot
motion. A specific illustration of C for the pull drawer task
can be found in the down-left corner of Fig. 2. This example
is broadly applicable to strictly asynchronous bimanual tasks
(e.g., CL
k ). While, for fully synchronous manipulation
k and CR
k in C keep the same. Currently, we
do not consider those long-horizon tasks where synchronized
and asynchronized keyframes are mixed.
In the following, we show that the extracted fine-grained
keyframes-based motion actions eA along with the correspond-
ing motion mask C will continue to play a vital role.
C. Demonstration Proliferation from One Teaching
Based on the one-shot teaching, we propose two demonstra-
tion proliferation schemes, the automatic rollout verification
of real robots and point cloud-level geometry augmentation of
manipulated objects. This solution is an efficient and reliable
route to quickly produce training data for imitation learning.
An example is shown in Fig. 2 (b).
1) Auto-Rollout Verification in Real-World: Formally, our
refined keyframes-based robot actions eA are interpretable
and editable. These properties assist us to conduct automated
demonstration rollout verification and collection on real robots.
and eAR belonging to the left and right robotic arms based
on the motion mask C. Below is for decomposing strictly
asynchronous tasks.
k  1, CR
k  0, CR
eAL   eAR
where we actually eliminate K redundant keyposes for unilat-
eral arm waiting (holding on actions). For synchronous tasks
( eAL   eAR  K), we always have to drive both arms, so
there is no need to apply the motion mask.
The above allows two arms to disengage smoothly. Then,
we can precisely edit any keyframe in eAL or eAR closely
related to the manipulated object to align with its changed
TABLE I: The time comparison of different data collection
or expansion methods. We report the average completion time
for 3 tasks, 10 valid trials in total for each task. The  means
it can be achieved by directly modifying the script.
Operators
Long-Horizon Bimanual Tasks
drawer (s)
water (s)
bottle (s)
Master-Slave
DragDrop
Auto-Rollout
Geo-Trans
keypose in real-world. We still take the pull drawer task (with
10 keyframes) as an example. When moving the object picked
up by the left arm, we need to adjust the 6-th keypose eaL
). For example, if we move the object 5 cm
along the X-axis positive direction, we then just add an offset
(0.05, 0.00, 0.00) to the position part eap,L
. Moreover, we can
also replace objects with similar shapes in the same position
to expand category diversity. Finally, we conduct the rollout
to get a new demonstration. The same is true for adjusting the
drawer manipulated by the right arm. Regardless of simplicity,
we compared auto-rollout with two popular data collection
the comparison. The other two ways are hampered by multi-
operators and higher failure rates.
2) Geometric Transformation of Point Clouds: Regarding
the above expansion of object positions and categories in real-
to reliably augment visual observations of manipulated objects
(the extracted 3D point clouds eO) any number of times, so
that theoretically infinite demonstrations can be obtained. In
the auto-rollout stage, we have initially figured out the cor-
respondence between manipulated objects and their relevant
keyframes. Now, we can perform geometric transformations
(mainly controlled rotations and translations) on the objects
at the point cloud level, and update the 6-DoF values in
the corresponding keyframes. In this way, matching pairs of
visual observations eO and keyframes-based actions eA can be
generated in batches, forming a series of new training data,
which no longer need to be verified in real robots. It should be
noted that the geometric transformation of eO is restricted, that
rational moving range of manipulated objects can be measured
during the auto-rollout phase incidentally. In Tab. I, we have
added the time comparison of this data proliferation, which
maintains the highest efficiency.
D. Bimanual Diffusion Policy Learning
In this part, we adapt popular visuomotor diffusion policies
[15, 98, 95], and propose a customized bimanual diffusion
policy (BiDP) to enable fast and robust imitation of long-
horizon tasks. We firstly shrink the input observations into
task-relevant object point clouds, allowing the policy model
to converge quickly and resistant to interference. Additionally,
we devise a motion mask to unify the action prediction and
address the dual-arm coordination problem.
Bimanual dataset composition. According to the defi-
nition in Sec. IV-A, we rewrite the training set as eD
{( eO, eA, C)i}N
C is the motion mask containing coordination strategies. eD is
generated by applying our proposed data proliferation solution
to expand the seeding one-shot teaching to get a large dataset
with hundreds or thousands of trajectories. Here, we update
eO{ eOk, SL
k1 and eA{(eap,
eOk is the observation containing 3D point clouds of manip-
ulated objects instead of the entire RGB image  or point
clouds scene [98, 95]. SL
k and SR
k are robot proprioception
states with similar formats as actions S
have discrete keyposes, rather than continuous and dense robot
states. Learning to predict keyposes is common in robotic
manipulation [55, 89, 41, 99]. The policy needs to learn a
mapping from the initial observation eO1 to all subsequent
keyposes eA for two arms. The history horizon and prediction
horizon is 1 and K, respectively. In evaluation, the policy
predicts all actions to be executed conditioned only on an one-
shot observation { eO1, SL
1 } at first sight.
Diffusion-based policy representation. Similar to [15, 98],
we utilize Denoising Diffusion Probabilistic Models (DDPMs)
to model the conditional distribution p( eAk eOk). Starting
from the random Gaussian noise eAT
actions with decreasing levels of noise, gradually from eAT 1
k. This process follows:
k ( eOk, eAt
The policy finally outputs eA0
k. Because point clouds are
used as the visual input instead of RGB images, we adopt
more robust SIM(3)-equivariant architectures [96, 95], rather
than policies based on CNNs  or transformers .
predicts the gradient E( eAk) for denoising the noisy action
input. It first uses a modified PointNet-based  encoder
with SIM(3)-equivariance to encode visual observations. The
encoded visual features and positional embeddings of t are
passed to FiLM layers . Then, the policy network applies a
convolutional U-Net  to process eAk, t and the conditioned
observations to predict denoising gradients. Note that eOk, eAk
k are processed to be invariant to scale and position.
Above-mentioned FiLM layers, convolutional U-net, and other
connecting layers are also modulated to be SO(3)-equivariant.
Please refer to [96, 95] for more details.
Customized bimanual diffusion policy. Since eAk and
k contain dual-arm actions in our task, it is important to
preprocess them appropriately. A vanilla approach is to pre-
dict all actions in each keyframe, including (eap,L
and (eap,R
). This not only needs to re-splice the
based policy network accordingly, but also learns redundant
actions for asynchronous tasks (as pointed out in Sec. IV-C),
3 drawers
9 daily objects
6 bottles
6 bottles  6 caps
4 delivery boxes
5 covered boxes
pull drawer
pour water
unscrew bottle
uncover lid
open box
Fig. 4: We collected a variety of manipulated objects in instance-level for each of five bimanual tasks to improve and verify
the generalizability of trained policies. All of these objects are from everyday life, not intentionally customized.
which is inefficient and error-prone. To this end, we reorganize
the action space into A  { eAL, eAR} based on the motion
mask C according to Eqn. 2. A contains a series of time-
ordered single-arm actions, which is a mixture of the left
and right with removing potential redundancy. Taking the pull
drawer task as an example, a demonstration consists of 10
keyframes { eAR
10}. For
synchronous tasks, the left and right sides appear alternately.
In this way, we unify the policy network form of bimanual
mentation details are in supplementary materials.
V. EXPERIMENTS
We aim to answer the following research questions. Q1:
What is the quality of our extracted hand motions? Q2: Can the
various strategies introduced in the YOTO framework enable it
to better learn bimanual manipulation policies? Q3: Do trained
BiDP models generalize outside of the in-distribution domain?
variety of long-horizon complex tasks?
A. Experiment Setups
1) Tasks: We evaluate YOTO on five real-world bimanual
lectively encompass two types of dual-arm collaborations:
strictly asynchronous and synchronous. The manipulated ob-
jects in these tasks might be rigid, articulated, deformable or
non-prehensile. They also involve many primitive skills such
as pullpush, pickplace, re-orient, unscrew, revolve and lift
up. Some skills must require both arms to complete. More
are quite complex due to containing multiple substeps. In the
pull drawer: A drawer and a daily pocketed object.
It consists of 6 substeps including stable the drawer (L), pull
the drawer (R), pick up the object (L), place the object into
the drawer (L), stable the drawer (L), and push the drawer (R).
TABLE II: Detailed statistics of five bimanual tasks. The
means we only count these auto-rollout demonstrations.
Task Names
Is Synchronous?
Manipulated Objects
Substeps
Keyframes
Avg. Duration (s)
Categories
Demonstrations
pour water: A capless bottle with water and an empty
mug. It consists of 6 substeps including pick up the mug (R),
pick up the bottle (L), bring the mug close to the bottle (R),
pour water in bottle into the mug (L), put down the bottle (L),
and put down the mug (R).
unscrew bottle: A capped bottle with water. It
consists of 5 substeps including pick up the bottle (L), bring
the bottle close to the right arm (L), unscrew the cap (R), put
down the cap (R), and put down the bottle (L).
uncover lid: A rectangular box with a top covered
lid and no handles. It consists of 3 substeps including go to
the lower middle part of the lid (LR), lift up the lid (LR), and
put down the lid to one side (LR).
open box: A delivery box with four handleable wings.
It consists of 4 substeps including go close to the two vertical
wings (LR), flick open two wings (LR), go close to the two
horizontal wings (LR), and flick open two wings (LR).
The statistics of these tasks are in Tab. II, where the number
of keyframes is counted based on the one-shot teaching.
Examples of each task are shown in Fig. 1 and Fig. 7.
2) Demonstrations: Imitation learning requires sufficient
training data, including diverse verified task trajectories, to
learn a closed-loop action prediction policy. To this end, as
described in Sec. IV-C, we start from a single-shot teaching
of every task and collect a considerable number of demonstra-
tions via the proposed rapid proliferation solution. Moreover,
to improve and evaluate the generalization of learned policies,
we have collected multiple objects within each task. All related
assets are shown in Fig. 4.
collect real robot data. We set 3 (for tasks pull drawer
and pour water) or 9 (for the other three tasks) position
variations for each manipulated object, and replace all al-
ternatives from the assets in each position. In this way, we
get training data with diverse positions and categories. The
demonstration number of every task is in the last row of
Tab. II, where we added statistics on their average duration.
We then processed these data into the form suitable for BiDP,
including extracting 3D point clouds of manipulated objects
and saving the corresponding multi-step end-effector keyposes.
Note that we also recorded the complete binocular video
observation and continuous robot actions during each auto-
methods [104, 15, 98, 95] for comparison. Next, we applied
3D geometric transformations to each demonstration, acting
only on task-relevant object point clouds. These synthetically
augmented data are only applicable to our proposed BiDP
algorithm. After formulating the script, we finally expanded
the data volume by 100 times, which results in 5K24K
trajectories per task. This magnitude is comparable to existing
large-scale bimanual teleoperation methods such as RDT
(6K self-created episodes) and 0  (5100 hours post-
training data), but our cost is extremely low.
3) Baselines: We compare our method to four strong base-
lines. (1) Action Chunking Transformers (ACT) . It is
proposed by ALOHA and uses a well-designed transformer
structure as the visual encoder. (2) Diffusion Policy (DP) .
The vanilla diffusion policy uses RGB images as inputs and
ResNet  as the visual encoder. We modified it by using
point cloud scenes as observations and a PointNet encoder
. (3) 3D Diffusion Policy (DP3) . It is a variant of
diffusion policy with a simpler point cloud encoder. It also
designs a two-layer MLP to encode robot proprioceptive states
before concatenating with the observation representation. (4)
EquiBot . It takes the point cloud scene as observation,
and learns to predict continuous undecomposed 7-DoF actions
of dual arms. Note that these baselines, including our BiDP,
are designed to learn task-independent policies, and do not
consider the multi-task model currently.
4) Metrics: We train all methods for 500 or 1,000 epochs
and only save the last checkpoint for testing. We evaluate
each model with 5 trials for each single object (last three
tasks with 30, 25 and 20 trials, respectively) or 2 trials
for paired objects (first two tasks with 54 and 36 trials,
respectively) in every task. These objects have randomized
initial placements. For a more detailed comparison, we report
the average length (following CLAVIN ) in each substep
for a sequenced long-horizon task, where the last substep
indicates the final success rate. Although above tests have new
variations in object placements, we choose two tasks pull
drawer and uncover lid to perform more challenging
out-of-distribution (OOD) evaluations on novel objects. We
omit the last object or paired objects from the training set
(a) Raw 3D Hand Points
(b) Projected 2D Hand Points
(c) Lifted Points in Keyframes
Fig. 5: Illustrations of extracted hand motion trajectories by
using (a) unhandled raw 3D hand center points, (b) projected
hand center points on the 2D image, and (c) lifted 3D points
in simplified keyframes. The first and second line represents
the task pull drawer and uncover lid, respectively.
TABLE III: Ablation studies of proposed strategies in YOTO
and the bimanual diffusion policy (BiDP). The task pull
drawer with 243 episodes is used to train all models.
observation
keyframes
reorganize
geometric
transforms
Not Expanded
Success Rate
Success Rate
Not Expanded
Average Length
Average Length
Fig. 6: Ablation studies on expanded training data at different
scales using geometric transformations. The task pull drawer
with 243 episodes is treated as the not expanded version.
and treat them as unseen objects to evaluate the final trained
model. The number of all OOD trials is quadrupled.
B. Results Comparison
by one, including basic in-distribution results and generaliza-
tions to out-of-distribution settings.
(Q1) Our extracted hand motions have good continuity
and consistency. We first discuss the quality of the extracted
motion trajectories, which is the core concept of this paper and
extremely important for the various strategies developed next.
As shown in Fig. 5, we compared the general effect of 3D hand
TABLE IV: Quantitative results of detailed long-horizon performance comparisons (in-distribution evaluations). The step-
wise success rates and average length of completed task sequences are reported. We use different colors such as teal, olive
and purple to indicate that each substep corresponds to the left arm, right arm and both arms, respectively.
pull drawer (243 episodes)
pour water (162 episodes)
close to
BiDP (Ours)
unscrew bottle (54 episodes)
uncover lid (45 episodes)
open box (36 episodes)
close to
close to
close to
BiDP (Ours)
TABLE V: Comparison of the average success rate of various
methods on all five tasks (in-distribution evaluations).
BiDP (Ours)
Success Rate
TABLE VI: Quantitative results of detailed long-horizon per-
formance comparisons (out-of-distribution evaluations). The
substeps are abbreviated as sequential numbers.
pull drawer
(144 episodes)
uncover lid
(36 episodes)
S1 S2 S3 S4 S5 S6 Avg.
motion trajectories extracted using different methods in two
different long-horizon bimanual tasks. Firstly, when directly
applying advanced 3D hand mesh reconstruction methods (ei-
ther HaMeR  or WiLoR ), the resulting hand trajectory
is always unstable and difficult to parse (see Fig. 5 (a)).
This is mainly because most of these methods are based on
monocular images, and the preset camera parameters such as
focus and focal length are directly calculated using the center
and size of each image. This makes the estimation results for
consecutive frames in the video not in a unified and invariant
camera space, and therefore unreliable and ambiguous in
depth. Nevertheless, this intuitive but sub-optimal approach
is still widely used by mainstream methods for learning from
human videos [42, 47, 23]. In comparison, after projecting
these 3D points onto a 2D image plane (with the Z-axis set to
0 for ease of visualization), it is clear that the trajectory trends
and estimated motion flow are improved (see Fig. 5 (b)). This
conclusion is generally applicable, for tasks like ours where the
camera is stationary and its intrinsic and extrinsic parameters
are known. Finally, as described in Sec IV-B, we filter out
sparse keyframes from these continuous points and lift the
corresponding position components into 3D points to obtain
the keyposes suitable for the end-effector (see Fig. 5 (c)). We
thus claim that our extracted hand motion trajectory based on
an one-shot human teaching has a more guaranteed quality.
And we expect that this motion extraction technology will
be used for retargeting to other more dexterous end-effectors,
such as multi-fingered hands.
(Q2) The various strategies we propose in YOTO are
effective. After extracting primary keyposes that could be
successfully injected into the robot, we continue to explore
YOTO including other strategies, which are closely related
to the visuomotor policy learning. As shown in Tab. III,
we quantitatively illustrate the effectiveness of each strategy
one by one through many ablation studies. We experimented
with task pull drawer which has 243 training trajectories.
regarded as the vanilla EquiBot , which takes the entire
point cloud scene as observation, learns to predict continuous
augmented training demonstrations. Despite being a solid
horizon task. Next, we replaced the input with point clouds
containing only manipulated objects (id-2) or predicted sim-
plified sparse keyposes (id-3), and the success rate and average
execution length of the task were improved. These results
suggest that reducing unnecessary distractions in the input
and learning fewer simplified actions are the right direction.
When both are used together (id-4), better performance can
be achieved. Based on these two strategies, we decoupled
the output action space and reconstructed it into a single-arm
format (id-5), the policy could also be superior, indicating the
importance of eliminating redundant actions. Alternately, if
3D geometric transformations were applied to further expand
training demonstrations (id-6), the resulting model effect was
much better, with the most prominent growth. This proves
that our developed demonstration proliferation is simple yet
efficient. We accordingly show in Fig. 6 the typical trend that
using more extended training data leads to better performance,
which is consistent with our consensus. Finally, combining the
above strategies together (id-7), our BiDP takes full advantage
pull drawer
pour water
unscrew bottle
uncover lid
open box
Fig. 7: Visualization of five bimanual tasks performed on real robots. We use different colors such as teal, olive and purple to
distinguish frames of left arm, right arm and both arms, respectively. Arrows are artificially added to show movement trends.
of all the strengths and has achieved the best results.
On the other hand, we need to compare and explain whether
BiDP is better than other visuomotor imitation methods [104,
following the mainstream in-distribution setting, we performed
extensive policies training and real robot evaluations on five
long-horizon tasks, and reported a detailed performance com-
parison of various methods. Generally speaking, we can draw
three conclusions from these quantitative data. (1) First, the
diffusion-based strategy always performed better than the
transformer-based ACT. This is mainly because the diffusion
model can model a higher-dimensional action space and is
highly malleable, while tranformer architectures usually do
not have these characteristics and require a large amount of
data to achieve scale effects and gain advantages. In addition,
ACT utilizes 2D images as observations instead of 3D input,
which also makes it achieve inferior results. (2) Second, a more
advanced and sophisticated 3D observation perception archi-
tecture can lead to higher policy performance. For example,
compared to the modified DP that directly uses PointNet to
process 3D point cloud input, DP3 and EquiBot adopt a self-
designed lightweight MLP encoder and SIM(3)-equivariant
backbone to extract point cloud features, respectively, and
always achieved better results. (3) Finally, for more complex
long-horizon bimanual manipulation tasks, the existing state-
of-the-art methods still have a lot of room for improvement,
such as the gradually decaying effect over multiple substeps
and less exploration of efficient utilization of training data.
Thanks to the proposed multiple strategies, our BiDP can
better cope with bimanual tasks, significantly better than all
compared policies. We summarized the average success rate
of each method on all five tasks in Tab V, where our method
BiDP achieved a success rate of nearly 60, demonstrating
good potential for practical robotic applications. To sum up,
it can be concluded that the various strategies we proposed in
YOTO are quite effective.
(Q3) BiDP has satisfactory out-of-domain generalization
ability. To further illustrate the superiority of BiDP, we de-
signed tests under out-of-distribution (OOD) settings. Results
are shown in Tab VI. From it, we can see that, except for
our method and EquiBot, the performance of the other three
methods has dropped significantly when it comes to OOD
paring to EquiBot, our BiDP still has a clear advantage, thanks
to the fact that we use explicit 3D geometric transformations
for expanding the training demonstrations instead of SIM(3)-
equivariant augmentation of the entire point cloud input in
EquiBot. In addition, using pure object point clouds as input
reorient pen  pick-place
flip basket upside down
contact-rich long-horizon bimanual task (asynchronous)
non-prehensile bimanual task (synchronous)
Fig. 8: Illustrations of another two bimanual tasks. Top: the visualization of hand motions extraction. Bottom: the corresponding
rollout examples by injecting actions on real robots. Refer to Fig. 1 and Fig. 7 for notes on different colors and curves.
also makes our model more robust compared to all baselines.
The core idea here is to rely on the still rapidly developing
capabilities of vision foundation models, such as the open vo-
cabulary detection  and segmentation , to more reliably
perceive various unseen scenes and objects. In summary, these
results verify that our BiDP indeed outperforms prior methods
with the least amount of performance degradation in OOD
generalization.
(Q4) YOTO is widely applicable to diverse bimanual
tasks. Our proposed framework YOTO is compatible with
most bimanual tasks, such as the selected five representative
long-horizon tasks, covering a variety of skills, multi-object
the above-mentioned quantitative results, we also qualitatively
demonstrate the visual effects of real robot execution on five
tasks in Fig. 7, mainly showing the sparse keyframes contained
in them. We can see that the two robot arms have learned the
movements demonstrated by human hands and complete these
complex tasks in an orderly manner.
ulation tasks and enabled the dual-arm robot to learn new given
tasks quickly and easily through one-shot human teaching. Due
to space limitations, we did not continue the demonstration
proliferation and policy training. The illustrations of extracted
actions that can be injected into real robots are shown in
Fig. 8. These results further reveal the simplicity, versatility
and scalability of YOTO. In the future, we will explore using
YOTO to handle more intricate, valuable, but less researched
bimanual manipulation tasks.
VI. CONCLUSION AND LIMITATION
In this paper, we propose a novel framework named YOTO
to address the challenge of efficient and robust bimanual
manipulation. Our approach learns from one-shot human video
and consecutive hand features like pose, joints, and states.
To ensure stable and precise manipulation, we simplify noisy
hand motion trajectories into discrete keyframes and introduce
a motion mask for better dual-arm coordination. Based on
the refined one-shot teaching, we develop a scalable data
proliferation solution using auto-rollout verification and 3D
geometric transformations to rapidly create diverse training
examples. With this enriched dataset, we design a dedicated
bimanual diffusion policy (BiDP) that simplifies observations,
predicts keyposes, and reorganizes action spaces for efficient
training. Validated on five complex bimanual tasks, our frame-
work demonstrates superior performance in both synchronous
and asynchronous scenarios. These contributions provide a
standardized method for transferring human motions to robots,
a scalable approach for data generation, and an effective
algorithm for mastering intricate dual-arm tasks, advancing
the field of bimanual manipulation.
formance on various long-horizon bimanual manipulation
(1) Our vision-based hand trajectory extraction schemes have
inherent errors. This means that we have to check carefully
and verify on the real robot whether the extracted position and
posture information is reliable, which still requires additional
manpower. (2) The primary version of YOTO adopts a fixed
carts or multi-legged robots. (3) The equipped parallel gripper
is not flexible enough and has limited functionality. Upgrading
the end-effector to a multi-fingered dexterous hand or equip-
ping it with force-tactile sensors can make the robot more
versatile and powerful. (4) More ultra-difficult bimanual tasks
are still under-explored, such as the specialized tool-based
manipulation (e.g., picking up a hammer to pound a nail or
twisting a screwdriver to tighten a screw), highly dynamic non-
quasi-stationary tasks, and friendly interactive collaboration
with people. In short, these limitations highlight the need for
further innovations to enhance robustness, generalization, and
scalability in bimanual robot manipulation.
ACKNOWLEDGMENTS
This work was supported by the Guangdong Provincial
Key Field RD Program (No. 20240104, the project name:
Research and Application of Common Key Technologies of
Robot Embodied Intelligence Based on AI Large Model),
and also received funding from the 2024 Shenzhen Science
and Technology Major Project (No. 202402002, the project
Robots to Learn Human Skills).
REFERENCES
Jeff Bingham, Sanky Chan, Kenneth Draper, De-
bidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer
Aloha 2: An enhanced low-cost
hardware for bimanual teleoperation.
arXiv preprint
Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten
ficient bimanual folding of garments. In 2022 IEEERSJ
International Conference on Intelligent Robots and Sys-
tems (IROS), pages 18. IEEE, 2022.
Arpit Bahety, Shreeya Jain, Huy Ha, Nathalie Hager,
Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, and
Shuran Song. Bag all you need: Learning a general-
izable bagging strategy for heterogeneous objects. In
2023 IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 960967. IEEE,
Arpit Bahety, Priyanka Mandikal, Ben Abbatematteo,
and Roberto Martn-Martn. Screwmimic: Bimanual im-
itation from human videos with screw space projection.
In Proceedings of Robotics: Science and Systems (RSS),
Ravin Balakrishnan and Gordon Kurtenbach. Exploring
bimanual camera control and object manipulation in
3d graphics interfaces. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
Christian Bersch, Benjamin Pitzer, and Soren Kam-
mel. Bimanual robotic cloth manipulation for laundry
folding.
In 2011 IEEERSJ International Conference
on Intelligent Robots and Systems, pages 14131419.
Kevin Black, Noah Brown, Danny Driess, Adnan Es-
vision-language-action flow model for general robot
control. arXiv preprint arXiv:2410.24164, 2024.
Johann Brehmer, Joey Bose, Pim De Haan, and Taco S
Cohen. Edgi: Equivariant diffusion for planning with
embodied agents.
Advances in Neural Information
Processing Systems, 36, 2024.
Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burch-
Canonicalized-alignment
purpose garment manipulation. In 2023 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 58725879. IEEE, 2023.
Lawrence Yunliang Chen, Baiyu Shi, Roy Lin, Daniel
David Held, and Ken Goldberg. Bagging by learning
to singulate layers using interactive perception.
2023 IEEERSJ International Conference on Intelligent
Robots and Systems (IROS), pages 31763183. IEEE,
Lawrence Yunliang Chen, Baiyu Shi, Daniel Seita,
Richard Cheng, Thomas Kollar, David Held, and Ken
Goldberg. Autobag: Learning to open plastic bags and
insert objects. In 2023 IEEE International Conference
on Robotics and Automation (ICRA), pages 39183925.
Sirui Chen, Chen Wang, Kaden Nguyen, Li Fei-Fei, and
C Karen Liu.
demonstrations for robot learning with augmented real-
ity feedback. arXiv preprint arXiv:2410.08464, 2024.
Yuanpei Chen, Chen Wang, Yaodong Yang, and Karen
Liu. Object-centric dexterous manipulation from human
motion data.
In 8th Annual Conference on Robot
Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, and
Xiaolong Wang.
immersive active visual feedback. In 8th Annual Con-
ference on Robot Learning, 2024.
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau,
Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shu-
ran Song. Diffusion policy: Visuomotor policy learning
via action diffusion.
The International Journal of
Robotics Research, page 02783649241273668, 2023.
Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, and
Abhinav Gupta. Efficient bimanual manipulation using
learned task schemas.
In 2020 IEEE International
Conference on Robotics and Automation (ICRA), pages
Ian Chuang, Andrew Lee, Dechen Gao, and Iman
Soltani. Active vision might be all you need: Exploring
active vision in bimanual robotic manipulation. arXiv
preprint arXiv:2409.17435, 2024.
Matei Ciocarlie, Corey Goldfeder, and Peter Allen.
Dexterous grasping via eigengrasps: A low-dimensional
approach to a high-complexity problem. In Proceedings
of Robotics: Science and Systems (RSS), 2007.
Adria Colome and Carme Torras. Dimensionality reduc-
tion for dynamic movement primitives and application
to bimanual manipulation of clothes. IEEE Transactions
on Robotics, 34(3):602615, 2018.
Runyu Ding, Yuzhe Qin, Jiyue Zhu, Chengzhe Jia,
Shiqi Yang, Ruihan Yang, Xiaojuan Qi, and Xiaolong
Wang. Bunny-visionpro: Real-time bimanual dexterous
teleoperation for imitation learning.
arXiv preprint
Michael Drolet, Simon Stepputtis, Siva Kailas, Ajinkya
A comparison of imitation learning algorithms for bi-
manual manipulation. IEEE Robotics and Automation
Dimitrios
Muhammed Kocabas, Manuel Kaufmann, Michael J
Hilliges.
for dexterous bimanual hand-object manipulation.
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 1294312954,
Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wet-
owing and imitation from humans.
arXiv preprint
Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mobile
low-cost whole-body teleoperation.
arXiv preprint
Jianfeng Gao, Xiaoshu Jin, Franziska Krebs, Noemie
visual imitation learning of bimanual manipulation
In 2024 IEEE International Conference on
Robotics and Automation (ICRA), pages 1685016857.
Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-
Wei Chao, and Dieter Fox.
manipulation from few demonstrations. In Proceedings
of Robotics: Science and Systems (RSS), 2024.
Jennifer Grannen, Priya Sundaresan, Brijen Thanan-
Goldberg.
Untangling dense knots by learning task-
relevant keypoints. In Conference on Robot Learning,
pages 782800. PMLR, 2021.
Jennifer Grannen, Yilin Wu, Suneel Belkhale, and Dorsa
Sadigh. Learning bimanual scooping policies for food
acquisition. In Conference on Robot Learning, pages
Jennifer Grannen, Yilin Wu, Brandon Vu, and Dorsa
Stabilize to act: Learning to coordinate for
bimanual manipulation. In Conference on Robot Learn-
Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.
In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Kristen Grauman, Andrew Westbury, Lorenzo Torre-
Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal,
Bikram Boote, et al. Ego-exo4d: Understanding skilled
human activity from first-and third-person perspectives.
In Proceedings of the IEEECVF Conference on Com-
puter Vision and Pattern Recognition, pages 19383
Markus Grotz, Mohit Shridhar, Tamim Asfour, and
Dieter Fox.
robotic bimanual manipulation tasks.
arXiv preprint
Valentin N Hartmann, Andreas Orthey, Danny Driess,
Ozgur S Oguz, and Marc Toussaint.
Long-horizon
multi-robot rearrangement planning for construction as-
sembly. IEEE Transactions on Robotics, 39(1):239252,
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 770778, 2016.
Paul Hebert, Nicolas Hudson, Jeremy Ma, and Joel W
Burdick. Dual arm estimation for coordinated bimanual
manipulation. In 2013 IEEE International Conference
on Robotics and Automation, pages 120125. IEEE,
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
diffusion probabilistic models.
Advances in Neural
Information Processing Systems, 33:68406851, 2020.
Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe
Dynamic handover: Throw and catch with
bimanual hands.
In Conference on Robot Learning,
pages 18871902. PMLR, 2023.
Stephen James, Kentaro Wada, Tristan Laidlow, and
Andrew J Davison. Coarse-to-fine q-attention: Efficient
learning for visual robotic manipulation via discreti-
In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kap-
Chelsea Finn. Bc-z: Zero-shot task generalization with
robotic imitation learning.
In Conference on Robot
Edward Johns. Coarse-to-fine imitation learning: Robot
manipulation from a single demonstration.
IEEE international conference on robotics and automa-
tion (ICRA), pages 46134619. IEEE, 2021.
Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina
Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d
scene representations. arXiv preprint arXiv:2402.10885,
Justin Kerr, Chung Min Kim, Mingxuan Wu, Brent Yi,
Qianqian Wang, Ken Goldberg, and Angjoo Kanazawa.
Robot see robot do: Imitating articulated object manipu-
lation with monocular 4d reconstruction. In 8th Annual
Conference on Robot Learning, 2024.
Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti,
Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael
et al. Openvla: An open-source vision-language-action
model. arXiv preprint arXiv:2406.09246, 2024.
Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun,
and Joshua B Tenenbaum.
Learning to act from ac-
tionless videos through dense correspondences. In The
Twelfth International Conference on Learning Repre-
Franziska Krebs and Tamim Asfour.
A bimanual
manipulation taxonomy. IEEE Robotics and Automation
Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-
Sevilla-Lara. Learning precise affordances from ego-
centric videos for robotic manipulation. arXiv preprint
Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo
ing humanoid robots manipulation skills through single
video imitation. In 8th Annual Conference on Robot
Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, and
Yi Wu. Efficient bimanual handover and rearrangement
via symmetry-aware actor-critic learning. In 2023 IEEE
International Conference on Robotics and Automation
(ICRA), pages 38673874. IEEE, 2023.
Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel,
and Jitendra Malik. Twisting lids off with two hands.
arXiv preprint arXiv:2403.02338, 2024.
Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi,
Sergey Levine, and Jitendra Malik. Learning visuotac-
tile skills with two multifingered hands. arXiv preprint
I-Chun Arthur Liu, Sicheng He, Daniel Seita, and
Gaurav S Sukhatme. Voxact-b: Voxel-based acting and
stabilizing policy for bimanual manipulation.
Annual Conference on Robot Learning, 2024.
Junjia Liu, Yiting Chen, Zhipeng Dong, Shixiong Wang,
Sylvain Calinon, Miao Li, and Fei Chen. Robot cooking
with stir-fry: Bimanual non-prehensile manipulation of
semi-fluid objects.
IEEE Robotics and Automation
Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai
Jun Zhu. Rdt-1b: a diffusion foundation model for bi-
manual manipulation. arXiv preprint arXiv:2410.07864,
Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yux-
iang Zhang, Yebin Liu, and Li Yi. Taco: Benchmarking
generalizable bimanual tool-action-object understand-
In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Xiao Ma, Sumit Patidar, Iain Haughton, and Stephen
Hierarchical diffusion policy for kinematics-
aware multi-task robotic manipulation. In Proceedings
of the IEEECVF Conference on Computer Vision and
Pattern Recognition, pages 1808118090, 2024.
Jinna Lei, and Pieter Abbeel.
Cloth grasp point de-
tection based on multiple-view geometric cues with
application to robotic towel folding.
In 2010 IEEE
International Conference on Robotics and Automation,
pages 23082315. IEEE, 2010.
Ajay Mandlekar, Danfei Xu, Roberto Martn-Martn,
Silvio Savarese, and Li Fei-Fei. Learning to generalize
across long-horizon tasks from human demonstrations.
In Proceedings of Robotics: Science and Systems (RSS),
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn.
What matters in learning from offline human demonstra-
tions for robot manipulation. In Conference on Robot
Oier Mees, Lukas Hermann, Erick Rosete-Beas, and
Wolfram Burgard. Calvin: A benchmark for language-
conditioned policy learning for long-horizon robot ma-
nipulation tasks. IEEE Robotics and Automation Let-
Andrew T Miller and Peter K Allen.
Graspit! a
versatile simulator for robotic grasping. IEEE Robotics
Automation Magazine, 11(4):110122, 2004.
Mirrazavi
Figueroa Fernandez, and Aude Billard.
Coordinated
multi-arm motion planning: Reaching for moving
objects in the face of uncertainty. In Proceedings of
Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen,
Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, and
Ping Luo. Robotwin: Dual-arm robot benchmark with
generative digital twins (early version). arXiv preprint
Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura
mediate representations for robot manipulation. arXiv
preprint arXiv:2411.02704, 2024.
Alexander Quinn Nichol and Prafulla Dhariwal.
proved denoising diffusion probabilistic models.
International Conference on Machine Learning, pages
Abby ONeill, Abdul Rehman, Abhiram Maddukuri,
Abhishek Gupta, Abhishek Padalkar, Abraham Lee,
Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya
Open x-embodiment: Robotic learning
datasets and rt-x models: Open x-embodiment collabo-
ration 0.
In 2024 IEEE International Conference on
Robotics and Automation (ICRA), pages 68926903.
Georgios Papagiannis, Norman Di Palo, Pietro Vi-
cution from everyday human videos.
arXiv preprint
Georgios Pavlakos, Dandan Shan, Ilija Radosavovic,
Angjoo Kanazawa, David Fouhey, and Jitendra Malik.
Reconstructing hands in 3d with transformers.
Proceedings of the IEEECVF Conference on Com-
puter Vision and Pattern Recognition, pages 98269836,
Angelika Peer, Yuta Komoguchi, and Martin Buss.
Towards a mobile haptic interface for bimanual manipu-
lations. In 2007 IEEERSJ International Conference on
Intelligent Robots and Systems, pages 384391. IEEE,
Weikun Peng, Jun Lv, Yuwei Zeng, Haonan Chen,
Siheng Zhao, Jichen Sun, Cewu Lu, and Lin Shao.
through a real-to-sim-to-real approach. In 8th Annual
Conference on Robot Learning, 2024.
Ethan Perez, Florian Strub, Harm De Vries, Vincent
with a general conditioning layer. In Proceedings of the
AAAI conference on artificial intelligence, volume 32,
Rolandos Alexandros Potamias, Jinglei Zhang, Jiankang
hand localization and reconstruction in-the-wild. arXiv
preprint arXiv:2409.12259, 2024.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet: Deep hierarchical feature learning
on point sets in a metric space. Advances in Neural
Information Processing Systems, 30, 2017.
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Rong-
hang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr,
Roman Radle, Chloe Rolland, Laura Gustafson, et al.
Sam 2: Segment anything in images and videos. arXiv
preprint arXiv:2408.00714, 2024.
Javier Romero, Dimitris Tzionas, and Michael J Black.
Embodied hands: Modeling and capturing hands and
bodies together. ACM Transactions on Graphics, 36(6),
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
segmentation.
In Medical Image Computing and
Computer-Assisted InterventionMICCAI, pages 234
241. Springer, 2015.
Hyunwoo Ryu, Jiwoo Kim, Hyunseok An, Junwoo
won Hwang, Jongeun Choi, and Roberto Horowitz.
Bi-equivariant
denoising
generative
modeling on se (3) for visual robotic manipulation. In
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 1800718018,
Gautam Salhotra, I-Chun Arthur Liu, and Gaurav S
Sukhatme.
Learning robot manipulation from cross-
morphology demonstration.
In Conference on Robot
Dandan Shan, Jiaqi Geng, Michelle Shu, and David F
Fouhey. Understanding human hands in contact at inter-
net scale. In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar
and Deepak Pathak. Bimanual dexterity for complex
tasks. In 8th Annual Conference on Robot Learning,
Mohit Shridhar, Lucas Manuelli, and Dieter Fox.
manipulation. In Conference on Robot Learning, pages
Doganay Sirintuna, Idil Ozdamar, and Arash Ajoudani.
Carrying the uncarriable: a deformation-agnostic and
human-cooperative framework for unwieldy objects us-
ing multiple robots. In 2023 IEEE International Confer-
ence on Robotics and Automation (ICRA), pages 7497
Christian Smith, Yiannis Karayiannidis, Lazaros Nal-
and Danica Kragic. Dual arm manipulationa survey.
Robotics and Autonomous systems, 60(10):13401353,
Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. In International
Conference on Learning Representations, 2021.
Octo Model Team, Dibya Ghosh, Homer Walke, Karl
open-source generalist robot policy.
arXiv preprint
Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang,
Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anand-
by watching human play.
In Conference on Robot
Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan
and portable mocap data collection system for dexterous
manipulation. In Proceedings of Robotics: Science and
Systems (RSS), 2024.
Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou,
Yang Gao, and Pieter Abbeel.
Any-point trajec-
tory modeling for policy learning.
arXiv preprint
Thomas Weng, Sujay Man Bajracharya, Yufei Wang,
Khush Agrawal, and David Held.
manual cloth manipulation with a flow-based policy. In
Conference on Robot Learning, pages 192202. PMLR,
Zhou Xian, Nikolaos Gkanatsios, Theophile Gervet,
Tsung-Wei Ke, and Katerina Fragkiadaki. Chaineddif-
tion for robotic manipulation. In 7th Annual Conference
on Robot Learning, 2023.
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai,
Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and
Lu Yuan. Florence-2: Advancing a unified representa-
tion for a variety of vision tasks. In Proceedings of the
IEEECVF Conference on Computer Vision and Pattern
Fan Xie, Alexander Chowdhury, M De Paolis Kaluza,
Linfeng Zhao, Lawson Wong, and Rose Yu.
imitation learning for bimanual robotic manipulation.
Advances in Neural Information Processing Systems,
Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin
Yang. Iterative geometry encoding volume for stereo
matching. In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages
Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Junda
tive multi-range geometry encoding volumes for stereo
matching. arXiv preprint arXiv:2409.00638, 2024.
Lei Yan, Theodoros Stouraitis, Joao Moura, Wenfu Xu,
Michael Gienger, and Sethu Vijayakumar. Impact-aware
bimanual catching of large-momentum objects. IEEE
Transactions on Robotics, 2024.
Sim (3)-equivariant diffusion policy for generalizable
and data efficient learning. In 8th Annual Conference
on Robot Learning, 2024.
Jingyun Yang, Congyue Deng, Jimmy Wu, Rika
Leonidas
Jeannette
yond rigid object manipulation. In 2024 IEEE Interna-
tional Conference on Robotics and Automation (ICRA),
pages 92499255. IEEE, 2024.
Zhaodong Yang, Yunhai Han, and Harish Ravichan-
motion in learning bimanual dexterity. arXiv preprint
Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu,
Muhan Wang, and Huazhe Xu.
3d diffusion policy:
Generalizable visuomotor policy learning via simple 3d
representations.
In Proceedings of Robotics: Science
and Systems (RSS), 2024.
Jia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia,
Li Chen, Hao Dong, Haoming Song, Dong Wang,
Di Hu, Ping Luo, et al.
Learning manipulation by
predicting interaction.
In Proceedings of Robotics:
Science and Systems (RSS), 2024.
Xinyu Zhan, Lixin Yang, Yifei Zhao, Kangrui Mao,
Hanlin Xu, Zenan Lin, Kailin Li, and Cewu Lu.
tion in complex task completion. In Proceedings of the
IEEECVF Conference on Computer Vision and Pattern
Fan Zhang and Michael Gienger.
Affordance-based
robot manipulation with flow matching. arXiv preprint
Tianle Zhang, Dongjiang Li, Yihang Li, Zecui Zeng,
Lin Zhao, Lei Sun, Yue Chen, Xuelong Wei, Yibing
for household tasks. arXiv preprint arXiv:2405.18860,
Xinyu Zhang and Abdeslam Boularias. One-shot im-
itation learning with invariance matching for robotic
manipulation. In Proceedings of Robotics: Science and
Systems (RSS), 2024.
Tony Z Zhao, Vikash Kumar, Sergey Levine, and
Chelsea Finn. Learning fine-grained bimanual manip-
ulation with low-cost hardware.
In Proceedings of
Tony Z Zhao, Jonathan Tompson, Danny Driess, Pete
recipe for robot dexterity. In 8th Annual Conference on
Robot Learning, 2024.
Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang,
Qingnan Fan, Kaichun Mo, and Hao Dong.
gripper manipulation.
In The Eleventh International
Conference on Learning Representations, 2023.
Bohan Zhou, Haoqi Yuan, Yuhui Fu, and Zongqing
Lu. Learning diverse bimanual dexterous manipulation
skills from human demonstrations.
arXiv preprint
Jihong Zhu, Michael Gienger, Giovanni Franzese, and
Jens Kober. Do you need a hand?a bimanual robotic
dressing assistance scheme.
IEEE Transactions on
APPENDIX
A. Implementation Details of Our BiDP
In this section, we describe in detail the architecture and
implementation of our proposed method BiDP.
1) Spaces of observation and action: We adopt a 13-
dimensional proprioception vector and a 7-dimensional action
space for each robot arm, respectively. The proprioception
data for each arm consists of the following information: a
3-dimensional end-effector position, a 6-dimensional vector
denoting end-effector orientation (represented by two columns
of the end-effector rotation matrix), a 3-dimensional vector
indicating the direction of gravity, and a scalar that represents
the degree to which the gripper is opened. The action space
for each arm consists of the following information: a 3-
dimensional vector for the end-effector position offset, a 3-
dimensional vector for the end-effector angular velocity in
axis-angle format, and a scalar denoting the gripper action.
For all our bimanual tasks, the observation horizon is set to
as one of the network inputs. And the initial state of the right
arm is always fixed in each task. For the number of the action
simplify it and set the prediction length of the three strictly
asynchronous tasks to the number of keyframes K, and the
prediction length of the two synchronous tasks to 2K, which
is not reducible. This is slightly different from the setup used
in the mainstream methods ACT , Diffusion Policy
and EquiBot , where the action horizon is always smaller
than the prediction horizon with redundant steps.
2) Network architecture: In all tasks, we use a SIM(3)-
equivariant PointNet [96, 95] with 4 layers and hidden
dimensionality 128 as the feature encoder. For the noise
prediction network, we inherits hyperparameters from the
original Diffusion Policy . Specifically, to optimize for
inference speed in all experiments, we use the DDIM scheduler
with 8 denoising steps, instead of the DDPM scheduler
which performs up to 100 denoising steps.
3) Sampling of point cloud: As we all known, setting the
number of points to sample in the point cloud observation is a
key hyperparameter to consider when designing an architecture
that takes point cloud inputs. In our experiments, we found out
that using 1024 points is sufficient for all tasks. In particular,
we have tried increasing the number of point clouds to 2048 or
and this will also cause the storage occupied by the training
observation data to be too large and the training time cost to
increase. Therefore, reducing the number of points to 1024
can make training faster without hurting performance. And all
our policy models can be trained on a GeForce RTX 3090 Ti
with 24 GB of memory.
4) Training and evaluation: When using fully expanded
training demonstrations (including 100 and 500), we train
all methods of the first two tasks and the last three tasks
for 500 and 1,000 epochs, respectively. The batch-size is set
to 64. Otherwise, when using these under-expanded training
Fig. 9: Examples of using vision foundation models (VFMs)
to detect and segment manipulated objects.
data (including 25 and 5 and not expanded), we train all
methods of the first two tasks and the last three tasks for 2,000
and 4,000 epochs, respectively. For all experiments, we only
evaluate the last one checkpoint saved at the end of training.
For every evaluation in the real world, we run the policy
in a randomly initialized placement of objects for dozens of
episodes (please refer the metrics part in the main content for
more details), and record the mean average length and success
rate achieved by the policy.
In addition, we have explained and demonstrated the im-
portance and advantages of object-centric point cloud input in
our main paper. At inference time, we also need to preprocess
the binocular RGB observations to obtain the point cloud
of manipulated objects. This core design relies on the still
rapidly developing capabilities of vision foundation models
(VFMs). Here we leverage the state-of-the-art open vocabulary
detection method Florence-2  and segmentation method
SAM2  to automatically extract object masks and then
filter out corresponding point clouds. Examples are shown
in Fig. 9. Despite this, occasionally we may fail to segment
desired objects accurately, and in these special cases we will
manually correct the masks. These cases are not counted
as failed evaluation trails due to not involving significant
elements of bimanual robot manipulation. Because we believe
that the next generation of VFMs can alleviate these problems,
or we can directly address them through domain adaptation,
test-time adaptation, or adjusting input prompts.
B. Details of Our Selected Bimanual Tasks
We here summarize details related to all manipulation tasks,
including object size, hand trajectory visualization, number of
keyframes and the valid manipulation area.
1) Real size of manipulated objects: In order to give readers
a more intuitive cognition of the size of all the objects used
in our experiment, we have summarized the centimeter-level
shape information of the objects involved in detail in Fig. 10.
3 drawers
9 daily objects
pull drawer
paperclip
vaseline
mouthwash
watercolor
headphone
26 cm  16 cm  7 cm
20.5 cm  20 cm  8 cm
gray wooden drawer
yellow plastic drawer
white plastic drawer
6 bottles
pour water
Bottle Id
Diameter
Mugcup Id
Diameter
6 bottles  6 caps
unscrew bottle
Bottle Id
Diameter
Diameter
5 covered boxes
uncover lid
Lid Height
Body Height
Body Length
Body Width
L  W  H (cm)
Box Color
Lid Height (cm)
4 delivery boxes
open box
Body L  W  H (cm)
Wing L1  L2 (cm)
Wing1 Length
Body Height
Body Length
Body Width
Wing2 Length
Fig. 10: We collected a variety of manipulated objects in instance-level for each of five bimanual tasks to improve and verify
the generalizability of trained policies. All of these objects are from everyday life, not intentionally customized. We also collect
the detailed size information in centimeter-level for all related objects. Best to view after zooming in.
pull drawer
pour water
unscrew bottle
uncover lid
open box
Fig. 11: Visualization of extracted hand trajectories for five long-horizon bimanual tasks. Best to view after zooming in.
Without loss of generality, we roughly abstract all objects into
two typical geometric shapes, namely cuboids (represented by
and diameter). The cuboids include 9 everyday objects and 3
drawers in the pull drawer task, 5 covered boxes in the
uncover lid task, and 4 express boxes in the open box
task. The cylinders include the 6 bottles and 3 mugs in the
pour water task, and the 6 caps in the unscrew bottle
task. In particular, we also counted the height of the lid in task
uncover lid and the length of the flippable wings on both
sides in task open box. These objects are all within the size
range that the gripper can grasp and the operating table can
place. We expect that these detailed statistics can help better
understand and reproduce each task.
2) Hand motion extraction results: The movement mode of
all bimanual tasks we designed mainly comes from a single-
shot teaching of both human hands. In Fig. 11, we show
the detailed visualization results of extracted hand motion
trajectories so that we can better understand the entire process
of manipulation, including which objects each arm contacts
Fig. 12: Illustration of the effective placement range of manipulated objects on the workbench for each task during training
and testing. We mainly show the predefined position points of the objects in the automatic verification phase (indicating by
black-white dots), as well as the approximate distribution of the object positions after using the geometry transformation
(indicating by spread translucent red scopes). Best to view after zooming in.
and the order of motion. Our subsequent demonstration pro-
liferation and learned action prediction policies will follow the
same motion pattern.
3) Determination of keyframes in each task: As described
in the main text, we use discrete keyframes (a.k.a. keyposes) to
simplify and represent each long-horizon task as in C2FARM
and PerAct . Keyframes can be auto-extracted using
simple heuristics, such as a change in the openclose end-
effector state or local extrema of velocityacceleration. This
abstraction way is extremely effective for the first three strictly
asynchronous tasks, but the latter two tasks that require the
synchronization of both arms do not perform well due to very
few remaining keyframes. Therefore, we artificially increase
sampling frames between keyframes with larger step spans to
make the manipulation action more stable and smooth. This
constraint can also be added to heuristic rules to complete
automatic keyframes extraction.
4) Effective area on the workbench: Since we used two
fixed manipulators, the accessible space is limited. Specifi-
effective areas where the objects were located in each task on
the table platform. In the training demonstrations we collected
and the real robot evaluation phase, we would not place objects
outside these areas to avoid exceeding the reach limit of two
arms. Note that this does not mean that the policies we trained
do not generalize to different locations. Even in a restricted
during testing may be completely new, so it is still a non-trivial
out-of-distribution (OOD) situation.
In Fig. 12, we use a series of black-white dots to represent
each pre-fixed point, which corresponds to the position of the
manipulated object in the auto-rollout phase. These points are
defined relative to the area where the object touches the table,
and can be at its geometric center (open box), the midpoint
of a side (uncover lid), or a corner in a fixed direction
(open box, pour water, unscrew bottle and open
box). We show some examples of semi-transparent manipu-
lated objects in each sub-image. The effective range drawn
on the table also refers to these proxy points. Therefore,
considering that the object itself may be quite large (refer
Fig. 10), the area it actually covers will be much larger. On the
other hand, we utilize the semi-transparent red area to indicate
the approximate distribution of objects after their positions
have been modified using the point cloud-based geometric
transformation. We apply measured parameters to control the
augmented objects to basically cover the entire valid area
without overlapping, so that the trained model has robust
generalization of position variations. Moreover, this design
ensures that the absolute displacement of the object will not
be too large, thus avoiding obvious violations of the imaging
rules under perspective projection.
Fig. 13: Qualitative rollout samples from the third-person perspective for all real robot evaluation scenarios. From top to
Fig. 14: Rollout examples of another two new tasks including reorient pen (top row) and flip basket (bottom row).
Fig. 15: From top to bottom, we have examples of failed cases in all five tasks during evaluation. We have outlined and
magnified the areas where the failures occurred so that we can quickly examine them. Best to view after zooming in.
C. Evaluation Results and Performance Analysis
1) More qualitative examples: In Fig. 13, we show qual-
itative rollout samples from the third-person perspective for
all evaluation tasks we mentioned in the main paper. We
still choose images near the keyframes to illustrate more
and location variations. These examples show more complete
scenes and the motion of two robot arms, and can be con-
sidered as a supplement to the limited field of view of the
binocular observation camera. Note that these third-person
video recordings do not participate in any training and testing.
open the gray wooden drawer, pick up the vaseline box and
put it into the drawer, and close the drawer; (1.2) open the
white plastic drawer, pick up the headphone case and put it
into the drawer, and close the drawer; (2.1)(2.2) grasp and
pick up the mug cup, grasp and pick up the drink bottle, pour
the water contained in the bottle into the mug cup, and place
back the mug cup and bottle; (3.1)(3.2) grasp and pick up
the drink bottle, unscrew the circular cap of the bottle, and
place back the bottle and cap; (4.1)(4.2) use two arms to
close to the lid of the covered box, lift up the box lid, and
place down the box lid; (5.1)(5.2) use two arms to close
to the two upper longer wings of the delivery box, open the
the wings. For more intuitive qualitative results, please refer
to the recorded videos.
2) Rollouts of two new tasks:
We show in the main
paper two additional tasks that can be injected into dual-
arm manipulators after a single-shot teaching following the
proposed YOTO paradigm. Fig. 14 shows a series of third-
person recordings of real rollouts. They involve two important
atomic skills: reorientation and rearrangement. The two tasks
directions and place them in a cup. (2) flip basket. Flip
the non-prehensile woven basket with the mouth facing down
180 degrees so that it faces upwards.
3) Analysis of failure cases: Although our method BiDP
outperforms many strong baselines [104, 15, 98, 95] for
addressing long-horizon bimanual manipulation tasks, it still
presents various failure cases during evaluation. Below, we fo-
cus our analysis on execution failure of our BiDP in real-world
experiments. In Fig. 15, we show some representative failure
examples of all real robot executions we have performed with
our method.
cases like: (1.1) The object was wrongly placed in the drawer
so that the drawer could not be closed successfully; (1.2) The
object collided with the drawer during the transfer process,
causing the drawer to move out of position and affecting
its closing operation; (1.3) A grasping error occurred during
the object picking process, causing the object to fail to be
picked up; (2.1) The mouth of the bottle is not aligned with
the mouth of the mug cup, causing more water to spill
out; (2.2) The contact point was biased when grasping the
mug handler, making it easy for water to spill out; (2.3) A
grabbing error occurred while picking up the mug causing it
to fall over; (3.1) When picking up the bottle, the gripper
squeezed the bottle cap, causing the grip to fail; (3.2) The
gripper fails to clamp the center of the bottle cap, causing
the cap to fail to be twisted off; (3.3) When picking up
the bottle, the gripper collided with the bottle body, causing
the bottle to fall and the gripping failed; (4.1) The box lid
fell off due to lack of coordination when opening it; (4.2)
The box lid fell off due to lack of coordination when trans-
ferring it. (4.3) Inappropriate distance between arms caused
that the entire covered box was lifted and moved with-
out opening the box lid; (5.1) After the delivery box was
fully opened, due to a defect in the pressing action (such as
not long enough or deep enough), a wing rebounded back
due to the tension at the hinges; (5.2) Due to the deliv-
ery box displacement or inaccurate action prediction, a lower
shorter wing could not be successfully opened. (5.3) Due
to inaccurate action prediction, a upper longer wing failed to
open successfully. These failure cases point out the direction
that needs further exploration in the future. For more intuitive
qualitative results, please refer to our recorded videos.
