=== PDF文件: Morpheus A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control.pdf ===
=== 时间: 2025-07-22 15:58:52.262432 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Hybrid Actuation and Diverse Emotion Control
Zongzheng Zhang1,2, Jiawen Yang1, Ziqiao Peng1,
Meng Yang4, Jianzhu Ma1, Lin Cheng5, Huazhe Xu3, Hang Zhao3, Hao Zhao1,2
1 Institute for AI Industry Research (AIR), Tsinghua University 2 Beijing Academy of Artificial Intelligence (BAAI)
3 Institute for Interdisciplinary Information Sciences(IIIS), Tsinghua University
4 MGI Tech, Shenzhen, China 5 Beihang University
Equal contribution Corresponding author. Email: zhaohaoair.tsinghua.edu.cn
Fig. 1: Morpheus: an animatronic face with diverse expressions. (a) Front view. Blue markers indicate the attachment points between
the underlying mechanical structure and the soft skin, while yellow arrows denote the directions of movement. (b) Side view highlighting
motion capabilities. Blue arrows indicate the three-axis neck movement: nodding, shaking, and rotation. The green arrow illustrates the jaws
ability for horizontal movement in addition to typical opening and closing motions, enabling more diverse expressions. (c) Four sequential
frames of the happy expression as the animatronic face says welcome! The first row illustrates the virtual expressions generated by our
algorithm rendered in Blender, while the second row displays the corresponding real-world expressions reproduced by the animatronic face.
AbstractPrevious animatronic faces struggle to express emo-
tions effectively due to hardware and software limitations. On
the hardware side, earlier approaches either use rigid-driven
design within constrained spaces, or tendon-driven mechanisms,
which are more space-efficient but challenging to control. In
the best of both worlds. The eyes and mouthkey areas for
emotional expressionare controlled using rigid mechanisms
for precise movement, while the nose and cheek, which con-
vey subtle facial microexpressions, are driven by strings. This
design allows us to build a compact yet versatile hardware
platform capable of expressing a wide range of emotions. On
the algorithmic side, our method introduces a self-modeling
network that maps motor actions to facial landmarks, allowing
us to automatically establish the relationship between blendshape
coefficients for different facial expressions and the corresponding
motor control signals through gradient backpropagation. We then
train a neural network to map speech input to corresponding
blendshape controls. With our method, we can generate distinct
emotional expressions such as happiness, fear, disgust, and
specific control signalsa feature that has not been demonstrated
in earlier systems. We release the hardware design and code
I. INTRODUCTION
Imagine a museum guide robot explaining exhibits to chil-
dren [4, 30]. Despite having flexible limbs and responsive
question-answering capabilities, these robots may still lack a
sense of engagement and immersion. The absence of nuanced
emotional expressions through facial features prevents them
from evoking a sense of closeness. In human-robot interaction,
a mechanical face capable of dynamic and expressive move-
ments is crucial for bridging the emotional gap, fostering trust
and empathy through a tangible, three-dimensional medium
for emotional communication. This makes it indispensable in
applications such as healthcare, education, and entertainment.
lenges in effectively conveying emotions due to constraints
in both hardware and software design. In terms of hardware
mechanisms or fully tendon-driven actuation. While rigid-
driven systems [8, 28, 31, 18, 40, 21, 54, 19] offer precise
control over movement trajectories, they struggle to handle
subtle facial regions like the nose and cheek due to space
On the other hand, tendon-driven systems  are more space-
efficient but fall short when it comes to controlling areas such
as the eyebrows and mouth, where larger and more dynamic
movements are essential for effective emotional expression. In
(b)), adopts a hybrid actuation approach, utilizing 33 actuators
to combine the strengths of both rigid and tendon-driven mech-
anisms. This design enables the creation of more dynamic and
diverse facial expressions within a compact space, effectively
addressing the limitations of previous systems.
Fig. 1(c) illustrates Morpheus generating a happy expression
while saying Welcome! in both the virtual and real domains.
The mouth shape, facial dynamics, and overall expression
closely align with the intended emotional content of the
speech. Precise control is essential for breathing life into the
mechanical structure, enabling the creation of complex and
diverse facial expressions. Traditional expression control meth-
ods which rely on predefined control programs [43, 6, 41, 2],
result in limited, static expressions and fail to produce smooth,
continuous transitions between different emotions. Currently,
learning-based methods mainly focus on replicating expres-
sions observed from human faces via sensors , such as
future frame . For instance, Li et al.  explores robot
facial expression generation from speech. However, it still falls
short in capturing the facial expressions that naturally occur
during speech, resulting in mechanical, emotionless detached
playback. Current methods remain far from achieving diverse,
facial expressions.
On the algorithmic side, digital human researchers have
developed blendshape bases that can generate a wide array
of facial expressions. However, the challenge remains in
effectively translating these blendshapes into motor control
signals for an animatronic face. The key issue is identifying a
control basis that aligns with the blendshapes, as animatronic
complex to model entirely for control purposes. The com-
plexity stems from the non-linear and often unpredictable
relationship between motor actions and facial expressions,
which is exacerbated when mechanical systems interact with
soft tissues. To tackle this, we adopt a self-modeling approach.
We train a network using random pairs of motor control signals
and corresponding animatronic face landmarks to establish
a relationship between them. Once trained, this network is
fixed. By applying backpropagation, we can then calculate
the necessary motor control signals to achieve specific blend-
shapes. This method provides a systematic and precise way to
control the animatronic faces motors to replicate the desired
expressions.
We then develop a neural network to translate speech into a
set of blendshape coefficients. When integrated with the motor
control signal coefficient determined by our self-modeling
spectrum of emotional expressions in response to spoken
language. We can now generate a diverse range of emotional
aligned with the emotional nuances of the spoken words. This
capability to produce nuanced and contextually appropriate
expressions in real-time represents a significant leap forward
in animatronic technology.
We summarize our three contributions below:
We propose a hybrid-actuated robotic face hardware
platform that combines the advantages of both rigid and
tendon-driven mechanisms, allowing for precise control
of key areas for emotional expression while maintaining
space efficiency.
We introduce a self-modeling network that maps mo-
tor actions to facial landmarks, enabling the automatic
establishment of the relationship between blendshape
coefficients and motor control signals through gradient
backpropagation.
We train a neural network to map speech input to cor-
responding blendshape controls, enabling the generation
of distinct emotional expressions with nuanced, emotion-
specific control signals.
II. RELATED WORKS
A. Audio-Driven Facial Animation
Facial animation and emotional expression synthesis have
gained significant attention in both computer graphics and
human-robot interaction, with applications ranging from vir-
tual avatars to robotic systems. Early work on facial anima-
matching facial movements to phonemes for lip-syncing but
fails to capture emotional variations in speech. This limitation
makes the generated expressions less realistic beyond basic lip
movements.
Recent advancements in deep learning [11, 12, 44, 47], par-
ticularly using Transformers, have improved facial animation
by predicting facial movements directly from audio features.
These methods capture more nuanced aspects of speech,
including the dynamics of facial expressions in response to
both speech content and emotional tone. However, many of
these models still prioritize the content of speech, with less
focus on the emotional dimension.
In response to this gap, models like Jis  emotion disen-
tangling approach and EmoTalk  have started integrating
emotional features in facial animation. Jis model separates
emotional and content representations in speech, allowing for
more expressive and dynamic facial movements. EmoTalk
further extends this concept to 3D face animation tasks. Our
work builds on these ideas by offering a more integrated
approach to emotion and content disentangling, improving
Fig. 2: Overview of the Mechanism for Generating Realistic Speech-driven Facial Expressions. We propose a method that disentangles
content and emotion from expressive speech and generates blendshape coefficients using a transformer-based decoder with emotion-guided
attention. These coefficients are then mapped to actuator commands through a face inverse learning module, ultimately producing realistic
facial expressions that match the emotional tone of the speech.
the stability of the process and enhancing the synchronization
between audio and facial movements.
B. Mechanical Face
Over the years, various humanoid robots have been devel-
oped to enhance human-robot interaction through facial ex-
pressions. Early works by Kobayashi and Hara , , ,
Hashimoto et al. , , Berns and Hirth  utilize flexible
micro-actuators and pneumatic systems to replicate human-
like facial expressions, improving emotional communication
[23, 43]. Later advancements, such as the use of rope-driven
movements. Brooks et al.  introduces a humanoid robot
head with 21 degrees of freedom (DOFs) to integrate facial
expressions with social behaviors. Additionally, bio-inspired
introduced to enhance expression flexibility. Humanoid robots,
HRP-4C [25, 42], Janet and Thomas and Eva , have
emphasized full-body integration with advanced facial expres-
sion capabilities. Allison et al.  proposes a unique muscle
actuation system for socially assistive robots, utilizing fewer
actuators to mimic muscle activity and create natural facial
expressions. Furthermore, the Flob robot  integrates high-
resolution sensors for improved social interaction through
facial expressions. The Affetto child robot  uses detailed
facial design to study caregiver-child attachment, while the
EveR-2 platform  emphasizes an expressive humanoid
design for emotional communication in human-robot interac-
More recently, Li et al.  has proposed a tendon-driven
approach for animatronic robots, using linear blend skinning
(LBS) for speech-synchronized facial movements. Yan et al.
develops a humanoid robot head with a rigid linkage
drive system to generate six basic facial expressions. Hu et al.
presents Emo, a rigid-driven humanoid robot platform
with 26 actuators and enhances facial expressivity through
anticipatory modeling, enabling real-time co-expression of
facial expressions with humans. In this paper, We adopt a
hybrid drive system combining tendon and rigid actuators,
utilizing 33 actuators to achieve diverse facial expression
control.
C. Inverse Expression Learning
Pioneering works by Breazeal , Berns and Hirth ,
Nakaoka et al.  and Yan et al.  rely on pre-
programmed systems to generate facial expressions, which
limits their flexibility in real-time emotional responses. Due
to the complexity and non-linearity of robot face, expression
recognition [14, 22, 37, 49], and learning-based method are
crucial. Recently, researchers have made significant progress
by integrating facial expression control techniques, such as
HEFES system , forward and inverse kinematics models
, genetic algorithms [15, 20], CNN-LSTM-based emotion
classification system , visual mimicry learning [9, 19],
Facial Action Coding System , Bayesian optimization
, MAP-Elites algorithm  and visual self modeling .
Since 2023, Gaussian Splatting  has introduced efficient
differentiable rasterization, and has been extended to model
dynamics [52, 35]. Given the motion retargeting approach
proposed in , we backpropagate gradients for optimizing
actuator commands, thereby establishing a connection between
virtual expressions and real-world actuator commands.
III. METHOD
This section begins with an overview of the architecture
(Sec. III-A) integrating the components of our system. Next,
we detail the algorithmic framework (Sec. III-B) for trans-
forming speech inputs into virtual emotional expressions and
the hardware design (Sec. III-C) of the robotic face. We then
introduce the face inverse learning module (Sec. III-D), which
Fig. 3: Hardware Design of Our Mechanical Face Platform. (a) Front view of the mechanical skeletal structure. (b) Transparent side
view of the mechanical structure, highlighting tendon-driven movements in the nose and cheek areas (indicated by blue arrows). (c) Four
rigid-driven modules (indicated by green arrows), including the eyebrow module (4 actuators), eye module (6 actuators), mouth module (16
actuators), and neck module (3 actuators), work together to form the complete mechanical face platform.
maps virtual expressions to motor commands. Finally, the
overall loss function (Sec. III-E) for optimizing the system
is outlined.
A. Architecture Overview
Our speech-driven robotic face system with varying ex-
pressions consists of three main components: the Emotional
Facial Animation module, the Mechanical Face Platform, and
the Face Inverse Learning module. As shown in the Fig. 2,
when a speech input with emotional tone A1:T is fed into the
Emotional Facial Animation module, the model generates a
sequence of blendshape coefficients B1:T , which define the
virtual facial expressions predicted for each frame. These co-
efficients are then passed to the Face Inverse Learning module,
which outputs actuator control commands P 1:T that closely
match the virtual facial expressions. Finally, the generated
facial expression sequence is implemented on the Mechanical
Face Platform, synchronized with the speech input.
B. Emotional Facial Animation
In this part, our goal is to create expressive 3D facial
animations that not only reflect the emotional tone of the spo-
ken content but also offer precise control over the emotional
intensity and personal expression style, which are crucial for
enhancing the realism of mechanical faces. The model takes
a sequence of speech snippets A1:T  (a1, . . . , aT ), where
each at RD is a frame of audio, and produces corresponding
facial blendshape coefficients B1:T  (b1, . . . , bT ) R33,
which define the facial movements at each time step. These
blendshape coefficients control the movement of a mechanical
speech in a natural and dynamic manner.
To achieve high-quality facial animations, we tackle the
challenge of disentangling the speech content from the emo-
tional tone. Our solution involves an emotion disentangling
of the speech signal. We use pre-trained audio models fine-
tuned for extracting distinct content and emotion features.
The encoder is trained on pseudo-training pairs with varying
emotional tones and content, ensuring that it learns to inde-
pendently represent speech content and emotional expression.
This disentanglement enables the model to accurately map the
emotional aspects of speech to facial movements, independent
of the specific content of the speech.
Since speech samples with the same content but different
emotions often have different speech rates, directly using these
samples without alignment may lead to temporal mismatches
in facial animations. To address this, we apply Dynamic
Time Warping (DTW)  to align the feature sequences.
pre-trained wav2vec model , which provides robust and
expressive audio representations. Given two wav2vec feature
sequences Sa and Sb of the same content but different lengths,
DTW calculates a set of index coordinate pairs {(i, j), . . . } by
dynamic programming to align Sa[i] and Sb[j]. The optimal
alignment path minimizes the cumulative distance between
corresponding features:
d(Sa[i], Sb[j]),
where d is the Euclidean distance cost and P is the alignment
path. The path constraint ensures that valid steps are (i1, j),
(i, j  1), or (i  1, j  1), enforcing forward progression
in at least one of the sequences at each step. By aligning
the speech samples in this manner, we ensure that temporal
inconsistencies in the emotional and content features are
animations.
Once the content and emotion features are extracted and
temporally aligned, they are fed into a feature fusion decoder
to generate the 3D facial blendshape coefficients. The decoder
integrates these features to produce realistic facial motions. We
adopt a transformer-based architecture for the decoder, which
effectively captures the temporal dependencies and dynamic
nature of facial movements. Positional encoding is used to
model the timing of facial actions, while a multi-head self-
attention mechanism helps the model focus on local, tem-
poral patterns crucial for generating smooth, coherent facial
animations. An emotion-guided attention mechanism is also
introduced to prioritize emotional features, ensuring that the
generated facial expressions are strongly influenced by the
emotional tone of the speech.
forward layer and decoded into 33 blendshape coefficients
using a fully connected layer. These coefficients control the
mechanical faces movements, producing expressive and emo-
tionally resonant facial animations. This approach enables the
generation of highly customized facial motions for mechanical
personal style. By combining emotion disentangling, tempo-
ral alignment through DTW, and emotion-guided attention,
our method provides a powerful tool for creating realistic,
mechanical avatars.
C. Hardware Design
The primary objective of the hardware design (Fig. 3(a))
is to mimic diverse human expression. The system utilizes
a total of 33 actuators, 29 of them are distributed in the
rigid-driven part: 4 actuators in the eyebrow module, 6 in
the eye module, 16 in the mouth module, and 3 in the neck
module (Fig. 3(c)); the rest 4 of them are distributed in the
tendon-driven part controlling the motion of nose and cheek
(Fig. 3(b)). Integrated movement of these modules allows for
fundamental and diverse human facial expressions, as well as
neck twists and nods. The main structure of the hardware is
built through 3D printing. Through the integration of these
of realism in mimicking human facial expressions, making
it a robust platform for advanced human-robot interaction.
For a more detailed demonstration of the mechanical faces
1) Module Design and Actuation: A sufficient number
of actuators provides the possibility of diverse expressions.
The decomposition of facial expressions is a fundamental
prerequisite for achieving such versatility.
In this work, the movement of the eyebrows is divided
into the movement of the head of the eyebrows and the peak
of the eyebrows. These two motions are relatively defined,
so two sets of four-bar mechanisms are employed to realize
them. It is worth noting that according to the motion principle
of the human eyelid, only the upper eyelid moves when
blinking. However, the lower eyelid moves upward due to
the contraction of the orbicularis oculi muscle. Consequently,
the lower eyelid also needs to be configured with degrees of
freedom to achieve realistic motion.
The eye movement is implemented as a two-degree-of-
freedom movement, reflecting the fact that human eye move-
ments are predominantly synchronous. To achieve realism
and reduce mechanical complexity, the motions of the two
eyeballs are linked, enabling them to rotate simultaneously
to any desired position. Realistic eyeballs are equipped with
miniature cameras for face detection. This setup fosters natural
engagement during conversations, enriching the overall inter-
action and creating a sense of connection with the external
environment.
In the mouth module, the lips are discretized into eight
control points: two at the corners of the mouth and three
at each of the upper and lower lips. The movement of the
two corner points is driven by a planar five-bar mechanism,
which has a wide range of movement. This mechanism allows
the platform to replicate facial movements associated with
as well as expressions of sadness or anger, characterized
by downward movements. Six points on the lips control the
opening and closing of the lips. Additionally, the jaw, beyond
its normal opening and closing motion, is capable of lateral
translation using a rack-and-pinion mechanism. This level
of discretization provides sufficient precision to emulate the
complex dynamics of lip movements during human speech.
2) Hybrid Actuation Control: The simulated face mecha-
nism uses a hybrid actuation system that combines tendon-
driven (Fig. 3(b)) and rigid-driven (Fig. 3(c)) components.
while the rest of the face is driven by traditional rigid mech-
anisms. Tendon drives are highly flexible and take up little
as the human face, where space is limited and the number of
degrees of freedom is high. The principle is also similar to
that of a real muscle stretching drive. Therefore, its suitable
for the complex movement of the cheek and nose. On the
other hand, rigid-driven systems offer greater reliability, load-
bearing capacity, and well-defined trajectories. The eyebrow,
eye and mouth modules utilize such drives. By combining
the flexibility of tendon-driven systems with the stability and
precision of rigid-driven mechanisms, this hybrid approach
enables a more natural and expressive simulation of facial
movements.
D. Face Inverse learning
The Face Inverse Learning module plays a critical role
in transforming blendshape coefficients into precise actuator
commands for the mechanical face platform. Due to the com-
plexity of soft skin as a nonlinear system, where the movement
state is influenced by factors such as material properties and
attachment points, traditional model-based control methods
are no longer applicable. In this work, we adopt an inverse
learning approach to bridge the gap between virtual facial
expressions and the real-world actuator-driven movements.
Excluding the two motors controlling eye movements, three
motors for neck movements (their movements are controlled
by an additional facial tracking system), and two motors
for tongue movements, the remaining 26 motors P
(p1, . . . , p26) are responsible for generating facial expressions.
As Fig. 2 shows, we build an MLP network to model di-
verse facial expressions. Using the face landmark detection
module in MediaPipe , we obtain the 2D landmarks
Cr  (cr1, . . . , crN ) R4682 of the real facial expressions.
This allows us to complete the mapping process using the
dataset we construct (Sec. IV-A). To accelerate the network
optimization process and reduce parameters, similarly to the
approach used in
Hu et al. , we decouple the facial
expression into two independent parts: the upper part (con-
trolled by the eyebrow and eye modules) and the lower part
(controlled by the mouth module), improving training speed
without sacrificing accuracy.
from virtual faces are also obtained. By calculating the 1
distance between these two sets of landmarks, we can back-
propagate the gradient p to the self-modeling face, thereby
optimizing the 26-dimensional motor control parameters.
E. Loss Function
To optimize our neural network for generating realistic
and expressive facial animations for mechanical faces, we
define a comprehensive loss function that incorporates two
key components: cross-reconstruction loss, self-reconstruction
loss. The overall objective function is a weighted sum of these
individual losses, as shown below:
L  1Lcross  2Lself,
where the hyperparameters 1  1.0, 2  1.0 are set
empirically based on our experiments. Each loss term plays
a distinct role in guiding the network toward learning high-
quality facial motions and emotional expressions, ensuring
smooth temporal dynamics, and improving emotional recogni-
tion accuracy. Below, we provide detailed descriptions of each
loss component.
Cross-Reconstruction Loss. This loss component is crucial
for disentangling the emotional e1, e2 and content features
struct facial blendshape coefficients based on different combi-
nations of content and emotion. Given audio inputs Ac1,e2 and
which are then processed by the decoder to produce new
combinations of facial movements. The cross-reconstruction
loss measures the discrepancy between the decoders output
and the corresponding ground truth blendshape coefficients,
Lcross  D (Ec (Ac1,e2) , Ee (Ac2,e1)) Bc1,e12
D (Ec (Ac2,e1) , Ee (Ac1,e2)) Bc2,e22
where D represents the decoder responsible for generating
the facial blendshape coefficients, guided by the emotion and
content features extracted by the encoders Ec and Ee.
Self-Reconstruction Loss. To maintain the quality of facial
which ensures that the network can accurately reconstruct
the original blendshape coefficients from the same content
and emotional input. This encourages the network to generate
consistent facial motions for each specific speech sample. The
self-reconstruction loss is given by:
Lself  D (Ec (Ac1,e2) , Ee (Ac1,e2)) Bc1,e22
This loss helps the model focus on maintaining high fidelity in
the generated facial expressions, ensuring that it can faithfully
reproduce the target blendshape coefficients from a given input
sequence.
In the Face Inverse Learning module, the loss function for
the MLP is defined as the 1 distance LL1 . Specifically, it is
calculated as:
LL1 (Cr, Cv)  1
cri cvi1,
where Cr  (cr1, . . . , crN ) and Cv  (cv1, . . . , cvN ) rep-
resent the set of 2D landmarks of the real and virtual facial
points. Minimizing this loss generates the precise actuator
commands for real face expressions that closely match the
given virtual faces.
IV. EXPERIMENTS
A. Implementation Details
Datasets. To train our Emotional Facial Animation module,
we use the 3D-ETF dataset . This dataset includes multi-
emotion facial animation data with a total duration of five
hours. Two widely used 2D audio-visual datasets are utilized to
construct the 3D-ETF dataset: RAVDESS  and HDTF .
By combining these datasets, the 3D-ETF dataset provides
different emotions and high-resolution facial animation data,
enabling our model to learn nuanced emotional expressions.
To train the Face Inverse Learning module, we construct a
dataset for our system. As described in Sec. III-D, 26 motors
control various facial expressions. To avoid structural inter-
ference during the random generation of control commands,
we impose constraints on each motors movement commands
beforehand. Since each motor can have multiple possible
be generated is exponentially large.
For dataset creation, we position an RGB camera in front of
the mechanical face and randomly send motor control instruc-
tions. These expressions are recorded by MediaPipes  face
landmark detection, capturing the 2D landmark values. This
methodology allows us to construct a dataset comprising 5000
samplesfive times larger than the dataset created in Hu et al.
which is used to train the MLP network. The resulting
dataset offers a comprehensive set of training samples that map
actuator commands to their corresponding facial expressions,
effectively completing the process of modeling the transition
from motor control signals to realistic facial movements.
Hardware Deployment. The simulated face mechanism
employs TD-8035MG servos in the neck module, with a
blocking torque of 35 kg  cm to carry the platforms weight
and the rest of the servos are GUOHUA 9g A0090 micro
servos with 4.6 kg  cm blocking torque. The soft skin is
made from zero-degree liquid silicone, molded by a custom
mole. The primary control board is an NVIDIA Jetson Xavier,
capable of processing audio, images and video data and send
control signals to the motors.
B. Diverse Expressions
Fig. 4 shows 25 different human facial expressions we se-
through precise control of the eyebrow, eye, mouth and neck
a wide variety of rich and complex facial movements. The
platforms ability to replicate such a broad range of emotional
states demonstrates its versatility and strong potential for
realistic human-robot interaction.
C. Quantitative Results
In this experiment, we demonstrate the capability of our
system to generate diverse facial expressions while speaking
complete sentences. Fig. 5 shows the results of our system
where three different expressions  happy, angry and fear
are exhibited while speaking specific sentences.
In terms of eyebrow movements, each expression exhibits
distinct changes that contribute to the emotional tone. For
are raised, conveying joy, while in the angry expression
(Fig. 5(b)), the brows furrow tightly, indicating intensity and
frustration. In the fear expression (Fig. 5(c)), the eyebrows
are lowered, further reinforcing the emotional state. Similarly,
mouth movements play a crucial role in differentiating the
expressions. The happy expression (Fig. 5(a)) is marked by the
corners of the mouth being raised, resembling a smile. Notably,
the mouth movements also align with the natural shape of the
mouth for speech, closely matching the typical phonemes and
enhancing the authenticity of the emotional expression.
The first row shows the virtual expressions generated by our
Emotional Facial Animation algorithm, rendered in Blender,
and it is clear that our Emotional Facial Animation algorithm
is effective in generating diverse expressions during speech.
Fig. 4: Expressions generated by the mechanical face platform.
The platform demonstrates 25 emotional expressions, from happy
to funny, showcasing its ability to replicate a variety of human facial
The second row shows the real-world expressions generated
by the animatronic face using our inverse learning algorithm,
successfully replicating the virtual expressions and demon-
strating the systems ability to bridge the gap between virtual
generation and real-world facial actuation, resulting in realistic
and expressive emotional responses.
D. Qualitative Results
To evaluate the overall effectiveness of our mechanical
face platform in generating realistic facial expressions, we
utilize a video-based facial expression recognition network
. This network is used to classify the generated facial
videos produced by our platform. The classification results
are represented using a confusion matrix, which provides
a detailed breakdown of the systems performance for each
expression type.
In the experiment, we randomly select 100 sentences, with
each paired with four basic expressions, resulting in a total
of 400 videos, each lasting approximately 2 seconds. Table I
presents the performance of the full system. The matrix shows
Fig. 5: Complete sentences with diverse facial expressions. The first row illustrates the virtual expressions generated by our algorithm
rendered in Blender, while the second row displays the corresponding real-world expressions reproduced by the animatronic face. (a) Happy
expression while saying Kids are sitting by the door. (b) Angry expression while saying I lost my keys. (c) Fear expression while saying
The weather is changing.
Fig. 6: Comparison of happy expression using different actuation
systems.
(a) Mouth-Only System. (b) Eyebrow-Only System. (c)
Rigid-Driven System wo tendon driven. (d) Hybrid-Driven System
high accuracies for happy (90) and angry (91) expressions,
reflecting its strong ability to generate dynamic and expressive
movements. While the relatively lower accuracies for disgust
(66) and fear (73) indicate that these expressions are more
easily confused, suggesting less distinct differences between
To evaluate lip synchronization, we use the Lip Vertex Error
(LVE), as applied in MeshTalk  and FaceFormer . LVE
measures the maximum 2 error among all lip vertices for a
given frame. Specifically, for ground truth and predicted lip
vertices Lgt and Lpred, LVE is defined as:
LVE  max
address this, we use the Emotional Vertex Error (EVE), which
calculates the maximum 2 error of the vertex displacement in
these regions. For vertices in the eye and forehead areas, the
EVE is given by:
EVE  max
V gt,j V pred,j2,
where V gt and V pred are the ground truth and predicted
coordinates for the selected regions.
The evaluation results in Table II show that our method
outperforms existing approaches in both LVE and EVE on the
RAVDESS  and HDTF  datasets. Our model achieves
the lowest LVE and EVE scores on both emotional and neutral
expressiveness. On the RAVDESS  dataset, we outperform
EmoTalk with an LVE of 2.426 mm and an EVE of 2.389 mm.
2.491 mm and an EVE of 2.252 mm, surpassing other methods
like MeshTalk  and FaceFormer . By introducing the
Dynamic Time Warping (DTW) algorithm, we achieve better
audio feature alignment, which leads to more accurate lip
synchronization and consequently a lower LVE.
E. Analysis of Hybrid Actuation Hardware Design
Fig. 6 compares the effectiveness of different actuation
systems in generating a happy expression. Fig. 6(a) shows the
result with only the mouth moving, which leads to a limited
expressiveness. Fig. 6(b) depicts the result with only the
TABLE I: Confusion Matrix of recognizing four basic expressions
True Predicted
Happy ()
Angry ()
Disgust ()
Happy ()
Angry ()
Disgust ()
TABLE II: Quantitative evaluation results on RAVDESS  and
HDTF datasets (3D-ETF dataset)
RAVDESS  (emotion)
HDTF  (no emotion)
LVE (mm)
EVE (mm)
LVE (mm)
EVE (mm)
MeshTalk
FaceFormer
eyebrow moving, which fails to capture the full range of facial
emotions. Fig. 6(c) demonstrates the outcome using a rigid
actuation system, which offers better control but still lacks
subtle micro-expressions. In contrast, Fig. 6(d) presents the
hybrid actuation system (ours), which combines both rigid and
tendon-driven mechanisms. This hardware design produces the
most dynamic and expressive facial motion, highlighting its
superiority in conveying emotional expressions.
To quantitatively assess the effectiveness of the hybrid actu-
ation system, we use the same method mentioned in Sec. IV-D.
The classification accuracy is measured using accuracy across
four configurations mentioned above. Following the same
experimental setup as the main experiment, 400 videos are
generated for each configuration and evaluated to compare per-
formance. Additionally, we use landmark distance to measure
the discrepancy between the mechanical faces expressions and
the virtual expressions. We observe a larger facial landmark
displacement in subtle areas between the rigid-only system and
ours (as shown in Table III), confirming the hybrid systems
superior expressiveness. For clarity, all landmark distances are
converted from pixel units to millimeters using camera-to-face
calibration based on known facial dimensions of the robot
platform.
TABLE III: Comparison of Facial Landmark Displacement
Subtle Area
Rigid-Only System (mm)
Hybrid System (mm)
TABLE IV: Classification Accuracy for Videos Generated by Dif-
ferent Hardware Configurations
Hardware Configuration
Happy ()
Angry ()
Disgust ()
Mouth-Only System
Eyebrow-Only System
Rigid-Driven System wo tendon driven
Hybrid-Driven System (Ours)
The results, summarized in Table IV, indicate that the
hybrid-driven system significantly outperforms other config-
urations across all metrics. The addition of the tendon-driven
module significantly improves the mAP from 64.5 to 80,
with notable gains in happy and angry expressions, increasing
from 72 to 90 and 75 to 91, respectively, highlighting
the crucial contribution of the tendon-driven module to these
expressions. In contrast, fear and disgust have slightly lower
all performance of the eyebrow-only system surpasses that of
the mouth-only system, emphasizing the greater importance of
eyebrow movements in facial expressions.
V. LIMITATIONS
In terms of hardware, the facial skin and actuators are
attached using adhesive, which complicates debugging and
adjustments. A more efficient solution could involve fixed
attachment points with magnets or clasps. Our experiments
also show that the material and thickness of the facial skin
impact expression quality, emphasizing the need for further
testing to select the best materials and thickness to better
replicate natural human facial anatomy.
On the software side, the current facial expression gener-
ation system takes emotional speech as input and produces
corresponding facial expressions, which results in relatively
high inference times. To enable more natural and real-time
interactions between humans and robotics, future work will
need to refine the network architecture, focusing on achieving
near real-time dialogue capabilities between the human and
the robotic.
VI. CONCLUSION
In this work, we present Morpheus, an animatronic face
system that combines hybrid actuation, self-modeling, and
speech-driven expression generation for lifelike facial move-
ments. Experimental results show that the hybrid actuation sys-
tem outperforms traditional rigid or tendon-driven approaches
in producing expressive facial movements. The self-modeling
network accurately links motor control to facial deformation,
enabling smoother expressions. Our speech-driven system
produces dynamic and natural facial animations, improving
human-robot interaction. We believe that Morpheus sets a new
benchmark for expressive robotic faces, bringing us closer to
REFERENCES
Brian Allison, Goldie Nejat, and Emmeline Kao. The design of
an expressive humanlike socially assistive robot. 2009.
Wagshum Techane Asheber, Chyi-Yeu Lin, and Shih Hsiang
Yen. Humanoid head face mechanism with expandable facial
expressions. International Journal of Advanced Robotic Sys-
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and
Michael Auli. wav2vec 2.0: A framework for self-supervised
learning of speech representations. Advances in neural infor-
mation processing systems, 33:1244912460, 2020.
Towards a humanoid museum
guide robot that interacts with multiple persons. In 5th IEEE-
RAS International Conference on Humanoid Robots, 2005.,
pages 418423. IEEE, 2005.
Donald J Berndt and James Clifford.
Using dynamic time
warping to find patterns in time series. In Proceedings of the
3rd international conference on knowledge discovery and data
Karsten Berns and Jochen Hirth. Control of facial expressions
of the humanoid robot head roman. In 2006 IEEERSJ inter-
national conference on intelligent robots and systems, pages
Cynthia Breazeal.
Emotion and sociable humanoid robots.
International journal of human-computer studies, 59(1-2):119
Rodney A Brooks, Cynthia Breazeal, Matthew Marjanovic,
Brian Scassellati, and Matthew M Williamson. The Cog project:
Building a humanoid robot.
In International workshop on
computation for metaphors, analogy, and agents, pages 5287.
Boyuan Chen, Yuhang Hu, Lianfeng Li, Sara Cummings, and
Hod Lipson.
Smile like you mean it: Driving animatronic
robotic face with learned models. In 2021 IEEE International
Conference on Robotics and Automation (ICRA), pages 2739
Boyuan Chen, Robert Kwiatkowski, Carl Vondrick, and Hod
Lipson. Fully body visual self-modeling of robot morphologies.
Science Robotics, 7(68):eabn1944, 2022.
Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ran-
speaking styles. In Proceedings of the IEEECVF conference on
computer vision and pattern recognition, pages 1010110111,
Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and
Taku Komura. Faceformer: Speech-driven 3d facial animation
with transformers. In Proceedings of the IEEECVF Conference
on Computer Vision and Pattern Recognition, pages 18770
Zanwar Faraj, Mert Selamet, Carlos Morales, Patricio Torres,
Maimuna Hossain, Boyuan Chen, and Hod Lipson.
Facially
expressive humanoid robotic face. HardwareX, 9:e00117, 2021.
Jianquan Gu, Haifeng Hu, and Haoxi Li. Local robust sparse
representation for face recognition with single sample per
person. IEEECAA Journal of Automatica Sinica, 5(2):547554,
Ahsan Habib, Sumit K Das, Ioana-Corina Bogdan, David Han-
for android phillip k. dick. In 2014 IEEE International Con-
ference on Automation Science and Engineering (CASE), pages
David Hanson, Giovanni Pioggia, S Dinelli, Fabio Di Francesco,
R Francesconi, and Danilo De Rossi. Identity emulation (ie):
Bio-inspired facial expression interfaces for emotive robots. In
AAAI Mobile Robot Competition, pages 7282, 2002.
Takuya Hashimoto, Sachio Hiramatsu, and Hiroshi Kobayashi.
Development of face robot for emotional communication be-
tween human and robot. In 2006 International Conference on
Mechatronics and Automation, pages 2530. IEEE, 2006.
Takuya Hashimoto, Sachio Hiramatsu, and Hiroshi Kobayashi.
Dynamic display of facial expressions on the face robot made
by using a life mask.
In Humanoids 2008-8th IEEE-RAS
International Conference on Humanoid Robots, pages 521526.
Yuhang Hu
