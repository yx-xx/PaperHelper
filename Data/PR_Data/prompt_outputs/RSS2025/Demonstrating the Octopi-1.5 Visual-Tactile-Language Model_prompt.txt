=== PDF文件: Demonstrating the Octopi-1.5 Visual-Tactile-Language Model.pdf ===
=== 时间: 2025-07-22 15:48:36.289120 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Demonstrating the Octopi-1.5
Visual-Tactile-Language Model
Samson Yu, Kelvin Lin, and Harold Soh
Dept. of Computer Science, National University of Singapore
NUS Smart Systems Institute
Contact Authors: samson.yuu.nus.edu, haroldcomp.nus.edu.sg
AbstractTouch is recognized as a vital sense for humans
and an equally important modality for robots, especially for
dexterous manipulation, material identication, and scenarios
involving visual occlusion. Building upon very recent work in
touch foundation models, this demonstration will feature Octopi-
1.5, our latest visual-tactile-language model. Compared to its
signals from multiple object parts and employs a simple retrieval-
augmented generation (RAG) module to improve performance
on tasks and potentially learn new objects on-the-y. The system
can be experienced live through a new handheld tactile-enabled
sensors. This convenient and accessible setup allows users to
interact with Octopi-1.5 without requiring a robot. During
the demonstration, we will showcase Octopi-1.5 solving tactile
inference tasks by leveraging tactile inputs and commonsense
knowledge. For example, in a Guessing Game, Octopi-1.5 will
identify objects being grasped and respond to follow-up queries
about how to handle it (e.g., recommending careful handling
for soft fruits). We also plan to demonstrate Octopi-1.5s RAG
capabilities by teaching it new items. With live interactions, this
demonstration aims to highlight both the progress and limitations
of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting eld. All code for Octopi-1.5 and design les for the
TMI gripper will be released as open-source resources.
I. INTRODUCTION
Touch has long been recognized as crucial for robots,
particularly in contact-rich tasks or situations with visual
occlusion. It enables robots to discern latent object properties,
such as determining the softness or deformability of objects.
In recent years, robot touch has made signicant strides, with
notable advances in sensing technologies  for example,
in visual tactile sensors like GelSight  and Digit
and in the interpretation and application of tactile signals
for manipulation. Notably, recent work has integrated tactile
perception with Vision-Language Models (VLMs), enabling
robots to perform various tactile-related tasks using natural
language prompts [3, 4, 5].
Building upon this line of research, we have developed
an improved visual-tactile language model (VTLM), which
we call Octopi-1.5. Compared to existing VTLMs, Octopi-1.5
features three key enhancements:
a new tactile encoder trained on a combination of ex-
isting Gelsight datasets with an expanded PhysiCLeAR
the use of the Qwen2-VL 7B base VLM , offering
Gelsight Sensor
TAC-02 Sensor
GoPro Camera
Octopi-1.5
Visual-Tactile-
Language Model
The Object is rm,
Describe
the object
Guessing Game
Free Interaction
Octopi-1.5 Demonstrations using a Tactile Manipulation Interface
(TMI) gripper. We plan to demonstrate how Octopi-1.5 can be used to describe
and make use of tactile sensations in a range of scenarios, e.g., without vision
as shown in the top image. We also a (i) Guessing Game where Octopi-1.5 is
tasked to guess which item is being grasped given only tactile inputs (without
vision), (ii) a Sorting task where Octopi-1.5 has to sort items according to
hardness or fruits according to ripeness, and (iii) a Free Interaction setting
where users can use visual, tactile, and language modalities together with
Octopi-1.5. in a free manner on a variety of objects to explore the strengths
and limitations of the system.
enhanced interaction and commonsense knowledge capa-
experimental retrieval-augmented generation (RAG) ,
which not only boosts performance but potentially allows
the model to learn new tactile-object pairings by adding
them to its database.
At R:SS 2025, we propose to demonstrate Octopi-1.5s
capabilities in real-time using a Tactile Manipulation Interface
(TMI) (Fig. 1), which is a Universal Manipulation Interface
(UMI)  modied with tactile sensors. The demo setup will
allow users to interact with objects using the TMI and query
Octopi-1.5 through natural language and physical interaction.
We designed the setup to be portable and not require physical
several interactive scenarios, including a Guessing Game
in which Octopi-1.5 must identify the object being touched
from a collection of items  to free interaction where users
can interact freely with Octopi and the TMI.
The remainder of this paper details Octopi-1.5 and the
demonstration setup. We begin with a brief review of related
work in tactile-language models (Sec. II), followed by a de-
scription of Octopi-1.5s model architecture, training process,
and the TMI (III). Sec. IV outlines the planned demonstrations,
supported by preliminary experimental results that validate
their feasibility. Finally, we conclude with a summary and a
discussion of current limitations (Sec. V).
Our demonstrations aim to showcase advancements in
tactile-enabled VLMs and their potential applications in
robotics. While Octopi-1.5 and similar models are still un-
der development, current limitations, such as challenges in
demonstration will inspire discussion and further exploration
in this eld. To support the community, we will release all
code for Octopi-1.5 and the TMI gripper design.
II. BACKGROUND AND RELATED WORKS
Octopi-1.5 builds upon a wide base of prior work in tactile
perception for robotics and multi-modal language models. In
the following, we give a brief overview of these areas, focus-
ing on advances in visual-tactile-language models. We refer
readers interested in general tactile sensing and perception for
robotics to survey articles [9, 10].
Tactile Sensing. Although not as well-developed as vision,
there has been signicant progress on the development of
tactile sensors over the years . Tactile sensors come
in a variety of types depending on their sensing modality,
e.g., piezoresistive, capacitive, and optical. In this work, we
primarily use an optical sensor called the Gelsight Mini 1.
Optical tactile sensors provide high-resolution tactile images
that capture surface deformations by converting them into
visual data . This makes them effective at detecting ne
surface features and texture, and their image outputs are easily
processed by modern machine learning methods. However,
compared to alternative sensor types, optical tactile sensors
tend to be larger in size and have lower sampling rates.
Our TMI gripper also incorporates a piezoresistive sensor
called the TAC-022. Unlike optical sensors, piezoresistive
sensors operate by detecting changes in resistance caused by
the deformation of elastic materials under applied forces .
The TAC-02 has 64 taxels, which is lower in resolution
compared to the Gelsight Mini, but is more compact, has a
higher sampling rate (1kHz), and can directly capture static
and dynamic pressure.
Visual-Tactile-Language Models. Octopi is closely related
to very recent work on multimodal large-language models
1Gelsight
uploadsproductsheetMiniGS Mini Product Sheet 10.07.24.pdf
OCTOPI-1.5 TRAINING DATASET STATISTICS
Num. Tactile
per Object
PhysiCLeAR-Plain
PhysiCLeAR-Dotted
Hardness
ObjectFolder-Real
(MLLMs)  that process data from real-world tactile sen-
sors [3, 4, 5]  we refer to these models as Visual-Tactile-
Language models (VTLMs). These prior works share simi-
larities in that they involve the collection of tactile, visual,
and language modalities into datasets. For example, the TVL
Dataset  includes over 44,000 paired tactile-visual samples
annotated with natural language, while other datasets, such
as ObjectFolder  and PhysiCLeAR , combine high-
resolution tactile data with semantic labels for material prop-
erties like hardness and roughness. These datasets are used
to train tactile encoders, often based on Visual Transformers
(ViTs) , employing contrastive and regression-based loss
functions to align tactile, visual, and language representations.
Octopi-1.5 shares these foundational approaches but (i) is
trained on a larger dataset compared to its predecessor and (ii)
incorporates a retrieval-augmented generation (RAG) module,
enabling it to retrieve and utilize similar objects from its
database to enhance predictions and allows for on-the-y
learning of new object-tactile pairings.
III. SYSTEM DESCRIPTION: OCTOPI-1.5 AND TMI
In this section, we discuss the main components of our
A. Octopi-1.5 VTLM
Octopi-1.5 is a visual-tactile-language model (VTLM). Be-
Octopi-1.5 Model Structure. Fig. 2.A provides a high-level
overview of the model architecture, including the GelSight
mini tactile encoder. Octopi-1.5 is based on the QWEN2-VL
7B open-source vision-language model (VLM) , whereas
the previous Octopi version utilized Llama . Encoders
are used to transform raw inputs into tokens, which are
subsequently processed by the VLM transformer.
A key improvement in Octopi-1.5 is in the tactile encoder,
which translates tactile frames from a GelSight mini sensor
into tokens for the VLM. Specically, the tactile encoder is
a ne-tuned CLIP  module augmented with a projection
layer. To optimize computational efciency, we process only
salient frames, which are selected using a heuristic that iden-
ties the top 10 frames with the largest differences compared
to their preceding frames.
Octopi-1.5 Training. Octopi-1.5 was trained on an expanded
PhysiCLeAR dataset (including both marker and markerless
GelSight pads), as well as the hardness  and ObjectFolder
datasets . Table I provides a summary of the dataset
statistics. Model training was conducted in two stages: rst, we
Transformer
Qwen2-VL
Word Encoder
Tactile Encoder
Vision Encoder
CLIP (L14)
Projection
Input Tokens
Encoders
Output Tokens
textured
Describe the object
CLIP (L14)
Constrastive
Regression
Averaging
Tactile Frames:
Retrieval Module
Most similar
baseball
(A) Octopi-1.5 model architecture. Octopi-1.5 is a ne-tuned Qwen2-VL 7B multimodal VLM with a tactile encoder and retrieval module. (B)
The CLIP module of the tactile encoder is pre-trained before netuning using contrastive and regression losses on PhysiCLeAR, Hardness, and ObjectFolder
datasets. Please see main text for details.
TABLE II
ANNOTATOR SCORING GUIDANCE FOR HARDNESS AND ROUGHNESS.
Property
Description
Example Object
Hardness
Easily compressible when pressed with little force
Cotton wool
Compressible upon pressing with moderate force
Foam mattress
Incompressible with high human pressing force
Roughness
Minimal feeling of friction upon nger sliding
Slight feeling of friction upon nger sliding
Signicant feeling of friction upon nger sliding
Toothbrush bristles
Training Task Example: Describe and Rank
Describe the objects in the following
tactile videos and rank them in
decreasing hardness and roughness.
Object 1
Part 1.1: [tactile frames]
Part 1.2: [tactile frames]
OCTOPI-1.5
Object 1
Part 1.1: [descriptions]
Part 1.2: [descriptions]
Object parts ranked in decreasing
Object parts ranked in decreasing
Training Task Example. Octopi-1.5 is trained to describe and rank
different parts of an object (in addition to different objects).
trained the CLIP module within the tactile encoder, followed
by end-to-end ne-tuning of the VTLM. This staged approach
resulted in more stable training outcomes.
The CLIP module was trained using both regression and
contrastive losses, as illustrated in Fig. 2.B. In the current
annotated hardness and roughness scores for objects. Anno-
tators followed the guidance shown in Table II for labeling.
RAG Description Example
Describe the object in the following
tactile video(s).
OCTOPI-1.5
Object 1: [Tactile Frames]
Object 1: textured, grippy,
Most similar objects (in order
of decreasing similarity): a new
baseballs seams (grippy, hard,
The outcome of a tactile description by Octopi-1.5 augmented with
RAG. The additional information added by the RAG module is shown in blue.
For the contrastive loss, the network distinguishes tactile inputs
from the same objectpart (positive class) versus those from
other objectsparts (negative class). To ensure consistency, both
positive and negative classes were sampled using the same type
of GelSight silicon pad (marker or markerless). We trained the
CLIP module for 30 epochs, selecting the best encoder based
on validation loss using a set of six unseen objects.
In the second phase, the CLIP module is frozen, and
the projection layers are trained alongside the decoder. The
decoder training employs Visual Prompt Tuning (VPT)  on
description and ranking tasks as illustrated in Fig. 3. Unlike its
Gelsight-mini
Gelsight-
TMI Gripper Fingers
CAD Drawing
TMI ngers and CAD drawings with compartments for inserting the
Gelsight-mini or the TAC-02 sensors.
comprising multiple parts, such as a hairbrush with distinct
handle and bristle components.
RAG-Modied Descriptions. We are currently experimenting
with the decomposition of a VTLM into two components:
one dedicated to processinginference and another serving as
a memory of previously seen items. As a step towards
this goal, Octopi-1.5 incorporates a simple RAG scheme to
enhance its tactile descriptions by augmenting them with
textual information from similar objects, thereby aiding down-
stream tasks. Specically, the tactile descriptions of objects
are supplemented with their labels and the tactile descriptions
of similar items, creating a more informative representation
(example in Fig. 4).
To generate these augmented descriptions, we follow a
straightforward process:
For a new set of salient tactile images, compute their
average embedding using the tactile encoder.
Perform a cosine-similarity search over an existing
dataset of tactile embeddings to identify the top-5 objects
with the most similar averaged embeddings.
Aggregate the unique objects and rank them by the
number of retrieved samples to form a prioritized list of
matches.
While straightforward, this RAG scheme improved Octopis
performance in our proposed demonstration tasks (see Section
IV). Future work could explore more advanced RAG setups,
such as retrieving and incorporating tactile embeddings di-
rectly; implementing such improvements will likely require
retraining the VTLM to optimally utilize the retrieved infor-
B. Tactile Manipulation Interface
To facilitate the demonstration, we propose using a modi-
ed Universal Manipulation Interface (UMI) . Our Tactile
Manipulation Interface (TMI) features ngers equipped with
tactile sensors (Fig. 5). Specically, one nger is mounted
with a GelSight Mini sensor, while the other houses a TAC-
02 piezoresistive tactile sensor. The GelSight sensor provides
high-resolution tactile imaging, whereas the TAC-02 sensor 3
Gelsight-mini Sensor
Tactile Video
Chat Interface
Manipulation
Interface (TMI)
TAC-02 Sensor
Our demonstration system is highly portable, consisting primarily of
the TMI and a laptop. The laptop is equipped with the necessary software to
interface with Octopi-1.5, which runs on a remote workstation or server. A
stable internet connection is required, with the option to use a mobile hotspot
if needed. The setup can be fully assembled in under 15 minutes. We will
provide a variety of items for grasping and participants can use their own
objects.
directly captures pressure readings.
Our demonstration will primarily utilize the GelSight Mini
training. Nevertheless, the TMI provides a proof-of-concept
integration of multiple sensors, which we believe will spur
discussion on integrating heterogeneous tactile modalities. We
also hope to show participants how manipulation data can
be collected using the TMI. We plan to engage in broader
discussions surrounding TMIs benets  such as ease of
use and versatility  and limitations including sensor wear
and tear, task constraints, difculties in annotation, and the
variability across different tactile sensors.
IV. PLANNED DEMONSTRATIONS
We propose a series of demonstrations to showcase Octopi-
1.5s capabilities in aiding tactile-related tasks. These demon-
strations will utilize the TMI, connected to a laptop with an
internet connection to a remote workstationserver at NUS
running Octopi-1.5 (Fig. 6). If available, we would prefer
to use a large-screen TV or projector to display presentation
slides and Octopi-1.5s feedback.
Our planned format is straightforward: we will begin with
a brief overview of Octopi-1.5 and the TMI, followed by an
introduction to the demonstration system. Currently, text inter-
action is facilitated through a simple chat interface, though we
are exploring the feasibility of integrating speech-to-text using
Whisper . However, we found Whisper can be unreliable
in noisy environments with multiple speaking participants, so
it may not be included in the nal setup.
We will then demo a tactile-based description task (Fig. 1),
followed by two structured demonstrations: the Guessing
Guessing Game Example (through Simplied
Interface)
Take a picture of the scene.
[System takes a picture using the
GoPro; see image above].
What fruits do you see in the image?
OCTOPI-1.5
Object 1: round, red, apple.
Object 2: round, orange, orange.
Object 3: oval, yellow, kiwi.
Touch an item on the table.
OCTOPI-1.5
Please use the gripper to grab the
[User grasps apple using the TMI (no
visual images taken)].
Finished collecting tactile data.
Describe what you just touched.
OCTOPI-1.5
Object 4: hard, firm, glossy, smooth.
Which fruit is it likely to be?
OCTOPI-1.5
To determine which option the object
is likely to be, we need to consider
the surface texture descriptions that
help to distinguish between the given
options. Lets analyze each option:
A) Apple: Apples typically have a
B) Orange: Oranges have a rough, bumpy
texture on the skin.
C) Kiwi: Kiwis have a fuzzy, textured
each option ...
option because it has a smooth,
slightly waxy texture on the skin,
which is characteristic of apples.
User Interaction Example for the Guessing Game.
TABLE III
AVERAGE ACCURACIES () ON THE GUESSING GAME.
(teaching)
Encoder-1.5
Octopi-1 (7B)
-Octopi-1 (13B)
Octopi-1.5 (8B)
Octopi-1.5 (8B)
Game and Sorting Task. Finally, users will have the oppor-
tunity to interact more freely with the system to explore its
capabilities and limitations.
A. Guessing Game
The guessing game is designed to demonstrate how tactile
information can be combined with commonsense knowledge
to make inferences:
We will provide octopi with a set of objects (described
via language or visual input)
Users will select an object and provide tactile inputs
by grasping the object with the TMI (no visual input
provided during grasping)
Octopi-1.5 will infer which object corresponds to the
tactile inputs.
Users can interact with the system by prompting Octopi-
1.5 to guess again if the initial guess was incorrect or
asking follow-up questions (e.g., how hard the object
should be grasped).
To simplify the interaction, the demo will make use of a chat
interface that hides the more complex prompts used to elicit
proper behavior from Octopi-1.5; see Fig. 7) for an example
of the interaction where the user grasps an apple. After the
behind-the-scenes.
Preliminary tests support the feasibility of this demonstra-
tion. We experimented with three categories of objects: balls
(baseball, plush ball, tennis ball) and fruits (apple, partially
ripe kiwi, orange) and completely unseen objects (hairbrush
are summarized in Table III, with averages computed across
multiple samples from the same object (25 samples for balls,
26 samples for fruits, and 41 unseen samples).
As a non-VLM baseline, the Encoder-1.5 model uses co-
sine similarity of tactile embeddings in the training set. The
encoder-only baseline achieves good results on familiar objects
but offers no out-of-the-box handling of unseen items without
database updates.
Octopi-1.5 shows higher overall accuracy compared to the
older Octopi-1. Simple RAG retrieval improves performance
and teaching further boosts results by adding new item
samples to the dataset on the y. With RAG enabled, accuracy
scores for Octopi-1.5 are high (96-100) for the seen objects
(Balls, Fruits) with g
