=== PDF文件: Action Flow Matching for Lifelong Learning.pdf ===
=== 时间: 2025-07-21 14:25:34.703415 ===

请从以下论文内容中，按如下JSON格式严格输出（所有字段都要有，关键词字段请只输出一个中文关键词，一个中文关键词，一个中文关键词）：
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Action Flow Matching
for Continual Robot Learning
Alejandro Murillo-Gonzalez and Lantao Liu
Indiana UniversityBloomington
{almuri, lantao}iu.edu
AbstractContinual learning in robotics seeks systems that can
constantly adapt to changing environments and tasks, mirroring
human adaptability. A key challenge is refining dynamics models,
essential for planning and control, while addressing issues such
as safe adaptation, catastrophic forgetting, outlier management,
data efficiency, and balancing exploration with exploitation all
within task and onboard resource constraints. Towards this goal,
we introduce a generative framework leveraging flow matching
for online robot dynamics model alignment. Rather than execut-
ing actions based on a misaligned model, our approach refines
planned actions to better match with those the robot would take
if its model was well aligned. We find that by transforming
the actions themselves rather than exploring with a misaligned
model as is traditionally done the robot collects informative
data more efficiently, thereby accelerating learning. Moreover,
we validate that the method can handle an evolving and possibly
imperfect model while reducing, if desired, the dependency
on replay buffers or legacy model snapshots. We validate our
approach using two platforms: an unmanned ground vehicle and
a quadrotor. The results highlight the methods adaptability and
strating its potential towards enabling continual robot learning.
I. INTRODUCTION
Humans exhibit a remarkable ability to adapt in dynamic
and uncertain environments, excelling at compensating for
mismatches between expectations and actual outcomes. For
they instinctively adjust their steering and braking actions to
align with the anticipated altered dynamics caused by the
changed road condition. This ability to plan effectively in
the face of discrepancies inspires the robotics community to
tackle a pressing challenge: enabling robots to operate robustly
with potentially misaligned or limited dynamics models, where
misalignment can emerge from simplifying assumptions, new
or unaccounted factors, incomplete or inaccurate data and
parameter uncertainty. This capability is crucial for long-
term autonomy, where unanticipated changes in environmental
can render pre-designed, pre-learned or pre-calibrated models
ineffective.
The dynamics model of a robot encapsulates how the robot
predicts the outcome of its actions based on its state and
control inputs. Accurate dynamics models form the foundation
for planning and control, dictating how a robot predicts the
outcomes of its actions to achieve desired goals. Techniques
such as Linear Quadratic Regulator (LQR) [18, 1] and Model
Predictive Control (MPC) [33, 8, 36] rely critically on the
fidelity of these models to generate and evaluate control
policies. Without a robust and adaptable dynamics model,
even advanced controllers falter, particularly in realistic sce-
narios where the robot must operate in diverse or evolving
circumstances. For instance, modeling the effects of reduced
traction on a slippery surface or compensating for actuator
wear are essential for maintaining safe and effective robot op-
eration. Dynamics models thus represent the link between the
robots perception, prediction, and control, enabling informed
decision-making under uncertainty.
Although the potential for robots to improve their dynamics
models through experiential learning is compelling, achieving
this safely and effectively remains a significant challenge. Fre-
quent model updates risk overfitting to transient phenomena or
acquired knowledge . Additionally, outliers and spurious
correlations in data can compromise the model, undermining
performance and safety. Conversely, overly conservative up-
dating may fail to capture legitimate shifts in dynamics .
Lifelong learning [37, 43, 19] introduces additional complex-
context of ongoing task execution, rather than solely focusing
on model improvement. The need to reconcile these trade-
offs is further complicated by the diversity of environments
a robot may encounter, each presenting unique and evolving
dynamics.
In this paper, we propose a novel method for continual
learning of robot dynamics models that efficiently leverages
the robots latest experiences. Our approach departs from the
conventional approach in current Model-Based Reinforcement
Learning (MBRL)  where exploration is mostly guided via
plans obtained with the latest version of the model [7, 15].
transform planned actions into those that more closely reflect
the robots actual intentdefined as the action the robot would
have selected if an aligned dynamics model was available.
are often suboptimal or even totally wrong. This happens
because the model fails to precisely account for the actual
environmental and dynamical factors influencing the robot.
planned or desirable future states. AFM leverages such signals
and learns a transformation that adjusts the initially planned
Adjust via AFM
Fig. 1: Action Flow Matching for efficient continual learning. The planner uses the latest version of the dynamics model and the desired goal
to select the next action a0
t. AFM transforms a0
t into a1
t whenever the misalignment between the model and the environment is unacceptable.
model inaccuracies, thus increasing the likelihood of reaching
the planned state. In other words, the realigned actions can
be viewed as the output of a calibration process correcting
for the imperfect understanding of system dynamics, ensuring
that the executed actions are more effective and informative
for subsequent learning and control. Figure 1 illustrates AFMs
role while updating a robots dynamics model in an online and
non-episodic manner.
Our key contributions can be summarized as follows:
Efficient Dynamics Space Reduction: The initial model
can be leveraged to reduce the state-action space to a
dynamically feasible region, which would otherwise be
inefficient to capture through random trajectory gener-
ation. Since at this stage we are not assuming access
to a ground-truth model, we can only obtain from the
initial model the reduction of the space to an approximate
subset of feasible regions. The intuition behind this idea
is demonstrated in Figure 3.
Representing the Dynamics Regime without Ground-
Truth Models: By focusing on state evolution patterns,
we can learn to represent different dynamics regimes
without needing an accurate ground-truth model. Here it
is important to highlight a crucial distinctionlearning
state evolution patterns means merely identifying re-
curring patterns of state changes. This is unrelated to
learning the underlying system dynamics which are
unavailable as long as the misalignment persists: we will
capture the dynamics model via continual model updates.
Intent Mapping for Accelerated Learning: Leveraging
flow matching within the reduced dynamics space and
the dynamics regime representations allows us to produce
guiding signals that the agent can exploit. This, in turn,
accelerates learning thanks to the collection of more in-
formative transition information via transformed actions.
II. RELATED WORK
This section reviews related work, starting with continual
dynamics learning for real-time system modeling, followed by
online methods for robotic adaptation. We also briefly touch on
batch-based approaches for robot adaptation, which, although
not continual, provide valuable insights for enhancing robot
performance and versatility.
A. Continual Robot Dynamics Learning
Learning a robots dynamics online to fine tune or correct
previous models has usually relied on strong inductive biases,
which helps reduce the learning complexity but leads to task-
or embodiment-specific methods. For example, Jiahao et al.
learn online the residual dynamics of a drone following a
reference trajectory. In this setting, the drones mass changes
as it operates, for instance because it needs to carry a load not
considered in its original dynamics. Their work builds upon
differential equation methods, used to augment a model
obtained from first principles . To turn KNODE-MPC
into an online method in , the authors relied on a set of
architecturally-equivalent models fine tuned sequentially and
in parallel to the drones operation. The robot then switches
to the most up-to-date weights. However, to train the models,
they require a replay buffer that relies on a window of past
transition information, which requires additional memory re-
quirements and selecting an additional hyperparameter (replay
buffer window size).
Meier et al.  learns the inverse dynamics error for a
robotic arm. They focus on getting the residual terms for
the model of a KUKA lightweight arm in simulation and
real-world evaluation. They leverage Gaussian processes with
an smoothed update rule to achieve controlled parameter
for robustness. Nguyen-Tuong and Peters  find a sparse
representation of the data using a test of linear independence
to get a fixed-size subset of the data that will be used for
training. Then, the subset of data is used with an online kernel
regression method to obtain the robots model. However, note
that this approach requires keeping both a database of the
observed data and the varying subset of training data, which
might be impractical for lifelong learning. The method is
shown to work with a 7 degree-of-freedom Barrett WAM arm.
Sun et al.  focus on dynamics learning in the con-
text of legged locomotion. They conduct evaluations using a
simulated biped and a real-world quadruped. Their approach
involves learning a time-varying, locally linear residual model
along the robots current trajectory, aiming to correct the
prediction errors of the controllers model. In particular, they
use ridge regression to find the residual weights and biases
of the nominal model. Initially, they rely on the nominal
model and only start learning the residual parameter after k
decide when enough data has been collected to be able to
learn a good set of coefficients. k also serves as the maximum
size of the replay buffer, thus as new data arrives, old data
is sequentially discarded to encourage learning only based on
the latest transition dynamics.
Jamone et al.  explore kinematics learning for a sim-
ulated humanoid working with tools with different shapes
and sizes, which induces a non-negligible context switch. The
context is recovered by identifying the transition discrepancy
from similar inputs, which signals the presence of a different
confounding factor (different tool in this case) that modifies
the true robots dynamics. To model the transitions they learn
a multi-valued function, that allows for multiple potential solu-
tions to be associated with the same query input. Nonetheless,
they rely on the Infinite Mixture of Linear Experts algorithm,
thus constraining the modeled relationships to collections of
local linear models.
B. Online Learning for Robot Adaptation
Pastor et al.  achieve online robotic arm movement
adaptation by associating experienced sensor traces with
stereotypical movements. The traces are leveraged to generate
an implicit model that can be used to track differences in what
the robot feels for future similar movements. The model then
enables planning reactive behaviors to compensate or adapt to
unforeseen events. In contrast to our work, this works adaptive
behavior results from the predictive controller correcting for
discrepancies based on the models learned a-priori, while we
focus on modeling the new dynamics.
Thor and Manoonpong  develop an online error-based
learning method for frequency adaptation of central pattern
generators (CPGs) to produce periodic motion patterns, which
can be leveraged for robot locomotion. In this setting, learning
happens using a dual integral learner that relies on a simple
objective function that does not need to explicitly consider
the systems dynamics. Although this method adds supporting
evidence to error-guided learning for control adaptation, it
is encapsulated to CPGs, which might not be relevant to
controlling certain robotic platforms.
Hagras et al. [13, 14] achieve a lifelong learning approach
for online controller learning and calibration. They focus on
agricultural mobile robots that adapt their navigation behaviors
to the dynamic and unstructured environment. Specifically,
they learn online the membership functions for a hierarchical
fuzzy genetic system. Additionally, each behavior is imple-
mented as a simple fuzzy logic controller, thus limiting the
number of inputs and outputs. Fuzzy coordination will then
allow behaviors to activate concurrently at varying levels,
providing smoother control than switching methods.
achieves continual learning in simulation environments by
applying online stochastic gradient descent (SGD) to update
the model parameters, while concurrently meta-learning a
mixture model to represent parameters across different tasks.
Chen and Wen  do trajectory tracking for flexible-joint
robots. The controllers network consists of a regressor and
output layer, designed to model the robots linear-in-parameter
dynamics. The parameters are updated guided by the trajec-
tory tracking error on separate time scales: fast adaptation of
output weights via Lyapunov-based laws and slower internal
weight updates through backpropagation, inspired by singular
perturbation theory.
C. Batch-based Learning for Robot Adaptation
The Rapid Motor Adaptation (RMA) family of algorithms
has emerged to enable a robot to rapidly adapt to envi-
ronmental challengesparticularities represented via a latent
vector that augments the input of a policy learned offline, in
simulation and using privileged information (e.g., ground truth
sensor readings and environment information) [20, 21, 32, 24].
Although they have shown good performance, online learning
is not possible given the need for multi-staged training, where
the latent representation is first learned together with the
model-free base policy, followed by learning an adaptation
module that learns to predict the latent representation using
past state-action data. Lee et al.  propose Context-aware
Dynamics Models (CaDM) for handling diverse dynamics.
The approach involves learning a context encoder alongside
forward and backward dynamics models, with both models
conditioned on a latent vector that encodes dynamics regime-
specific information. This work highlights the need for con-
tinual dynamics learning, as it is hard to exhaustively learn
a robots transition model and the challenge is exacerbated
by the possibility of ambiguous state representations due to
non-observed state variables (e.g., broken or missing sensors).
III. PRELIMINARIES
We begin with an introduction to model-based planning
and its reliance on predictive dynamics models for effective
decision-making. Then, we introduce flow matching, a gen-
erative framework for transforming random variables from a
source to a target distribution.
A. Model-Based Planning and Control
Let S Rn be the state space representing all possible
configurations of the system. Let A Rm be the action space
representing all possible control actions that influence the sys-
tems dynamics. Model-based planning leverages a dynamics
model f : S  A S to predict the state evolution. This
predictive capability allows planners to optimize a sequence
of actions a A from a state s S by minimizing a cost
function J (s, a) that balances the multiple objectives of the
and resource efficiency:
a arg min
aAH J (st, a; H).
where the trajectory cost J is defined as:
J (st, a; H)
(sth, ath)  (stH).
actions over the planning horizon H, while and  denote
the running cost at each timestep and the terminal cost,
respectively. The sequence of states {st, st1, . . . , stH} is
determined recursively using the dynamics model f as:
st1  f(st, at),
where st S represents the state of the system at time t, and
at A is the action at t.
In this setting, the dynamics model f serves as the mathe-
matical representation of the constraints governing the evo-
lution of the systems state. That is, the feasibility of any
proposed sequence a is dictated by the systems dynamics.
strained sequential decision-making problem:
a arg min
a J (st, a, H),
subject to
sth1  f(sth, ath),
This formulation emphasizes that f, the dynamics model,
governs admissible trajectories by encoding critical physical
constraints such as non-linearities, dynamics limits, and uncer-
tainties. Any inaccuracies in f shift the feasible solution space,
resulting in plans {st} that may violate real-world constraints
or be suboptimal when executed. Consequently, maintaining
an aligned dynamics model is essential for effective planning
and control.
We solve the optimization problem in Eq. (4) using
sampling-based methods, which are well-suited for high-
dimensional and non-linear dynamics. These methods optimize
action sequences by sampling trajectories, evaluating their
The differences among this family of methods lie in how
the candidate distribution is selected and updated at decision
(MPPI) [12, 28] and the Cross Entropy Method (CEM) .
The critical role of the dynamics model highlights the need
for mitigating the impact of model misalignment, particularly
in complex, unstructured environments where exhaustively
anticipating all changes in transition dynamics is impractical or
even infeasible. AFM, introduced later in this work, addresses
this by transforming planned actions into those that better align
with the agents true intention, compensating for errors caused
by misalignment between the true and modeled dynamics.
B. Flow Matching (FM)
In the following overview we adopt the notation and de-
scription of FM by Lipman et al. : FM is a framework
for generative modeling that constructs bijective, deterministic
flows through a learned velocity field, designed to interpolate
between a source p and target q distribution. Formally, given
a training dataset with samples from a target distribution
q Rd, the goal is to train a neural network parameterized as
a time-dependent velocity field u
: [0, 1]Rd Rd such that
the associated flow transforms the source distribution p into q.
This is achieved by constructing a time-continuous probability
path {p}1
method can be decomposed as follows:
1) Constructing the Flow: The flow transformation  :
Rd Rd is defined through the solution of an Ordinary
Differential Equation (ODE) parameterized by the velocity
transforming samples X0 p through X  (X0).
2) Designing the Probability Path: This is used to interpo-
late samples from the source to the target distribution. Among
the alternatives, we choose the conditional optimal-transport
combination of the source X0 p and target X1 q samples:
3) Training Objective: The core learning objective is to
minimize the discrepancy between the learned velocity field
and the target velocity u, defined via a regression loss:
LFM()  EU[0,1],X p
(X) u(X)
the joint transformation between two high-dimensional distri-
butions .
EU[0,1],X0p,X1q
whose key property is that it shares the same gradient as the
nominal flow matching loss:
LFM()  LCFM(),
thus enabling learning a valid velocity field u
without the
need for the ground-truth u.
4) Sampling from the Model: After training, new samples
X1 q are drawn by solving the ODE over the interval
[0, 1] for an input X0 p:
Specific to our work, we obtain X1 using the learned velocity
solver .
IV. METHOD: ACTION FLOW MATCHING
We introduce Action Flow Matching (AFM), a method
towards continual learning in robotics built on a set of key
time model alignment. In this context, misalignment can arise
from simplifying assumptions, new or unaccounted latent
uncertainty.
Our identified insights include: (1) efficient dynamics space
reduction using the initially available model; (2) dynamics
regime representation learning without ground-truth models
by focusing on state change evolution patterns; and (3) intent
mapping for accelerated learning by using a generative model
to produce guiding signals based on information about the
intent of the robot and the current dynamics regime, instead
of solely relying on the possibly misaligned model.
A. Formal Overview
Given the available dynamics model ft at time t, with
weights t updated every step using the previous state tran-
horizon H using planned actions a0
st1  ft(st, a0
t is chosen to minimize a trajectory cost J (st, at, H),
which evaluates deviations from desired outcomes and control
effort. However, when ft significantly deviates from the true
environment dynamics f, the predicted states st1 fail to
accurately reflect the systems behavior, leading to suboptimal
plans and limiting both learning and control performance.
To address these discrepancies, we deviate from the conven-
tional approach of only relying on the imperfect dynamics f
to gather experience with which the model can be improved,
since complex systems can make it challenging and time
consuming. Instead, we opt to add a more direct and efficient
t itself.
The essence of our proposed framework can be likened
to optometry, where instead of attempting to correct the
biological intricacies of a myopic persons eyesan inherently
complex and delicate systemwe place corrective lenses that
adjust the incoming light path, ensuring the perceived image
is properly aligned with reality. Similarly, rather than solely
modifying the intricate and often imperfect system dynamics,
we introduce an additional transformation step that compen-
sates for discrepancies, ensuring that the executed actions are
better aligned with the intended outcomes. This additional step
in our case is AFM, whose overview is presented in Figure 2.
t aims to reduce the
deviation between the planned next state:
t1  ft(st, a0
and the state the robot would actually reach if a0
t is executed
(realized next state):
st1  f(st, a0
that is,
f(st, a1
t12 f(st, a0
It is important to highlight that the constraint in Eq. (14)
aims to reduce the deviation, instead of minimizing it. The
main drivers for this problem formulation are twofold:
Limited access to environment dynamics. We cannot
assume access to the current environment dynamics f.
Minimizing st1s
t12 would require full knowledge
of f, which remains partially unknown and is the exact
target we seek to learn. As such, attempting to directly
minimize this quantity would render the learning of the
AFM model g both intractable and redundant.
Enhancing data informativeness and model alignment.
By targeting the more tractable constraint in Eq. (14),
the agent is encouraged to collect more informative state
evolution data. This not only facilitates faster alignment
of ft, but also improves planning efficacy by ensuring
that actions a1
t more closely correspond to the desired
outcomes. These features are especially advantageous for
lifelong learning algorithms [37, 43].
B. Learning to Map Plans to Intentions
To enable efficient continual learning of evolving dynamics,
we focus on generating informative exploratory trajectories.
This addresses the challenge posed by misaligned dynamics
models ft, where planned actions a0
t may fail to achieve the
desired outcomes, resulting in data collection inefficiencies in
regions of the state space with low informational value.
To mitigate this, we propose the AFM model g, designed
to transform the distribution of planned actions a0
t into actions
t that align more closely with the agents intended actions.
have been chosen had the agent used a well-aligned dynamics
model. Consequently, AFM guides the agent to select actions
that promote data collection in regions of higher utility, thereby
expediting model alignment.
The key contribution of AFM stems from its novel training
a potentially misaligned dynamics model to actions that more
closely achieve the intended outcomes. Standard approaches
to flow matching would require datasets consisting of samples
from the source and target distributions. Specifically, AFM
would necessitate a dataset comprising planned actions a0
(source distribution) and intended actions a1
t (target distribu-
tion). However, creating such datasets is infeasible due to the
challenging hurdle of comprehensively identifying the causes
Training
ODE SOLVER
Action Flow Matching
Dynamics
Representation
Intended
Representation
Flow Model
Deployment
Fig. 2: AFM Pipeline. During training we leverage counterfactual-inspired transitions by simulating the state evolution with one randomly
sampled action but registering a different random action as the cause. At deployment time, the agent tries to identify the current dynamics
regime and based on that transforms the planned actions a0
t to a corrected or intended action a1
t that AFM considers has a higher chance
of accomplishing the desired state transition.
and consequences of model misalignments. Additionally, di-
rect access to f or the desired action distribution is unavailable
(both are what we are actually trying to learn). Therefore, the
diversity and extent of possible source distributions introduce
the following two critical challenges:
Q1 How can we generate diverse datasets that simulate
dynamics model misalignments while relying solely on
the initial dynamics model f0? This model, learned from
data representing a specific dynamics regime (e.g., flat
office carpet), may not generalize to other regimes (e.g.,
icy sidewalk).
Q2 How can we identify and characterize the current dy-
namics regime using only observed state evolution data,
thereby informing AFM of environmental characteristics
and the predictive reliability of the current model ft?
Fig. 3: Feasible Dynam-
ics Space Intuition. Blue:
State-Action space. White:
Dynamically
feasible
gion. Orange: Approxima-
tion of the feasible region
using the dynamics model.
approach would involve randomly
generating
alongside corresponding planned
actions a0
and using them to
train the flow matching model.
state-action
constraints
introduced
embodiment.
we leverage the initial dynamics
model f0
to approximate the
dynamically
feasible
depicted
constrain
exploration space, ensuring focus
on regions of practical relevance.
f0(s, a) F,
s S, a A,
where F Rn represents the approximated feasible dynamics
region derived from f0.
train g, we simulate how actions influence state transitions
under varying dynamics regimes. Specifically, given an initial
state st S, a planned action a0
t A, and an intended action
t A, we compute the next state st1 under the intended
st1  f0(st, a1
but register the transition as if it resulted from the planned
st1 f0(st, a0
: si S, a0
We find this counterfactual-inspired misaligned dynamics sim-
ulation generates diverse and useful source and target distri-
butions for AFM.
To address Q2, we propose a learned representation ZD
that encodes the current dynamics regime. This representation
is produced by a model that receives the current state st, the
planned action a0
) between the planned state transition s
t1 (Eq. 12) and the
realized state transition st1 (Eq. 13). Note, however, that we
need to consider how to compute et in two distinct scenarios:
During the dataset generation phase, an error value et
is calculated as the difference between the predicted
state transitions when using two actions, a1
t (the action
that results in the observed state transition) and a0
planned action):
et  f0(st, a1
t) f0(st, a0
During operation in the environment, the error is instead
defined as the difference between the observed next state,
(untransformed) action a0
et  st1 ft(st, a0
In addition to the dynamics regime representation model,
a transformed action encoder, EZT : [0, 1]
A Rdim(ZT ),
A Rm symbolizes the action transformations space.
The latent representations ZD and ZT are passed through a
flow model, FM : Rdim(ZD)  Rdim(ZT ) Rdim(S), responsible
for predicting the velocity field u Rdim(at):
u  g(st, a0
EZD(st, a0
t ; ZT ); FM
where   [ZD; ZT ; FM] represents the learnable parame-
ters of the model. Using the collected dataset E, learning the
model g involves optimizing the parameters  to minimize
the conditional flow matching loss LCFM defined in Eq. (8).
t from a0
used by an explicit midpoint ODE solver . This solver
takes incremental steps, transforming a
t into a
(starting
from   0), iteratively refining the action toward a1
C. Continual Learning with AFM
Once the AFM model g is trained, it can be leveraged for
accelerated continual dynamics learning. This process involves
using decision-time planning to select actions that explore
regions of the space most informative for reducing model
regimes.
A crucial aspect is determining when to rely solely on the
learned model ft and when to utilize the AFM model to
mitigate potential model misalignments. This decision process
is formalized as follows. At each iteration, we record the
predicted state transition spred  ft(st1, a0
t1) and the
realized next state sreal. The prediction error et is computed
as the L2 norm of the difference between the realized and
predicted states:
if sreal spred2 < M,
sreal spred,
where M is a predefined threshold for acceptable dynamics
misalignment. From et, a binary model misalignment flag M
is generated:
The flag M dictates whether action transformations are re-
quired. If M transitions from True to False, it implies that
the current model ft aligns with the dynamics, allowing
actions to be executed without transformation. Conversely,
a transition from M  False to M  True indicates a
dynamics regime change. In this case, transformed actions
should be executed to facilitate faster model re-alignment.
Upon detecting a regime change, a representation of the
current dynamics ZD is computed as:
ZD  EZD(st, a0
where EZD encodes the relevant contextual information for
g. The AFM model then maps the planned action a0
transformed action a1
t  ODE SOLVER
t ; ZT ); FM
s  0, e  1, step
which is executed every time step until the model is realigned
(flag M transitions again).
(st1, at1, st) is collected and used to update the model
parameters t1 to t. Note that our approach makes no
assumptions about the specifics of f, as it is agnostic to the
of the dynamics model. Instead, our focus is on generating
more informative actions that facilitate adapting f, thereby
enabling and accelerating continual learning.
V. RESULTS
We evaluate our method on two robotic platforms shown
in Figure 4: the Jackal UGV and the Crazyflie quadrotor.
The experiments assess whether AFM enables faster transi-
tion model alignment under evolving world or embodiment
conditions that affect the robots dynamics. Adapting to these
changes is crucial for successfully completing the tasks.
Fig. 4: Platforms employed for validation of our method and
comparison against the baselines in simulation. (Left) Jackal UGV
. (Right) Crazyflie quadrotor .
A. Baselines
We are interested in comparing with methods that achieve
dynamics model adaptation by learning from the continuous
stream of robot observations during deployment1:
1Approaches that focus on offline adaptation strategies , latent-space
dynamics learning , assume some structure over the task space [4, 35],
or directly learn a policy  fall outside the scope of this paper.
X Coordinate
Y Coordinate
X Coordinate
Y Coordinate
Fig. 5: ETHZ Maps  where the online dynamics learning tasks
took place. The UGV has to reach a series of sparse waypoints (blue
dots) to complete the lap. At certain points the dynamics change
and the robot needs to adapt to the new and unexpected world
characteristics.
stream-x PE : This work introduces strategies to
enable batch-based model-free reinforcement learning
methods to operate in a streaming fashion. We adopt these
ideas and integrate them into the probabilistic ensemble
(PE) framework  to evaluate their performance in a
model-based context. We provide an introduction to the
stream-x family of algorithms in Appendix A.
Online-KNODE-MPC
utilizes
Knowledge-based Neural Ordinary Differential Equations
(KNODE) combined with transfer learning techniques
to achieve continuous dynamics model alignment for
a quadrotor. A significant limitation of this approach
is its reliance on prior knowledge about the system
This requirement can be impractical for systems with
highly nonlinear or poorly understood dynamics. It
also requires maintaining multiple past copies of the
resource constrained platforms.
model predictive control (MPC), serve as an essential
baseline for comparison. These models rely on well-
known principles of physics for each platform. For
the UGV we use the Dubins  model and for the
Quadrotor we adopt the model from . They provide a
strong benchmark for evaluating the benefits of adaptive
dynamics models in tasks requiring high accuracy and
adaptability.
B. Navigating under Evolving Dynamics
Unmanned Ground Vehicle: The Jackal UGV shown
in Figure 4 (left) is a robust wheeled platform commonly
used for research in ground vehicle dynamics and navigation.
This platform allows us to rigorously evaluate the ability of
adaptive dynamics models to handle complex interactions such
as varying traction conditions and actuator degradation during
long-term deployment. Its state st  [xt, yt, t]T S R3
corresponds to the pose and heading of the robot. The control
actions at  [vt, t] A R2 are the linear vt [1, 1] and
angular t [2, 2] velocities.
Environments and Tasks: We conduct our experiments on
two upscaled ETHZ test tracks . These tracks present a
stream-x PE
Average Success Rate
stream-x PE
Average Number of Steps
Fig. 6: Aggregated results for online and non-episodic UGV dynam-
ics learning. Mean  standard deviation over five seeds.
balanced mix of operational challenges, combining smooth,
straightforward sections with tighter curves and more complex
geometry. The task requires the robot to follow a set of sparse
arises when, after reaching 15 of the waypoints, the environ-
ment dynamics undergo a significant shift, necessitating rapid
adaptation for the robot to successfully complete the task. To
further evaluate the system, the dynamics revert to the nom-
inal configuration after 80 of the waypoints are achieved,
introducing another adaptation requirement. The change in
the dynamics regime is achieved via gains intervening the
simulators control variables. Notably, the robot is unaware of
the timing or occurrence of these dynamic transitions, relying
solely on its internal dynamics regime representation to predict
and adapt effectively. The agent is allotted 5,000 steps to
complete each task; exceeding this limit results in the task
being deemed incomplete. We evaluate the models according
to their success rate and average number of steps to reach all
waypoints.
Dynamics Model: As the initial or base model we em-
ploy an ensemble of probabilistic neural networks . The
architecture explicitly models both epistemic and aleatoric
inherent stochasticity. Despite its success in various settings,
the PE baseline traditionally assumes offline or batch training,
limiting its applicability to streaming or continuously evolv-
ing environments. To train f0 we use the Negative Log-
Likelihood (NLL) loss with respect to the predicted mean
and variance 2: L()  log 2, y2  log 2 using
50.000 randomly generated training samples from the Dubins
physics-based model , 256 batch size and the Adam
optimizer with the default parameters in PyTorch 2.5.1 for 32
epochs or until the loss change between epochs was below
1e-3. During deployment we update the models weights
t1 to get the latest dynamics model ft using the same
loss function and optimizer, but only leveraging the latest
transition tuple (i.e., replay buffer size is 1). We use an
ensemble of size 5 with two hidden linear layers, each with
200 units, and Leaky ReLU nonlinearities in between. For
planning with ft we use MPPI  with population size 1000,
TABLE I: UGV Lifelong Dynamics Learning Results in Map 1 with M  1. Results averaged ( standard deviation) over five seeds.
Best results are shown in bold, while the least favorable results are highlighted in gray.
Scenario
AFM (Ours)
stream-x PE
Success Rate
Success Rate
Success Rate
TABLE II: UGV Lifelong Dynamics Learning Results in Map 2 with M  1. Results averaged ( standard deviation) over five seeds.
Best results are shown in bold, while the least favorable results are highlighted in gray.
Scenario
AFM (Ours)
stream-x PE
Success Rate
Success Rate
Success Rate
AFM Model: The action encoder maps the input action and
a time-dependent parameter through two linear layers with 64
hidden units and ELU activation in between. The dynamics
encoder processes the current state, action, and next state
using a similar structure. The flow network combines both
encodings through three linear layers (with 128, 64 and 64
hidden units, respectively) to predict the velocity field. The
model is trained using the Adam optimizer (learning rate 0.01)
with an MSE loss and a batch size of 256. The training data
is generated using the procedure introduced in Section IV in
batches of 2048 samples for 75.000 iterations. Note that the
same AFM model (weights and architecture) was used for
all scenarios, supporting its usefulness across tasks without
needing to assume any kind of task structure.
forms all baselines. Specifically, as shown in Tables I and
is 34.2 higher than the best-performing baseline that has
47.2 success rate. Moreover, our approach requires 15.1
fewer steps on average (3277.95 vs 3865.24), confirming its
superior efficiency.
A detailed breakdown of results reveals that our method
exhibits robustness across diverse environments, maintaining a
high success rate with minimal variance (0.09 for AFM vs 0.14
for the best baseline). In contrast, baseline methods exhibit
significant performance degradation in challenging scenarios,
especially in environments requiring substantial velocity adap-
tation by the robot. In 15 out of 22 scenarios, the best-
performing baseline exhausted the entire allotted time without
successfully completing the task. In contrast, our method
encountered this issue in only 5 out of 22 scenarios. This
means AFM had a 66 improvement in the most challenging
scenarios. Moreover, in none of these five cases did the
other methods consistently complete the task, highlighting the
overall difficulty of the task and the robustness of our approach
in comparison.
A key distinction of our method is that it does not assume
any inherent task structure, making the adaptation process
inherently more challenging. Unlike other approaches that
leverage predefined inductive biases to facilitate learning
[4, 35], our method must generalize effectively without re-
lying on prior knowledge of task distributions. This constraint
necessitates a more robust and flexible adaptation mechanism,
capable of handling diverse and previously unseen scenarios
with minimal assumptions about underlying task properties.
The observed improvements stem from our methods ability
to balance exploration and exploitation based on the misalign-
ment level and correcting actions that seek more informative
regions of the space. Overall, these results highlight the
efficacy of AFM in achieving both higher task success and
greater sample efficiency. More importantly, they represent a
significant step towards the ultimate goal of lifelong robot
dynamics learning, where robots continuously adapt and refine
their models over extended periods in diverse environments.
C. Flying under Evolving Dynamics
(right) is a lightweight, highly agile aerial robot that offers
unique challenges due to its underactuated nature and sensitiv-
ity to aerodynamic and other exogenous effects. Its state st
[xt, yt, zt, xt, yt, zt, qx,t, qy,t, qz,t, qw,t, x,t, y,t, z,t]T
S R13 includes the position, linear velocity, orientation
represented as a quaternion (qx,t, qy,t, qz,t, qw,t), and angular
velocity. The control actions at  [u1,t, u2,t]T A R4
consist of the total thrust u1,t R and the moment vector
Environment and Task: We follow the same settings
and evaluation metrics as . The quadrotor needs to track
circular trajectories with a 2 m radius at target speeds of 0.3,
0.8, 1.0, 1.2, and 1.7 ms. Each simulation runs for 8 seconds
at 500 Hz. To assess adaptiveness, the quadrotors mass is
altered mid-flight: reduced by 50 at 2 seconds and increased
to 133 of the nominal mass at 5 seconds. Performance is
assessed as the MSE between the reference trajectory and the
state of the robot every time step.
Dynamics Model: To verify that AFM is agnostic to the
underlying dynamics model, we leverage the author-provided
weights and implementation of Online-KNODE-MPC  as
the initial model f0. All hyperparmeters are left unchanged.
AFM Model: The model is analogous to the one used for
the UGV platform. The only difference is that we generate
15M training samples using the initial model and the procedure
from Section IV to learn g.
AFM (Ours) Online-KNODE-MPC
Average Tracking MSE
Fig. 7: Quadrotor trajectory
tracking
ing dynamics. Mean  stan-
dard deviation over five exper-
that integrating AFM with the
Online-KNODE-MPC quadro-
tor dynamics model reduces
tracking
significant
expert-designed physics-based
models that already enabled
tracking
provided
the designers assumptions re-
mained valid. Since all set-
tings of Online-KNODE-MPC
remain unchanged except for
its planned actions a0
are refined by AFM, this improvement can be attributed to
AFMs corrections. By guiding the robot toward informative
ing overall performance.
A closer analysis of the results reveals that this reduction
in tracking error is most pronounced in scenarios with rapidly
changing dynamics, where conventional MPC-based physics-
only approaches struggle due to growing model inaccuracies.
The ability of AFM to refine actions ensures that the quadrotor
can better compensate for the multiple unmodeled disturbances
and dynamic shifts it experienced during task execution.
D. Ablation Studies
We conduct ablation experiments on the UGV task using
two variations of the base PE dynamics model: one with frozen
weights (PE) and another where the weights are updated every
timestep using only the previous transition information (Online
PE). The results, summarized in Tables III and IV, show that
in general our method (AFM) outperforms the ablations across
all evaluations, with the Online PE ablation placing second.
For example, in 9 out of 22 scenarios, the best-performing
ablation exhibited the worst efficiency, either by exhausting the
allotted time or taking longer than all other approaches, which
supports AFMs ability to effectively guide lifelong dynamics
learning. Furthermore, when compared with Online PE, AFM
had the best performance in 55 more scenarios.
during the task, where AFM consistently achieves lower loss
and recovers to a nominal loss level after a dynamics regime
change faster than the best ablation. It also reaches new
dynamics regimes faster than the ablations, signaling that it
took it less time to adapt to the new regimes. Additional
results with more scenarios are provided in Figures 10 and
11 in Appendix B. Figure 9 compares the UGVs trajectory
with different continual dynamics learning methods. The plots
clearly show AFMs action adjustments help the robot recover
faster after an intervention.
We further investigate the potential of domain randomiza-
tion (DR) to enhance the performance of AFM. To this end,
we evaluate AFM in a setting where the initial dynamics
model is trained with actions perturbed by 10, simulating
inaccuracies arising from domain shift or real-world uncer-
tainties. Our findings indicate that this AFMDR configuration
yields improved outcomes. Specifically, AFMDR achieves an
additional 2.2 increase in success rate (83.6 vs. 81.4)
and requires, on average, 146.7 fewer steps to reach the goal
VI. DISCUSSION
We have shown that AFM offers a compelling step towards
continual dynamics model learning, addressing challenges of
adaptability and efficiency. We now analyze the key aspects
of our findings and their implications for robotic systems.
AFM is Agnostic to the Dynamics Model Architecture:
Rather than being constrained to a specific model structure,
AFM seamlessly integrates with multiple dynamics models
without requiring modifications to the underlying framework.
TABLE III: Ablation Results. UGV Lifelong Dynamics Learning in Map 1 with M  1. Results averaged ( standard deviation) over
five seeds. Best results are shown in bold, while the least favorable results are highlighted in gray.
Scenario
AFM (Ours)
Online PE
Success Rate
Success Rate
Success Rate
TABLE IV: Ablation Results. UGV Lifelong Dynamics Learning in Map 2 with M  1. Results averaged ( standard deviation) over
five seeds. Best results are shown in bold, while the least favorable results are highlighted in gray.
Scenario
AFM (Ours)
Online PE
Success Rate
Success Rate
Success Rate
Average Loss
Online PE
AFM (Ours)
Time Steps
Average Loss
Online PE
AFM (Ours)
(a) Map 1
Average Loss
Online PE
AFM (Ours)
Time Steps
Average Loss
Online PE
AFM (Ours)
(b) Map 2
Fig. 8: UGV Lifelong Learning Loss Comparison. We present the results on Map 1 (left) and Map 2 (right). Vertical lines indicate the
average step at which a change in the environmental dynamics occurred, thus presenting a new scenario in which the robot needs to learn
to operate. These transitions coincide with the robot entering new regions of the map upon successful completion of prior objectives, thus
earlier transitions are desired. For ease of visualization we only compare against the best ablation and use the moving average of the mean
over five seeds with window size 20. For more details, refer to Appendix B.
This flexibility allows practitioners to select the most suitable
model for a given platform or task, rather than adapting to
restrictive assumptions or architectural constraints. To demon-
strate this versatility, we evaluated AFM on two distinct
robotic platforms: an UGV and a quadrotor, each employing a
different class of dynamics model. For the UGV, we leveraged
a PE-based model, which is entirely data-driven and learns the
system dynamics from experience. In contrast, the quadro-
tor utilized Online-KNODE-MPC, a hybrid approach that
combines prior ODE-based physics models with an adaptive
learning component. Despite the fundamental differences in
these modelsone being purely learned and the other physics-
informedAFM consistently improved performance across
both cases. Specifically, for the UGV, AFM led to a 34.2
increase in task success rate, highlighting its ability to rapidly
correct for discrepancies in learned models. Meanwhile, for
Average Trajectory with intervention vgain  2.0 and
gain  2.0
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  1.0
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  0.5
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  1.0
Online PE
stream-x PE
Average Trajectory with intervention vgain  2.0 and
gain  2.0
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  2.0
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  0.5
Online PE
stream-x PE
Average Trajectory with intervention vgain
gain  1.0
Online PE
stream-x PE
Fig. 9: Trajectory heatmaps with continual dynamics learning in Map 1 (top) and Map 2 (bottom) under multiple interventions.
For clarity, we compute a moving average over 5 seeds with window size 14 and scale the standard deviation by 0.2.
the quadrotor, where the best baseline model already incor-
porated physics-based priors and was highly specialized for
the task, AFM further reduced the tracking error by 6.6
on average. These results underscore AFMs broad applica-
bility in robotics, where the choice of dynamics model often
varies due to platform constraints, task requirements, or data
availability. By enabling effective adaptation regardless of the
underlying model, AFM paves the way for more generalizable
and scalable learning-based control strategies in lifelong robot
autonomy.
Reconciling Exploration and Exploitation: The results pro-
vide strong evidence in favor of AFM as a method to balance
exploration and exploitation in continual learning scenarios.
Robots must navigate the tension between exploiting existing
knowledge to achieve immediate goals and exploring unknown
areas to adapt to a dynamics regime change and enhance
long-term performance. By correcting actions to account for
discrepancies in dynamics, our framework enables the robot to
pursue its objectives while still uncovering useful information
about its environment. Through the assessment of model
alignment has been achieved, allowing the robot to fully
exploit the learned model without requiring continued AFM-
guided exploration. These adaptive trade-offs ensure that the
robot remains productive in the short term without compro-
mising its potential for future improvement.
VII. CONCLUSION AND LIMITATIONS
We have introduced AFM, a framework designed to accel-
erate lifelong robot dynamics model learning. AFM enables
a robot to rapidly align its dynamics model, enhancing both
the speed and safety of task completion. By refining the
actions proposed by a model-based planner, AFM adjusts them
to align more closely with those the robot would choose
using a well-aligned model. This approach not only improves
task performance but also facilitates more efficient learning,
enabling the robot to adapt to changing environments and
dynamics. The method is flexible, model-agnostic, capable of
generating its own training data, and has shown significant
improvements across diverse robotic platforms. This paves the
way for more robust, scalable, and self-sustaining solutions for
lifelong robotic autonomy.
performance over competitive baselines, AFMalong with
the broader challenge of continual dynamics learningstill
requires further advancements. For example, while AFM effec-
tively addresses the exploration phase, further improvements
are needed in the generalization of the dynamics model
itself. Enhancing the models ability to extrapolate beyond
previous observations could accelerate adaptation and reduce
exploration. Another challenge is the data-intensive nature
of training FM models . While AFM mitigates this by
generating high-quality training data (as described in Section
IV), the overall data efficiency could still be improved. Fu-
ture algorithmic advances in this area, should translate into
improved training efficiency for AFM.
ACKNOWLEDGMENTS
This work is supported by the National Science Foundation
under Grant No. 2047169. The authors thank the anonymous
reviewers for their valuable feedback, which helped improve
the quality of this work.
REFERENCES
Brian DO Anderson and John B Moore. Optimal control:
linear quadratic methods. Courier Corporation, 2007.
Bitcraze.
Crazyflie simulation.
Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein,
and Pierre LEcuyer.
The cross-entropy method for
optimization. In Handbook of statistics, volume 31, pages
3559. Elsevier, 2013.
Xinyuan Cao, Weiyang Liu, and Santosh Vempala. Prov-
able lifelong learning of representations. In International
Conference on Artificial Intelligence and Statistics, pages
Kong Yao Chee, Tom Z Jiahao, and M Ani Hsieh.
control framework for aerial robots. IEEE Robotics and
Automation Letters, 7(2):28192826, 2022.
Shuyang Chen and John T Wen.
Adaptive neural
trajectory tracking control for flexible-joint robots with
online learning. In 2020 IEEE International Conference
on Robotics and Automation (ICRA), pages 23582364.
Kurtland Chua, Roberto Calandra, Rowan McAllister,
and Sergey Levine.
Deep reinforcement learning in a
handful of trials using probabilistic dynamics models.
Advances in neural information processing systems, 31,
Charles Ray Cutler. Dynamic matrix control: an optimal
multivariable control algorithm with constraints. Univer-
sity of Houston, 1983.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Tinne Tuytelaars. A continual learning survey: Defying
forgetting in classification tasks. IEEE transactions on
pattern analysis and machine intelligence, 44(7):3366
L. E. Dubins. On curves of minimal length with a con-
straint on average curvature, and with prescribed initial
and terminal positions and tangents. American Journal
of Mathematics, 79(3):497516, 1957. ISSN 00029327,
Mohamed Elsayed, Gautham Vasan, and A Rupam Mah-
Streaming deep reinforcement learning finally
works. arXiv preprint arXiv:2410.14606, 2024.
Manan S Gandhi, Bogdan Vlahov, Jason Gibson, Grady
predictive path integral control: Analysis and perfor-
mance guarantees. IEEE Robotics and Automation Let-
Hani Hagras, Victor Callaghan, and Martin Colley. Pro-
totyping design and learning in outdoor mobile robots
operating in unstructured outdoor environments. IEEE
international robotics and automation magazine, 8(3):
Hani Hagras, Martin Colley, Victor Callaghan, and Mal-
colm Carr-West.
Online learning and adaptation of
autonomous mobile robots for sustainable agriculture.
Autonomous Robots, 13:3752, 2002.
Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2:
arXiv preprint arXiv:2310.16828, 2023.
Lorenzo Jamone, Bruno Damas, Jose Santos-Victor, and
Atsuo Takanishi.
Online learning of humanoid robot
kinematics under switching tools contexts. In 2013 IEEE
international conference on robotics and automation,
pages 48114817. IEEE, 2013.
Tom Z Jiahao, Kong Yao Chee, and M Ani Hsieh.
Online dynamics learning for predictive control with an
application to aerial robots.
In Conference on Robot
Rudolf Emil Kalman et al. Contributions to the theory of
optimal control. Bol. soc. mat. mexicana, 5(2):102119,
Khimya Khetarpal, Matthew Riemer, Irina Rish, and
Doina Precup. Towards continual reinforcement learning:
A review and perspectives. Journal of Artificial Intelli-
gence Research, 75:14011476, 2022.
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra
Malik. Rma: Rapid motor adaptation for legged robots.
arXiv preprint arXiv:2107.04034, 2021.
Ashish Kumar, Zhongyu Li, Jun Zeng, Deepak Pathak,
Koushil Sreenath, and Jitendra Malik.
Adapting rapid
motor adaptation for bipedal robots. In 2022 IEEERSJ
International Conference on Intelligent Robots and Sys-
tems (IROS), pages 11611168. IEEE, 2022.
Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak
for generalization in model-based reinforcement learning.
In International Conference on Machine Learning, pages
Timothee Lesort, Vincenzo Lomonaco, Andrei Stoian,
Rodrguez. Continual learning for robotics: Definition,
lenges. Information fusion, 58:5268, 2020.
Yichao Liang, Kevin Ellis, and Joao Henriques. Rapid
motor adaptation for robotic manipulator arms.
Proceedings of the IEEECVF Conference on Computer
Vision and Pattern Recognition, pages 1640416413,
Alexander Liniger, Alexander Domahidi, and Manfred
Morari. Optimization-based autonomous racing of 1:43
scale rc cars. Optimal Control Applications and Methods,
Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta
guide and code, 2024. URL
Franziska Meier, Daniel Kappler, Nathan Ratliff, and
Stefan Schaal. Towards robust online inverse dynamics
learning. In 2016 IEEERSJ International Conference on
Intelligent Robots and Systems (IROS), pages 40344039.
Ihab S Mohamed, Kai Yin, and Lantao Liu. Autonomous
navigation of agvs in unknown cluttered environments:
log-mppi control strategy. IEEE Robotics and Automa-
tion Letters, 7(4):1024010247, 2022.
Anusha Nagabandi, Chelsea Finn, and Sergey Levine.
learning
Continual
adaptation
model-based
preprint
Duy Nguyen-Tuong and Jan Peters. Incremental online
sparsification for model learning in real-time robot con-
trol. Neurocomputing, 74(11):18591867, 2011.
Peter Pastor, Ludovic Righetti, Mrinal Kalakrishnan, and
Stefan Schaal.
Online movement adaptation based on
previous sensor experiences. In 2011 IEEERSJ Inter-
national Conference on Intelligent Robots and Systems,
pages 365371. IEEE, 2011.
Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and
Jitendra Malik. In-hand object rotation via rapid motor
adaptation.
In Conference on Robot Learning, pages
Jacques Richalet, Andre Rault, JL Testud, and J Papon.
Model predictive heuristic control. Automatica (journal
Clearpath Robotics.
Jackal ugv - small weatherproof
robot - clearpath., 2025. URL
comjackal-small-unmanned-ground-vehicle.
Paul Ruvolo and Eric Eaton. Ella: An efficient lifelong
learning algorithm. In International conference on ma-
chine learning, pages 507515. PMLR, 2013.
Dale E Seborg, Thomas F Edgar, Duncan A Mellichamp,
and Francis J Doyle III. Process dynamics and control.
John Wiley  Sons, 2016.
Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong
machine learning systems: Beyond learning algorithms.
In 2013 AAAI spring symposium series, 2013.
Kyle Stachowicz, Lydia Ignatova, and Sergey Levine.
Lifelong autonomous improvement of navigation foun-
dation models in the wild. In 8th Annual Conference on
Robot Learning, 2024.
Endre Suli and David F Mayers.
An introduction to
numerical analysis. Cambridge university press, 2003.
Yu Sun, Wyatt L Ubellacker, Wen-Loong Ma, Xiang
Masayoshi Tomizuka, Koushil Sreenath, and Aaron D
Ames. Online learning of unknown dynamics for model-
based controllers in legged locomotion. IEEE Robotics
and Automation Letters, 6(4):84428449, 2021.
Richard S Sutton and Andrew G Barto. Reinforcement
Mathias Thor and Poramate Manoonpong. Error-based
learning mechanism for fast online adaptation in robot
motor control.
IEEE transactions on neural networks
and learning systems, 31(6):20422051, 2019.
Sebastian Thrun.
Lifelong learning algorithms.
Learning to learn, pages 181209. Springer, 1998.
APPENDIX
A. Streaming Model-Free Reinforcement Learning
Streaming reinforcement learning methods aim to process
data one sample at a time without storing previous samples,
thereby eliminating the use of a replay buffer. This is partic-
ularly relevant for a robot that needs to incorporate the latest
information into its model of the world. Traditional methods
rely on a dataset of transitions which is then used to fine-
tune the model. However, this is not practical nor desirable
for a robot, since that would require stopping its operation
while training with the whole dataset. Furthermore, additional
engineering challenges would emerge, such as deciding when
to stop to fine-tune or how to store all the data in a resource-
constrained platform.
To achieve streaming learning in environments with non-
stationary data streams several challenges need to be ad-
dressed. For example, instability due to issues such as large
per-sample gradient fluctuations, activation non-stationarity,
and improper data scaling. We summarize next how the
stream-x family of algorithms addresses this :
1) Sample Efficiency: To enhance sample efficiency, the
method employs sparse initialization and eligibility traces:
Sparse Initialization: Most weights are initialized to zero
while retaining a sparse set of non-zero weights, reducing
interference and improving learning robustness.
Eligibility Traces: Accumulating traces facilitate credit
assignment over time. The trace updates recursively using
zt  zt1  v(St, wt), where  and  are the
discount and trace parameters, respectively.
2) Update Stability: Step sizes are stabilized using a mod-
ified backtracking line search that bounds updates, avoiding
overshooting errors in single-sample learning. An efficient
step-size control mechanism mitigates computational overhead
associated with traditional backtracking.
3) Normalization and Scaling: Layer normalization is ap-
plied across network layers to maintain stable activation distri-
butions under non-stationary data. The normalization scale and
bias parameters are not learned. Additionally, observations and
rewards are scaled dynamically using unbiased mean-variance
estimates to ensure effective learning despite unbounded state
These innovations collectively enable streaming deep re-
inforcement learning to achieve performance comparable to
batch learning methods, despite the constraints of one-sample
learning and non-stationary environments.
B. Additional Experimental Results
Loss Comparison: Figures 10 and 11 illustrate the loss
comparison for both maps across multiple scenarios. Notably,
our method consistently demonstrates accelerated learning
of the environments new dynamics. This is evident in two
key observations: 1) The loss curves associated with our
method generally exhibit lower values and exhibit fewer abrupt
vertical lines, signifying transitions between dynamic regimes,
typically appear earlier in the curves generated by our method.
This implies that the agent employing AFM reaches objec-
tives more rapidly and transitions to subsequent goals more
the underlying dynamics model.
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Time Steps
Average Loss
Online PE
AFM (Ours)
Fig. 10: Loss Comparison for Map 1. Vertical lines indicate the average step at which a change in the environmental dynamics occurred,
thus presenting a new scenario in which the robot needs to learn to operate. These transitions coincide with the robot entering new regions
of the map upon successful completion of prior objectives, thus earlier transitions are desired. For ease of visualization we only compare
against the best baseline and use a moving average with window size 20. We present the mean loss over five seeds.
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Average Loss
Online PE
AFM (Ours)
Time Steps
Average Loss
Online PE
AFM (Ours)
Fig. 11: Loss Comparison for Map 2. Vertical lines indicate the average step at which a change in the environmental dynamics occurred,
thus presenting a new scenario in which the robot needs to learn to operate. These transitions coincide with the robot entering new regions
of the map upon successful completion of prior objectives, thus earlier transitions are desired. For ease of visualization we only compare
against the best baseline and use a moving average with window size 20. We present the mean loss over five seeds.
