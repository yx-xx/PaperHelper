=== PDF文件: V-HOP Visuo-Haptic 6D Object Pose Tracking.pdf ===
=== 时间: 2025-07-22 09:41:27.126046 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Hongyu Li1, Mingxi Jia1, Tuluhan Akbulut1, Yu Xiang2, George Konidaris1, and Srinath Sridhar1
1Brown University
2The University of Texas at Dallas
AbstractHumans naturally integrate vision and haptics for
robust object perception during manipulation. The loss of either
modality significantly degrades performance. Inspired by this
multisensory integration, prior object pose estimation research
has attempted to combine visual and haptictactile feedback.
Although these works demonstrate improvements in controlled
environments or synthetic datasets, they often underperform
vision-only approaches in real-world settings due to poor gen-
eralization across diverse grippers, sensor layouts, or sim-to-real
environments. Furthermore, they typically estimate the object
pose for each frame independently, resulting in less coherent
tracking over sequences in real-world deployments. To address
these limitations, we introduce a novel unified haptic represen-
tation that effectively handles multiple gripper embodiments.
Building on this representation, we introduce a new visuo-haptic
transformer-based object pose tracker that seamlessly integrates
visual and haptic input. We validate our framework in our
dataset and the Feelsight dataset, demonstrating significant per-
formance improvement on challenging sequences. Notably, our
method achieves superior generalization and robustness across
novel embodiments, objects, and sensor types (both taxel-based
and vision-based tactile sensors). In real-world experiments,
we demonstrate that our approach outperforms state-of-the-art
visual trackers by a large margin. We further show that we
can achieve precise manipulation tasks by incorporating our
real-time object tracking result into motion plans, underscor-
ing the advantages of visuo-haptic perception. Project website:
I. INTRODUCTION
Accurately tracking object poses is a core capability for
robotic manipulation, and would enable contact-rich and dex-
terous manipulations with efficient imitation or reinforcement
learning [68, 31, 23]. Recent state-of-the-art object pose
estimation methods, such as FoundationPose , have sig-
nificantly advanced visual tracking by leveraging large-scale
datasets. However, relying solely on visual information to per-
ceive objects can be challenging, particularly in contact-rich or
in-hand manipulation scenarios involving high occlusion and
rapid dynamics.
The cognitive science findings show that humans naturally
integrate visual and haptic information for robust object per-
ception during manipulation [46, 12, 28]. For instance, Gordon
et al.  demonstrated that humans use vision to hypothesize
object properties and haptics to refine precision grasps. The
human sense of touch consists of two distinct senses [42, 6]:
the cutaneous sense, which detects stimulation on the skin
and dynamic body posture. This integration, known as haptic
Fig. 1: Visuo-haptic sensing for 6D object pose tracking. We
fuse egocentric visual and haptic sensing to achieve accurate
real-time in-hand object tracking.
nipulate objects . In robotics, analogous capabilities are
achieved through tactile sensors (cutaneous sense) and joint
sensors (kinesthesis) .
Drawing inspiration from these human capabilities, re-
searchers have explored the integration of vision and touch
in robotics for decades. As early as 1988, Allen  proposed
an object recognition system that combined these modalities.
More recently, data-driven approaches have emerged to tackle
object pose estimation using visuo-tactile information [32, 54,
face two major barriers that hinder their broader applicability:
(i) Cross-embodiment: Most approaches overfit specific grip-
pers or tactile sensor layouts, limiting their adaptability. (ii)
Domain generalization: Compared to visual-only baselines,
visuo-tactile approaches struggle to generalize, hindered by
insufficient data diversity and model scalability. Moreover,
they typically process each frame independently, which can
result in less coherent object pose tracking over sequences
in real-world deployments. As a result, existing methods are
challenging to deploy broadly and often require significant
customization to specific robotic platforms.
To address these challenges, we propose V-HOP (Fig. 1):
a two-fold solution for generalizable visuo-haptic 6D object
pose tracking. First, we introduce a novel unified haptic
representation that facilitates cross-embodiment learning. We
consider the combination of tactile and kinesthesis in the
form of a point cloud, addressing a critical yet often over-
looked aspect of visuo-haptic learning. Second, we propose
a transformer-based object pose tracker to fuse visual and
haptic features. We leverage the robust visual prior captured
by the visual foundation model while incorporating haptics. V-
HOP accommodates diverse gripper embodiments and various
objects and generalizes to novel embodiments and objects.
We build a multi-embodied dataset with eight grippers using
the NVIDIA Isaac Sim simulator for training and evaluation.
Compared to FoundationPose , our approach achieves 5
improvement in the accuracy of object pose estimation in terms
of ADD-S  in our dataset. These results highlight the
benefit of fusing visual and haptic sensing. In the FeelSight
optimization-based visuo-tactile object pose tracker, achieving
a 32 improvement in the ADD-S metric and ten times faster
run-time speed. Finally, we perform the sim-to-real transfer
experiments using Barrett Hands. Our method demonstrates
remarkable robustness and significantly outperforms Founda-
When integrated into motion plans, our approach achieves 40
higher average task success rates. To the best of our knowl-
to demonstrate robust generalization across both taxel-based
tactile sensors (e.g., Barrett Hand) and vision-based tactile
sensors (e.g., DIGIT sensors), as well as on novel embodiments
and objects.
In conclusion, our contributions to this paper are two-fold:
1) Unified haptic representation: we introduce a novel
haptic representation, enabling cross-embodiment learn-
ing and addressing the cross-embodiment challenge by
improving adaptability across diverse embodiments and
objects.
2) Visuo-haptic transformer: We present a transformer
model that integrates visual and haptic data, improving
pose tracking consistency and addressing the domain
generalization challenge.
II. BACKGROUND
In this section, we first define the problem formally and
then review existing haptic representations and our proposed
unified representation.
A. Problem Definition
We tackle the model-based visuo-haptic 6D object pose
tracking problem, assuming access to:
Visual observations: An RGB-D sensor observes the
object in the environment.
Haptic feedback: The object is manipulated by a rigid
gripper equipped with tactile sensors.
Our approach takes the following as input:
1) The CAD model Mo of the object.
2) A sequence of RGB-D images O  {Oi}t
each observation Oi  [Ii, Di] includes an RGB image
Ii and a depth map Di.
3) An initial 6D pose T0  (R0, t0) SE(3), where
R0 SO(3) is 3D rotation and t0 R3 is 3D trans-
In practice, the ground-truth initial pose T0 is hard to obtain
and can only be estimated through pose estimation [72, 62,
bT0  T0 in the following. At each timestep i, our model
estimates the object pose bTi given all the inputs with the
initial pose being the estimate bTi1 at the previous timestep.
The above inputs are the standard inputs from the model-
based visual pose tracking problem [66, 7], while the inputs
below will serve our haptic representation and will be detailed
in later sections.
4) Gripper description in Unified Robot Description Format
5) Gripper joint positions j  {j1, j2, . . . , jDoF }.
6) Tactile sensor data S, including Positions Sp and read-
ings Sr of tactile sensors, which will be formally defined
in the next section.
7) Transformation between the camera and the robot frames
obtained through hand-eye calibration .
B. Haptic Representation
The effectiveness of haptic learning hinges on its repre-
sentation. Prior approaches using raw value , image ,
or graph-based [75, 33, 50] representations often struggle
to generalize across diverse embodiments. For instance, Wu
et al.  and Guzey et al.  project tactile signals from
Xela sensors into a 2D image format. While this allows
efficient processing with existing visual models, extending
the method to different grippers or sensor layouts proves
challenging. Similarly, Li et al.  and Rezazadeh et al.
employ graph-based mappings, where taxels are represented
as vertices. However, variations in sensor layouts result in
different graph distributions, creating significant generalization
In contrast, we adopt a point cloud representation, which
naturally encode 3D positions and can flexibly accommodate
multi-embodiments. We broadly classify tactile sensors into
taxel-based and vision-based. A more comprehensive review
on tactile sensors can be found at . Below, we outline how
they are converted into point clouds in prior works [8, 54, 64,
13], paving the way for our unified framework.
Taxel-based Sensors. The tactile data is defined as S
taxels. The tactile data consists of S  (Sp, Sr):
Positions (Sp): Defined in the gripper frame and trans-
formed into the camera frame using forward kinematics.
Readings (Sr): Capturing contact values. Readings are
commonly binarized into contact or no-contact states [78,
The set of taxels in contact is:
Sc  {si S  Sr(si) > },
and the corresponding tactile point cloud Sp,c is defined as
Vision-based sensors. For vision-based tactile sensors [29,
Positions (Sp): Sensor positions in the camera frame,
similar to taxel-based.
Images (SI): Capturing contact states using regular RGB
image representation. Using the tactile depth estimation
model [3, 54, 26, 53, 52, 2], we can convert SI into tactile
point cloud Sp,c.
Yet we are not the first to employ point cloud representations
for tactile learning, prior works [8, 54, 64, 13] focus on a
single type 
