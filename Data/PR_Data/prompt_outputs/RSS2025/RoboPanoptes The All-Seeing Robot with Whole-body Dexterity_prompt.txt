=== PDF文件: RoboPanoptes The All-Seeing Robot with Whole-body Dexterity.pdf ===
=== 时间: 2025-07-22 15:46:55.594717 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：The All-seeing Robot with Whole-body Dexterity
Xiaomeng Xu1
Dominik Bauer2
Shuran Song1,2
1Stanford University
2Columbia University
Whole-body Dexterity
Whole-body Vision
21 Cameras
Diverse Objects
Environment  Tasks
c) Unbox in Narrow Space
d) Multi-step Stowing
a) Sweep Small Objects
b) Move Large Objects
Fig. 1: RoboPanoptes, a robot system that utilizes all of its body parts to sense and interact with its environment. Whole-body vision (via
21 cameras distributed over the robots body) enables whole-body dexterity, with the robot utilizing its entire surface for manipulation. This
design enables new robot capabilities such as a) simultaneously sweeping multiple small objects, b) moving large objects using whole-body
AbstractWe present RoboPanoptes1, a capable yet practical
robot system that achieves whole-body dexterity through whole-
body vision. Its whole-body dexterity allows the robot to utilize
its entire body surface for manipulation, such as leveraging
multiple contact points or navigating constrained spaces. Mean-
the robots surface to provide comprehensive, multi-perspective
visual feedback of its own and the environments state. At its
learns complex manipulation skills directly from human demon-
cameras while maintaining resilience to sensor failures. Together,
these design aspects unlock new capabilities and tasks, allowing
RoboPanoptes to unbox in narrow spaces, sweep multiple or
oversized objects, and succeed in multi-step stowing in cluttered
1Argos Panoptes is a many-eyed giant in Greek mythology, with panoptes
meaning all-seeing. RoboPanoptes is the many-eyed, all-seeing robot.
ciency. Results are best viewed on
I. INTRODUCTION
The vast majority of todays robot manipulation systems
focus on controlling the robots end-effector (e.g., a hand or
gripper) using a centralized camera (e.g., an environment- or
wrist-mounted camera). Despite being the common practice,
both designs unnecessarily limit a robots capability. In this
body dexterity through whole-body vision.
By whole-body dexterity, we mean the manipulation skills
that utilize the robots entire body surface to interact with the
environment. For example, the robot uses all its links to sweep
many objects at once (Fig. 1 a), instead of using only the end-
effector to push objects one by one. To demonstrate the benefit
of whole-body dexterity, we design a high degree-of-freedom
allows high motion flexibility and adaptability.
to coordinate all its movable body parts, understand their
spatial relationships with respect to the environment, and plan
their interactions therewith. Attaining the required level of
perception is challenging with a single centralized camera,
regardless of its placement. To address this challenge, we intro-
duce the concept of whole-body vision  an integrated camera
system that is distributed on the robots surface, providing
comprehensive visual feedback for whole-body dexterity.
robotic manipulation systems, including:
Multi-contact manipulation using all body parts, such as
simultaneously sweeping multiple objects or moving an
object larger than the robot itself (Fig. 1 a, b).
Manipulation in constrained spaces, such as unboxing a
container from its side opening or organizing objects in a
cluttered environment (Fig. 1 c, d).
Omni-directional manipulation, allowing the robot to
perceive and manipulate its environment from any angle
without reorienting its body, thereby increasing its efficiency
in all tasks by maximizing the share of task-related motion.
create challenges for controlling such a system, and con-
Control complexity: Programming complex behaviors for
such a high degree-of-freedom system is demanding. Previous
approaches often rely on manually designed state represen-
tations and carefully tuned motion patterns (e.g., sinusoid
functions [21, 40]) for every task, which are not transferable
for new task requirements. We drastically simplify this process
by introducing an intuitive demonstration interface and an
efficient policy learning algorithm, allowing even novice users
to quickly and easily teach the robot new manipulation skills.
Learning efficiency: For whole-body vision, numerous
cameras are distributed on the robots surface and move in tan-
dem with the robot. Consequently, the policy must efficiently
process this complex and high-dimensional input space to infer
the appropriate actions. We address this challenge through a
novel whole-body visuomotor policy architecture based on
diffusion transformers . Cross-attention captures semantic
correlations between multi-view images and robot actions,
while a view-dependent positional encoding preserves crucial
spatial information. As we demonstrate for diverse tasks, this
architecture enables efficient learning of complex behaviors
from limited data.
System robustness: Multi-sensor systems often become
increasingly vulnerable to failure as the number of sensors
out-of-distribution observations that compromise the overall
system performance. To address this challenge, we develop a
Blink Training strategy that randomly masks camera inputs
during the training process. By simulating these unpredictable
sensor disruptions  analogous to random blinking of the
robots cameras  we are able to train policies that remain
reliable even under partial sensor failure or latency.
In summary, our primary contribution is the RoboPanoptes
through whole-body vision. Specifically, it is enabled by:
A modular hardware design that integrates distributed vi-
sion and actuation in a scalable and reconfigurable system.
An intuitive demonstration interface that streamlines the
acquisition of complex whole-body manipulation behaviors,
dramatically reducing the barrier to teaching new skills.
A whole-body visuomotor policy that efficiently processes
whole-body visual input through cross-attention transform-
ers and view-dependent positional encoding, while improv-
ing resilience to sensor failures through blink training.
hardware
training
publicly
available
stanfordRoboPanoptes.
II. RELATED WORK
By discussing prior work on designing high-DoF robots, on
leveraging them for whole-body manipulation and the closely
related challenge of whole-body sensing, we illustrate the
novel capabilities of RoboPanoptes with regards to achieving
whole-body dexterity by exploiting whole-body vision.
A. High Degree-of-Freedom Robot Systems
High-DoF robotic systems are characterized by their ability
to achieve complex configurations and motions through a large
number of controllable joints. This hyper-redundancy enables
them to emulate their biological role models  such as snakes,
vines [6, 1], and elephant trunks   to perform tasks in
confined and complex environments. Snake robots [21, 40,
such high-DoF systems. Leveraging hyper-redundancy, they
are able to achieve undulating, side-winding, and tumbling
movement strategies. While snake robots demonstrate these
impressive locomotion capabilities, their use in manipulation
remains limited, often restricted to simple grasping [10, 24,
based path planning with manual primitive designs . In
by incorporating whole-body vision, intuitive data collection
manipulation across diverse and complex tasks.
B. Whole-body Manipulation
Stasse and Righetti  define whole-body manipulation
as requiring high redundancy, a floating base, and multiple
contact points. Previous whole-body manipulation systems
typically consist of a quadruped with an arm mounted on
top [23, 16, 11, 14], or a bimanual system with legs or a mobile
base [29, 12, 32, 42, 33, 37]. However, while existing systems
optimize body configurations to achieve desired end-effector
contact to the end-effector alone. In contrast, whole-body
dexterity actively leverages all available body surfaces for
environmental contact and manipulation (with or without a
movable base). Moreover, the sensing capabilities of existing
whole-body manipulation systems are limited. By commonly
using a single environment-, wrist- or head-mounted camera,
occlusions are unavoidable and hinder their ability to fully
observe the state of cluttered environments and the robots
state relative to it.
C. Whole-body Sensing
Prior work on whole-body sensing has explored range,
ception and interaction, addressing challenges in collision
sensing enables collision avoidance and spatial awareness
in manipulation systems. Tanaka et al.  implement a
semi-autonomous collision avoidance system for snake robots
using range sensors distributed along the body, enabling it
to navigate complex terrains. Qi et al.  further develop a
laser-ranging sensor ring design for human-robot interaction.
Kim et al.  employ Time-of-Flight sensors distributed on
a humanoid robot for collision-free motion planning. Whole-
body force sensing, as implemented by Kollmitz et al. ,
leverages force-torque sensors embedded within mobile robots
to achieve compliant motion. Tactile sensors allow robots to
interact delicately with the environment by detecting pressure,
robot  uses various contact and force sensors throughout
the robots chest, arm, and hand to enable different resolutions
of contact signals across the body, which is crucial for contact-
rich whole-body manipulation tasks.
While effective in detecting nearby obstacles and con-
remains limited using range, tactile, and force sensing. In
environments beyond contact or geometry. Most related to
our work, Yamaguchi and Atkeson  propose the idea of
whole-body vision with an optical skin system; combining
embedded cameras with transparent, flexible skin. However,
their implementation is constrained by high data transmis-
sion requirements and integration complexity such as bulky
Raspberry Pi cameras and processors. In contrast, we fully
implemented an effective and robust whole-body vision setup
capable of autonomously performing dexterous manipulation
skills enabled by our modular hardware design ( IV), intuitive
data collection interface ( V), whole-body visuomotor policy
( VI), as well as several practical considerations during
system implementation ( VII). Through our experiments, we
demonstrate that the whole-body vision data is sufficient for
imitation learning of complex manipulation tasks ( VIII).
III. DESIGN OBJECTIVES
We design RoboPanoptes to be capable yet practical; able
to perform a wide range of whole-body dexterity tasks, while
remaining easy to build, extend, and deploy.
Motion flexibility, enabling the system to leverage all body
parts for object manipulation and collision avoidance.
Visual coverage, minimizing occlusions caused by the en-
vironment or the robots body itself.
Rapid adaptability to new tasks and applications without
explicit reprogramming, instead directly leveraging human
demonstration.
The practicality of RoboPanoptes is reflected in its:
Robustness to sporadic sensor delays and failures; an issue
that compounds as the number of cameras increases.
Modular design, allowing easy customization with varying
degrees of freedom (DoFs) and camera configurations.
the-shelf and 3D printed components, enabling researchers
to easily replicate it.
We achieve these objectives through a series of careful
hardware design decisions  IV, an intuitive data collection
interface  V, and robust visuomotor policy learning  VI.
In  VII, we also discuss important practical considerations
for designing whole-body vision systems.
LED light
5 on head,
2 on body module
Actuator
Wire Fixtures
Rigid Hook
no actuation
a) Body Module
b) Head Module
Fig. 2: Modular Hardware Design including a) a body module
consisting of an actuator, two cameras, and wire fixtures, as well
as b) a head module with five cameras and an LED light.
IV. MODULAR HARDWARE DESIGN
RoboPanoptes hardware consists of nine modular body
units and one head unit. The design objectives achieved
thereby are extendability to higher DoFs, comprehensive visual
Vision-Actuation Body Module. Each body module (shown
in Fig. 2 b) contains an actuator, two cameras, and fixtures for
wire management. The actuator is a standard DYNAMIXEL
XC330-M288-T servo motor with one rotational degree
of freedom. This motor is small (20  34  26 mm) and
lightweight (23 g), with a 288.4:1 gear ratio. These motors
are daisy-chained together; a single wire powers the robot.
We control them with current-based position control com-
municated through the TTL protocol. The body modules are
chained together, with neighboring links DoF perpendicular to
each other, as shown in Fig. 1. Each motor can freely rotate up
to 90, allowing flexible and complex motions of the chain
of body modules.
To maximize the visual coverage while minimizing the
number of cameras, we mount two cameras on each link
one on the front and one on the back  with cameras pointing
outwards and perpendicular to the links DoF. Using eight
body modules, the whole robot is thus equipped with 16
cameras  four cameras on each of its four sides , enabling
omnidirectional (all-seeing) vision. We design the camera
slots on the links to hold the cameras mechanically stable to
avoid relative movement w.r.t. the link, and caging the cameras
inside the body to avoid accidental damage. We use the
Adafruit Ultra Tiny USB Camera with a GC0307
image sensor, characterized by an extremely small dimension
(8254.5 mm) that easily fits on the small links. It offers
a sufficient resolution of 640  480 and a 50field of view.
the camera boards JST connector to a USB-A port, and the
cameras cannot be daisy-chained. If not managed properly, the
resulting large number of cables would constrain the robots
motion and occlude camera views. To address this, we design
wire fixtures that are incorporated on each link. These slots
allow wires to pass through on the remaining two sides of the
module that are not holding cameras, ensuring that the wires
will not occlude the cameras.
Head Module. The final body module is connected to a head
module that acts as an end-effector to unlock more manipula-
tion skills and provides additional perception capabilities. We
design this module in the shape of a parallel hook that enables
the picking and pulling of small extrusions, e.g., hooking and
pulling the drawers handle in Fig. 1 b. Even though the
robots body modules observe all four sides around it, some
blind angles (facing top and down) remain, which we address
through the head modules camera mounting strategy. Four
downwards-facing cameras are placed on the lower part of the
is placed in the center of the head and pointing upwards.
upward camera, illuminating the area that the head module
navigates towards and that its hooks might interact with. This
allows the robot to visually perceive even narrow dark spaces,
e.g., inside the box in Fig. 1 a. Alternatively, this head design
may be flexibly swapped with other types of end-effectors for
specific tasks.
V. DATA COLLECTION INTERFACE
Directly programming such a high-DoF robot to execute
complex (and natural) behaviors is difficult. Inverse kinematics
and standard joystick controls, commonly used in manipu-
lation systems, can only control end-effector motions and
are hard to solve for high-DoF systems. For locomotion of
leader robot
follower robot
observations
Fig. 3: Data Collection Interface. The operator uses both hands to
control the leader robot, whose joint angles are sent to the follower
robot in real-time as position targets. The joint angles of the leader
robot are recorded as target actions, while the images and joint angles
of the follower robot are recorded as observations.
similarly structured high-DoF robots, previous approaches use
carefully coded motion patterns (e.g., sinusoid functions [21,
While RL exploration offers an alternative, it can only be
safely and scalably implemented in simulation, introducing
significant sim2real observation gaps.
as shown in Fig. 3. The leader and the follower robot have the
exact same structure. During teleoperation, torque is disabled
for the leader robot while being enabled for the follower.
To demonstrate a task, a human operator uses both hands to
move the leader robot. The leaders joint positions are sent
to the follower in real time, allowing it to mirror the leader
using PID position control at a control rate of 30 Hz. During
the demonstration, images from all cameras and robot joint
positions are recorded at 10 Hz.
VI. WHOLE-BODY VISUOMOTOR POLICY
Using the collected demonstrations, we can train a whole-
body visuomotor policy that infers whole-body actions (i.e.,
nine joint angle sequences) given whole-body vision (i.e.,
images from 21 cameras). Compared to a common manipula-
tion system, RoboPanoptes needs to handle significantly more
complex observation spaces due to the following factors:
Many cameras: The policy needs to efficiently extract task-
relevant semantic information from the raw pixels captured
by a large number of cameras.
Non-stationary cameras: As the robot moves, the cameras
are in constant motion. This requires the policy to under-
stand the dynamic spatial relations between the cameras,
the robot and its environment.
Unreliable cameras: A system of many cameras is prone
to unpredictable failures and delays, requiring the policy to
be robust to such disturbances.
Whole-body Actions
(9 joint angles At for T steps)
9 Joint Angles Jt
Diffusion Transformer
T Noise Tokens (0,I)
Diffusion Transformer
Conditional Action Diffusion
T Whole-body Action Tokens
K Denoising         Steps
Vision Encoder
Camera-pose
Encoding (MLP)
Fwd. Kinematics
21 Camera Poses Pt
21 Whole-body     Vision Tokens
as condition
Action Decoder (MLP)
proprioception
diffusion step
Whole-body Vision (21 RGB Images It)
Fig. 4: Whole-body Visuomotor Policy leverages whole-body vision for whole-body dexterity. Left: The current robot and environment
state is observed via RoboPanoptes 21 cameras and its 9 joints angles, converted to 21 camera poses using forward kinematics. Each image
(green) is represented by the class token of a vision foundation model. Each camera pose (purple) is embedded using our view-dependent
positional encoding. The concatenation of each cameras image and pose tokens yields a whole-body vision token; 21 in total. Middle: Our
whole-body visuomotor policy consumes these vision tokens, proprioception, and denoising-step tokens as condition via cross attention. We
diffuse T whole-body dexterity tokens (blue), each corresponding to an action time step. Right: Per time step, we project the predicted
dexterity token to the 9 joint angles to be achieved by the dexterity action.
As shown in Fig. 4, we base our policy design on Diffu-
sion Policy , which infers the robots joint-space actions
through denoising diffusion, conditional on previous observa-
tions. Concretely, at each time step t, the policy takes the
previous To observations Ot as conditional input. It predicts
Tp actions At, of which Ta Tp are executed on the robot.
In our implementation, we set the observation horizon To  2,
the action prediction horizon Tp  16, and the action execution
horizon Ta  8. The observations Ot  {It,Pt,Jt}, taken from
the follower robot during training, consist of the previous RGB
observations It  {itTo1,...,it} NTo216404803
the corresponding joint angles Jt  { jtTo1,..., jt} RTo9.
The actions, taken from the leader robot during training, are
the following joint angles At  {at1,...,atTp} RTp9.
In the following, we elaborate on the policy design details
and outline how these key design decisions address the afore-
mentioned challenges.
Coordinating moving cameras with view-dependent posi-
tional encoding. The robots motions continuously change the
camera poses and, thereby, the region of the environment each
camera is observing. The observation space of this dynamic
multi-view camera system is thus significantly larger than that
cameras and make policy learning more efficient, we employ a
view-dependent positional encoding strategy. Specifically, we
compute the 6D camera poses in the base frame using the
forward kinematics based on the current joint angles. Each
camera pose is represented by a 3-dimensional vector for
position and a 6-dimensional vector for orientation, using the
first two columns of rotation matrix . The position and
orientation vectors are each projected to a 192-dimensional
vector using fully-connected linear layers. Our experiments
demonstrate that this additional positional encoding enables
the policy to have better data efficiency and 3D awareness.
Learning semantic correspondences using a multi-view
cross-attention
transformer. To efficiently extract task-
relevant information from many cameras, we employ a pre-
trained vision encoder and a multi-view cross-attention trans-
former to learn the task-relevant semantic correspondence
between different camera observations. To this end, we exploit
pretrained vision foundation models such as CLIP  or
DINO  that enable advanced semantic understanding
and visually-complex robot manipulation tasks [5, 43]. Specif-
and apply color jitter augmentation. We batch the 21 images
and feed them into the (frozen) ViT-B16  encoder of a
CLIP-pretrained model to predict a 768-dimensional class-
token feature for each image. These per-image features are
projected to 384-dimensional vectors and concatenated with
the corresponding cameras position and orientation embed-
concatenation of these whole-body vision tokens with the
projected joint angles yields a (21  9)  768-dimensional
representation of the robot and environment state at time t.
We adopt a time-series diffusion transformer architec-
ture . Noisy actions Ak
t (where k is the denoising step) are
passed in as input tokens for the transformer decoder blocks,
while the observations Ot are passed into the multi-head cross-
attention layers of the transformer decoder stack as conditional
input. The cross-attention mechanism [38, 4] efficiently lever-
ages the rich information captured by all cameras, learning
the correspondences between multi-modal observations (multi-
view images and robot proprioception) and robot actions.
Robustness to unreliable cameras with blink training.
Low-cost cameras often have unreliable connections, leading
to dropouts and variable latencies. On average, the dropout
rate (i.e., camera discount) observed for the used cameras is
4.4 and latency ranges from 15 ms to 100 ms. To make the
system robust to such camera failure at test time, we employ
a blink training strategy that randomly drops out camera
inputs during training. Concretely, we simulate a 5 failure
rate for each camera, independently masking out entire images
at each time step. This means that there is a 65.9 probability
that at least one camera is being dropped out at each time
step. Consistent with observations in previous work , this
simple strategy significantly improves the robustness of the
completely disabled during test time.
VII. PRACTICAL CONSIDERATIONS
This section highlights critical implementation details for
developing an effective RoboPanoptes system. Although we
do not consider these aspects to be novel technical contribu-
performance. As such, our insights provided here may serve as
guidelines for the future development of whole-body dexterity
and vision systems.
Camera Selection. When selecting a camera, we need to
consider its size, field of view (FOV), and connectivity. We
choose the Adafruit Ultra Tiny USB Camera due to
its versatility and ease of integration. It offers a compact
design (8  25  4.5 mm) that fits well within the spatial
constraints of the robot while providing a reasonable FOV
(50) for the tasks. Moreover, these USB cameras support
standard UVC (USB Video Class) interfaces, making them
plug-and-play compatible with various devices. This simplifies
the deployment and ensures ease of integration.
We also explored other options, such as wireless cameras
and Raspberry Pi cameras, but encountered impractical limita-
tions. Wireless cameras introduce significant latency, limiting
the reactivity of the overall system during deployment. The
Raspberry Pi Camera Module requires a Raspberry Pi
single-board computer as the interface, and we find the used
ribbon cable connection to be sufficiently flexible but unstable,
leading to potential reliability issues. In contrast, USB cameras
provide a reliable and standardized interface and, through
the need for specialized (interface) hardware.
Wire Management. Managing the USB and power wires
for all 21 cameras and 9 motors turns out to be a critical
engineering challenge. It directly impacts the overall sys-
tem performance as poor wire management causes a range
of issues, including obstructed camera views and restricted
robot motion. Tangled wires also cause physical strain and
potentially lead to an unreliable, or even damaged, system.
To prevent wires from obstructing camera views, we design
custom fixtures on the robots links, as described in  IV,
that secure the wires in place. However, fixing the wires to
the links introduces a new issue: the stiffness of the wire
could reduce the range of the robots movements. Note that
the USB cameras wiring, which consists of four internal wires
(power, ground, data, and data), requires proper insulation
to prevent common-mode interference and ensure stable data
transmission. However, we find that we can still increase the
wires flexibility by removing the bulky rubber jacket and
retaining only the thin aluminum foil below for insulation.
Data Communication Bandwidth. A common desktop PC
often has limited bandwidth, making it challenging to stream
data from multiple high-bandwidth devices, such as cameras.
For our choice of camera, the main constraint is the number
of USB devices and corresponding video streams a PC can
support; usually only four or five simultaneously. To address
this bottleneck, we use four PCIe USB extension boards, each
providing 20 Gbps of additional bandwidth and allowing the
system to reliably stream data from 23 cameras simultane-
Multiprocessing. To efficiently manage data acquisition and
processing from multiple cameras, we use multiprocessing to
parallelize tasks and minimize interference between camera
streams. Each camera is acquired in its own process at 30
FPS. During demonstration and inference, a shared memory
space (with the size of N216404803
) across all processes
is maintained to store the latest image observations, updated
every time a new frame is received. This approach ensures
seamless data flow and reduces processing delays, allowing
the system to handle large amounts of visual data concurrently.
c) Top-down Camera
Fig. 5: Alternative Observation Spaces. We compare RoboPanoptes
with several baselines, including using a) only the head camera, b)
the four neck cameras, and c) a top-down camera. Variants using
all of RoboPanoptes cameras but without view-dependent positional
encoding or without blink training serve as ablations of our design.
VIII. EXPERIMENTS
In the following, we study RoboPanoptes ability to perform
a wide range of real-world manipulation tasks that require
whole-body dexterity. To ensure fair comparisons, all methods
use the same set of demonstrations for training and identical
initial configurations during testing.
direction
Head Camera Only
cant open
wo Camera Pose
wo Blink Training
(c) Comparisons
(b) RoboPanoptes Policy Rollout
Initial pose
Drag the box closer
Enter through the hole
Lift the lid up
Slide the lid open
Training Configurations
Topdown Camera
(a) Test Scenarios
Fig. 6: Unboxing Task. a) Different test scenarios. The first two images show all initial configurations overlayed. b) Policy rollout of
c) Typical failure cases of the baselines. The Top-down Camera policy struggles to determine the correct reaching height. The Head
Camera policy moves in the wrong direction. The wo Camera Pose Encoding policy struggles to open the lid, and at locating and
entering the hole. The wo Blink Training policy underperforms in scenarios with camera dropout (T2).
A. Unboxing Task
in a random location around the robot (Fig. 6). To do so, the
robot first needs to locate the small hole on the side of the
is out of reach), extend its body inside the box, lift the lid,
and finally slide the lid aside to fully open the box.
The task success is measured by whether the box is opened
in the end. No partial credit is given for this task.
through a narrow hole and operate within the confined space
inside the box. Visual occlusions: The holes location is often
occluded (with respect to a camera at the scenes center) due
to the random box placement. Dexterity: Actions like lifting
and sliding the lid require fine-grained control over multiple
DoFs involving multiple links. Multiple contacts: The robot
must drag the box closer, leveraging multiple contacts with its
body links and stabilizing the box during lid manipulation.
Test scenarios: We conduct 18 rollouts for each policy. To
ensure a fair comparison, we use the same set of initial robot
and object configurations for all different methods. We achieve
the same object configuration by adjusting it to an overlayed
top-down view of the reference configuration. As shown in
Fig. 6 (a), the test configurations can be grouped into five
for each camera a 5 failure rate at each time step (5
rollouts); i.e., a 65.9 failure rate for at least one camera.
such as different colors and patterns (5 rollouts).
per time step and delays sampled from a uniform distribu-
tion U(0,0.5s) (2 rollouts).
googly-eye stickers all over the robot (1 rollout).
ing approaches in this task:
Top-down Camera: Observations from a single, static
top-down camera (Fig. 5).
Head Camera:
Observations
mounted at the center of the head (Fig. 5).
wo Camera Pose: A whole-body visuomotor policy
trained without view-dependent positional encoding.
wo Blink Training: A whole-body visuomotor pol-
icy trained without randomized camera dropouts.
tion episodes, with each demonstration averaging 15 s. We
report both qualitative and quantitative results in Fig. 6,
and illustrate typical failure cases. RoboPanoptes achieves an
occluded objects
Initial pose
miss blocks
under the shelf
knock down tall object
miss objects
out of view
all objects are in
the target zone
Time cost
comparison
(b) RoboPanoptes Policy Rollout
(c) Comparisons
(a) Test Scenarios
Fig. 7: Sweeping Task. a) Different test scenarios. b) RoboPanoptes policy rollouts, highlighting the capability of leveraging multiple whole-
body contacts with objects. c) Typical failure cases of baselines. The Top-down Camera policy fails to detect objects under the shelf and
often knocks down tall objects. The Neck Cameras policy struggles with objects located behind the robot due to self-occlusion.
overall 94.4 success rate, outperforming all baselines.
The Top-down Camera policy typically fails to locate
the hole due to occlusion. While the robot correctly moves
towards the box, it often approaches the hole too high or too
low. Since the hole cannot be seen from the top-down camera,
it is ambiguous at which height the robot should reach forward.
The Head Camera policy fails consistently to move to-
wards the box. It always executes a fixed trajectory with
the robot moving forward, independent of where the box is
located. We hypothesize that this is because the box often
cannot be seen from the head camera and, thus, the policy
has overfitted to use proprioception instead. During our ex-
by coincidence and managing to open the lid smoothly since
this action mostly relies on proprioception.
wo Camera Pose, the policy is less accurate in locating
and getting inside the hole. This demonstrates that our view-
dependent positional encoding design enables better spatial
awareness and more effective skill learning. And even if the
robot has already gotten inside, it often fails to open the
lid. We suspect that this is because vision tokens outnumber
proprioception tokens, making it more challenging for the
policy to learn how to utilize proprioception information.
wo Blink Training policy performs worse in T2
when we randomly drop out cameras and is slightly less robust
than ours in other scenarios as well. This proves that our blink
training strategy is critical to the robustness of the policy,
especially during unexpected test-time sensor failures.
B. Sweeping Task
randomly placed on a table or under a shelf) into a target
region around its base. The target zone is 2020cm and cen-
tered around the robot. Training data includes demonstrations
with 24 small blocks and two cylindrical objects. The task
success rate for sweeping multiple small objects is measured
by the ratio of objects inside the target zone. For sweeping a
large object, the task success rate is measured by whether the
object is dragged into the target zone without being knocked
objects requires the robot to leverage its whole body and ad-
ditional inter-object contact. The robot can also sweep objects
larger than itself by wrapping around them and dragging them
with multiple body links. Dexterity requirements: The robot
must conform to objects, sweep under narrow spaces (i.e.
under the shelf), and lift itself to varying heights to sweep
tall objects while preventing them from toppling over. Visual
robot are frequently occluded from certain viewpoints.
Test scenarios: We evaluate this task across 18 rollouts,
divided into the following scenarios:
following approaches:
Top-down Camera: Observations from a single, static
top-down camera (Fig. 5 a).
Neck Cameras:
Observations
mounted along the robots neck (Fig. 5 b).
ResNet Encoder: A whole-body visuomotor policy
with a ResNet-34  vision encoder trained from scratch
instead of using a pretrained vision encoder.
wo Camera Pose: A whole-body visuomotor policy
trained without view-dependent positional encoding.
wo Blink Training: A whole-body visuomotor pol-
icy trained without randomized camera dropouts.
Teleop EE: Teleoperating the robot to move small ob-
jects one by one, using only the head link. This approach
is used to demonstrate the inefficiency of the end-effector-
only manipulation strategy, even under a near-perfect policy
(i.e., teleoperation).
the methods success rate and time per block in Fig. 7 (bottom
right). RoboPanoptes achieves a 96.6 success rate, outper-
forming all baselines.
The Top-down Camera policy typically misses objects
under the shelf due to occlusion; especially failing to fetch
objects that were not successfully swept out on the first try. It
also sweeps tall objects at a low height, toppling them over.
As in the unboxing task, we hypothesize this behavior is due
to the ambiguous height when observed from the top-down
The Neck Cameras policy misses out-of-view objects
(e.g., when objects are hidden behind the robot itself) and
fails to recover in cases when, e.g., the robot pushed objects
too far away. Occasionally, we observe the robot getting stuck
in a pose without interacting with any object. We suspect this
being due to the policy overfitting to the proprioception signal
due to occlusion.
The ResNet Encoder policy often sweeps toward in-
correct or empty r
