=== PDF文件: Demonstrating Shared Force-Language Embeddings for Natural Human-Robot Communication.pdf ===
=== 时间: 2025-07-22 15:50:00.394915 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Cross-modality Force and Language Embeddings
for Natural Human-Robot Communication
Ravi Tejwani
Electrical Engineering and
Computer Science
Massachusetts Institute of Technology
tejwanirmit.edu
Karl Velazquez
Electrical Engineering and
Computer Science
Massachusetts Institute of Technology
kvelazmit.edu
John Payne
Electrical Engineering and
Computer Science
Massachusetts Institute of Technology
johnpaynmit.edu
Paolo Bonato
Department of Physical Medicine, Harvard Medical School
pbonatomgh.harvard.edu
Harry Asada
Mechanical Engineering
Massachusetts Institute of Technology
asadamit.edu
AbstractA method for cross-modality embedding of force
profile and words is presented for synergistic coordination of
verbal and haptic communication. When two people carry a
communication about the intended movements and physical
forces applied to the object. This natural integration of verbal and
physical cues enables effective coordination. Similarly, human-
robot interaction could achieve this level of coordination by inte-
grating verbal and haptic communication modalities. This paper
presents a framework for embedding words and force profiles
in a unified manner, so that the two communication modalities
can be integrated and coordinated in a way that is effective and
synergistic. Here, it will be shown that, although language and
physical force profiles are deemed completely different, the two
can be embedded in a unified latent space and proximity between
the two can be quantified. In this latent space, a force profile and
words can a) supplement each other, b) integrate the individual
need for cross-modality embedding is addressed, and the basic
architecture and key building block technologies are presented.
Methods for data collection and implementation challenges will
be addressed, followed by experimental results and discussions.
I. INTRODUCTION
As stipulated in Asimovs laws, robots must perform tasks
based on human instruction. A central challenge in robotics
has been developing effective ways for humans to communi-
cate and give instructions to robots.
Early roboticists attempted to develop robot languages,
e.g. VAL , to describe desired robot actions and tasks.
motions and behaviors are difficult, inefficient, or impossi-
ble to describe through language alone. This limitation is
particularly evident in tasks requiring environmental contact
and forcecompliance control, where forces and moments are
not directly visible. Such manipulative tasks, now termed
contact-rich manipulation [20, 7], often involve subconscious
knowledge that humans find difficult to articulate through any
Fig. 1: Physical therapist from Spaulding Rehabilitation Hos-
pital is seen demonstrating hamstring curl therapy on the
patient with neurological injuries 1. She is instructing a patient
to move gently forward while providing an assistive force.
form of language. To address this challenge, roboticists devel-
oped alternative approaches that bypass the need to translate
subconscious skills into language. These approaches include
teaching by demonstration , programming by demonstration
, skill acquisition  and, more recently, imitation learning
With recent advances in natural language processing,
language-grounded robot control has gained significant mo-
mentum [27, 18]. This approach will play a crucial role
in scenarios in which robots and humans interact closely.
effectively convey human intentions, instructions, and desired
behaviors to robots?
Consider situations where a human wants a robot to gently
touch an object or their body. Such actions are difficult
to describe verbally; instead, humans prefer to physically
demonstrate the desired gentleness of touch. Yet, physical
1Detailed therapy sessions can be seen in our online appendix: https:
shared-language-force-embedding.github.iotherapy-sessions
demonstration alone cannot convey important context, nuance,
and reasoning behind the action. This illustrates how language
and touchforce are complementary modalities that must be
integrated and coordinated for effective human-robot commu-
nication.
To study this problem, we collaborated with physical ther-
apists from Spaulding Rehabilitation Hospital who demon-
strated various therapeutic techniques. Figure
1 shows one
such demonstration, where the therapist first explains the
procedure verbally and then demonstrates by gently turning
the patients leg. If a robot were to suddenly move the patients
On the other hand, the verbal explanation, I will lift your leg
gentle. They may be anxious to see whether it is painful. The
therapist starts pushing the leg immediately after giving the
brief explanation, demonstrating what she means by lifting the
leg gently. This example highlights how language and physical
touchforce serve as two distinct but complementary modalities
for describing tasks and communicating intended behaviors.
The challenge lies in integrating them effectively.
The goal of the current work is to establish a unified
method for representing language and force that facilitates
their integration and coordination. We make the following
1) A framework for cross-modality embedding of force
profiles and words 2, enabling translation between phys-
ical force curves and natural language descriptions;
2) A paired data collection methodology with 10 par-
ticipants performing language-to-force and force-to-
language translations, capturing human intuition about
force-language relationships;
3) Evaluation metrics and results validating the frame-
works effectiveness and generalization on unseen data;
II. RELATED WORK
A. Force-Based Human-Robot Interactions
Force-based interactions have been studied in the past for
human-robot collaborative tasks. Early work in  demon-
strated automatic program generation from force-based teach-
ing data. Furthermore,  established the significance of
force feedback in human-robot interfaces, putting down the
groundwork for new interaction paradigms. Recent work has
significantly improved our understanding of force-based ma-
force constraints and  further extending this to robust
multi-stage manipulation tasks.
Research on using force sensing for improved physical
human-robot interaction was explored in  showing methods
for learning from demonstration using force data  and
explaining human intent from contact forces . However,
these works are applicable to tasks under specific conditions;
broader task variability, diverse conditions and contexts, and
subtle nuance that language can describe are not considered.
B. Grounding Natural Language in Robot Actions
Natural language has been investigated in literature for
grounding language phrases to robot actions and behaviors.
developed probabilistic approaches for mapping natural
language instructions to robot trajectories. Building on this,
showed methods for learning semantic parsers that ground
language to robot control policies. Recent work has shown the
use of large language models to improve language understand-
ing for robotics   . While these approaches map language
to robot actions, the tasks are mostly pick-and-place, and more
complex manipulative tasks that involve contact forces are
excluded.
C. Multimodal Embeddings in Robotics
Research in learning shared embedding spaces between
different modalities for robotic learning has been explored
in the past.  developed cross-modal embeddings between
visual and force data for manipulation tasks.  showed
learning joint embeddings of language and vision for a
robot instruction navigation task. Although, these approaches
have demonstrated the potential of multimodal embeddings
in robotics, none have specifically addressed the challenge
of creating shared embeddings between force trajectories and
natural language descriptions.
The current work aims to fill this gap by developing and
providing a framework of bidirectional translation between
physical forces and their linguistic descriptions. Inspired by
physical therapists interactions with patients, we will address
the needs for unified representation of language and force
profiles and effectiveness of force-language cross-modality
embedding to better understand how these strikingly distinct
modalities can be integrated.
III. PRELIMINARIES
A. Coordinate System
We first introduce a coordinate system to consistently define
spatial directions and interpret force profiles (Table 2). Each
force measurement is a vector F(t) R3 with components
(Fx(t), Fy(t), Fz(t)). Fig. 2 shows linguistic direction to its
corresponding axis:
backward
Fig. 2: Coordinate system mapping direction words to spatial
axes for interpreting force profiles.
B. Force Profile
We record time-varying force data using a force-torque sen-
sor mounted on the UR robots end-effector. For a recording
T seconds, we store each timestamp ti [0, T] along with the
measured force vector F(ti). We refer to the set of samples,
ordered chronologically as a force profile. Formally, a force
profile is represented as a 4  N tensor:
Force Profile
Fx(tN 1)
Fz(tN 1)
where t0  0 and tN1  T. Figure 5 shows examples of
force profiles, paired with textual instructions.
To interpret forces quantitatively, we adopt Newtons second
law of motion:
p(t) p(0)
F(t)dt  J(t)
where at time t, p(t) is the momentum, F(t) is the applied
the elementary pillars that describe force profiles: direction
F(t), magnitude F(t), and duration T.
C. Language
Throughout this paper, we define a phrase as an ordered list
of words describing a motion or force profile (e.g. slowly for-
we consider two distinct vocabularies.
1) Minimal Viable Vocabulary: This vocabulary contains 18
direction words (e.g. left, right, forward-down) and 12 modifier
words (e.g. slowly, quickly, harshly) that describe variations in
force magnitude and duration .
Direction
Modifier
backward
slightly
backward-down
backward-left
smoothly
backward-right
backward-up
down-forward
down-left
significantly
down-right
forward-left
gradually
forward-right
immediately
forward-up
right-up
TABLE I: The minimal viable vocabulary. Direction words de-
scribe the overall direction of the force profile, while modifier
words describe the magnitude and duration.
Binary Phrase Vectors: The direction words require 18
dimensions and modifier words require 12 dimensions (as
lightly right
smoothly backward
Cross-modal
Latent Space
Fig. 3: Conceptual illustration of the desired properties of
the cross-modality latent space. A pair of corresponding force
profile and phrase should be located near each other, measured
by a distance metric such as cosine similarity. However,
force profiles and phrases that do not correspond should be
positioned far away. This demonstrates that similar inputs
would be close together and dissimilar inputs would be far
apart in the latent space.
shown in Table I). Each word is encoded as a 31-dimensional
basis vector, where the additional dimension represents an
empty or null word. These binary phrase vectors are then
concatenated to form a 62-dimensional vector, where exactly
two positions contain 1 - one in the first 31 dimensions
identifying the modifier (or empty modifier) and one in the
second 31 dimensions identifying the direction (or empty
direction). This binary encoding scheme ensures consistent
representation while allowing for partial or incomplete phrases
through the accomodation of empty words.
2) Extended SBERT Vocabulary:
We leverage SBERT
(Sentence-BERT) embeddings , a contextual language
model that produces semantically meaningful representations.
This representation allows for encoding beyond our minimal
viable vocabulary, as the SBERT model has been pretrained
on diverse textual data. When processing a phrase, we encode
the complete text rather than individual words, resulting in a
768-dimensional vector that preserves the semantic meaning
of the entire expression (Fig. 4). This approach enables our
system to handle open-vocabulary inputs while maintaining
the shared structure of our force-language embedding space.
D. Cross-Modality Embedding (Shared Latent Space)
We learn the cross-modality embedding as a shared latent
space Z R16 to align force profiles and phrases. Rather than
treating phrases and force profiles as purely distinct modalities,
we emphasize on their common representational ground. In
this unified latent space, certain force profiles can be naturally
described by words (e.g., gentle push), and conversely,
phrases can be manifested as force trajectories. Specifically,
we define encoders Eforce, Ephrase that map force profiles and
define decoders Dforce, Dphrase that map shared embeddings
back to forces and phrases, respectively. A contrastive learning
objective  encourages embeddings of paired forcephrase
"immediately
backward"
"gradually
forward"
LLM Embedding
"I want you to
ease ahead
over time"
(Similar
Concepts)
(Dissimilar
Concepts)
Fig. 4: Conceptual illustration of the process of matching an
arbitrary text input with the most semantically similar phrase
using SBERT embeddings.
data to lie close together in Z while pushing apart non-
matching pairs. This alignment supports:
Force-to-Language Translation: Observed force profiles
can be decoded into textual instructions.
Language-to-Force Translation: Written phrases can be
transformed into corresponding force trajectories for
robotic execution.
in human-robot collaboration by unifying physical force sig-
nals and language instructions within a single latent represen-
tation .
IV. SYSTEM OVERVIEW
In this work, we aim to develop a unified representation
for physical forces and natural language phrases. Our primary
goal is to learn a shared embedding space that allows robots to
translate human-applied force profiles into linguistic descrip-
tions and, conversely, generate appropriate force outputs from
language instructions.
We use a UR  robot for the magnitude, direction, and
duration of applied forces over time. To build a dataset that
naturally pairs force signals with language, we designed two
human-participant procedures. In the Phrase-to-Force proce-
dure (Sec. V-C1), participants receive a brief textual phrase
and physically move the robot arm. In the Force-to-Phrase
procedure (Sec. V-C2), participants observe an externally
applied force trajectory and then describe it in natural language
from our minimal viable vocabulary (e.g., gradually left). By
collecting these paired samples, we obtain a diverse dataset for
learning force-language correspondences.
Our proposed model is a dual autoencoder architecture (Fig.
6), which processes both time-series force profiles and textual
phrases. We encode each force profile into a latent repre-
sentation and similarly embed each phrase into a matching
latent space. We train the model with three core objectives
that facilitate robust multimodal alignment: (1) reconstruction,
which ensures that both forces and phrases can be faithfully
recovered from their respective embeddings, (2) contrastive
close in latent space while pushing apart mismatched pairs,
and (3) translation, which enables the network to generate a
force profile from a given phrase and to describe a given force
profile with a textual output.
By optimizing these objectives, the system learns to em-
bed semantically related force and language inputs in close
proximity. During inference, a robot can interpret a previously
unseen force in linguistic terms or synthesize an appropriate
force response for a phrase. We describe the details of the
training procedure, architecture, and data preprocessing in
Sec. V. In subsequent sections, we evaluate how well the
learned embeddings capture force-language relationships and
demonstrate the systems capability to perform bidirectional
translation between physical forces and natural language.
V. ARCHITECTURE
A. Model
1) Force Profile Input: Each raw force profile consists of
time-series measurements for the x, y, and z components of
durations. To create a uniform representation, we first resample
each force profile to N  256 evenly spaced time steps
spanning a fixed duration T  4. This yields a 3  256 tensor
where Fx,i, Fy,i, Fz,i denote the resampled forces at the i-th
time step in each axis. Next, we integrate each axis of F over
time to obtain an impulse profile
where Ja,i
0 Fa(ti)dt and ti is the time associated
with the i-th sampled resampled point. J is flattened to form
a 768-dimensional vector
R768 which serves
as input to the force encoder.
2) Phrase Input: Since neural networks cannot directly
process raw text, each phrase must be converted into a fixed-
size vector representation that preserves its semantics. We
consider two embedding approaches: Binary Phrase Vectors
from the Minimal Viable Vocabulary (Section
III-C1), and
contextual embeddings from SBERT (Section III-C2).
a) Binary Phrase Vectors: In our minimal viable vo-
modifier and a direction. Each word is represented as a one-hot
encoded vector, with the modifier requiring 31 dimensions and
the direction requiring 31 dimensions (including an empty
position for each). These binary vectors are then concatenated
to yield a 62-dimensional phrase representation. This discrete
encoding provides a direct mapping between the linguistic
components and physical force characteristics.
Fig. 5: Examples of corresponding force profiles and phrase pairs. (a,b) Basic motions in forward and backward directions,
showing dominant positive and negative y-components respectively. (c,d) Effect of adding modifiers (softly and greatly) to
forward motion, demonstrating how they alter force magnitude while maintaining direction.
b) SBERT Embeddings: To handle arbitrary verbal in-
structions beyond our fixed vocabulary, we implement a
mapping mechanism using SBERT that identifies the most
semantically similar phrase in our minimal viable vocabulary.
For any input text x, we find the matching MVV phrase p
arg maxpP s(E(p), E(x))
if s(E(p), E(x)) >
otherwise
where P is the set of MVV phrases, E is the SBERT encoder,
s is cosine similarity, and  is a threshold parameter. This
approach enables our system to accept open-vocabulary input
while leveraging our trained force-language mappings.
3) Dual Autoencoders: Our framework employs two au-
toencoders
profiles
phrasesto map inputs from distinct modalities into a shared
latent space. For the phrase autoencoder, we utilize SBERTs
768-dimensional embeddings or the Binary Phrase Vector (62-
dimensional embedding) as input, which are then processed
through several layers before reaching the 16-dimensional bot-
tleneck layer. This architecture enables the model to compress
the semantic information into a compact representation that
aligns with force profiles in latent space.
During training, the encoder learns to capture the essential
latent features of the input data, while the decoder learns
to reconstruct the input from these latent variables. This
unsupervised learning process enables the network to extract
a compact representation that generalizes well to novel inputs
sharing similar underlying structure. In our dual autoencoder
to output latent vectors of the same dimension, ensuring
compatibility in the shared cross-modal latent space. This
design choice allows us to perform bidirectional translation
between force profiles and phrases by using the decoder of one
modality on the latent representation produced by the encoder
of the other.
Online Appendix 3details all the specific layers and activa-
tion functions and hyperparameters used for each modality. In
all cases, layers consist of a linear transformation (i.e., a matrix
multiplication) followed by the application of a nonlinear
activation function; we use the rectified linear unit (ReLU)
for this purpose.
Appendix
framework
shared-language-force-embedding.github.ioframework
for each Cartesian axis
samples across
Force Profile Input
Feature Extraction
Unflatten
Feature Post-processing
Constructed Force Profile
for each Cartesian axis
samples uniform across
Phrase Input
lightly right
Constructed
slightly right
per-component features
resampled uniform across
(Ex. Impulse Profile)
, force profile features input,
, phrase embedding
, shared latent space embedding,
Fig. 6: Architecture diagram of the cross-modality dual autoencoder that represents phrases as a 768  1 embedding generated
by S-BERT (). The phrase embedding input is then passed into the phrase autoencoder, which encodes it into the shared
latent space. There the 16  1 embedding can take 2 paths: either be decoded back into a phrase or be translated into a force
profile by using the force profile decoder. Force profile inputs are first preprocessed to extract meaningful features for the
force profile autoencoder to digest. They are then encoded into the shared latent space. Like phrase inputs, they can either be
decoded back into a force profile or be translated into a phrase by using the phrase decoder.
B. Multitask Learning
To encourage the model to learn a robust cross-modality la-
tent representation, we adopt a multitask learning strategy that
jointly optimizes three related objectives: (1) reconstruction of
the original inputs from their latent representations, (2) con-
trastive alignment of corresponding force profile and phrase
embeddings in the shared latent space, and (3) translation
between modalities by decoding a latent embedding obtained
from one modality into the other. Joint training with these tasks
compels the model to extract meaningful latent features that
generalize well across modalities while mitigating over-fitting.
function; however, by incorporating multiple loss functions
corresponding to related tasks , our network is guided
to form a representation that simultaneously serves several
objectives. The overall loss function is a weighted sum of the
individual losses:
L  krLr  kzLc  ktLt,
where hyperparameters kr, kz, kt control the relative im-
portance of the reconstruction loss Lr, contrastive loss Lc,
and translation loss Lt, respectively. In our experiments, these
constants are all set to 1, indicating equal weighting for each
task. The loss functions are defined as:
1) Reconstruction Loss (Lr) : It measures how accurately
each autoencoder (force and phrase) reproduces its own input
from the latent vector. For force profiles, we use mean squared
error; for phrases, the reconstruction metric depends on the
chosen representation, with cross-entropy for binary phrase
vectors and mean squared error for S-BERT embeddings.
For phrase inputs xp with encoder Ep and decoder Dp, the
reconstruction loss varies:
H(xp, softmax(Dp(Ep(xp))))
xp Dp(Ep(xp))2
where H is cross-entropy loss.
2) Contrastive Loss (Lc): To align the force and phrase em-
(corresponding) latent vectors closer together while pushing
apart unpaired (non-corresponding) vectors. This ensures that
shared features of matching force and language inputs are
learned and represented similarly in the latent space.
The contrastive loss for a batch of n force-phrase pairs
p) in the shared latent space is:
max(0, mzi
where  controls the negative pair weighting and m is the
margin parameter.
3) Translation Loss (Lt): Finally, we measure how well the
model translates between modalities. Given a force profile and
its paired phrase, we encode one and decode into the other,
then compare the result to the corresponding ground truth.
lightly right
smoothly backward
Cross-modal Latent Space
Embeddings of Phrases
Cross-modal Latent
Space Embeddings of
Force Profiles
Fig. 7: Conceptual illustration of the contrastive loss function.
Given a batch of corresponding force profiles and phrases, a
distance metric is measured for each pair across modalities.
This results in a distance metric matrix. The diagonals refer
to the distance metrics of corresponding force profiles and
negative pairs. The contrastive loss function encourages the
distances of positive pairs to be minimized. It also penalizes
negative pairs if they start to get too close to each other.
This cross-decoding step drives the model to capture modality-
agnostic features in the shared embedding, facilitating natural
force-to-language and language-to-force translation.
The translation loss for corresponding force profile and
phrase inputs xf and xp is:
Lt(xf, xp)  err(xf, Df(Ep(xp)))  err(xp, Dp(Ef(xf)))
where err(, ) is the appropriate reconstruction error metric
for each modality.
C. Data Collection
To train and evaluate the cross force-language embedding,
we collected data from 10 participants (6 males, 4 females)
aged 21-32 years (mean26.4) who interacted with a UR10
manipulator arm. To address individual variations in force
profile application, we employed min-max normalization and
relative pattern analysis across participants. This normaliza-
tion procedure helps account for differences in strength and
movement patterns between individuals, allowing the model
to focus on the semantic relationship between language and
force trajectories rather than absolute force magnitudes. Each
participant completed two procedures: phrase-to-force trans-
lation and force-to-phrase translation. Both procedures used
a consistent reference frame as in (Section III-A). The study
was approved by the Institutional Review Board (IRB Protocol
2212000845R001) to ensure ethical human subject research
guidelines were followed.
(a) Example Phrase-to-Force trial: A participant interprets the phrase
smoothly down forward by guiding the robot arm while force data
is recorded.
(b) Example Force-to-Phrase trial: The robot executes a predefined
force profile, and the participant describes the felt motion as sharply
backward right
Fig. 8: Demonstration of bidirectional force-language translation through human trials.
Trial Number
Phrases Provided to User 1
backward
forward and down
backward quickly
smoothly right
quickly right and down
forward and left sharply
up smoothly
TABLE II: List of phrases provided to a user participant during
their Phrase-to-Force procedure. Note the first six phrases
across all users were a random ordering of the basic directions.
composition structure.
1) Phrase-to-Force:
Participants were sequentially pre-
sented with 42 distinct phrases (Table
II illustrates an ex-
ample sequence of prompts.). Each phrase was generated by
randomly combining a direction word (e.g., left, forward-
down) with an optional modifier word (e.g., quickly,
smoothly) from the vocabulary described in Section III-C.
The first six phrases were always basic directions, while the
remaining 36 incorporated modifiers and compound directions.
For each phrase, participants gripped the robots end-effector
and demonstrated what they felt was an appropriate motion
matching the description. The system recorded force measure-
ments for each demonstration, typically lasting 2-4 seconds.
A user trial demonstrating this is shown in Figure 8a.
2) Force-to-Phrase: In this procedure, participants gripped
the robots end-effector while the manipulator executed ran-
domly generated force trajectories. Each motion was created
by combining three randomized parameters: a primary direc-
tion vector, a force magnitude between 0.5 and 15 Newtons,
and a duration ranging from 1 to 4 seconds. After experiencing
each motion, participants constructed descriptive phrases using
the same vocabulary (Section III-C) from the phrase-to-force
procedure - selecting direction and modifier words they felt
best characterized the force profile they had just experienced.
A user participant trial demonstrating this is shown in Fig. 8b.
Complete details on the dataset with all 10 participants are in
appendix 4.
VI. EVALUATION
Our experiments aim to address three key research ques-
1) ForceLanguage Translation Performance. How ef-
fectively does the dual autoencoder architecture translate
force profiles into phrases and vice versa? This evalu-
ates whether the shared latent space successfully aligns
semantically similar force and language inputs.
2) Generalization to Unseen Examples. Can the model
generalize to force profiles and phrases outside its
training distribution? This assesses whether the learned
shared representation captures essential features that
extend beyond the training data.
3) Impact of Phrase Representation. How does the
choice of phrase representation affect model perfor-
mance? Specifically, do richer semantic embeddings (via
pretrained S-BERT embeddings) enhance the models
ability to associate forces with language compared to
binary phrase vectors from the Minimal Viable Vocab-
To investigate these questions, we evaluate two variants of
our dual autoencoder (DAE) framework. The first variant, de-
noted DAEB, utilizes the binary phrase vector representation
of phrases (described in Section III-C2). The second variant,
Appendix
showcasing
collected
participants
phrase-to-force
force-to-phrase
shared-language-force-embedding.github.iodata-collection
scribed in Section III-C2. By comparing results across these
two models, we assess the influence of language embedding
choices on overall performance and generalization.
A. Baseline Models
1) SV M KNN: As a baseline, we use Support Vector
Machines (SVM) to map force signals to phrases and K-
Nearest Neighbors (KNN) to map phrases back to force
profiles. This approach is limited to our Minimal Viable
Vocabulary since SVMs cannot generate continuous S-BERT
embeddings.
Each force profile is reduced to a final impulse vector I
We train one SVM for each word slot (direction and modifier).
Given the impulse vector, these SVMs predict the most likely
direction(s) and modifier.
For the inverse mapping, we treat each phrase as a com-
bination of direction and modifier classes, then apply KNN
in the same 3D impulse space to identify the closest training
example. Since no full time series is predicted, we approximate
the complete impulse profile by linear interpolation from zero
impulse at t  0 to the predicted final impulse at t  T. This
simple interpolation scheme serves as a coarse placeholder for
the temporal structure of the motion.
2) DMLPB: In this baseline, we train two independent
Multi Layer Perceptron (MLP) networks without any shared
latent space. One MLP maps force profiles from flattened 768-
dimensional impulse vectors to 62-dimensional binary phrase
vector (Section V-A1), while the other maps phrases to force.
We call this approach DMLPB. Each MLP is trained via a
reconstruction objective to directly map from one modality to
the other. The training loss corresponds to the standard mean
squared error for forces (Section V-B1) and cross-entropy for
binary phrase embeddings. Since there is no shared latent
space or cross-modal alignment, the model simply learns direct
forward and backward transformations.
3) DMLPS: DMLPS is analogous to DMLPB except it
uses the S-BERT embedding representation of phrases. Instead
of two binary phrase vector outputs, the first MLP maps the
force profile to a 150-dimensional concatenation of S-BERT
word embeddings, and the second MLP performs the inverse
mapping. Each MLP is again optimized with a reconstruction-
style loss suited to its respective output space. This direct
mapping baseline allows us to isolate the effect of introducing
richer semantic information via S-BERT embeddings, inde-
pendent of a shared latent representation.
B. Metrics
We define a set of performance metrics to evaluate the force-
language translations. These metrics evaluate both the fidelity
of generated force profiles relative to a ground-truth trajectory
(force-to-phrase translation) and the accuracy of generated
phrases relative to a reference text description (phrase-to-force
translation).
1) Force Profile Accuracy (FPAcc): For a predicted force
profile xf and ground-truth xf, we compute the mean
squared error (MSE), averaged across both temporal and
spatial axes:
FPAcc  MSE(xf, xf)
A lower value indicates closer alignment with the refer-
ence force profile.
2) Force Direction Accuracy (FDAcc). For a predicted
total impulse J(T) and ground-truth J(T) (see eq. 4)
we compute the cosine similarity:
J(T)  J(T)
J(T)  J(T)
Value near 1 indicate strong directional alignment,
whereas near 1 signify an opposite direction.
3) Modifier Similarity (ModSim): We compare the pre-
dicted modifier wm with the ground-truth modifier wm
by embedding each word using SBERT , producing
768-dimensional vectors. We then compute the cosine
E( wm)  E(wm)
E( wm)  E(wm)
where E is the SBERT sentence embedder.
4) Direction Similarity (DirSim): As with modifiers, we
embed both the predicted direction word(s) wd and
ground-truth wd using SBERT, then compute the cosine
similarity of the resulting vectors. If the phrase contains
two direction words, we concatenate them into a single
short text snippet (e.g., left and up) before embedding.
E( wd)  E(wd)
E( wd)  E(wd)
High similarity values indicate strong agreement in
directional semantics.
5) Full Phrase Similarity (PhraseSim): We compute the
overall phrase similarity as the average of the Modifier
Word Similarity and Direction Word(s) Similarity:
PhraseSim  1
2 (ModSim  DirSim)
C. Experiments
To evaluate both in-distribution and out-of-distribution per-
1) In-Distribution Evaluation. We randomly split the
dataset into training (90) and testing (10) subsets.
Each model is trained and tested on the same splits,
and this process is repeated for 30 independent trials
with different random seeds. We then average the per-
formance metrics over these trials to reduce variance,
yielding a robust estimate of in-distribution accuracy.
2) Out-of-Distribution Modifiers. To assess the models
ability to generalize to unseen adverbial cues, we iso-
late a single modifier (e.g., slowly). All data points
containing this modifier are excluded from the training
set but retained in the test set. After training, we measure
how effectively each model handles data points that
correspond to the held-out modifier. We repeat this pro-
cedure for each modifier in the vocabulary and average
the results to obtain a modifier-level generalization score.
3) Out-of-Distribution Directions. Similarly, we evaluate
direction-level generalization by holding out each di-
rection (e.g., up) from the training set. The model is
then tested on data points where this direction appears.
Repeating this protocol for all direction words and aver-
aging the results provides a direction-level generalization
D. Results and Analysis
We present the evaluation outcomes for the three experimen-
tal settings: in-distribution, out-of-distribution (O
