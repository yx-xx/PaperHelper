=== PDF文件: DexterityGen  Foundation Controller for Unprecedented Dexterity.pdf ===
=== 时间: 2025-07-22 09:59:15.885931 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Foundation Controller for Unprecedented Dexterity
Zhao-Heng Yin1,2, Changhao Wang2, Luis Pineda2, Francois Hogan2, Krishna Bodduluri2, Akash Sharma2,
Patrick Lancaster2, Ishita Prasad2, Mrinal Kalakrishnan2, Jitendra Malik2, Mike Lambeta2,
Tingfan Wu2, Pieter Abbeel1, Mustafa Mukadam2
2FAIR at Meta
zhaohengyin.github.iodexteritygen
Motion Command
Foundation Dexterity
Controller
Dexterous Action
Fig. 1: We introduce DexterityGen (DexGen) as a foundation controller that achieves unprecedented dexterous manipulation
behavior with teleoperation. DexGen is a generative model that can translate an unsafe, coarse motion command produced
by external policy to safe and fine actions. With human teleoperation as a high-level policy, DexGen exhibits unprecedented
dexterity from diverse object rotation and regrasping to using pen, syringe, and screwdriver.
AbstractTeaching robots dexterous manipulation skills, such
as tool use, presents a significant challenge. Current approaches
can be broadly categorized into two strategies: human teleop-
eration (for imitation learning) and sim-to-real reinforcement
learning. The first approach is difficult as it is hard for humans to
produce safe and dexterous motions on a different embodiment
without touch feedback. The second RL-based approach struggles
with the domain gap and involves highly task-specific reward en-
gineering on complex tasks. Our key insight is that RL is effective
at learning low-level motion primitives, while humans excel at
providing coarse motion commands for complex, long-horizon
tasks. Therefore, the optimal solution might be a combination of
both approaches. In this paper, we introduce DexterityGen (Dex-
Gen), which uses RL to pretrain large-scale dexterous motion
leverage this learned dataset to train a dexterous foundational
controller. In the real world, we use human teleoperation as a
prompt to the controller to produce highly dexterous behavior.
We evaluate the effectiveness of DexGen in both simulation and
real world, demonstrating that it is a general-purpose controller
that can realize input dexterous manipulation commands and
significantly improves stability by 10-100x measured as duration
of holding objects across diverse tasks. Notably, with DexGen
we demonstrate unprecedented dexterous skills including diverse
object reorientation and dexterous tool use such as pen, syringe,
and screwdriver for the first time.
DexGen Controller
(Generative Model)
Multi-task Dataset in Simulation
Rotation
Translation
Learned Action Distribution
Likelihood
DexGen Controller
Teleop  Policy
(Dangerous)
(Safe and Useful)
Projection
Training Phase
Inference Phase
Fig. 2: Overview of proposed framework. Left (Training): We collect a large multi-task dexterous in-hand manipulation dataset
in simulation to pretrain a generative model that can generate diverse actions conditioned on the current state. The pretrained
generative model can produce useful actions including rotation, translation, and more intricated behaviors. Right (Inference):
During inference, we can project dangerous motion produced by teleoperation or policy back to a high-likelihood action with
guided sampling. This makes DexGen capable of assisting a coarse high-level policy to perform complex object manipulations.
I. INTRODUCTION
Dexterous robotic hands are increasingly capturing attention
due to their potential across various fields, including manu-
systems can replicate the fine motor skills of the human hand,
enabling complex object manipulation [50, 4]. Their ability
to perform tasks requiring human-like dexterity makes them
valuable in areas where traditional automation falls short.
skills to robotic hands remains a key challenge in robotics.
Recent data-driven approaches to teach robots dexterous
manipulation skills can be boardly categorized into two cate-
limitations in practical applications. For human teleoperation,
a major bottleneck is the collection of high-quality demon-
strations [32, 62]. In contact-rich dexterous manipulation, it
is challenging for humans to perform safe and stable object
manipulation actions, often resulting in objects falling from
the hand. This makes teleoperation impractical for dexterous
manipulation tasks. For sim-to-real RL, challenges arise from
the significant domain gap between simulation and the real
specifications when training an RL agent for complex tasks.
We will discuss these challenges in more detail in Section II.
While each approach has its own set of challenges, com-
bining their strengths offers a promising strategy to address
the complexities of dexterous manipulation. Specifically, re-
cent sim-to-real RL works [41, 63] have shown that it is
possible to train simple dexterous in-hand object manipulation
primitives (e.g. rotation) that can be transferred to a robot
in the real world. This suggests that RL can be leveraged
to generate a large-scale dataset of dexterous manipulation
grasp transitions. Meanwhile, humans excel at composing
these skills through teleoperation to address more challenging
tasks. For example, Yin et al. have shown that they can perform
in-hand reorientation by calling several rotation primitives
sequentially . However, the external inputs in these studies
are limited to a few discretized commands, lacking control
over low-level interactions, such as finger movements and
object contact. This limitation makes it difficult to prompt ex-
isting models to generate more detailed, finger-level interaction
Motivated by these observations, in this paper, we propose
a novel training framework called DexterityGen (DexGen) to
address the challenges of teaching dexterous in-hand manip-
ulation skills. Our main idea is to use a broad, multitask
simulation dataset generated via RL to pretrain a generative
behavior model (DexGen) that can translate a coarse motion
command to safe robot actions which can maximally preserve
the motion while guaranteeing safety. In real-world appli-
an imitation policy, can be used to prompt DexGen to exe-
cute meaningful manipulation skills. Our approach effectively
decouples high-level semantic motion generation from fine-
grained low-level control, serving as a foundational low-level
dexterity controller.
We validate our DexGen framework through both simulated
and real-world experiments. In simulation, we demonstrate
that DexGen significantly enhances the robustness and per-
formance of a highly perturbed noisy policy, extending its
stable operation duration by 10-100 times and enabling success
even when input commands are predominantly noise. In real-
world scenarios, we employ human teleoperation as a proxy
for high-level motion commands and test the framework on
various challenging dexterous manipulation tasks involving
complex hand-object interactions across a diverse set of ob-
jects. Notably, it successfully synthesizes trajectories to solve
challenging tasks, such as reorienting and using syringes and
screwdrivers for the first time (with human guidance).
II. EXISTING APPROACHES:
CHALLENGES AND OPPORTUNITIES
In this section, we review the challenges and opportunities
with existing approaches to dexterous manipulation that mo-
tivate our work.
A. Human Teleoperation for Imitation Learning
challenging for humans due to the following reasons:
a) Partial Observability: During in-hand manipulation,
the object motion is determined by the contact dynamics
between hand and object [58, 38, 23]. Successful manipulation
requires perceiving and understanding contact information,
such as normal force and friction, to generate appropriate
torques. However, human operators face challenges in ob-
serving this information due to occlusion and limited tactile
feedback. Additionally, existing discrete haptic feedback (e.g.
binary vibration) alone is often inadequate for conveying
complex touch interactions and contact geometries.
b) Embodiment Gap: Although human and robot hands
may appear similar at first glance, they differ significantly in
their kinematic structures and geometries. For example, human
fingers have smooth and compliant surfaces, while the robot
fingers often have rough edges. These differences result in
discrepancies in contact dynamics, making it challenging to
directly transfer our understanding of human finger motions
for object manipulation to robotic counterparts. In our early
change of fingertip shape.
c) Motion Complexity: Dexterous in-hand manipulation
involves highly complex motion. The process requires precise
control of a high degree-of-freedom dynamical system. Any
suboptimal teleoperation motion at any DOF can lead to
failures such as breaking grasping contacts.
d) Inaccuracy of Actions (Force): Existing robot hand
teleoperation systems are based on hand retargeting with po-
sition control, which lacks an intuitive force control interface
to users. As a result, users can only influence force through
position-control errors, making teleoperation particularly chal-
lenging in force-sensitive scenarios. Moreover, the presence of
noise in real world robot system further complicates control.
While humans may find it challenging to provide fine-grained,
low-level actions directly, human teleoperation or even video
demonstrations can still offer valuable coarse motion-level
guidance for a variety of complex real-world tasks. Humans
possess intuitive knowledge, such as where a robot hand
should make contact and what constitutes a good grasp. Thus,
human data can be leveraged to create a high-level semantic
action plan. In locomotion and whole-body control research,
recent studies have proposed using teleoperation commands
as high-level motion prompts [10, 17]. However, extending
this approach to finegrained dexterous manipulation remains
an open question.
B. Sim-to-real Reinforcement Learning
dexterous manipulation involves two main challenges:
a) Sim-to-Real Gap: It is difficult to reproduce real-
world sensor observation (mainly for vision input) and physics
in simulation. This gap can make sim-to-real transfer highly
challenging for complex tasks. In particular, transferring a
vision-based control policy from simulation to real world
for dexterous hands is a huge challenge and requires costly
visual domain randomization . For instance, Dextreme
leveraged extensive visual domain randomization with 5M
rendered images to train a single object rotation policy.
b) Reward Specification: A more important issue, be-
yond the sim-to-real gap, is the notorious challenge of design-
ing reward functions for long-horizon, contact-rich problems.
Existing methods often involve highly engineered rewards or
overly complicated learning strategies , which are task-
specific and limit scalability.
though sim-to-real RL can be difficult, especially for those
complex long-horizon or vision-based tasks, some recent
works have shown that sim-to-real RL is sufficient to build
diverse transferable manipulation primitives based on propri-
oception and touch . Therefore, one opportunity for sim-
to-real RL is to create rich low-level action primitives that can
be combined with the high-level action plan discussed above.
In this paper, we achieve this through generative pretraining.
Object Model
Grasp Set
Simulation RL
Data Collection
Grasp-to-Grasp
Trajectories
Diverse Wrist Movement
Diverse Grasp Set
Diverse Objects
Grasp Generation
Training Dataset Pipeline
Fig. 3: Dataset: The Anygrasp-to-Anygrasp dataset generation pipeline is designed for the generative pretraining of DexGen.
For a wide variety of objects, we extensively search for potential grasp configurations, using these as both the initial and
goal states for RL policies. To ensure our diffusion model can manage diverse scenarios, we incorporate varied wrist poses,
Diffusion Model
Gradient Guidance
Inverse Dynamics Model
Conditioning
Conditioning
DexGen Controller
Fig. 4: Model: Architecture of the DexGen controller. The whole system takes robot state, external motion conditioning, and
mode conditioning as input. A diffusion model first generates the motion as the intermediate action representation. The motion
conditioning is not fed into the diffusion model directly but as the gradient guidance during the diffusion sampling. Then,
another inverse dynamics model will translate the generated motion to executable robot action. We implement our diffusion
model as a UNet in this paper. The inverse dynamics model is a residual multilayer perceptron.
III. THE DEXGEN CONTROLLER
We propose to pretrain a generative behavior model p(ao)
on the simulation dataset to model prior action distribution so
that it can generate stable and effective actions a conditioned
on the robot state o. During inference, we can sample actions
from this distribution and further aligned with external motion
commands using gradient guidance. We detail the dataset used
for training the model in section III-B, the model architecture
in section III-C, and the inference procedure in section III-D.
A. Preliminaries
a) Diffusion Models: Diffusion Model  is a pow-
erful generative model capable of capturing highly complex
probabilistic distributions, which we use as our base model.
The classical form of the diffusion model is the Denoising
Diffusion Probabilistic Model (DDPM) . DDPM defines a
forward process that gradually adds noise to the data sample
x0 pdata(x):
xt  txt1
is some noising schedule. We have xt
N(tx0, (1 t)I) where t  Qt
s1 s goes to 0 as
t . DDPM trains a model (xt, t) to predict denoised
sample x0 given the noised sample xt with its timestep t.
During sampling, DDPM generates the sample by removing
the noise through a reverse diffusion process:
p(xt1xt)  N(xt1; (xt, t), 2
DDPM can generate high-fidelity samples in both vision and
robotics applications. In addition to the power to generate
data samples faithfully, diffusion models also support guided
p(x) pdata(x)h(x) where h(x) is given by a differentiable
energy function h(x)  exp J(x). To do this, we only need
to introduce a small modification to the reverse diffusion
step. Given the current sample , we add a correction term
J() to . Here,  is a step size hyperparameter and
is the variance in each diffusion step. This will guide the
sample towards high-energy regions in the sample space. We
can set h(x) to control the style of generated samples.
b) Robot System and Notations: In this paper, we assume
the robot hand is driven by a widely used PD controller. At
each control timestep, we command a joint target position qt
and the controller will use torque   Kp(qt qt) Kd qt to
drive the joints. Here, qt is the current joint position, and qt
is the joint velocity. Kp and Kd are two constant scalar gains.
We use xt to denote the key point positions of finger links at
time t in the wrist frame. Note that our algorithm does not rely
on a specific system implementation and can be extended to
other robot systems. We can also specify keypoints and actions
for other robots to implement our proposed algorithm.
B. Large-Scale Behavior Dataset Generation
Since human teleoperation or external policies will control
the robot hand to interact with the object in diverse ways, our
model should be capable of providing refinement for all these
potential scenarios (states). To achieve this, we require a large-
scale behavior dataset to pretrain our DexGen model, ensuring
comprehensive coverage of the state space. We accomplish
this by collecting object manipulation trajectories in simulation
through reinforcement learning.
Anygrasp-to-Anygrasp To ensure our dataset can cover a
broad range of potential states, we introduce Anygrasp-to-
Anygrasp as our central pretraining task. This task captures
the essential part of in-hand manipulation, which is to move
the object to arbitrary configurations. For each object, we
define our training task as follows. We first generate a set
of object grasps using Grasp Analysis and Rapidly-exploring
Random Tree (RRT) , similar to the Manipulation RRT
procedure . Each generated grasp is defined as a tuple
(hand joint position, object pose). In each RL rollout, we
initialize the object in the hand with a random grasp. We set
the goal to be a randomly selected nearby grasp using the
k Nearest-Neighbor search. After reaching the current grasp
to select a nearby reachable goal during the training process,
as learning to reach a distant grasp directly can be difficult.
After training, we use this anygrasp-to-anygrasp policy to
rollout grasp transition sequences to cover all the possible
hand-object interaction modes. We sample over 100K grasp
for most objects during grasp generation to ensure coverage.
This training procedure yields a rich repertoire of useful
the high-level policy can leverage for solving downstream
tasks (Figure 5). In addition to the Anygrasp-to-Anygrasp task,
we also introduce other tasks such as free finger moving and
fine-grained manipulation (e.g. fine rotation) to handle tasks
that have special precision requirements.
Pretraining Dataset
State Space
action distribution
Motion Prompt
Fig. 5: Our large-scale, multi-task pretraining dataset covers
diverse grasp to grasp transitions (arrows). DexGen controller
learns the dataset action distribution (purple shaded area) at
each state, and we can use sequential motion prompting (pur-
ple triangle) to perform a useful long-horizon skill, connecting
two distance states.
During RL training, we use a diverse set of random objects
and wrist poses. For each task, we include random geometrical
objects with different physical properties. To enhance the
robustness of our policy, we randomly adjust the wrist to dif-
ferent poses throughout the process, in addition to employing
commonly used domain randomizations, so the policy will
learn to counteract the gravity effects and exhibit prehensile
manipulation behavior (Figure 3). By combining all these data,
the robot hand can manipulate different kinds of objects in
different wrist configurations against gravity rather than being
limited to manipulating a single object at a certain pose. More
details of the RL training can be found in Appendix.
We collect a total of 1  1010 transitions as our simulation
Generating this dataset (by rolling out trained RL policies)
requires 300 GPU hours. Although the dataset is large, we
hypothesize it can still be far from sufficient as the human
dexterity emerges from millions of years of evolution. Never-
behavior that have not been showed before.
C. DexGen Model Architecture
We illustrate our DexGen model architecture in Figure 4.
The DexGen model has two modules. The first module is a
diffusion model that characterizes the distribution of robot fin-
ger keypoint motions given current observations. Here we use
3D keypoint motions x RT K3 in the robot hand frame
as an intermediate action representation, This representation is
particularly advantageous for integrating guidance from human
teleoperation. In this context, T is the future horizon, K is the
number of finger keypoints. The second module in DexGen
is an inverse dynamics model, which converts the keypoint
motions to executable robot actions (i.e. target joint position)
We use a UNet-based  diffusion model to fit the complex
keypoint motion distribution of our multitask dataset. Our
Wrist Tracker
Hand Tracker
Hand Tracker
Wrist Tracker
Fig. 6: Real world experimental setup based on Allegro Hand
with a Franka Panda Arm (Left). We use human teleoperation
(Right) as a proxy for high-level policy.
model learns to generate several future finger keypoint offsets
xi  xti xt conditioned on the robot state at timestep
t and a mode conditioning variable. The state is a stack of
historical proprioception information. The mode conditioning
variable is a one-hot vector to explicitly indicate the intention
of the task. For instance, when placing an object we do not
want the model to produce actions that will make the robot
hold the object firmly. Without introducing a release object
if most of the actions in the dataset will keep the object
in the palm. In our dataset, the majority of transitions are
labeled with a default (unconditional) label, and only a small
portion of them corresponding to specialized scenarios has
a special mode label. We only use a specialized precision
rotation mode label for screwdriver in our experiments. For
releasing object, we find that disabling DexGen controller is
sufficient in practice.
The inverse dynamics model is a simple residual multilayer
perceptron that outputs a normal distribution to model the
actions conditioned on the current robot state and motion
command. We train both the diffusion model and inverse
dynamics model with our generated simulation dataset using
the standard diffusion model loss function and the MSE loss
for regression respectively. We train these models with the
AdamW optimizer [35, 29] for 15 epochs using 96 GPUs,
which takes approximately 3 days. The detailed network setup
can be found in the appendix.
D. Inference: Motion Conditioning with Guided Sampling
Our goal is to sample a keypoint motion that is both safe (i.e.
from our learned distribution p(xo)) and can maximally
preserve the input reference motion. Formally, this can be
written as x p(xo) exp(Dist(x, xinput)). Here,
xinput RK3 is the input commanded fingertip offset, and
Dist is a distance function that quantifies the distance between
the predicted sequence and the input reference. There can be
many ways to instantiate this distance function. In this paper,
Fig. 7: Part of our real world testing objects, which are
not present in our pretraining dataset. We include objects of
different sizes, masses, and aspect ratios.
we find the following simple distance function works well
Dist(x, xinput)
xi xinput2.
The above function encourages the generated future fingertip
position to closely match the commanded fingertip position.
Since the action of the robot hand has a high degree of
freedom (16 for the Allegro hand used in this paper), naive
sampling strategies become computationally intractable. To
address this, we propose using gradient guidance in the
diffusion sampling process to incorporate motion conditioning.
In each diffusion step, we adjust the denoised sample x by
subtracting xDist(x, xinput) as a guide. Here  is
a parameter of the strength of the guidance to be tuned, which
we will study in experiments. The generated finger keypoint
movement is then converted to action by the inverse dynamics
model. We use DDIM sampler  during inference for 10Hz
control. The total sampling time is around 27ms (37Hz) on
a Lambda workstation equipped with an NVIDIA RTX 4090
IV. EXPERIMENTS
In the experiments, we first validate the effectiveness of
DexGen through simulated experiments, demonstrating its
ability to enhance the robustness and success rate of extremely
suboptimal policies. Then, we test our system in the real world
with a focus on its application in shared autonomy. Our results
show that DexGen can assist a human operator in executing
unprecedented dexterous manipulation skills with remarkable
generalizability.
A. System Setup
In this paper, we use Allegro Hand as our manipulator
and we attach the Allegro Hand to a Franka-panda robot
arm. In the teleoperation experiments in real world, we use a
retargeting-based system to control the robot with human hand
Guidance
Duration
Guidance
Duration
Guidance
Duration
Mul U(0, 0.5)
Guidance
Duration
Achieved Goals
Achieved Goals
Achieved Goals
Ours (Duration)
Ours ( Goals)
Baseline (Duration)
Baseline ( Goal)
Fig. 8: Results of simulation evaluation. We use DexGen to correct several noise-corrupted expert policies. Note that each
dimension of action space is bounded by [-1, 1] and these noises ruin the expert action most of the time. We measure the
average duration (in seconds) and number of achieved goals per trial over a 20-minute simulated experiment. As shown in the
duration by 10-100x and even help an extremely perturbed policy to achieve success where the baseline fails.
gestures. The human hand pose is captured by Manus Glove
and retargeted to the Allegro hand through the Geometric
Retargeting algorithm . We obtain the 6D human wrist
pose via the Vive tracking system and use it to control the
robot arm separately. Although we use this single robot setup
in our experiments, we believe our method is general and can
be applied to other hand setups.
B. Simulated Experiments
1) Experimental Setup: We first test the capability of Dex-
Gen in assisting suboptimal policies in solving the Anygrasp-
to-Anygrasp task in simulation. We simulate 2 kinds of
suboptimal policies with an expert RL policy exp. The first
one is noisy(as)  exp(as)  U(, ), which simulates
an expert that can perform dangerous suboptimal actions
through additive uniform noise. The second is slow(as)
U(0, )exp(as), which is a slowdown version of expert.
We compare these suboptimal experts  to their assisted
counterparts DexGen . We record the average number of
critical failures (drop the object) and the number of goal
achievements within a certain time of different policies.
2) Main Results:
We plot the result of different policies
in Figure 8. We find that without our assistance, the noisy
expert has much more frequent failures. As a result, it can
only hardly achieve any goals in the evaluation. In contrast,
with the assistance of DexGen, we can partially recover
the performance of this noisy expert. We also find that for
different policies, the optimal guidance value is also different.
these policies. Moreover, when the guidance is relatively small,
although we can maintain the object in hand, we can not
achieve the desired goal as well because DexGen does not
know what the goal is. When the guidance becomes too large,
the potentially suboptimal external motion command may take
over DexGen guidance and lead to a lower duration in some
Fig. 9: DexGen can maximally preserve input action while cor-
recting dangerous actions. DexGen can reject users behavior
(open up the palm) and keep holding the object.
C. Real World Experiments:
We have demonstrated that our system can provide effective
assistance through simulated validation. Then, we further
design several tasks for benchmarking in the real world. In
the first set of experiments, we ask a human teleoperator to
act as an external high-level policy and we evaluate whether
our system can assist humans to solve diverse dexterous
manipulation tasks. We introduce a set of atomic skills that
covers common in-hand dexterous manipulation behavior.
In-hand Object Reorientation The user is required to
control the hand to rotate a given object to a specific
pose. In the beginning, we initialize the object in the air
over the palm, and the user needs
