=== PDF文件: Learning to Act Anywhere with Task-centric Latent Actions.pdf ===
=== 时间: 2025-07-22 16:12:40.232227 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个最核心的最有代表性的中文关键词，要中文关键词，如果是英文关键词就尝试翻译成中文（不能是英文，不能是多个，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：Learning to Act Anywhere with
Task-centric Latent Actions
Qingwen Bu1,2, Yanting Yang2, Jisong Cai2, Shenyuan Gao2, Guanghui Ren3,
Maoqing Yao3, Ping Luo1,2 and Hongyang Li1,2
1 The University of Hong Kong 2 OpenDriveLab
3 AgiBot
Latent Action Space
Any Embodiment
Action-label Free
Any view
Real-world
Room2Room
SimplerEnv
Robot Manipulation
Indoor  Outdoor Navigation
Human Videos
Task-centric
Fig. 1: We introduce UniVLA, a unified vision-language-action (VLA) framework that enables policy learning across different
environments. By deriving task-centric latent actions in an unsupervised manner, UniVLA can leverage data from arbitrary
embodiments and perspectives without action labels. After large-scale pretraining from videos, UniVLA develops a cross-
embodiment generalist policy that can be readily deployed across various robots by learning an action decoding with minimal
cost. Compared to OpenVLA , UniVLA exhibits unanimous improvement on multiple manipulation and navigation tasks.
AbstractA generalist robot should perform effectively across
various environments. However, most existing approaches heavily
rely on scaling action-annotated data to enhance their capa-
bilities. Consequently, they are often limited to single physical
specification and struggle to learn transferable knowledge across
different embodiments and environments. To confront these
cross-embodiment vision-language-action (VLA) policies. Our
key innovation is to derive task-centric action representations
from videos with a latent action model. This enables us to exploit
extensive data across a wide spectrum of embodiments and
perspectives. To mitigate the effect of task-irrelevant dynamics,
we incorporate language instructions and establish a latent action
model within the DINO feature space. Learned from internet-
scale videos, the generalist policy can be deployed to various
robots through efficient latent action decoding. We obtain state-
of-the-art results across multiple manipulation and navigation
superior performance over OpenVLA with less than 120 of
pretraining compute and 110 of downstream data. Continuous
performance improvements are observed as heterogeneous data,
even including human videos, are incorporated into the training
pipeline. The results underscore UniVLAs potential to facilitate
scalable and efficient robot policy learning.
I. INTRODUCTION
Empowered
emergence
large-scale
datasets [78, 63, 38, 18], robot policies based on vision-
language-action models (VLA) have made encouraging strides
recently [9, 28, 39]. However, they typically rely on ground-
truth action labels for supervision, which limits their scalability
in utilizing internet-scale data from diverse environments. Fur-
across different embodiments (e.g., Franka, WidowX, and even
human hands) and tasks (e.g., manipulation and navigation)
poses a significant challenge to effective knowledge transfer.
This raises a crucial question: could we learn a unified action
representation that enables the generalist policy to plan ef-
facilitating knowledge transfer across different embodiments
and environments?
To address these challenges, we propose UniVLA, a gener-
alist policy learning framework that enables scalable and effi-
cient planning across various embodiments and environments.
Much like large language models (LLMs) learn cross-lingual
shared knowledge [22, 17], we aim to construct a unified
action space that facilitates knowledge transfer across video
human videos. Our recipe for generalist policy consists of three
key stages: 1) Task-centric Latent Action Learning, where
we extract task-relevant action representations from massive
cross-embodiment videos in an unsupervised manner. This
is achieved by discretizing latent actions from the inverse
dynamics of paired frames using a VQ-VAE . 2) Next-
latent action prediction, where we train an auto-regressive
vision-language model with discretized latent action tokens,
endowing it with embodiment-agnostic planning capabilities.
3) Latents decoding, where we decode latent plans into
physical behaviors and specialize the pretrained generalist
policy for deployment in unseen tasks efficiently.
While recent studies [87, 15] have investigated the via-
bility of learning latent actions from web-scale videos, they
suffer from a critical limitation: their naive reconstruction-
based objectives often capture task-irrelevant dynamics, such
as movements of non-ego agents or unpredictable camera
shifts. These noisy representations hinder policy pretraining
by introducing distractions unrelated to the task. To address
patch-level representations from pixels, providing both spatial
and object-centric priors that better capture task-relevant infor-
mation. By using the readily available language instructions
as conditions, we further disentangle movements into two
complementary action representations, one of which explicitly
represents task-centric actions.
UniVLA achieves state-of-the-art performance across mul-
tiple manipulation benchmarks and navigation tasks, outper-
forming OpenVLA  by a significant margin while re-
quiring merely 120 of the pretraining cost (in GPU hours).
This efficiency stems from its task-centric latent action space,
which decouples task-relevant dynamics from extraneous vi-
sual changes. Our action representation not only reduces
computational overhead but also enables efficient scaling - as
dataset size grows, UniVLAs performance improves, effec-
tively leveraging cross-embodiment, cross-view robot datasets
and even unlabeled human videos to expand its pretraining
corpus and extract transferable knowledge. Remarkably, when
pretrained solely on the Bridge-V2 dataset , UniVLA
surpasses OpenVLA and LAPA trained on the larger Open
X-Embodiment  dataset, underscoring its ability to distill
transferrable knowledge from limited data.
In addition, we employ a lightweight decoder with only
10.8M parameters to translate latent actions into executable
tuning. This design leverages the compact and informative
nature of the task-centric latent action space, enabling UniVLA
to adapt efficiently to diverse tasks and embodiments with
minimal downstream data. Our comprehensive evaluation,
spanning manipulation, navigation, and real-world deploy-
next-generation generalist robotic policies.
In summary, our main contributions are three-folds:
We propose UniVLA, a recipe towards generalist policy
by planning in a unified, embodiment-agnostic action
learning from web-scale videos.
We introduce a novel approach for extracting task-
relevant latent actions from cross-embodiment videos,
decoupling task-centric dynamics from irrelevant visual
changes. Both qualitative and quantitative experiments
highlight its merits and advantages over existing works.
UniVLA achieves state-of-the-art performance on mul-
tiple benchmarks and real-robot tests, achieving an
18.5 increase in success rate over OpenVLA on the
LIBERO  benchmark, 29.6 in navigation tasks ,
and a 36.7 improvement in real-world deployments.
II. RELATED WORK
A. Vision-language-action Models
Building on the success of pretrained vision foundation
models (VLMs), VLAs have been introduced to process
multimodal inputsvisual observations and language instruc-
tionsand generate robotic actions for completing embodied
tasks. RT-1  and Octo  employ a transformer-based
policy that integrates diverse data, including robot trajectories
across various tasks, objects, environments, and embodiments.
In contrast, some prior works [9, 39, 46] leverage pretrained
VLMs to generate robotic actions by tapping into world knowl-
edge from large-scale vision-language datasets. For instance,
RT-2  and OpenVLA  treat actions as tokens within
the language models vocabulary, while RoboFlamingo
introduces an additional policy head for action prediction.
Building on these generalist policies, RoboDual  proposes
a synergistic dual-system that combines the strengths of both
generalist and specialist policy. Other works incorporate goal
image  or video [24, 82, 13] prediction tasks to generate
with these visual cues subsequently guiding the policy in
action generation. However, these methods heavily rely on
interactive data with ground-truth action labels, which sig-
nificantly restricts the scalability of VLAs. In contrast, our
approach unlocks the potential of internet-scale, action-free
videos by learning a unified latent action representation from
visual changes, independent of action labels.
B. Cross-embodiment Learning
Training a general-purpose robot policy is challenging due
to the diversity in camera perspectives, proprioceptive inputs,
joint configurations, action spaces, and control frequencies
across robotic systems. Early approach  focused on align-
ing action space manually between navigation and manipula-
tion but were limited to wrist cameras in manipulation. Recent
transformer-based approaches [28, 23] address these chal-
lenges by accommodating variable observations and actions,
with CrossFormer  co-training across four distinct action
spaces without imposing constraints on observation spaces
or requiring explicit action-space alignment. Flow representa-
point clouds, have been widely explored for cross-embodiment
learning [81, 88, 26, 83]. ATM  learns flow generation
from human demonstrations, while Im2Flow2Act  predicts
object flows from human videos without in-domain data.
alternative approach, with SPOT  predicting object trajec-
tories in SE(3) to decouple embodiment actions from sensory
inputs. Existing approaches demand extensive, diverse datasets
to cover all possible state-transition patterns and need explicit
sets itself apart by using a discrete codebook to encode latent
actions in an unsupervised manner. Our approach effectively
filters out visual noise and achieves efficient information com-
pression via vector quantization, thereby enhancing training
efficiency and lessening the reliance on data diversity.
C. Latent Action Learning
Several prior works focus on learning variational auto-
encoders [64, 76] on raw action trajectories to structure new
action spaces, emphasizing compact latent representations that
facilitate behavior generation and task adaptation, as seen in
VQ-BeT  and Quest . These methods are also adopted
in reinforcement learning to accelerate convergence . Re-
cent works [79, 74] explore vector quantization as action space
adapters to better integrate actions into large language models.
on ground-truth action labels, which limits their scalability.
To leverage broader video data, Genie  extracts latent
actions via a causal latent action model, conditioning on next-
frame prediction. Similarly, LAPO  and DynaMo
learn latent actions directly from visual data, bypassing meth-
ods using explicit action labels on in-domain manipulation
tasks. LAPA  and IGOR  introduce unsupervised
pretraining methods to teach VLAs discrete latent actions,
aiming to transfer knowledge from human videos. However,
these approaches encode all visual changes from raw pixels,
capturing task-irrelevant dynamics such as camera shakiness,
movements of other agents, or new object appearances, which
ultimately degrade policy performance. We propose a novel
training framework to decouple task-centric dynamics from
irrelevant visual changes, structuring a more effective latent
action space to enable robust policy planning.
III. METHODOLOGY
implement
(Sec. III-A) Leveraging language-based goal specifications,
we extract inverse dynamics from extensive video datasets in
an unsupervised manner, yielding a discretized set of task-
centric latent actions that generalize across diverse embodi-
ments and domains; 2) (Sec. III-B) Based on this, we train
an auto-regressive transformer-based vision-language-action
inputs to predict latent action tokens in a unified latent space;
3) (Sec. III-C) To facilitate efficient adaptation to various
robotic control systems, we introduce specialized policy heads
that decode latent actions into executable control signals.
A. Task-centric Latent Action Learning
The first step establishes the foundational groundwork of
our framework by generating the pseudo action labels (i.e.,
latent action tokens), which serve as the basis for training our
generalist policy in subsequent stages.
Latent action quantization. Fig. 2 illustrates the two-stage
training pipeline and overall architecture of our latent action
model. We start with a pair of consecutive video frames,
denoted as {ot, otk}, separated by a frame interval k. To
ensure a uniform time interval of approximately 1 second
across diverse datasets, the frame interval is calibrated ac-
cording to the recording frequency specific to each dataset.
To derive latent actions from videos, our latent action model
is constructed around an Inverse Dynamics Model (IDM)
based encoder I(atot, otk) and a Forward Dynamics Model
(FDM) based decoder F(otkot, at). The encoder infers
latent action given consecutive observations, and the decoder
is trained to predict future observations given specified latent
actions. We implement the encoder as a spatial-temporal trans-
former  with casual temporal masks, following Villegas
et al. . A group of learnable action tokens aq RNd,
with predefined dimension d, are concatenated sequentially to
the video features to extract the dynamics.
To further compress the information and align it with the
learning objective  of an auto-regressive transformer-based
Quantized action tokens az RNd are optimized with
VQ-VAE  objective, with a codebook of C vocabulary
size. The decoder, implemented as a spatial transformer, is
optimized to predict future frames utilizing only the quantized
action tokens. We do not feed decoder with historical frames to
prevent the model from over-relying on contextual information
or merely memorizing the dataset.
While recent works [11, 27, 87] employs raw pixels for
models to attend to noisy, task-irrelevant details (e.g., tex-
and crowd-sourced video datasets , where uncontrolled
capture conditions introduce further variability. Inspired by
joint-embedding predictive architectures (JEPA) [4, 5, 96], we
propose using DINOv2  spatial patch features as semanti-
cally rich representations. Their object-centric and spatially
aware properties make them ideal not only as inputs but
also as prediction targets for latent action models. Our self-
supervised objective minimizes the embedding reconstruction
the DINOv2 feature of paired video frames {ot, otk}. The
compact latent action must thus encode the transformation
between observations to minimize prediction error.
Latent action decoupling. As discussed earlier, the actions of
the robots are often entangled with irrelevant environmental
variations in web-scale videos. To mitigate the unfavorable
effect of task-irrelevant dynamics, we incorporate readily
available language instructions into the first training stage of
latent action model (Fig. 2 Left). The language inputs are
encoded using a pretrained T5 text encoder  and serve
as conditioning signals in the context for both the encoder
and decoder. This process can be formally described as:
aTI  I([Ot; Otk; aTI; ]), aTI  VQ(aTI),
Otk  F([Ot; aTI; ]),
Task Instruction
Spatial Transformer
Latent Action Quantize
Task-irrelevant
Supervise
Spatial-Temporal Transformer
Task-centric
Spatial-Temporal Transformer
Spatial Transformer
Latent Action Quantize
Supervise
Fig. 2: Task-centric latent action learning. We propose a two-stage training framework aimed at disentangling task-centric
visual dynamics and changes from extraneous factors. In Stage 1, task instruction embeddings, derived from a pre-trained T5
text encoder , are utilized as inputs to both the encoder and decoder. These embeddings provide task-relevant semantic
information to enhance predictive accuracy. In Stage 2, a novel set of latent actions is introduced, specifically designed to
replace the role of language and to capture task-centric dynamics from DINOv2-encoded features of video frames.
where [; ] denotes sequence-wise concatenation, VQ repre-
sents the codebook for vector quantized action representation,
and is the instruction embedding from the T5 text encoder.
Sending task instructions to the decoder provides high-level se-
mantic guidance regarding the underlying actions. As a result,
the quantized latent actions are optimized to encode only the
environmental changes and visual details , omitting higher-
level task-relevant information due to the constrained capacity
of the codebook . This stage establishes a set of latent
actions that encapsulate task-irrelevant information, such as
the emergence of new objects, movements of external agents,
or camera-induced motion artifacts. These dynamics, while
critical for grounding the model in the visual environment,
are orthogonal to the specific objectives of the task.
Following this, we repurpose the task-irrelevant codebook
and parameters of the latent action model trained in Stage 1
for the following stage (depicted in Fig. 2 Right), where the
objective is to learn a new set of task-centric latent actions
aTC upon which the policy is trained. In this stage, the model
extracts action information through:
{aTI, aTC}  I([Ot; Otk; aTI; aTC]),
aTI  VQ(aTI), aTC  VQTC(aTC),
Otk  F([Ot; aTI; aTC]),
where VQTC denotes the newly initialized codebook for
learning task-centric dynamics. Building upon the acquired
task-irrelevant representations, we freeze the corresponding
izing the new set of latent actions. This specialization facili-
tates the precise modeling of task-related dynamics, such as
object manipulation or goal-directed motion trajectories. The
explicit decoupling of latent action representations enhances
our generalist policys generalization capability across diverse
environments and tasks. Compared to naive latent action
learning approaches (e.g., LAPA ), training exclusively
on task-centric representations yields faster convergence while
achieving robust performance, suggesting these latent actions
are more informative for subsequent policy learning.
B. Pretraining of Generalist Policy
With the latent action model trained in the preceding step,
we proceed to label any video frame ot with latent actions
generalist policy. To align with Kim et al. , our gen-
eralist policy is built upon the Prismatic-7B  vision-
language model (VLM). The architecture integrates a fused
visual encoder derived from SigLip  and DINOv2 , a
projection layer to align visual embeddings with the language
Unlike prior LLM-based generalist policies (i.e., RT-2  and
OpenVLA ) that directly plan in low-level action spaces
by mapping infrequently used words in the LLaMA tokenizer
vocabulary to uniformly distributed action bins within [1, 1],
we extend the vocabulary with C special tokens, specifically
{ACT1, ACT2, ACT3,..., ACTC}. Latent actions are
projected into this vocabulary based on their indices in the
action codebook. This approach preserves the original model
architecture and training objectives of the VLM, fully lever-
aging its pretrained knowledge for transfer to robotic control
tasks. Specifically, our policy model  receives observation
and is optimized to minimize the sum of next-latent-action
negative log-probabilities:
L  Eot,l,az,<i
log (az,i  az,i  ot, l, az,<i)
where N represents the total length of action tokens. We set
N  4 for all our experiments. Moreover, empirical evidence
indicates that a compressed action space (e.g., reducing from
2567 in OpenVLA  to 164 when C  16) signifi-
cantly accelerates model convergence. Our approach achieves
competitive results with only 960 A100-hours of pretraining,
a substantial reduction compared to the 21,500 A100-hours
required for OpenVLA pretraining.
Auto-regressive Transformer
Tokenizer
Third-view RGB
Task Instruction
Latent Action
Detokenizer
Action Decoder
<ACT1>,<ACT2>,,<ACTC>
Latent Action Tokens:
Heterogeneous Action-space
Fig. 3: Architecture of the generalist policy. Our policy
architecture is founded on the Prismatic-7B Vision-Language
Model (VLM) , which processes projected visual em-
beddings and tokenized task instructions as inputs to predict
latent action tokens in an auto-regressive manner. To adapt
to specific robotic systems, specialized action decoder heads
are employed. These decoders leverage visual information
to extract context-specific features from latent actions and
subsequently translate them into executable control signals of
robotic systems with heterogeneous action spaces.
By training our policy within a unified latent action space,
the model capitalizes on transferable knowledge derived from
cross-domain datasets. Unlike Yang et al.  which neces-
sitates manual alignment of action spaces through visually
similar egocentric motions, such as wrist camera movements
in manipulation tasks and egocentric navigation, our method
eliminates this requirement. Consequently, UniVLA expands
the scope of utilizable datasets and enhances overall perfor-
latent action representations for scalable policy learning.
C. Post-training for Deployment
Latent action decoding. During downstream adaptation,
the pre-trained generalist policy maintains its embodiment-
agnostic characteristics by predicting the next latent action
during downstream adaptation. To bridge the gap between
latent actions and executable behaviors, additional action de-
coders are employed (as depicted in Fig. 3). Specifically, the
sequence of visual embeddings is first aggregated into a single
token through multi-head attention pooling , which then
functions as the query to extract information from the latent
action embeddings. This process is formulated as:
Visual Embed.:
v  A(Q  qv, K  V  Ev),
Action Embed.:
a  A(Q  qa  E
where A represents multi-head attention, {Ev, Ea} are visual
and latent action embeddings from the last layer of VLM, and
{qv, qa} are randomly initialized queries to extract visual and
action information respectively. The resultant action embed-
a is subsequently projected linearly into the desired
action space of the target robotic system. Given that latent
actions are designed to represent actions occurring within
approximately a one-second interval (mentioned in Sec. III-A),
they can be naturally decoded into action chunks . The
chunk size can be easily customized for specific embodiments
to achieve smoother and more precise control.
In practice, we employ parameter-efficient fine-tuning using
LoRA  to achieve efficient adaptation. With the addition
of the action head comprising merely 12.6M parameters, the
total number of trainable parameters is approximately 123M.
The entire model is trained end-to-end, optimizing both the
next-latent action prediction loss and the L1 loss between the
ground-truth and predicted low-level actions.
Learn from history outputs. Historical observations have
been demonstrated to play a critical role in enhancing sequen-
tial decision-making processes for robotic control [60, 42, 45].
els with multiple historical observations introduces significant
inference latency and results in redundant information within
visual tokens [94, 45]. Drawing inspiration from the well-
established Chain-of-Thought (CoT) reasoning paradigm
in large language models (LLMs), which generates interme-
diate reasoning steps to address complex tasks, we propose
leveraging historical latent action outputs to facilitate decision-
making in robotic control. Much like LLMs resolve questions
at each timestep during rollouts. This establishes a feedback
loop for the robot policy, enabling policy to learn from its own
decisions and adapt to dynamic environments.
To operationalize this approach, we employ the latent action
model to annotate actions extracted from historical frames.
These annotated actions are then mapped into the LLaMA
token vocabulary and appended to task instructions. During
to endow the model with in-context learning capabilities. At
inference time, one step of historical latent action (encoded
as N  4 tokens) is incorporated at each timestep, with the
exception of the initial step. Empirical results demonstrate
that this straightforward design improves model performance,
particularly in long-horizon tasks (see Sec. IV-C).
IV. EVALUATIONS
To demonstrate the performance of our proposed generalist
UniVLA across a diverse suite of benchmarks (including
manipulation benchmarks: LIBERO , CALVIN , Sim-
world scenarios. Additionally, we conduct latent action analy-
sis to quantify the task-centric property, and perform ablation
studies to explore critical design choices. With comprehensive
1) Performance  Adaptability. Can UniVLA success-
fully transfer the knowledge acquired during pretrain-
ing to novel embodiments and tasks and adapt effi-
ciently? (See Sec. IV-A for manipulation performance
and Sec. IV-A2 for adaptability to navigation.)
Pick up the bowl in the
top drawer and place it
on the plate
Pick up the salad dressing
and put it in the basket
Open the middle drawer
of the cabinet
Put the yellow and white
mug in the microwave
and close it
Same object, unseen layout
Seen layout, novel objects
Same object  layout, different goals
Long-horion Manipulation
LIBERO-Long
LIBERO-Goal
LIBERO-Object
LIBERO-Spatial
Fig. 4: Task setup on the LIBERO benchmark.
TABLE I: Results on LIBERO benchmark across four evaluation
suites. Our proposed UniVLA exhibits superior performance across all
benchmarked tasks compared to existing baseline methods, attributable
to its enhanced knowledge transferability and generalization capabil-
ities. Our model achieves state-of-the-art results despite being pre-
trained exclusively on either the Bridge-V2  dataset or action-free
human video data (denoted as Bridge and Human respectively).
Methods use additional wrist-view camera inputs. We reproduced
results of LAPA using the Prismatic-7B VLM.
Diffusion Policy
2) Generalizability. How does UniVLA generalize to un-
seen scenarios? (See Sec. IV-A3 for the analysis of its
generalizability in novel settings.)
3) Scalability. Can UniVLA effectively utilize diverse data
able benefits from the continuously expanding dataset?
(See Sec. IV-C for data scalability analysis.)
A. Main Results
1) Manipulation Benchmark on LIBERO
Experiment setup. We pretrain our full latent action model
on manipulation data, navigation data and human videos
) respectively. The pretraining details can be found in
Appendix A1. The LIBERO benchmark  comprises four
task suites specifically designed to facilitate research on life-
long learning in robotic manipulation. Our experiments exclu-
sively focus on supervised fine-tuning within the target task
through behavioral cloning on successful task demonstrations.
As illustrated in Fig. 4, our experimental setup includes the
following task suites, each consisting of 10 tasks with 50
human-teleoperated demonstrations per task:
1) LIBERO-Spatial requires the policy to infer spatial
relationships to accurately place a bowl, evaluating the
models ability to reason about geometric configurations;
2) LIBERO-Object maintains identical scene layouts but
introduces variations in object types, assessing the pol-
icys capacity to generalize across object instances;
3) LIBERO-Goal retains consistent objects and layouts
while assigning diverse task objectives, challenging the
policy to exhibit goal-oriented behavior and adaptability;
4) LIBERO-Long focuses on long-horizon manipulation
tasks involving multiple sub-goals, incorporating hetero-
geneous objects, layouts, and task sequences to evaluate
the models proficiency in complex, multi-step planning.
We adhere to the data processing pipeline introduced in
OpenVLA  to exclude failure cases from the demonstration
data used for training. UniVLA is trained on LIBERO-Long
for 40k steps and other test suites for 30k steps, with a
global batch size of 128. We only use third-person image and
language instructions as inputs. Notably, none of the samples
in LIBERO is included in the pretraining dataset of policy,
and the training data for our latent action model, necessitating
generalizability for both. In addition to presenting the results
of our most performant model, which is pre-trained on the
full dataset, we also provide results from models pre-trained
exclusively on the Bridge-V2  and human data, denoted
as Bridge and Human in Tab. I, respectively. To minimize
suite (i.e., 50 trials per task), with the reported performance
reflecting the average success rate across three seeds.
Baselines. Our selected baseline models include the following
five representative models, where OpenVLA and LAPA are
more closely related to our method:
LAPA  introduces an unsupervised framework for
learning latent actions from unlabeled human videos.
Octo  is a transformer-based policy trained on di-
verse robotic datasets, which employs a unified action
representation to handle heterogeneous action spaces.
MDT  leverages diffusion models to generate flexible
action sequences conditioned by multimodal goals.
OpenVLA  is a vision-language-action model that
leverages large-scale pretraining on diverse datasets, in-
cluding OpenX, to enable generalist robotic policies.
MaIL  enhances imitation learning by incorporating
selective state space models, which improve the efficiency
and scalability of policy learning.
Store the screwdriver
Clean the cutting board
Stack tower of hanoi
UniVLA (Ours)
Fold towel twice
Diffusion Policy
Average Score
Average Success Rate
Fig. 5: Real-world robot experiments. We propose four different tasks: Store the screwdriver, Clean the cutting board,
Fold towel twice, and Stack tower of hanoi, towards the evaluation of four axis of policys capabilities. UniVLA outperforms
previous state-of-the-art with an average elevation of 36.7 success rate and 0.68 average score across all tasks.
Oracle Success Rate
LLaVA-Nav
Fig. 6: Oracle success rate on R2R in VLN-CE. With only a
single-frame RGB input, UniVLA demonstrates performance
on par with NaVid, a navigation model that incorporates the
entirety of historical observations, while markedly outperform-
ing OpenVLA in success rate.
Results. The results presented in Tab. I demonstrate the
exceptional performance of UniVLA across all four evalua-
tion suites, significantly outperforming prior generalist poli-
cies such as OpenVLA, LAPA, and Octo. Notably, UniVLA
achieves an average performance of 95.4 by pretraining on
the full dataset, surpassing OpenVLA and LAPA by margins of
18.5 and 29.3 respectively. Despite being pretrained solely
on the Bridge-V2 dataset, UniVLA attains 93.0 average
MDT (76.1) that leverage additional wrist-view camera
inputs. Pretraining our policy with human data outcompetes
margin of 12.2. In conclusion, UniVLA shows unparalleled
knowledge transfer capability and establishes a new state-of-
the-art on LIBERO benchmark. We provide additional results
on CALVIN and SimplerEnv benchmark in Appendix B.
2) Navigation Benchmark on Room2Room
Experiment setup. In this experiment, we evaluate UniVLA
on the VLN-CE benchmarks  to assess its performance on
navigation tasks. These benchmarks offer a set of language-
guided navigation tasks and continuous environments for exe-
cuting low-level actions in reconstructed photorealistic indoor
scenes. Specifically, we focus on the Room2Room (R2R)
task in VLN-CE, one of the most widely recognized bench-
marks in vision-and-language navigation (VLN). All methods
are trained on the 10,819 samples in the R2R training split
and evaluated on the 1,839 samples in the R2R val-unseen
split. We use the oracle success rate to evaluate navigation
performance. An episode is considered successful if the agent
arrives within 3 meters of the goal in the VLN-CE.
Baselines. To ensure a fair comparison with UniVLA, we
evaluate RGB-only methods that operate without depth or
odometry data, directly predicting low-level actions within the
VLN-CE environments. Selected baselines are as follows:
Seq2Seq  is a recurrent sequence-to-sequence policy
that predicts actions from RGB observations.
CMA  employs cross-modal attention to integrate
instructions with RGB observations for action prediction.
LLaVA-Nav is a modified version of LLaVA , co-
finetuned with data proposed by NaVid , and encodes
history using an observation-to-history technique.
OpenVLA  is a vision-language-action model. We
introduce several special tokens to tokenize navigation
actions and finetune the model on the R2R training split.
NaVid  is a video-based large vision-language model
that encodes all historical RGB observations. It uses a
pretrained vision encoder to encode visual observations
and a pretrained LLM to predict actions.
Results. In Fig. 6, we report the oracle success rate for
each method. UniVLA significantly outperforms Seq2Seq and
Given the high computational cost of prompting history in
a 100-episode subset of the VLN-CE R2R val-unseen split.
UniVLA surpasses the oracle success rate of LLaVA-Nav
by 33.1 and OpenVLA by 29.6. Furthermore, UniVLA
achieves an oracle success rate comparable to NaVid, which
TABLE II: Generalizability evaluation. UniVLA demonstrates superior performance across all evaluated tasks, showcasing
its exceptional ability to generalize from high-level semantic comprehension to low-level visual robustness.
Lightning Variation
Visual Distractor
Novel Object
Diffusion Policy
UniVLA (Ours)
Visual Distractor
Novel Object
Lightning Variation
Original Setting
Fig. 7: Setting on generalizability evaluations. We evaluate
the generalizability of policies in 3 different settings. (a)
Lightning Variation: We dimmed the ambient light and applied
strong lighting in a specified direction. (b) Visual Distractor:
We added a bowl, notebook, and tape on the tabletop. (c)
Novel Object: We replaced the object to be manipulated from
a screwdriver to an unseen marker pen.
encodes all historical observations, while UniVLA conditions
only on the current observation and historical latent action.
3) Real-world Robot Deployment
Experiment setup. All real-world experiments are conducted
with a Piper arm from AgileX Robotics featuring a 7-DoF
action space and a third-view Orbecc DABAI RGB-D camera,
which we only utilize RGB images as input. To evaluate
various dimensions of policy capabilities, including:
1) Spatial Awareness: Pick up the screwdriver to put it into
the cabinet and close the door (Store the screwdriver).
2) Tool-usage and Nonprehensile Manipulation: Pick up
the broom and sweep the items on the cutting board into
the dustpan (Clean the cutting board).
3) Deformable Objects Manipulation: Fold the towel in
half twice (Fold towel twice).
4) Semantic Understanding: Stack the medium tower on
top of the large one first, then stack the small one on
top of the medium one.(Stack tower of hanoi)
For each task, we collect 2080 trajectories, scaled accord-
ing to task complexity, to finetune our model. To evaluate
generalization comprehensively, we design experiments that
span multiple axes of unseen scenarios, including lighting vari-
Recognizing that success rate alone inadequately captures
policy performance or distinguishes their capabilities, we
introduce a step-wise scoring system. For each of the four
completion of distinct stages during task execution. Detailed
scoring criteria, task setup and experiment results are provided
in Appendix C.
Baselines. We choose Diffusion Policy , alongside gener-
alist policies, OpenVLA  and LAPA  as our baselines.
Diffusion Policy is trained in a single-task manner, whereas
the generalist models are trained on all tasks simultaneously
with instruction inputs. For a fair comparison, we reproduce
LAPA with Prismatic-7B VLM  and action decoder heads,
aligning its architecture with our method. This setup allows us
to isolate and emphasize the contribution of our task-centric
latent action space. Specific parameters and architectural de-
tails can be found in Appendix C.
Results. We plot task success rates in Fig. 5. The single-
task Diffusion Policy (DP), optimized for trajectory fidelity
and low-latency control, excels in tasks like towel folding,
where success hinges on executing a fixed trajectory once the
correct towel edge is selected. This specialization allows DP
to achieve a higher success rate (53.3) compared to UniVLA
(46.7) in this task. However, UniVLA achieves a higher
step-wise score (2.47 vs. DPs 2.33, detailed in Appendix C),
reflecting its ability to reliably complete intermediate stages
(e.g., edge selection, partial folding) even when final execution
faltersa critical advantage in dynamic real-world environ-
ments where partial progress is valuable.
This trade-off arises from UniVLAs generalist design: while
DPs single-task training maximizes trajectory precision for
specific workflows, it struggles in tasks requiring semantic
reasoning (e.g., stack tower of hanoi, where DP achieves only
6.7 success). In contrast, UniVLA demonstrates superior
generalization and semantic understanding, achieving an un-
paralleled 86.7 success rate. This is further evidenced by
a 93.3 success rate in scenarios requiring precise object
manipulation and spatial reasoning (where the object is placed
at varied poses and positions in Store the screwdriver task).
In addition, our method achieves a real-time, closed-loop
inference frequency of 10Hz on an NVIDIA RTX 4090 GPU
by planning in a compact latent action space, and allowing
efficient action chunk prediction (we use a chunk size of 12
Group A.
Group B.
GNM-Outdoor
GNM-Indoor
Dobbe (Wrist-view)
Group C.
Fig. 8: Latent action analysis. We plot image pairs labeled with the same latent action from different sources of data and
embodiments. Each group of latent actions exhibits semantic-consistent actions. More examples are in Appendix B.
Real-robot
Bridge v2
OpenX (Manip.  Navi.)
Manip.  Navi.  Human
Fig. 9: Data scalability. UniVLA effectively expands its
pretraining corpus by incorporating cross-embodiment data
from OpenX and unlabeled human demonstrations, leading to
continuously improved downstream performance.
in practice). OpenVLA, despite extensive training on large-
scale robot datasets, suffers from execution stuttering due to
inference latency (e.g., 0.18s when predicting a single action
ing in poor real-world performance with only a 38.3 average
success rate. In a nutshell, UniVLA outperforms LAPA, the
second best policy, by 36.7 in success rate and 0.68 in
average score, demonstrating its real-world effectiveness and
the advantages of our proposed task-centric latent action space.
Generalizability Analysis. We investigate the generalizabil-
ity of policies from 3 different aspects, with the specific
experiment setups shown in Fig. 7. The results in Tab. II
highlight UniVLAs exceptional generalizability, significantly
outperforming baseline methods in success rates and step-wise
scores. It achieves a 66.7 success rate under varying lighting
(13.3), and LAPA (26.7), demonstrating robustness to
environmental change. In scenarios with visual distractors,
policies that rely more on semantic information, such as LAPA
and UniVLA, experience a relatively notable performance
drop. In the novel object setting, we replaced the screwdriver
with a marker and adjusted the language inputs for the
generalist policy accordingly. This change had minimal impact
on our policy as the success rate only drops by 6.6. Overall,
UniVLA achieves an average success rate of 68.9 and an
average score of 2.49, significantly outperforming prior VLAs
like LAPA (28.9, 1.36) and OpenVLA (20.0, 0.98). We
also provide video demos in the supplementary material.
B. Discussion on Latent Action
Qualitative analysis. We investigate the cross-domain trans-
ferability of latent actions by visualizing image pairs from
different data sources sharing the same latent action in Fig. 8.
Each group of latent actions maps to semantically consistent
behaviors across embodiments (e.g., latent actions representing
Pick up things in Group A). Notably, our latent action
unseen domain. Furthermore, LAM learns to align wrist-view
observations in manipulation with ego-centric movements in
to bridge diverse modalities and embodiments.
Quantitative analysis. To evaluate the effectiveness of our
proposed dynamics decomposition approach for task-centric
latent action learning, we assess the deployment performance
of policies trained with labels derived from different latent
actions. The results on LIBERO are shown in Tab. III. We
pre-train policies using only human videos, which contain
significant amounts of unpredictable motion, to amplify the
advantages of our method. In comparison to the latent ac-
tion construction approach introduced in Genie , which
captures all visual changes, our method demonstrates clear
superiority. Specifically, we achieve a 6.4 improvement in
TABLE III: Performance on LIBERO using various latent
actions. We pretrain policies using different latent actions
on Ego4D , which features human videos with diverse
movements and task-irrelevant dynamics, to demonstrate our
successful decoupling of task-centric dynamics. While task-
irrelevant ones yield poor performance, task-centric latent ac-
tion learning produces more meaningful action representations,
ultimately achieving superior deployment success rates.
Latent Action
Task-irrelevant
Task-centric
Fig. 10: Data efficiency. We present the success rate of
UniVLA across varying dataset proportions (10, 20, 50,
and the full dataset). Our policy can be adapted to an unseen
environment without requiring extensive expert demos for
average success rate, with substantial gains in LIBERO-Goal
and LIBERO-Long (13 and 9.8 improvement, respec-
tively). In contrast, latent actions that are task-irrelevant are
poorly aligned with true actions, making it difficult for policies
to infer them from observations and task instructions. This is
reflected in both lower action token prediction accuracy during
training and poorer inference performance. Notably, training
with task-irrelevant latent actions results in near-zero success
rates on the challenging LIBERO-Long benchmark.
C. More Ablations
Data scalability. We show how UniVLA evolves with the
growing data scale and the incorporation of data from dis-
tinct domains in Fig. 9. Though UniVLA already sets a
new state-of-the-art on LIBERO by pretraining only with
Bridge-V2 . Cross-embodiment data in OpenX  and
Ego4D  further amplifies the average success rate by 2.0.
While the performance on the LIBERO benchmark appears to
across our challenging real-world test suites. In real-world
the average score by 0.3, compared to Bridge-only pretraining.
Further incorporating human data, despite the absence of
action labels and the substantial embodiment gap it introduces,
yields an additional 0.28 increase. This trend of performance
improvement is similarly observed in the R2R navigation
effectively leverages diverse data sources.
TABLE IV: Ablations on decoder design. Auto-regressive
represents that we follow the approach of OpenVLA and
bins in an auto-regressive fashion. wo visual indicates that
visual embeddings are not utilized as query inputs for decoding
latent actions, as depicted in Fig. 3. The proposed action
decoder head, augmented by visual features, proves to be the
most effective, yielding the highest results on all test suites.
Action Decoder
Auto-regressive
Ours wo Visual
TABLE V: Ablations on the use of history action. Incor-
porating latent action outputs from previous steps as prompt
larly in long-horizon tasks, such as LIBERO-Long and R2R.
Prompt Input
LIBERO (Manip.)
R2R (Navi.)
Instruction-only
w History Action
Data efficiency. The preceding section highlights UniVLAs
scalability with respect to pretraining data, consistently en-
hancing its capabilities. We next explore its ability to adapt
efficiently to unseen environments with minimal data, as de-
tailed in Fig. 10. Specifically, we evaluate performance on the
LIBERO-Goal and LIBERO-Long benchmarks using partial
training data. UniVLA demonstrates superior data efficiency
compared to prior generalist policy, such as OpenVLA ,
and explicit point prediction methods like ATM . Notably,
with only 10 of the demonstration data, UniVLA achieves
a higher success rate on LIBERO-Goal (86.3 vs. 79.2)
than OpenVLA trained on the full dataset. Moreover, it sets
a new state-of-the-art performance on both LIBERO-Goal
and LIBERO-Long with only 10 and 50 of the training
action space, UniVLA maximizes pretraining knowledge, en-
abling highly efficient adaptation to new environments.
Latent action decoder. We compare our proposed action
decoding scheme with the auto-regressive approach, which
sequentially generates discretized actions as in OpenVLA
and LAPA. As shown in Tab. IV, our method consistently
achieves higher success rates across all test suites, with a
striking 42.1 improvement in LIBERO-Long. Leveraging
visual embeddings as queries enhances action decoding by
reducing ambiguity in the multimodal distribution, yielding
an additional 2.2 gain in average success rate.
designed to encapsulate dynamics over a one-second time
horizon. Given this temporal structure, decoding latent actions
as action chunks  is an intuitive choice, aligning the chunk
size with the control frequency of the target embodiment. This
is achieved by simply expanding the output dimension of the
final linear projection layer, while introducing negligible addi-
tional inference cost compared to the auto-regressive approach.
History latent actions. As detailed in Sec. III-C, we aug-
ment the instruction input with historical latent actions to
enhance sequential decision-making. We evaluate the efficacy
of this minimal architectural modification on manipulation and
navigation tasks, with quantitative results in Tab. V. The ap-
proach proves particularly impactful in long-horizon scenarios:
using only four input tokens (representing one latent action
group) improves success rates by 16.5 (R2R) and 3.9
(LIBERO-Long). Extending the history horizon yields dimin-
ishing returns. Unlike methods requiring redundant multi-
frame visual tokens for temporal context (e.g., [91, 45]), our
design provides compact historical guidance while enabling
iterative policy refinement through self-referential outputs.
This streamlined integration enhances contextual awareness
without incurring unnecessary computational overhead.
V. CONCLUSION
In this work, we introduce UniVLA, a vision-language-
action model that plans within a unified, task-centric latent
action space, enabling efficient adaptation to novel robotic
setups. Through extensive evaluations, we demonstrate that
UniVLA establishes state-of-the-art perfo
