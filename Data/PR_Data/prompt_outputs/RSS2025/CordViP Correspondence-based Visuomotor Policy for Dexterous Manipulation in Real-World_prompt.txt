=== PDF文件: CordViP Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World.pdf ===
=== 时间: 2025-07-22 09:41:59.725306 ===

请你只输出如下JSON，所有字段都必须有，且每个“关键词”字段只允许输出一个中文词语（不能是英文，不能是多个，不能是短语，不能有逗号、分号、空格），否则视为不合格。不要输出任何解释或正文，只输出JSON。
{
  "论文标题": "",
  "研究主题关键词": "",
  "应用场景关键词": "",
  "主要方法关键词": "",
  "创新点关键词": "",
  "主要结论关键词": ""
}
内容：for Dexterous Manipulation in Real-World
Yankai Fu1, Qiuxuan Feng1, Ning Chen1, Zichen Zhou1, Mengzhen Liu1,
Mingdong Wu1, Tianxing Chen2, Shanyu Rong1, Jiaming Liu1, Hao Dong1, Shanghang Zhang1,3B
1State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University,
2The University of Hong Kong, 3Beijing Academy of Artificial Intelligence
Equal Contribution, BCorresponding author
PickPlace
Different lighting conditions
Different viewpoints
Different scenarios
Unseen objects
Assembly
ArtiManip
Generalization
Cluttered
Object-3
Object-2
Object-1
Robust Viewpoint
Interaction-aware PC
FoundationPose
Fig. 1: We propose CordViP, a correspondence-based visuomotor policy for dexterous manipulation in the real world. (a)
correspondences between the object and the hand. (b) Right: Our method achieves promising results across multiple real-world
dexterous manipulation tasks, showcasing exceptional generalization capabilities.
AbstractAchieving human-level dexterity in robots is a key
objective in the field of robotic manipulation. Recent advance-
ments in 3D-based imitation learning have shown promising
key problems: (1) the quality of point clouds captured by a
single-view camera is significantly affected by factors such as
camera resolution, positioning, and occlusions caused by the
dexterous hand; (2) the global point clouds lack crucial contact
information and spatial correspondences, which are necessary
for fine-grained dexterous manipulation tasks. To eliminate
these limitations, we propose CordViP, a novel framework that
constructs and learns correspondences by leveraging the robust 6D
pose estimation of objects and robot proprioception. Specifically,
we first introduce the interaction-aware point clouds, which
establish correspondences between the object and the hand.
These point clouds are then used for our pre-training policy,
where we also incorporate object-centric contact maps and hand-
arm coordination information, effectively capturing both spatial
and temporal dynamics. Our method demonstrates exceptional
dexterous manipulation capabilities, achieving state-of-the-art
performance in six real-world tasks, surpassing other baselines by
a large margin. Experimental results also highlight the superior
generalization and robustness of CordViP to different objects,
I. INTRODUCTION
Dexterous manipulation is a fundamental capability in human
daily life such as assembling small parts and opening boxes.
Achieving human-level dexterity in real-world scenarios is
crucial for integrating robots into everyday human activities.
Recent advancements in imitation learning have demonstrated
significant potential in various robotic manipulation tasks. Some
existing methods leverage 2D images as input to directly
predict actions [7, 70, 54, 63]. While these vision-based
imitation learning approaches are capable of handling a wide
range of tasks, they typically demand extensive demonstrations
[19, 7, 13, 57] and, at the same time, fail to capture intricate
spatial relationships and 3D structures essential for dexterous
manipulation [69, 23, 49, 11], limiting their ability to perform
Recent research has increasingly focused on 3D imitation
to help robots perceive 3D environments and reason about
spatial relationships [69, 13, 56, 14, 64, 51]. For example,
3D diffusion-based policies, which aim to enhance robots
ability to perform complex manipulation tasks by providing a
more accurate and holistic representation of the environment.
These methods for obtaining 3D representations in real-world
scenarios often depend on a single-view RGB-D camera to
generate point clouds, imposing significant demands on both
the cameras quality and its position. However, the multi-
fingered nature of dexterous hands often leads to occlusions
during object manipulation, which not only loses the spatial
information of the object point clouds but also obscures critical
correspondence information for precise manipulation. Besides,
some works have explored the use of tactile sensors to enhance
contact information [17, 66, 18, 62, 2]. For instance, Wu et al.
proposed a canonical representation of 3D tactile data,
which is concatenated with other visual information. Although
tactile sensors hold promise, their high cost and susceptibility
to interference from factors such as temperature fluctuations
and electromagnetic fields undermine their stability and limit
their practical applicability in real-world scenarios. Amidst all
the challenges, leveraging efficient and robust 3D imitation
learning to understand spatial information in complex, dynamic
environments is crucial for dexterous manipulation.
In this paper, we propose a correspondence-based visuo-
motor policy (CordViP), which focuses on the spatial and
temporal consistency between the manipulated object and the
dexterous hand, even under significant occlusions. CordViP
separately extracts the 3D representations of robotic hands
joint structure and the manipulated object, and utilizes the
contact map between them as well as arm-hand cooperative
motion information to pretrain the observation encoder. This
approach enhances the dexterous hands ability to understand
spatial interactions and better facilitate a range of downstream
tasks. Specifically, our framework operates in three phases: (1)
We leverage the robust 6D pose estimation of objects combined
with the robots proprioceptive state to construct interaction-
aware point clouds, providing ideal 3D observations that
facilitates effective learning and inference for the visuomotor
policy. (2) We pretrain the encoder network on the play data we
collected to predict the contact map between the dexterous hand
and the object, and reconstruct the cooperative relationship
between the hand and the arm for further understanding of
spatial and temporal correspondences. (3) We utilize the pre-
trained encoder to extract the semantic features of 3D point
clouds and robot state, which encapsulates spatial consistency,
contact dynamics and collaborative information, and then are
used as conditions for the diffusion policy to predict actions.
To comprehensively evaluate our proposed CordViP, we
conduct extensive real-world experiments on six dexterous
LongHoriManip. Comparative results demonstrate that our
method not only exhibits superior effectiveness, but also
achieves remarkable performance with a minor number of
expert demonstrations, highlighting its capability to learn
efficiently from limited data. Furthermore, we find that Cord-
ViP generalizes well to various environmental perturbations,
including varying lighting conditions, unseen objects, diverse
baselines by a large margin.
In summary, our contributions are as follows:
1) We develop a pipeline based on robust 6D pose estimation
of objects and robot proprioception. This pipeline enables
the real-time acquisition of complete 3D representations
and semantic flow in real-world environments, and can
effectively address the challenges posed by occlusions
during dexterous manipulation.
2) We propose CordViP, a correspondence-based visuomotor
policy that utilizes the contact map and hand-arm
coordination information to facilitate the understanding
of spatial and temporal consistency.
3) We demonstrate the effectiveness and generalization of
our method through a range of real-world experiments
using a dexterous hand.
II. RELATED WORK
A. Dexterous Manipulation
Dexterous Manipulation is a long-standing research topic in
robotics that aims to give robots the ability to perform delicate
operations like humans [5, 1, 45, 65]. Traditional methods often
rely on trajectory optimization based on dynamic models to
solve operational problems [26, 33, 58], but these methods have
limitations in complex tasks because they simplify the contact
dynamics and are difficult to deal with uncertainties in dynamic
environments. In contrast, Reinforcement Learning (RL) does
not rely on accurate physical models but learns operational
policies through interaction with the environment, which is
highly adaptable. RL has achieved remarkable results in many
dexterous manipulation tasks, such as object reorientation [3,
methods often suffer from several challenges, such as the need
for extensive reward engineering and system design, as well
as limited generalization to unseen scenarios. Additionally,
while Sim-to-Real is a common technique employed in RL,
the gap between simulations and the real world degrades the
performance of the policies once the models are transferred into
real robots . Imitation learning (IL), as another effective
learning method, can quickly learn effective control policies
by imitating expert demonstrations [67, 54]. In this work, we
propose a correspondence-based visual imitation learning policy
that utilizes spatial information between various components,
enabling the acquisition of complex skills with a minimal
number of expert demonstrations.
B. Imitation Learning
Imitation learning (IL) allows a robot to directly learn
from experts. Behavioral Cloning (BC) is one of the simplest
imitation learning algorithms, which treats the problem of
learning behavior as a supervised learning task . The
modeling methods commonly used in traditional BC, such as
when modeling complex action distributions. They fail to
effectively capture the diversity and nuances of human behavior
. Over the past few years, diffusion models have emerged as
a new modeling approach in BC, becoming powerful tools that
enable robots to learn from demonstrations, handle uncertainty,
and perform complex multi-step tasks with precision. From the
early applications of DDPMs to the recent innovations in BESO
, OCTO , and CrossFormers , these models have
continually pushed the boundaries of whats possible in robotic
behavior generation. While traditional BC policies typically
rely on 2D image-based representations [7, 70, 54, 63, 28, 30],
recent advancements have extended imitation learning to 3D
visual repres
